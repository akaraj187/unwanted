{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9b512e4d-8ded-4428-847c-4362b1464ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Preprocessing...\n",
      "Training Logistic Regression Model...\n",
      "\n",
      " Model Accuracy: 89.93%\n",
      "\n",
      "Detailed Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.99      0.92        81\n",
      "           1       0.98      0.78      0.87        58\n",
      "\n",
      "    accuracy                           0.90       139\n",
      "   macro avg       0.92      0.88      0.89       139\n",
      "weighted avg       0.91      0.90      0.90       139\n",
      "\n",
      " Model saved as 'issue_classifier.pkl'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import joblib\n",
    "\n",
    "# 1. LOAD DATA\n",
    "print(\"Loading data...\")\n",
    "df = pd.read_csv(\"all_issues.csv\")\n",
    "\n",
    "# ... (after pd.read_csv) ...\n",
    "\n",
    "# 1.5 BALANCE THE DATA (Undersampling)\n",
    "print(\"âš–ï¸ Balancing the dataset...\")\n",
    "# Separate the two groups\n",
    "df_easy = df[df['is_beginner_friendly'] == 1]\n",
    "df_hard = df[df['is_beginner_friendly'] == 0]\n",
    "\n",
    "# Count how many easy ones we have (123)\n",
    "min_count = len(df_easy)\n",
    "\n",
    "# Randomly pick 123 \"Hard\" ones to match the 123 \"Easy\" ones\n",
    "df_hard_balanced = df_hard.sample(n=min_count, random_state=42)\n",
    "\n",
    "# Combine them back together\n",
    "df = pd.concat([df_easy, df_hard_balanced])\n",
    "\n",
    "print(f\"âœ… New Counts -> Easy: {len(df_easy)}, Hard: {len(df_hard_balanced)}\")\n",
    "# Now you have a perfectly 50/50 dataset!\n",
    "# ... (Continue with Preprocessing & Training) ...\n",
    "\n",
    "# ... (Load Data) ...\n",
    "\n",
    "# 2. PREPROCESSING\n",
    "print(\"Preprocessing...\")\n",
    "\n",
    "# Fill empty text\n",
    "df['title'] = df['title'].fillna('')\n",
    "df['body'] = df['body'].fillna('')\n",
    "df['labels'] = df['labels'].fillna('')\n",
    "\n",
    "# --- NEW: CLEAN LABELS ---\n",
    "# 1. Remove punctuation like [' '] from the CSV string\n",
    "df['labels_clean'] = df['labels'].str.replace(r\"[\\[\\]']\", \"\", regex=True)\n",
    "\n",
    "# 2. REMOVE \"CHEAT WORDS\" (Data Leakage Prevention)\n",
    "# We remove 'good first issue' so the model relies on the actual content, not just the tag.\n",
    "cheat_words = ['good first issue', 'beginner', 'easy', 'help wanted']\n",
    "for word in cheat_words:\n",
    "    df['labels_clean'] = df['labels_clean'].str.replace(word, \"\", case=False)\n",
    "\n",
    "# 3. COMBINE EVERYTHING\n",
    "# Now the model sees: \"Fix typo in readme documentation ui\"\n",
    "df['combined_text'] = df['title'] + \" \" + df['body'] + \" \" + df['labels_clean']\n",
    "# 3. VECTORIZATION (The \"Secret Sauce\")\n",
    "# Convert text to numbers. \n",
    "# max_features=1000 means \"Only look at the top 1000 most common words\"\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=3000,ngram_range=(1,2))\n",
    "# ngram_range=(1, 2) means \"Look at single words AND pairs of words\"\n",
    "X = vectorizer.fit_transform(df['combined_text'])\n",
    "y = df['is_beginner_friendly']\n",
    "\n",
    "# 4. SPLIT TRAIN/TEST\n",
    "# 80% for Training, 20% for Testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 5. TRAIN MODEL\n",
    "print(\"Training Logistic Regression Model...\")\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 6. EVALUATE\n",
    "predictions = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "\n",
    "print(f\"\\n Model Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(\"\\nDetailed Report:\")\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "#7. SAVE THE ENGINE (Optional, for later use in the App)\n",
    "joblib.dump(model, 'issue_classifier.pkl')\n",
    "joblib.dump(vectorizer, 'vectorizer.pkl')\n",
    "print(\" Model saved as 'issue_classifier.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1643a1-7119-40d7-beb0-01db5f2351ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------\n",
      "ðŸ“ Text Snippet: 'Fix typo in the README documentation for installation steps....'\n",
      "âœ… VERDICT: Beginner Friendly (Confidence: 64.0%)\n",
      "   -> Go claim this!\n",
      "------------------------------------------------\n",
      "\n",
      "\n",
      "------------------------------------------------\n",
      "ðŸ“ Text Snippet: 'Memory leak in the kernel thread scheduler causes segfault....'\n",
      "â›” VERDICT: Complex/Hard (Confidence: 67.1%)\n",
      "   -> Stay away!\n",
      "------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  Refactor the entire legacy authentication module to support OAu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------\n",
      "ðŸ“ Text Snippet: 'Refactor the entire legacy authentication module to support ...'\n",
      "â›” VERDICT: Complex/Hard (Confidence: 56.4%)\n",
      "   -> Stay away!\n",
      "------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  Bump requests library version from 2.25 to 2.28 in requirements.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------\n",
      "ðŸ“ Text Snippet: 'Bump requests library version from 2.25 to 2.28 in requireme...'\n",
      "â›” VERDICT: Complex/Hard (Confidence: 65.3%)\n",
      "   -> Stay away!\n",
      "------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# 1. LOAD THE BRAIN\n",
    "# We load the saved model and the vectorizer (the translator from Text -> Numbers)\n",
    "try:\n",
    "    model = joblib.load('issue_classifier.pkl')\n",
    "    vectorizer = joblib.load('vectorizer.pkl')\n",
    "except FileNotFoundError:\n",
    "    print(\"âŒ Error: You need to run train_model.py first to create the .pkl files!\")\n",
    "    exit()\n",
    "\n",
    "def predict_issue(text):\n",
    "    # 2. PREPARE THE INPUT\n",
    "    # Convert the text to the same number format the model learned on\n",
    "    text_vectorized = vectorizer.transform([text])\n",
    "    \n",
    "    # 3. ASK THE BRAIN\n",
    "    prediction = model.predict(text_vectorized)[0]\n",
    "    probability = model.predict_proba(text_vectorized)[0][1] * 100  # Confidence score\n",
    "    \n",
    "    # 4. SHOW RESULT\n",
    "    print(\"\\n------------------------------------------------\")\n",
    "    print(f\"ðŸ“ Text Snippet: '{text[:60]}...'\")\n",
    "    \n",
    "    if prediction == 1:\n",
    "        print(f\"âœ… VERDICT: Beginner Friendly (Confidence: {probability:.1f}%)\")\n",
    "        print(\"   -> Go claim this!\")\n",
    "    else:\n",
    "        print(f\"â›” VERDICT: Complex/Hard (Confidence: {100-probability:.1f}%)\")\n",
    "        print(\"   -> Stay away!\")\n",
    "    print(\"------------------------------------------------\\n\")\n",
    "\n",
    "# --- TEST ZONE ---\n",
    "# Paste some real titles or descriptions here to test!\n",
    "\n",
    "# Test 1: Should be Easy (Simulated)\n",
    "predict_issue(\"Fix typo in the README documentation for installation steps.\")\n",
    "\n",
    "# Test 2: Should be Hard (Simulated)\n",
    "predict_issue(\"Memory leak in the kernel thread scheduler causes segfault.\")\n",
    "\n",
    "# Test 3: YOUR TURN - Copy/Paste a real issue title below!\n",
    "while True:\n",
    "    user_input = input(\" \")\n",
    "    if user_input.lower() == 'exit': break\n",
    "    predict_issue(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f349f62-0a33-4a09-b902-3b7d410328f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
