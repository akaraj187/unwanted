repo,title,body,labels,is_beginner_friendly
ArduPilot/ardupilot,TemperatureSensor: Add Rangefinder to the TEMPx_SRC sensor source options,"## Feature request

**Is your feature request related to a problem? Please describe.**
Rover (and [hopefully soon Sub](https://github.com/ArduPilot/ardupilot/pull/31622)) make use of a `WATER_DEPTH` MAVLink message, which includes temperature data from a downward-facing rangefinder. Rangefinders don't always include temperature sensors (or they may not be super accurate), so it could be nice to be able to sub that in from a dedicated temperature sensor.

**Describe the solution you'd like**
Add a `Rangefinder` option to the temperature sensor [source options](https://github.com/ArduPilot/ardupilot/blob/master/libraries/AP_TemperatureSensor/AP_TemperatureSensor_Params.h#L43), to allow the rangefinder driver temperature data to be overshadowed via the [`TEMPx_SRC`](https://ardupilot.org/sub/docs/parameters.html#temp1-src-sensor-source) parameters.

**Describe alternatives you've considered**
1. The `WATER_DEPTH` message could automatically fall back to another temperature source if one is provided, but that either reliant on the rangefinder not providing temperature data, or makes it ambiguous as to what is being used
2. The source could be specified as ""water temperature"" instead of specifically rangefinder, which could make it more broadly useful in future, but also makes it less clear when and where it's likely to be used.
3. We could add dedicated MAVLink reporting for temperature sensors, and/or specifically for water temperature, which are both potentially good ideas, but then it's potentially unclear what source is being used for the temperature data in the `WATER_DEPTH` message

**Platform**
[  ] All
[  ] AntennaTracker
[  ] Copter
[  ] Plane
[x] Rover
[x] Submarine

**Additional context**
#31616 may be a good reference for the temperature sensor library side of things.
","['Rover', 'Sub', 'good first issue', 'FeatureRequest']",1
ArduPilot/ardupilot,Sub: add support for `WATER_DEPTH` MAVLink reporting,"## Feature request

**Is your feature request related to a problem? Please describe.**
Many underwater vehicles have downward facing sonar sensors that are capable of measuring the distance to the water floor. Currently those are reported via [`DISTANCE_SENSOR`](https://mavlink.io/en/messages/common.html#DISTANCE_SENSOR) and [`RANGEFINDER`](https://mavlink.io/en/messages/ardupilotmega.html#RANGEFINDER) MAVLink messages, but both of those have limited export utility without contextual information that then needs to be cross-referenced with other messages (at risk of being incorrect from timing deltas).

Water temperature is also ambiguously reported in ArduSub via `SCALED_PRESSURE*` messages, which requires a receiver to know which and how many temperature sensors are connected to determine which one(s) refer to the water (e.g. outside the enclosure).

**Describe the solution you'd like**
Rover [has support for](https://github.com/ArduPilot/ardupilot/pull/28875) a [`WATER_DEPTH`](https://mavlink.io/en/messages/ardupilotmega.html#WATER_DEPTH) message for boats, which seems to include most of the relevant information to be directly usable in map creation / bathymetry situations, and for high level autopilot state reporting. It seems reasonable to expand that support to ArduSub firmware.

**Describe alternatives you've considered**
Do nothing, which keeps things difficult for control station software, and keeps them reliant on cycling/filtering through `DISTANCE_SENSOR` and `SCALED_PRESSURE` messages for water depth and temperature estimates.

**Platform**
[  ] All
[  ] AntennaTracker
[  ] Copter
[  ] Plane
[  ] Rover
[X] Submarine

**Additional context**
_Add any other context or screenshots about the feature request here._
","['MAVLink', 'Sub', 'good first issue', 'FeatureRequest']",1
ArduPilot/ardupilot,Add support for OPTICAL_FLOW_RAD,"## Feature request

ArduPilot consumes OPTICAL_FLOW.  We should also consume OPTICAL_FLOW_RAD, treating all fields in exactly the same way as OPTICAL_FLOW except for the conversion to radians.

**Describe the solution you'd like**
Code!

**Describe alternatives you've considered**
Continuing to not have support

**Platform**
[  ] All
[  ] AntennaTracker
[X ] Copter
[X] Plane
[X] Rover
[X] Submarine
",['good first issue'],1
ArduPilot/ardupilot,AP_Gripper: support HOLD command option,"## Feature request

**Is your feature request related to a problem? Please describe.**
Explicit gripper control is currently limited to active releasing and grabbing, which leaves out stopped/neutral operation. The EPM gripper driver seemingly has an option to automatically return to neutral some fixed time after a grab or release, but that is privately scoped and has no direct control action.

**Describe the solution you'd like**
1. MAVLink [now supports a HOLD command for grippers](https://github.com/mavlink/mavlink/pull/2393), which I think should be used to trigger stopped / neutral functionality of the gripper.
2. With drivers that expose that functionality, it could also be integrated into AUX functions / joystick button function support, as the return point when a grab or release button stops being held.

**Describe alternatives you've considered**
We could just not implement this, but it's a missing functionality that seems useful for certain gripper types and applications.

**Platform**
[x] All
[  ] AntennaTracker
[  ] Copter
[  ] Plane
[  ] Rover
[  ] Submarine

**Additional context**
The lack of this functionality is what has thus far stopped ArduSub from regularly using ArduPilot's gripper functionality (opting to instead use momentary servo commands via joystick buttons), so this would reduce ArduSub's reliance on joystick button functions, while also making gripper configuration more intuitive.",['good first issue'],1
ArduPilot/ardupilot,"DO_SET_ROI_LOCATION and DO_SET_ROI_NONE ""gimbal device id"" fields should be supported","AP supports the  [DO_SET_ROI_LOCATION](https://mavlink.io/en/messages/common.html#MAV_CMD_DO_SET_ROI_LOCATION) and [DO_SET_ROI_NONE](https://mavlink.io/en/messages/common.html#MAV_CMD_DO_SET_ROI_NONE) as ""immediate"" and mission commands but we do not correctly consume ""param1"" (""Gimbal device ID"").  Instead it is always the first camera gimbal that is controlled.  We should consume this field and control the gimbal device specified

PR https://github.com/ArduPilot/ardupilot/pull/29567 added mission support for these commands",['good first issue'],1
ArduPilot/ardupilot,AP_Mount: mount_open servo output should work for all backends,"The SRV_Channel library has a [mount_open](https://github.com/ArduPilot/ardupilot/blob/master/libraries/SRV_Channel/SRV_Channel.h#L58) and mount2_open functions but they are only used by the [Mount_Servo backend](https://github.com/ArduPilot/ardupilot/blob/master/libraries/AP_Mount/AP_Mount_Servo.cpp#L112).

We should move the little bit of code that controls to servo into a function in [AP_Mount_Backend](https://github.com/ArduPilot/ardupilot/blob/master/libraries/AP_Mount/AP_Mount_Backend.h) and then call it from every backend as part of the update() call.

This issue was raised [here in the Plane forums](https://discuss.ardupilot.org/t/feature-request-add-support-for-retract-mount-in-arduplane/114894).","['BUG', 'Enhancement', 'Driver', 'good first issue']",1
ArduPilot/ardupilot,EAHRS is performing duplicate temperature compenstation,"## Bug report

**Issue details**

Certain devices exposed through EAHRS, like the MicroStrain7, supply data that is already temperature compenstated.

https://s3.amazonaws.com/files.microstrain.com/GQ7+User+Manual/external_content/dcp/Data/sensor_data/data/mip_field_sensor_scaled_mag.htm
https://s3.amazonaws.com/files.microstrain.com/GQ7+User+Manual/external_content/dcp/Data/sensor_data/data/mip_field_sensor_scaled_accel.htm

When the External AHRS driver posts data to sensor libraries, they perform temperature compenstation on it again, as long as `HAL_INS_TEMPERATURE_CAL_ENABLE` is defined, which is based on board size.  

_Please describe the problem_

This causes double temperature compensation, leading to incorrect values.

**Proposed Solution**
Add `bool is_temperature_compensated;` to `AP_ExternalARHS::ins_data_message_t`, and then add the necessary hooks in `AP_InertialSensor::handle_external(const AP_ExternalAHRS::ins_data_message_t &pkt)` to stop doing temperature compensation if the supplied value is already temp-compensated.

See `ryanf55:eahrs-precompensated-sensors` for a prototype fix (WIP)

**Version**
master `1e644d56a51806b63dcd68d5e6566ec0f10248fc`

**Platform**
[ x] All
[  ] AntennaTracker
[  ] Copter
[  ] Plane
[  ] Rover
[  ] Submarine

","['BUG', 'good first issue', 'ExternalAHRS']",1
ArduPilot/ardupilot,AP_GPS_GSOF: Support GPS_RAW_DATA for external logging,"## Feature request

**Is your feature request related to a problem? Please describe.**

Right now, there is no way to enable the internal logging on the PX1 GNSS  because the API is only available over the ethernet web UI. Internal logging is used for tech support to analyze system performance. Since PX1 has little flight hours on small fixed wing drones, this is particularly useful to have the ArduPilot community be able to supply logs.

**Describe the solution you'd like**

* Consume [GPS_RAW_DATA](https://ardupilot.org/copter/docs/parameters.html#gps-raw-data-raw-data-logging) in AP_GPS_GSOF
* Handle disabling logging on disarm, and log continuation on arm
* Send enable logging/disable logging commands as appropriate
* Stretch: Handle runtime param updates
* Update wiki entry for PX1: https://github.com/ArduPilot/ardupilot_wiki/pull/5540/files
* Create follow up issue for dealing with log data filling up and whether old logs are overwritten, the user is notified that logs are full, or something else

**Describe alternatives you've considered**

Connect laptop temporarily.

**Platform**
[  ] All
[  ] AntennaTracker
[  ] Copter
[  ] Plane
[  ] Rover
[  ] Submarine


## Further details

The following binary data commands are what need to be sent for logging. There is no TransNumber, so just hard code these in the implementation file. If there is an ACK, parse the ACK.
```
Start Logging: 02 00 4C 09 08 44 45 46 41 55 4C 54 00 62 03 
Stop Logging: 02 00 4C 09 09 44 45 46 41 55 4C 54 00 63 03
```

## Dependencies

The configuration ack/nack pattern from `gsof-49-50` branch needs to be merged in before there is ability to do configuration responses. This ticket could be done without acks, however it will need to be changed.

","['Enhancement', 'GPS', 'Driver', 'good first issue']",1
ArduPilot/ardupilot,Set Servo options triggered by failsafe,"

## Feature request


For UAS which perform a function controlled by Set_servo outputs, such as crop spraying, it would be desirable for the UAS to stop doing the function if a failsafe mode is triggered. It is not clear if the Sprayer functionality includes this but it may be useful for other  applications such as vectored thrust VTOLs etc

Currently the consequence of RC signal loss while spraying would be the UAS performing a RTL while still spraying. This would be VERY BAD as the spray would be released at a high altitude and subsequently drift to unintended locations. 

**Describe the solution you'd like**
_A clear and concise description of what you want to happen._

A parameter to set servo outputs should failsafe be triggered Eg SERVOXX_FS_VALUE = 1000 


**Describe alternatives you've considered**
_A clear and concise description of any alternative solutions or features you've considered._
Lua scripting?

checked the Sprayer functionality in ardupilot, it is unclear if this issue is addressed. 

(Also documentation on ""SPRAY_SPINNER Spinner rotation speed: Spinner's rotation speed in PWM (a higher rate will disperse the spray over a wider area horizontally)"" is inaccurate and could cause serious issues, increased RPM actually creates smaller droplets.  see https://en.wikipedia.org/wiki/Rotary_atomizers)

**Platform**
[ x ] All
[x  ] AntennaTracker
[ x ] Copter
[  x] Plane
[ x ] Rover
[  x] Submarine

**Additional context**
_Add any other context or screenshots about the feature request here._
","['Copter', 'good first issue']",1
ArduPilot/ardupilot,AP_DDS: Support IMU+gyro+accel data for rate control or offboard fusion,"From AP_AHRS, the data is filtered and good for rate control. This is the data after notch+low pass, phase lag and all. This goes on `imu_filtered` topic.

For offboard fusion, we can do it ""badly"". We would need delta angle, delta velocity, all the data products from INS that are not filtered. We have to know the lag to do delayed time fusion with lag on the GPS. This would be on `imu_raw`.

Current EKF does delayed time fusion (in delayed time) with forward propagation to the current time. Doesn't work as well in dynamic flight.  Researchers want to do this. 

Adding things like vision odometry back into ArduPilot (visual mavlink odometry) - this is preferred for performance so ArduPilot's EKF can fuse the data. ","['EKF', 'ROS', 'good first issue']",1
ArduPilot/ardupilot,Remove mavlink enum dependencies on non related library,"## Feature request

**Is your feature request related to a problem? Please describe.**
In numerous place, we were using mavlink enum. Those are well defined and conveniant but they bring a hard link to mavlink that can be harmfull codewise.

Therefore, we got a recent effort to reduce this dependency : see https://github.com/ArduPilot/ardupilot/pull/23229 . In this PR, we decouple the GPS status from the mavlink enum. That lower our inclusion mess.

**Describe the solution you'd like**
We should now update our codebase to remove the mavlink GPS enum in favor of the GPS_Status enum.

Some other work could be done to make the same for EKF and other mavlink enums !

**Describe alternatives you've considered**
_A clear and concise description of any alternative solutions or features you've considered._

**Platform**
[ X ] All
[  ] AntennaTracker
[  ] Copter
[  ] Plane
[  ] Rover
[  ] Submarine

**Additional context**
_Add any other context or screenshots about the feature request here._
","['Enhancement', 'good first issue']",1
ArduPilot/ardupilot,Custom Build Server Enhancements,"## Feature request

Our Custom Build Server (https://custom.ardupilot.org/) is great, but is lacking a lot of features which would make it very useful.  Given that when adding one feature it is often convenient to add several, this is a compendium issue of features which people have mentioned as being potentially useful.

 - ~~ability to select a branch to build (instead of ""only master"")~~
    - ~~this will require inspecting the selected branch to work out which option are available to present to the user~~
    - selecting branches from a white-listed github account name list would be a useful follow-up to this
 - ability to modify interface based on which vehicle is to be built
 - ability to build AP_Periph (linked to above point)
 - ability to upload a hwdef.dat (and perhaps other files) to populate the interface.  We already allow downloading of the hwdef.dat
 - ability to specify default parameter values to be included in the build
 - ability to specify some parameters as read-only
 - ability to enable assertions
 - ~~better dependency tracking / magic update of interface with deps selected (with warning to user that deps have been selected)~~
 - ~~download features.txt from (e.g.) https://firmware.ardupilot.org/Plane/latest/mRoX21-777/features.txt to pre-populate interface when it would be otherwise empty~~
",['good first issue'],1
ArduPilot/ardupilot,AP_SmartRTL algorithm breaks on Circle mode,"## Bug report

**Issue details**

A flight on the weekend showed a problem where the SmartRTL buffer filled up while the vehicle was in circle mode, and SmartRTL disengaged.

This is trivially reproducible in SITL:
```
mode loiter
arm throttle
rc 3 2000
mode circle
rc 3 1500
set sim_speedup 100
```

**Version**
master

**Platform**
[  ] All
[  ] AntennaTracker
[ X ] Copter
[  ] Plane
[  ] Rover
[  ] Submarine

**Airframe type**
Chimera

**Hardware type**
Lumeneer H743

**Logs**
n/a - reproducible in SITL
","['Copter', 'good first issue']",1
ArduPilot/ardupilot,Write autotest for the no-short-failsafe option in auto mode for Plane,"## Feature request

**Is your feature request related to a problem? Please describe.**
We had a regression where a user set the ignore-short-failsafe-in-auto option and it didn't have an effect

**Describe the solution you'd like**
An autotest to ensure the option works

https://discuss.ardupilot.org/t/fs-long-actn-and-fs-short-actn/77900/8

**Describe alternatives you've considered**
Having the feature break again and we don't know about it

**Platform**
[  ] All
[  ] AntennaTracker
[  ] Copter
[ X ] Plane
[  ] Rover
[  ] Submarine
","['Plane', 'good first issue']",1
ArduPilot/ardupilot,Make more features configurable,"## Cleanup request

Our new build server (https://custom.ardupilot.org) allows users to make their own firmwares by selecting and de-selecting features.

Sometimes the amount of code required to run a feature exceeds the flash space available on boards.

The more things we allow users to remove from the compilation the more likely they are to get a firmware that does everything they want.

A good example of this is scripting - users of 1MB boards would love to have it, but it is a large chunk of code and excluded from the 1MB builds by default.

*note* - each of these libraries can and should be done as a separate Pull Request!

There are many places in the code that we could allow users to exclude chunks of code on a piece-by-piece basis, for example:
 - rangefinder library
 - visual odometry backends
 - AC_Avoid

 - DONE AP_Winch backends
 - DONE GPS backends (@Hwurzburg is currently looking at these)
 - DONE rangefinder backends
 - DONE precision landing library
 - DONE compass backends
 - DONE airspeed backends
 - DONE AP_Mount backends
 - DONE battery monitor backends
 - DONE AP_Winch
 - DONE AP_MSP backends
 - DONE optical flow library
 - DONE optical flow backends
 - DONE RCInput backends (e.g. srxl)
 - DONE FRSky library
 - DONE FRSky backends

The larger the amount of code excised the more useful the change is.  `nm` can be used to show the largest symbols in the compiled code:
```
pbarker@bluebottle:~/rc/ardupilot(pr/sitl-on-hw)$ arm-none-eabi-nm --demangle --size-sort --print-size build/NucleoH743/bin/ardurover | tail -20
3000de30 00001200 B NavEKF_core_common::nextP
080e58ac 00001264 T NavEKF3_core::FuseDragForces()
3000a484 000013c8 b xsimstate
080e4454 0000144c T NavEKF3_core::FuseSideslip()
080880c8 00001540 T SoloGimbalEKF::predictCovariance()
0810a8cc 0000163c T NavEKF2_core::FuseSideslip()
300183d0 00001658 b supported_frames
30011e20 000017e4 b icm40609
3001be50 000018a0 B SDU1
08111918 00001e78 t __static_initialization_and_destruction_0(int, int)
0810d548 00001f5c T NavEKF2_core::FuseOptFlow()
081330b0 00002014 T AP_Param::param_defaults_data
080e80fc 000022f8 T NavEKF3_core::FuseOptFlow(NavEKF3_core::of_elements const&, bool)
080d4e08 00002814 T NavEKF2_core::FuseMagnetometer()
0805ff8c 000029ee T NavEKF3_core::FuseBodyVel()
081460a4 00002ddb t ap_romfs_0
0805597c 00002edc T NavEKF3_core::FuseMagnetometer()
080df5ac 00004d0c T NavEKF2_core::CovariancePrediction()
0806626c 00005e70 T NavEKF3_core::CovariancePrediction(Vector3<double>*)
30002518 00006740 B rover
pbarker@bluebottle:~/rc/ardupilot(pr/sitl-on-hw)$ 
```


There are also some large wins available by filling out `AP_NavEKF3_features.h` with more options.  This work will be much more complicated, but the ability to, for example, exclude magnetometers from the EKF's calculations would save a very large amount of code. ~~(see https://github.com/ArduPilot/ardupilot/issues/19070 however)~~

The pattern we use is `AP_<foo>_ENABLED`.  An example of this is `AP_QUADPLANE_ENABLED`.  Typically the main library header will contain:
```
#ifndef AP_GPS_NOVA_ENABLED
#define AP_GPS_NOVA_ENABLED 1
#endif
```

The new option should also be added to the custom build server configuration: https://github.com/ArduPilot/ardupilot/blob/master/Tools/scripts/build_options.py 

The code *must* be tested for compilation and working-ness with and without the option set.

**Platform**
[ X ] All
[  ] AntennaTracker
[  ] Copter
[  ] Plane
[  ] Rover
[  ] Submarine
",['good first issue'],1
ArduPilot/ardupilot,Move test_onboard_logging_generation outside autotest.py,"## Feature request

**Is your feature request related to a problem? Please describe.**
Move test_onboard_logging_generation outside autotest.py to all to validate logger documentation before running the full autotest suite. Ideally, on CI it should run on build_ci.sh with param_parse.py to allow to validate soon that the documentation is correct and not wait that all test pass to trigger a documentation failure...

**Describe the solution you'd like**
_A clear and concise description of what you want to happen._

**Describe alternatives you've considered**
_A clear and concise description of any alternative solutions or features you've considered._

**Platform**
[ X ] All
[  ] AntennaTracker
[  ] Copter
[  ] Plane
[  ] Rover
[  ] Submarine

**Additional context**
_Add any other context or screenshots about the feature request here._
","['AllVehicles', 'BuildSystem', 'good first issue', 'CI']",1
ArduPilot/ardupilot,Add visualisation for data in build_binaries_history.sqlite,"## Feature request

This file is generated as part of an autotest run.  It contains information about which boards built and which boards did not for a particular tested git sha.

It would be nice to be able to visualise this data on the autotest results page.

Previously when I've done this I've created a table, the left-most column being the most recent tested master commit, the columns sorted by git history.  This allows you to narrow down to a narrow range of commits where a board's build was broken.

However, a quicker patch giving a lot of value would be to extract the data in autotest.py's build_binaries.py and do a `results.add()` for the failing boards.  Only the failures for the master commit (as opposed to the stable/beta tags) would be reported as the story is too difficult to tell otherwise.  Rejigging autotest to run on the different tags (as opposed to running build_binaries on the different tags) is not in scope here :-)
",['good first issue'],1
ArduPilot/ardupilot,Modify autotest to notify Discord when run is complete,"## Feature request

We need to cut back on the number of tests we run on CI (so they finish faster, given we can't reasonably increase the resources allocated there).  However, we still want more tests overall.  The autotest results tend to get ignored a lot of the time.

A push notification to Discord (summarising improvements or regressions) may allow us to quickly notice and fix regressions that occur because we're not running all of the tests in the CI environment.

**Describe alternatives you've considered**
Continue to run more and more tests in CI.  We already take too long and randomly time out, which is no good.
Run fewer tests, catch fewer regressions.
Spend time optimising the time taken to run tests.

**Platform**
[ X ] All
[  ] AntennaTracker
[  ] Copter
[  ] Plane
[  ] Rover
[  ] Submarine
",['good first issue'],1
ArduPilot/ardupilot,Copter: POSITION_TARGET_LOCAL_NED doesn't correctly reflect SET_POSITION_TARGET_LOCAL_NED,"## Bug report

**Issue details**

POSITION_TARGET_LOCAL_NED doesn't correctly reflect SET_POSITION_TARGET_LOCAL_NED. 
It always returns MAV_FRAME_LOCAL_NED, regardless of what was specified in SET. 
The type mask is also incorrect - it can only return 3 different masks, when there are many more possible combinations. 
Finally, yaw and yaw_rate are always zero. 

**Version**
4.03

**Platform**
[  ] All
[  ] AntennaTracker
[x] Copter
[  ] Plane
[  ] Rover
[  ] Submarine
","['Enhancement', 'Copter', 'good first issue']",1
ArduPilot/ardupilot,Copter: implement Circle within Guided (aka LOITER_TURNS),"This enhancement request comes from this discussion: https://discuss.ardupilot.org/t/copter-not-supporting-mav-cmd-nav-loiter-turns-question/51785

The request is to implement Circle mode (aka LOITER_TURNS) from within Guided mode.

Support for LOITER_TURNS is already implemented in Auto mode so this should be possible.","['Enhancement', 'Copter', 'good first issue']",1
ArduPilot/ardupilot,Make error more explicit on install script,"### Feature request

**Is your feature request related to a problem? Please describe.**
We got some report of use that have failure to use SITL after using the install script.
I am pretty confident that they are just typing the command from the wiki and don't read output and miss the failure

**Describe the solution you'd like**
I propose that we make the error on the install script very visual with a big banner and if possible color !
","['Enhancement', 'good first issue']",1
ArduPilot/ardupilot,AP_Notify: Create automatic audible lost/crashed vehicle alarm,"### Feature request

**Is your feature request related to a problem? Please describe.**
When a vehicle crashes or is otherwise lost, it can be difficult to find it if you cannot see it.  If you do not have and RC connection, you cannot activate the lost vehicle alarm.  The lost vehicle alarm can only be activated by stick gesture or aux switch.  If you do not have a GCS connection and GPS, you cannot see its coordinates.

**Describe the solution you'd like**
- Lost vehicle alarm sound activate and stay activated when the RC and GCS are lost
- Lost vehicle alarm sound activate and stay activated on crash detection
- Change the lost vehicle alarm sound to something more unique, such as the sound of an ELT
- Change RGB lights to flash as bright white as possible and remain flashing after crash detection

**Describe alternatives you've considered**
Pop smoke and launch flares. But probably not practical at this time :)

**Platform**
[X] All
[  ] AntennaTracker
[  ] Copter
[  ] Plane
[  ] Rover
[  ] Submarine
","['Safety', 'Library', 'AllVehicles', 'good first issue']",1
ArduPilot/ardupilot,Plane: BUG with auto_state.checked_for_autoland,"### Bug report
Switch AUTO->RTL again an again,  because checked_for_autoland always set to false when change mode

```
bool Mode::enter()
{
    // cancel inverted flight
    plane.auto_state.inverted_flight = false;

    // don't cross-track when starting a mission
    plane.auto_state.next_wp_crosstrack = false;

    // reset landing check
    plane.auto_state.checked_for_autoland = false;

```

**Version**
4.0
**Platform**
[  ] All
[  ] AntennaTracker
[  ] Copter
[*] Plane
[  ] Rover
[  ] Submarine

**Airframe type**
_What type of airframe (flying wing, glider, hex, Y6, octa etc)_

**Hardware type**
_What autopilot hardware was used? (Pixhawk, Cube, Pixracer, Navio2, etc)_

**Logs**
_Please provide a link to any relevant logs that show the issue_
","['Plane', 'good first issue']",1
ArduPilot/ardupilot,"Copter: pre-arm check ""Terrain data missing"" when attempting to arm without GPS lock",,['good first issue'],1
ArduPilot/ardupilot,"ADSB: _OFFSET_LAT, OFFSET_LON should be renamed to POS_X/Y/Z","ADSB appears to use non-standard position offset parameter names, ADSB_OFFSET_LAT/LON which I think should be renamed to be ADSB_POS_X/Y/Z to be consistent with other sensors ([wiki](http://ardupilot.org/copter/docs/common-sensor-offset-compensation.html))

Maybe I'm misunderstanding but I don't see how a ""LAT"", ""LON"" parameter can be a good name for something that is in body frame.

Because users look at parameters directly a lot, I think we need to be careful about how we name things.",['good first issue'],1
ArduPilot/ardupilot,nmea output code should use fixed buffer and fix snprintf issues,"The AP_NMEA_Output code should allocate buffers once for max size of strings, and also needs to fix the use of snprintf for the lng_string to ensure it doesn't end up with an unterminated string in %s for next printf","['BUG', 'Library', 'good first issue']",1
ArduPilot/ardupilot,AC_Avoidance: Dijkstra's VisGraph should hold distances in meters,Dijkstra's [VisGraph class should hold distances in meters](https://github.com/ArduPilot/ardupilot/blob/master/libraries/AC_Avoidance/AP_OADijkstra.h#L85).  It currently holds the distances in cm perhaps because this saves a small amount in computation but even so we should change this to be meters.,"['Copter', 'Rover', 'good first issue']",1
ArduPilot/ardupilot,Rover: boats entering SmartRTL should loiter during path cleanup and during failure,"There are two places in mode_smart_rtl.cpp where stop_vehicle() is called ([see here](https://github.com/ArduPilot/ardupilot/blob/master/APMrover2/mode_smart_rtl.cpp#L40)).  

This is fine for Rovers but for boats they should ideally Loiter (i.e. hold position) if they can during the SmartRTL_WaitForPathCleanup and SmartRTL_Failure stages.

[Guided mode's behaviour when _guided_mode == Guided_WP](https://github.com/ArduPilot/ardupilot/blob/master/APMrover2/mode_guided.cpp#L35) can be used as a reference on how this can be done.","['Enhancement', 'Rover', 'good first issue']",1
ArduPilot/ardupilot,Rover: Guided mode's wp_bearing is incorrect for some sub modes,"While working on this PR: https://github.com/ArduPilot/ardupilot/pull/11283 I found that Guided mode's wp_bearing valueis incorrect for some submodes.

Guided uses the [mode.cpp's wp_bearing method](https://github.com/ArduPilot/ardupilot/blob/master/APMrover2/mode.cpp#L163) which calls the navigation library. This works when _guided_mode == Guided_WP but is incorrect when it's Guided_HeadingAndSpeed, Guided_TurnRateAndSpeed and Guided_Loiter.

We should override the wp_bearing for Guided mode and then handle these other sub modes.  For example, for Guided_HeadingAndSpeed we should use the target heading.  For Guided_TurnRateAndSpeed we should return 0.0f.  For Guided_Loiter we should call ModeLoiter's wp_bearing.","['BUG', 'Rover', 'good first issue']",1
ArduPilot/ardupilot,SITL: add correct battery estimator from vehicle type,"### Feature request

**Is your feature request related to a problem? Please describe.**
Following https://github.com/ArduPilot/ardupilot/pull/9900, we should create a correct way to implement battery consumption on SITL : 
Maybe with a battery object in SITL and with model of consumption on each vehicle backend.

**Platform**
[ X] All
[  ] AntennaTracker
[  ] Copter
[  ] Plane
[  ] Rover
[  ] Submarine

","['Enhancement', 'SITL', 'good first issue']",1
ArduPilot/ardupilot,RangeFinder: Using existing  Analog offset for other interfaces (I2C or Serial),"### Feature request
Expand rangefinder offset 

**Is your feature request related to a problem? Please describe.**
Need some easy fix to adjust sensor offset.
For example, the Garmin Lidar Lite, has an offset of approx. 25 cm on low range, and users do not have a mean to compensate.

**Describe the solution you'd like**
Just tested using the existing analog offset  within this code 
https://github.com/ArduPilot/ardupilot/blob/master/libraries/AP_RangeFinder/AP_RangeFinder_PulsedLightLRF.cpp#L94

    uint16_t _distance_cm = be16toh(val);
    float offset  = state.offset;
    // remove momentary spikes
    if (abs(_distance_cm - last_distance_cm) < 100) {
        state.distance_cm = (_distance_cm-offset);


**Platform**
[x  ] All
[  ] AntennaTracker
[  ] Copter
[  ] Plane
[  ] Rover
[  ] Submarine


","['Enhancement', 'Library', 'good first issue']",1
ArduPilot/ardupilot,PRX_ORIENT does not work (Teraranger EVO Tower),"**IF YOU DON'T REMOVE THESE FOUR LINES, THEN YOUR REPORT WILL BE CLOSED AUTOMATICALLY**
Questions and user problems should be directed at the forum (http://discuss.ardupilot.org)
_**Please be very sure you have found a bug when opening this issue**_
If there was a previous discussion in the forum, link to it

### Bug report

**Issue details**

When setting PRX_ORIENT = 1 with a Teraranger Tower Evo hub board, mounted upside down, the sensors stop working and no data comes to proximity screen in MP. Otherwise, setting back to = 0 gives distance data.

**Version**
3.6.0

**Platform**
[  ] All
[  ] AntennaTracker
[ X] Copter
[  ] Plane
[  ] Rover
[  ] Submarine


**Hardware type**
Pixhawk 2.1 cube


",['good first issue'],1
ArduPilot/ardupilot,AP_Param can set parameters by name which it shouldn't,"### Bug report

**Issue details**

The ArduSub tests currently pass when they probably shouldn't.

**Version**
master
**Platform**
[  ] All
[  ] AntennaTracker
[  ] Copter
[  ] Plane
[  ] Rover
[ X ] Submarine

**Airframe type**
Sub

**Hardware type**
SITL

Reproduce with:

```
--- a/libraries/GCS_MAVLink/GCS_Param.cpp
+++ b/libraries/GCS_MAVLink/GCS_Param.cpp
@@ -329,6 +329,9 @@ void GCS::send_parameter_value(const char *param_name, ap_var_type param_type, f
         if ((1U<<i) & mavlink_active) {
             const mavlink_channel_t _chan = (mavlink_channel_t)(MAVLINK_COMM_0+i);
             if (HAVE_PAYLOAD_SPACE(_chan, PARAM_VALUE)) {
+                if (!strcmp(param_name, ""ARMING_RUDDER"")) {
+                    abort();
+                }
                 mavlink_msg_param_value_send(
                     _chan,
                     param_name,
pbarker@bluebottle:~/rc/ardupilot(master)$ 
```

`./Tools/autotest/autotest.py --gdb --debug build.ArduSub dive.ArduSub`

At that point in the code we've set the value in AP_Param when we shouldn't have.

We're not honouring `check_frame_type` in the find-by-name codepaths.
",['good first issue'],1
ArduPilot/ardupilot,Rover: servo output range not set after SERVOx_FUNCTION changed,"Servo output ranges are set as part of the AP_MotorsUGV::init function.  This is only called once soon after startup meaning that if a user changes a SERVOx_FUNCTION value the servo's output range will not be correct.  This could lead to odd values being sent out to the servo until the board is rebooted.

We should first verify that the issue is real by setting a second SERVOx_FUNCTION for throttle and then compare it's output to the the servo3 outputs and see if they're different.

The solution might to one of these:

- call the motors init regularly (1hz?) while the vehicle is disarmed
- add boolean values of whether various frame types have been enabled (i..e the return value of have_skid_steering()) and then regularly check if the true/false value has changed and if it has call the init() function again.  Note that this might not work in the edge case where the user is setting up a second channel for throttleLeft or throttleRight because setting up this second channel won't affect the return value of the has_skid_steering() function.","['BUG', 'Rover', 'good first issue']",1
ArduPilot/ardupilot,SITL: AP_Camera object,"### Feature request

**Is your feature request related to a problem? Please describe.**
We should add a camera object like for SPRAYER and GRIPPER but for CAMERA.
","['SITL', 'good first issue']",1
ArduPilot/ardupilot,Rover : Forward to waypoint # when failsafe triggered,"#### Issue details
_Please describe the problem, or desired feature_
Current only have RTL, hold etc., it is great if it can be set a waypoint# when failsafe triggered
#### Version
_What version was the issue encountered with_
Rover3.3
#### Platform
[  ] All
[  ] AntennaTracker
[  ] Copter
[  ] Plane
[x  ] Rover
[  ] Submarine

#### Airframe type
_What type of airframe (flying wing, glider, hex, Y6, octa etc)_

#### Hardware type
_What autopilot hardware was used? (pixhawk, pixracer, PX4FMU etc)_
Pixhawk 2.1
#### Logs
_Please provide a link to any relevant logs that show the issue_

","['Enhancement', 'Rover', 'good first issue']",1
ArduPilot/ardupilot,AP_RangeFinder: TerraRangeI2C should use non-block semaphore in update(),"As discussed in [this PR related to the Maxbotix I2C rangefinder driver](https://github.com/ArduPilot/ardupilot/pull/8471), the TeraRangerI2C driver should take use semaphore using non-blocking instead of BLOCK_FOREVER from within the [update function](https://github.com/ArduPilot/ardupilot/blob/master/libraries/AP_RangeFinder/AP_RangeFinder_TeraRangerI2C.cpp#L156).

The reason is that the update() function is called from the mainloop so in the unlikely case that the I/O thread locks up, this would cause the main thread to also lockup.  Of course, the I/O thread locking up would likely lead to a crash anyway as I think we would not receive updates from the main IMU either.","['Library', 'good first issue']",1
ArduPilot/ardupilot,Copter: add pre-arm check if frame can't find the output for a motor,"In Copter-3.5 (and higher) the AP_Motors library requires that the SERVOx_FUNCTION be set for each motor.  Although these parameter default to good values, it's possible that a user could mess up the SERVOx_FUNCTION settings so that there are no defined outputs for some motors.  We should raise a pre-arm check failure in such cases.","['Enhancement', 'Copter', 'good first issue']",1
ArduPilot/ardupilot,Copter: vehicle may backup if mission with takeoff as first command is initiated,"Try this in the simulator:

- set up a mission with a take-off command to 50m followed by a waypoint command
- set MIS_RESTART = 1
- arm and takeoff in stabilize or loiter, fly the vehicle forward at a reasonably fast speed but do not climb above 50m
- initiate AUTO
- the take-off command sets a horizontal target based on the vehicle's position at the moment the take-off command was initiated.  If the vehicle is moving horizontally it will continue flying forward for a bit but then slow and return to where it was.

The solution is that if the vehicle is not-landed, the take-off command should use the horizontal stopping point as it's target instead of the current horizontal position.","['BUG', 'Copter', 'good first issue']",1
ArduPilot/ardupilot,SITL: reimplement the -P command ,"#### Issue details
The use of -P is broken as the parameter is erase when the default param file is loaded. 
See @OXINARF comments on for an initial solution https://github.com/ArduPilot/ardupilot/pull/6040
","['Enhancement', 'SITL', 'good first issue']",1
ArduPilot/ardupilot,Plane: Landing permanently disable geofence,"#### Issue details
Once you begin a landing sequence the geofence can be disabled ( https://github.com/ArduPilot/ardupilot/blob/649ef0fc1614cacf823115cf8433319a4f8da563/ArduPlane/commands_logic.cpp#L404 ) but if you abort the landing or change modes for any reason the fence is never re enabled. (This is not related to AP_Landing changes, and is a pre existing bug)

#### Version
Master

#### Platform
[  ] All
[  ] AntennaTracker
[  ] Copter
[ x ] Plane
[  ] Rover

EDIT: Updated the link to the code to be useful/correct.","['BUG', 'Plane', 'Safety', 'good first issue']",1
ArduPilot/ardupilot,MAVlink library prevents OSD from receiving RSSI from 3DR modems,"#### Issue details
file https://github.com/ArduPilot/ardupilot/blob/master/libraries/GCS_MAVLink/MAVLink_routing.cpp

lines starting 103

```
if (msg->msgid == MAVLINK_MSG_ID_RADIO ||
        msg->msgid == MAVLINK_MSG_ID_RADIO_STATUS) {
        // don't forward RADIO packets
        return true;
    }
```

this code fully filters out MAVLINK_MSG_ID_RADIO_STATUS and MAVLINK_MSG_ID_RADIO messages so no one module (and OSD too) can't see RSSI from 3DR modems.

So in Copter 3.1 3DR RSSI is working, and definitely not working on Copter 3.3/3.4. All OSD users logically [consider](https://www.rcgroups.com/forums/showpost.php?p=36273947&postcount=1861) that it is problem of OSD itself

This messages should be filtered out only on port from which they are comes, and be routed to all another MAVlink ports. Or even better to route this message to all ports and filter them out in 3DR modem firmware.","['Enhancement', 'Library', 'MAVLink', 'good first issue']",1
ArduPilot/ardupilot,handle_mission_write_partial_list is too strict,"#### Issue details

Currently `GCSCommon.cpp@handle_mission_write_partial_list` checks if the partial waypoint list is inside the current mission count boundary unnecessarily. This limits the utility of partial waypoint writing modifications because we can never expand the mission count.

A very simple case is appending a waypoint to a waypoint list. As far as I know a full rewrite of the mission plan would be required because the only other way to change the waypoint list is sending a `mavlink_msg_mission_count*` message with the previous mission count plus one. This is exactly what a partial list is for.

Removing the upper bound checks would not result in a problem because `GCS_Common.cpp@GCS_MAVLINK::handle_mission_item:839` already handles the case where the mission seq is bigger than mission.num_commands().

Am I wrong in these assumptions? If theoretically there is no problem I can produce a patch removing the upper bound checks.
","['Library', 'AllVehicles', 'MAVLink', 'good first issue']",1
ArduPilot/ardupilot,Calibration messages come through as high severity,"For example ""Place vehicle on LEFT side and press any key"" come through at high error severities instead of info level severities. This cause them to pop up as errors in QGroundControl. 
",['good first issue'],1
ArduPilot/ardupilot,Copter: add ch7 ch8 opt for set wp yaw behavior to 0 or 2 in flight,,"['Enhancement', 'Copter', 'good first issue']",1
ArduPilot/ardupilot,Copter: easier test of benchtest the parachute,"At the moment, because of the land-check in the parachute driver (which helps ensure we don't have unintended deployments on the ground near people) it can be difficult for a user to ensure the parachute servo is working during the setup.
We should perhaps clarify the testing procedure on the wiki and/or allow an override of the landing check - perhaps by allowing the mission planner to send a deploy-chute command with a magic number in param2 to specify that the chute should deploy no-matter what!
","['Copter', 'Safety', 'good first issue']",1
ArduPilot/ardupilot,Plane: make throttle slewrates be uint16,"this will help with takeoff throttle slew for catapult launch
","['Enhancement', 'Plane', 'good first issue']",1
ArduPilot/ardupilot,Copter: Pre-Arm Safety Check for Auto Missions,"Hi all,
for safety reasons I suggest a pre-arm check that either disables Auto mode or refuses to arm if the waypoints are further away from the current/home location than a user specified value in meter. 
This would prevent from ""fly aways"" when one accidentally switches to Auto mode and an ""old"" mission is executed which is located some n km away.
Kind regards,
Thorsten
","['Enhancement', 'Copter', 'Safety', 'good first issue']",1
ArduPilot/ardupilot,Copter: Activate buzzer when autolanding / buzzer modes,"AC3 does autolanding in certain failsafe conditions.
For safety reasons and to help locate the copter after a failsafe-induced autolanding, buzzing the buzzer in intervals when in autoland mode would be good. Interval-buzzing is also likely to carry a psychological ""do not touch me""-imperative in case, bywalkers find the copter after a failsafe-induced autolanding.

Buzzer could be controlled by a parameter:
BUZ_WARN: 0=never (only ARM/DISARM beeps), 1=failsafe autolandings, 2=any autolanding (including manual set LAND mode), 3=any failsafe condition
","['Enhancement', 'Copter', 'good first issue']",1
ArduPilot/ardupilot,Copter: ensure visible (led) and verbal (mission planner) warnings appear when Fence is breached,,"['Enhancement', 'NeedAnswer', 'Copter', 'Safety', 'good first issue']",1
ArduPilot/ardupilot,DDS: Vehicle to report SITL state,"ROS visualization and integration with other sensors require the vehicle's actual state rather than the estimated one, especially if we are using cameras or lidars as a measurement for estimating the state itself.

## Feature request

A new publisher `/ap/sim/groundtruth` with a custom message type, published at at least 50Hz in SITL only.

The message could be like:
```
std_msgs/Header header

# Geographic position
float64 latitude    # degrees
float64 longitude   # degrees
float64 altitude    # meters above mean sea level

# Cartesian position in ENU frame relative to home position
geometry_msgs/Vector3 position

# Orientation quaternion in ENU frame
geometry_msgs/Quaternion orientation

# Linear velocity in ENU frame (m/s)
geometry_msgs/Vector3 linear_velocity

# Angular velocity in body frame (rad/s)
geometry_msgs/Vector3 angular_velocity
```

**Platform**
[ x ] All
[  ] AntennaTracker
[  ] Copter
[  ] Plane
[  ] Rover
[  ] Submarine


",[],0
ArduPilot/ardupilot,`LOG_FILE_DSRMROT`: allow to work with replay,"## Feature request

**Is your feature request related to a problem? Please describe.**

Our setup currently uses a CubeOrange+ flight controller paired with a separate system-controller MCU, each with independent logging systems. To troubleshoot recurring EKF issues (such as altitude discontinuities), we have enabled EKF_REPLAY with LOG_DISARMED set to always on, as recommended in the documentation for effective replay functionality.

However, LOG_DISARMED causes the flight controller to continuously log to a single .bin file across multiple flights until reboot is performed. We usually like to separate our system's log per flight. This makes it relatively troublesome to correlate flight controller logs with our system-controller logs on a per-flight basis, as each flightâ€™s data is embedded within one large file rather than segmented by flight event.

At the moment, the workaround that we have done to correlate the flight controller logs and the system controller logs is that we have a separate lua script that captures the current bin file number and transmit it over CAN which is stored in the system controller log. This makes it relatively easier to find the correct flight logs, but the issue where the flight logs gets really big (~300MB or potentially even more) still stands.


**Describe the solution you'd like**
I was wondering if it would be possible that another feature be added where maybe, the log stops being recorded X seconds after the drone is disarmed, and then it creates a new log file afterwards. This would potentially allow us to have 1 log per flight, instead of having multiple flights being recorded (before a reboot is done) in a single bin file which can be quite annoying to analyse.

**Describe alternatives you've considered**
Up until now, to reduce the file size, we performed the following workaround:
- Reboot the flight controller prior to every flight test (not entirely practical once out in the field) to capture one flight per log
- Transmit the current .bin file number over CAN through the use of lua script to simplify the process of finding the corresponding BIN files to the system controller logs. We can then use the timestamps from our system controller logs and the flight controller logs to find the exact flight event.
- Disable INS logging unless necessary to reduce the file size

**Platform**
[  ] All
[  ] AntennaTracker
[x] Copter
[  ] Plane
[  ] Rover
[  ] Submarine
",[],0
ArduPilot/ardupilot,Add support for the u-Blox ZED-X20P GNSS module,"## Feature request

**Is your feature request related to a problem? Please describe.**
No

**Describe the solution you'd like**
Add support for the u-Blox ZED-X20P GNSS module in AP_GPS / UBLOX . 

**Describe alternatives you've considered**
_A clear and concise description of any alternative solutions or features you've considered._

**Platform**
[ X ] All
[  ] AntennaTracker
[  ] Copter
[  ] Plane
[  ] Rover
[  ] Submarine

**Additional context**
This module is the latest version from u-Blox and it supports L1/L2/L5/L6 with RTK. It has current support for u-Blox Point Perfect corrections service (IP and L-band) and future support for in Q1 26 for centimeter positioning with Galileo High Accuracy services delivered via E6b.
",[],0
ArduPilot/ardupilot,DDS: External Odometry overflows the timestamp,"The function AP_DDS_External_Odom::handle_external_odom() converts the reported timestamp from microseconds (uint64) to ms (uint32). This causes an overflow when calculating `time_ms`.

https://github.com/ArduPilot/ardupilot/blob/master/libraries/AP_DDS/AP_DDS_External_Odom.cpp#L42

For example, it remote_time_us = 1769032352500220, time_ms = 3800825856

## Suggestion

We can convert timestamp to time since boot right in place, accepting some minor jitter.

**Platform**
[ x ] All
[  ] AntennaTracker
[  ] Copter
[  ] Plane
[  ] Rover
[  ] Submarine

",[],0
ArduPilot/ardupilot,"RTL_ALT_FINAL is relative to home, not terrain, causing hard landing","## Bug report

**Issue details**
RTL_ALT_FINAL is calculated relative to HOME, even if RTL_TYPE is terrain and TERRAIN is enabled. 
This causes the drone to descend all the way to ground at LAND_SPEED_HIGH if RTL target (RALLY point) is located higher in the terrain relative to HOME and RTL_ALT_FINAL > 0

**Version**
Arducopter 4.5.2 in IRL. Verifired in Arducopter 4.7 in SITL.

**Platform**
[  ] All
[  ] AntennaTracker
[x] Copter
[  ] Plane
[  ] Rover
[  ] Submarine

**Airframe type**
Hex

**Hardware type**
Cube orange

**Logs**
https://www.pahagen.com/ardupilot/00199-2026-01-15_10-34-13.tlog
(RTL start at approx 95% to end. Mode changed to break approx 5 meters above ground to prevent hard landing).
",['Copter'],0
ArduPilot/ardupilot,fix MAV_CMD_DO_START_MAG_CAL autoreboot to only reboot after a successful calibration,"## Feature request
**Is your feature request related to a problem? Please describe.**
`MAV_CMD_DO_START_MAG_CAL` has a autoreboot param which reboots upon any final state of calibration, including failures and user cancellation. This seems unnessicary and incorrect

**Describe the solution you'd like**
patch `MAV_CMD_DO_START_MAG_CAL` param5 (autoreboot) so that setting it only causes a reboot when the calibration succeeds.

**Describe alternatives you've considered**
maintain the legacy reboot behavior if param5 is set to 1, but introduce a ""reboot only on success"" option by passing 2 to param5. I'm curious if this odd reboot behavior has to be preserved or not, as ""fixing"" it is much simpler and doesn't require a new enum to be created for the parameter.

**Platform**
[x] All
[  ] AntennaTracker
[  ] Copter
[  ] Plane
[  ] Rover
[  ] Submarine

**Additional context**
In the calibration code there is a guard for this for the autosave param, but not the reboot param
```
        if (_calibrator[i]->running()) {
            running = true;
        } else if (_cal_autosave && !_cal_saved[i] && _calibrator[i]->get_state().status == CompassCalibrator::Status::SUCCESS) {
            _accept_calibration(uint8_t(i));
        }
```
```
   if (is_calibrating()) {
        _cal_has_run = true;
        return;
    } else if (_cal_has_run && _auto_reboot()) {    // <- no check of status, always reboots even on user cancellation
        hal.scheduler->delay(1000);
        hal.scheduler->reboot();
    }
```

I'm interested in creating a PR to fix this issue, but wanted clarification on which approach is best before starting.",[],0
ArduPilot/ardupilot,Ubuntu 24.04 ROS2 Jazzy build fails fastcdr version,"## Bug report

Setting up Ardupilot on Ubuntu 24.04.03 and ROS2 Jazzy, when I try to run colcon build on the ardupilot_dds_package, the build fails for ROS2 Jazzy giving a fastcdr version mismatch error - 

```
--- stderr: micro_ros_agent
CMake Error at CMakeLists.txt:153 (find_package):
  Could not find a configuration file for package ""fastcdr"" that is
  compatible with requested version ""1"".

  The following configuration files were considered but not accepted:

    /opt/ros/jazzy/lib/cmake/fastcdr/fastcdr-config.cmake, version: 2.2.5
    /usr/local/lib/cmake/fastcdr/fastcdr-config.cmake, version: 2.2.6

```

**Version**
master branch

**Platform**
[  ] All
[  ] AntennaTracker
[  ] Copter
[  ] Plane
[  ] Rover
[  ] Submarine

**Airframe type**
_What type of airframe (flying wing, glider, hex, Y6, octa etc)_

**Hardware type**
_What autopilot hardware was used? (Pixhawk, Cube, Pixracer, Navio2, etc)_

**Logs**

<img width=""1655"" height=""779"" alt=""Image"" src=""https://github.com/user-attachments/assets/f7c3693e-2955-4eab-bd44-46a5b13785e5"" />
",['ROS'],0
ArduPilot/ardupilot,"gimbal_device_id == 0 should control all gimbals, not primary","## Bug report

For the commands listed below, a gimbal device id == 0 is, per the MAVLink spec, intended for *all* gimbal devices.
On current `master`, it sometimes (always?) instead refers to the ""primary"" device.
This seems a simple mistake. This issue's purpose is to fix it.

Because fixing it will cause a change in behavior, the fix should be done concurrent with usage spots. Here are some, please comment below if you know of others:
- MP's 'payload' screen uses a default value of 0. Users expect this to control the primary gimbal, not all. (Perhaps switch default to 1, with a dropdown for user to select ""ALL"" if that's desired.)
- The auto mode similarly defaults to 0. (Perhaps do the same type of fix: default to 1 instead.)

[Related info]
Forum post confirming this is bug: https://discuss.ardupilot.org/t/possible-bug-behavior-for-gimbal-device-id-0-in-ap-mount/141900/2

MAVLink commands it may affect:
- GIMBAL_MANAGER_SET_ATTITUDE (282)
- GIMBAL_MANAGER_SET_PITCHYAW (287)
- GIMBAL_MANAGER_SET_MANUAL_CONTROL (288)
- MAV_CMD_DO_SET_ROI_LOCATION (195)
- MAV_CMD_DO_SET_ROI_WPNEXT_OFFSET (196)
- MAV_CMD_DO_SET_ROI_NONE (197)
- MAV_CMD_DO_SET_ROI_SYSID (198)
- MAV_CMD_DO_GIMBAL_MANAGER_PITCHYAW (1000)
- MAV_CMD_DO_GIMBAL_MANAGER_CONFIGURE (1001)

**Version**
Current master (f18e69a24e075163b419580355d2b164b71f5f31).

**Platform**
[x] All
[  ] AntennaTracker
[  ] Copter
[  ] Plane
[  ] Rover
[  ] Submarine

**Airframe type**
N/A (Is a mavlink + gimbal + AP_Mount issue).

**Hardware type**
N/A

**Logs**
N/A","['BUG', 'Gimbal']",0
ArduPilot/ardupilot,Native Ship / Moving Platform Landing Support for ArduCopter,"Currently, ArduCopter does not provide a native, documented way to land on a moving platform (e.g. ship deck) comparable to the existing Plane / QuadPlane ship landing support.

While Plane has:

SIM_SHIP_ENABLE

Built-in ship simulation

Documented RTL-based landing logic

Official examples and Lua scripts

ArduCopter users must rely on:

External Rover â€œbeaconâ€ vehicles

Undocumented FOLLOW behavior

Custom Lua scripts

Trial-and-error configuration

This makes ship landing with Copter difficult, fragile, and inconsistent, especially in SITL and for research/testing use cases.",[],0
ArduPilot/ardupilot,Bosch BMP581 baro intermittent I2C read errors,"## Bug report

**Issue details**

We are testing Bosch BMP581 as a replacement barometer on Pixhawk 6X due to ICP20100 and BMP388 being discontinued.

It seems like 
	1.	BMP581 works fine on I2C2.
	2.	On I2C4, we see intermittent bad samples that show up as large spikes in pressure or baro altitude. Logic analyzer suggests occasional I2C glitches. This seems like bus reliability or recovery behavior rather than a sensor algorithm issue.

<img width=""813"" height=""265"" alt=""Image"" src=""https://github.com/user-attachments/assets/51068d07-2f27-4fd0-9251-fb634ef2f821"" />
baro0 (i2c4, BMP581 glitches) 

<img width=""770"" height=""328"" alt=""Image"" src=""https://github.com/user-attachments/assets/00904aef-9db4-45f5-bb2a-5e95dd4f2155"" />
Baro1 (I2C2,BMP581, No issue) 

<img width=""1283"" height=""555"" alt=""Image"" src=""https://github.com/user-attachments/assets/e3718308-2979-4f7d-ac88-2d59b1f3a647"" />

**Version**
Master

**Platform**
[ X ] All
[  ] AntennaTracker
[  ] Copter
[  ] Plane
[  ] Rover
[  ] Submarine

**Airframe type**
Copter

**Hardware type**
Holybro PIxhawk 6X

**Logs**
[Log ](https://drive.google.com/file/d/1VpvUOkHgXlUoMBhySO-Vg0Qk_fINYPsm/view?usp=sharing)
",[],0
ArduPilot/ardupilot,speedybeef4v4  failed to initialize MAVFTP when using gps (serial6) + mavlink on (serial3),"ArduCopter 4.6.1, trying 4.6.3 - same result - this config make  - failed to initialize MAVFTP - when connecting to FC. 
Disabling serial 6 - fix problem!

[config.zip](https://github.com/user-attachments/files/24607303/config.zip)",['NeedAnswer'],0
ArduPilot/ardupilot,SIGFPE via incorrect type conversion on fence file upload,"## Bug report

**Issue details**
A **SIGFFPE** signal is raised in functions **_convert_MISSION_ITEM_INT_to_AC_PolyFenceI_item_int_** ([MissionItemProtocol_Fence.cpp](https://github.com/ArduPilot/ardupilot/blob/master/libraries/GCS_MAVLink/MissionItemProtocol_Fence.cpp)) and/or _**is_positive**_ ([AP_Math.h)](https://github.com/ArduPilot/ardupilot/blob/master/libraries/AP_Math/AP_Math.cpp)) during an attempted upload of a crafted fence files with invalid values of parameter **_param1_**. Floating point exception occurs due to unchecked type conversions, leading to immediate autopilot process termination and loss of GCS â€“ vehicle communication. 


**Steps to reproduce**
Vulnerability can be triggered by uploading fence files with parameters set to invalid values:

1. **_fence-oor.txt_** â€“ with **_param1_** set to â€œ_**-1e+10**_â€ (minus 10 billion)
```
QGC WPL 110
0	0	0	5001	-1e+10		0	0	0	0	0	0	0
```
2. _**fence-nan.txt**_  - with **_param1_** set to â€œ_NaN_â€ (Not-a-Number)
```
QGC WPL 110
0	0	0	5003	 NaN		0	0	0	0	0	0	0
```
To reproduce the vulnerability for ArduCopter in SITL environment, follow below steps:

1. Run SITL simulation for ArduCopter in debug mode
    `$ Tools/autotest/sim_vehicle.py -v ArduCopter --console --map -w -D -G -ubsan`

2. Wait for vehicle to initialize into PreArm state
   `pre-arm good`

3. Upload one of the fence files to the vehicle using MAVProxy GCS console
```
> fence load fence-oor.txt
> fence load fence-nan.txt
```

4. Observe GCS connection and vehicle process termination

```
Sent all 1 fence items
no link
link 1 down
```

Exception raised for **_fence-oor.txt_** 
```
Thread 1 ""arducopter"" received signal SIGFPE, Arithmetic exception.
MissionItemProtocol_Fence::convert_MISSION_ITEM_INT_to_AC_PolyFenceItem (mission_item_int=..., ret=...)
    at ../../libraries/GCS_MAVLink/MissionItemProtocol_Fence.cpp:117
117             ret.vertex_count = mission_item_int.param1;
(gdb) print ret.vertex_count
$5 = 0 '\000'
(gdb) print mission_item_int.param1
$6 = -1e+10

```

Exception raised for **_fence-nan.txt_**
```
Thread 1 ""arducopter"" received signal SIGFPE, Arithmetic exception.
0x00005555555cd03c in is_positive<float> (fVal1=nan(0x400000)) at ../../libraries/AP_Math/AP_Math.h:68
68          return (static_cast<float>(fVal1) >= FLT_EPSILON);
(gdb) print fVal1
$7 = nan(0x400000)
```

Backtrace shows that both input files had a similar effect upon upload, resulting in the FPE due to unchecked type conversions, but raised in different parts of the code:

1. **_fence-oor.txt_** - function **_convert_MISSION_ITEM_INT_to_AC_PolyFenceI_item_int_** responsible for converting MAVLink mission item into an **_AC_PolyFenceItem_** structure,
performs an unchecked assignment of **_param1_** value to **_vertex_count_** of type _uint_8_t_.
2. **_fence-nan.txt_** - function **_is_positive_**, which simply checks whether a float is greater than zero, makes a float-to-NaN comparison between **_fVal1_** (holding _**param1**_ value) and **_FLT_EPSILON_** (of type float), which raises floating-point operation exception. 

**Root cause**
The root cause of this vulnerability is the lack of sufficient validation of parameters carried by **_param1_**, allowing out-of-range or non-numerical values to trigger an unhandled exception. Existing sanitization routine performs none or minimal input validation by doing only upper bound check (>255) for selected parameters, making it highly insufficient. 
Also keep in mind that demonstrated examples exploit only single case for specific 
commands (5001 and 5003), but the same vulnerability is present in all fence-specific commands:

- _MAV_CMD_NAV_FENCE_POLYGON_VERTEX_INCLUSION_  (5001) for  **_vertex_count_** =<(_-1e+10_)
- _MAV_CMD_NAV_FENCE_POLYGON_VERTEX_EXCLUSION_  (5002) for **_vertex_count_** =<(_-1e+10_)
- _MAV_CMD_NAV_FENCE_CIRCLE_EXCLUSION_  (5003) for **_radius_** == _NaN_
- _MAV_CMD_NAV_FENCE_CIRCLE_EXCLUSION_  (5004) for **_radius_** == _NaN_


**Impact**
The impact of this vulnerability is severe, affecting both operational reliability and flight safety across all ArduPilot vehicles. When an attacker in the network or unintentional operator uploads improperly constructed fence file, the autopilot software immediately crashes due to unhandled exception. As a direct result, the connection between the GCS and the vehicle is abruptly lost, so any ongoing automated mission is terminated. For vehicle in the air, this loss of control may lead to dangerous situations including uncontrolled flight, failure to perform failsafe procedures, or even a crash.

**Recommendation**
Enforce strict input validation and bounds checking on all externally supplied fence parameters before they are stored or used in arithmetic operations.

**Version**
_4.6.3_

**Platform**
[  ] All
[  ] AntennaTracker
[X] Copter
[X] Plane
[X] Rover
[X] Submarine


**Logs**

Backtrace for **_fence_oor.txt_**
```
#0  MissionItemProtocol_Fence::convert_MISSION_ITEM_INT_to_AC_PolyFenceItem (mission_item_int=..., ret=...)
    at ../../libraries/GCS_MAVLink/MissionItemProtocol_Fence.cpp:117
#1  0x000055555595b877 in MissionItemProtocol_Fence::replace_item (this=0x555555b79260, mission_item_int=...)
    at ../../libraries/GCS_MAVLink/MissionItemProtocol_Fence.cpp:155
#2  0x000055555595ad8b in MissionItemProtocol::handle_mission_item (this=0x555555b79260, msg=..., cmd=...)
    at ../../libraries/GCS_MAVLink/MissionItemProtocol.cpp:291
#3  0x000055555570e1a3 in GCS_MAVLINK::handle_mission_item (this=0x555555b84ea0, msg=...) at ../../libraries/GCS_MAVLink/GCS_Common.cpp:1028
#4  0x0000555555715f2a in GCS_MAVLINK::handle_common_mission_message (this=0x555555b84ea0, msg=...) at ../../libraries/GCS_MAVLink/GCS_Common.cpp:4694
#5  0x0000555555715a60 in GCS_MAVLINK::handle_message (this=0x555555b84ea0, msg=...) at ../../libraries/GCS_MAVLink/GCS_Common.cpp:4425
#6  0x00005555555d224e in GCS_MAVLINK_Copter::handle_message (this=0x555555b84ea0, msg=...) at ../../ArduCopter/GCS_MAVLink_Copter.cpp:1250
#7  0x000055555570f96b in GCS_MAVLINK::raw_packetReceived (this=0x555555b84ea0, framing_status=1 '\001', status=..., msg=...)
    at ../../libraries/GCS_MAVLink/GCS_Common.cpp:1924
#8  0x000055555570fbcb in GCS_MAVLINK::update_receive (this=0x555555b84ea0, max_time_us=1000) at ../../libraries/GCS_MAVLink/GCS_Common.cpp:1976
#9  0x0000555555711aaa in GCS::update_receive (this=0x555555ae4030 <copter+40240>) at ../../libraries/GCS_MAVLink/GCS_Common.cpp:2797
#10 0x00005555555cd930 in Functor<void>::method_wrapper<GCS, &GCS::update_receive> (obj=0x555555ae4030 <copter+40240>)
    at ../../libraries/AP_HAL/utility/functor.h:88
#11 0x0000555555684774 in Functor<void>::operator() (this=0x555555aa6980 <Copter::scheduler_tasks+1344>) at ../../libraries/AP_HAL/utility/functor.h:54
#12 0x00005555556e72dd in AP_Scheduler::run (this=0x555555ada458 <copter+344>, time_available=2500) at ../../libraries/AP_Scheduler/AP_Scheduler.cpp:270
#13 0x00005555556e778e in AP_Scheduler::loop (this=0x555555ada458 <copter+344>) at ../../libraries/AP_Scheduler/AP_Scheduler.cpp:399
#14 0x00005555556ed841 in AP_Vehicle::loop (this=0x555555ada300 <copter>) at ../../libraries/AP_Vehicle/AP_Vehicle.cpp:559
#15 0x000055555583d4b8 in HAL_SITL::run (this=0x555555af6860 <hal_sitl_inst>, argc=12, argv=0x7fffffffdb68, callbacks=0x555555ada300 <copter>)
    at ../../libraries/AP_HAL_SITL/HAL_SITL_Class.cpp:298
#16 0x00005555555cce9f in main (argc=12, argv=0x7fffffffdb68) at ../../ArduCopter/Copter.cpp:997
```


Backtrace for **_fence-nan.txt_**
```
#0  0x00005555555cd03c in is_positive<float> (fVal1=nan(0x400000)) at ../../libraries/AP_Math/AP_Math.h:68
#1  0x00005555557751f7 in AC_PolyFence_loader::validate_fence (this=0x555555adfa60 <copter+22368>, new_items=0x555555b78370, count=1)
    at ../../libraries/AC_Fence/AC_PolyFence_loader.cpp:1032
#2  0x000055555577554f in AC_PolyFence_loader::write_fence (this=0x555555adfa60 <copter+22368>, new_items=0x555555b78370, count=1)
    at ../../libraries/AC_Fence/AC_PolyFence_loader.cpp:1110
#3  0x000055555595ba9c in MissionItemProtocol_Fence::complete (this=0x555555b79260, _link=...) at ../../libraries/GCS_MAVLink/MissionItemProtocol_Fence.cpp:192
#4  0x000055555595af40 in MissionItemProtocol::transfer_is_complete (this=0x555555b79260, _link=..., msg=...)
    at ../../libraries/GCS_MAVLink/MissionItemProtocol.cpp:325
#5  0x000055555595ae63 in MissionItemProtocol::handle_mission_item (this=0x555555b79260, msg=..., cmd=...)
    at ../../libraries/GCS_MAVLink/MissionItemProtocol.cpp:312
#6  0x000055555570e1a3 in GCS_MAVLINK::handle_mission_item (this=0x555555b84ea0, msg=...) at ../../libraries/GCS_MAVLink/GCS_Common.cpp:1028
#7  0x0000555555715f2a in GCS_MAVLINK::handle_common_mission_message (this=0x555555b84ea0, msg=...) at ../../libraries/GCS_MAVLink/GCS_Common.cpp:4694
#8  0x0000555555715a60 in GCS_MAVLINK::handle_message (this=0x555555b84ea0, msg=...) at ../../libraries/GCS_MAVLink/GCS_Common.cpp:4425
#9  0x00005555555d224e in GCS_MAVLINK_Copter::handle_message (this=0x555555b84ea0, msg=...) at ../../ArduCopter/GCS_MAVLink_Copter.cpp:1250
#10 0x000055555570f96b in GCS_MAVLINK::raw_packetReceived (this=0x555555b84ea0, framing_status=1 '\001', status=..., msg=...)
    at ../../libraries/GCS_MAVLink/GCS_Common.cpp:1924
#11 0x000055555570fbcb in GCS_MAVLINK::update_receive (this=0x555555b84ea0, max_time_us=1000) at ../../libraries/GCS_MAVLink/GCS_Common.cpp:1976
#12 0x0000555555711aaa in GCS::update_receive (this=0x555555ae4030 <copter+40240>) at ../../libraries/GCS_MAVLink/GCS_Common.cpp:2797
#13 0x00005555555cd930 in Functor<void>::method_wrapper<GCS, &GCS::update_receive> (obj=0x555555ae4030 <copter+40240>)
    at ../../libraries/AP_HAL/utility/functor.h:88
#14 0x0000555555684774 in Functor<void>::operator() (this=0x555555aa6980 <Copter::scheduler_tasks+1344>) at ../../libraries/AP_HAL/utility/functor.h:54
#15 0x00005555556e72dd in AP_Scheduler::run (this=0x555555ada458 <copter+344>, time_available=2500) at ../../libraries/AP_Scheduler/AP_Scheduler.cpp:270
#16 0x00005555556e778e in AP_Scheduler::loop (this=0x555555ada458 <copter+344>) at ../../libraries/AP_Scheduler/AP_Scheduler.cpp:399
#17 0x00005555556ed841 in AP_Vehicle::loop (this=0x555555ada300 <copter>) at ../../libraries/AP_Vehicle/AP_Vehicle.cpp:559
#18 0x000055555583d4b8 in HAL_SITL::run (this=0x555555af6860 <hal_sitl_inst>, argc=12, argv=0x7fffffffdb68, callbacks=0x555555ada300 <copter>)
    at ../../libraries/AP_HAL_SITL/HAL_SITL_Class.cpp:298
#19 0x00005555555cce9f in main (argc=12, argv=0x7fffffffdb68) at ../../ArduCopter/Copter.cpp:997
```",[],0
ArduPilot/ardupilot,SIGFPE via incorrect type conversion on mission file upload,"## Bug report

**Issue details**

A **SIGFFPE** signal is raised in the function **_mavlink_int_to_mission_cmd_** ([AP_Mission.cpp](https://github.com/ArduPilot/ardupilot/blob/master/libraries/AP_Mission/AP_Mission.cpp))  during an attempted upload of a crafted mission file with invalid values of parameter **_param1_**.
Floating point exception occurs due to unchecked float-to-integer type conversion, leading to immediate autopilot process termination and loss of Ground Control Station (GCS)   â€“ vehicle communication.

**Steps to reproduce**

Vulnerability is triggered by uploading mission file with **_param1_** set to out-of-range value. 
That leads to failure during unchecked _**float**_ (32 bit) to **_uint_16_t_** (16 bit) conversion in _**mavlink_int_to_mission_cmd**_ ([AP_Mission.cpp](https://github.com/ArduPilot/ardupilot/blob/master/libraries/AP_Mission/AP_Mission.cpp))  function. Below demonstration uses a sample crafted mission file following plain-text format, with param1 set to **_1e+10_** (10 billion), that was uploaded to ArduCopter using MAVProxy _**wp load**_ method. 

```
QGC WPL 110
0	0	0	0	1e+10	0	0	0	0	0	0	0
```
To reproduce the vulnerability for ArduCopter in SITL environment, follow below steps:

1. Run SITL simulation for ArduCopter in debug mode
    `$ Tools/autotest/sim_vehicle.py -v ArduCopter --console --map -w -D -G `
2. Wait for vehicle to initialize into PreArm state
     `pre-arm good`
3. Upload mission file to the vehicle using MAVProxy GCS console
    `> wp load mission.txt`
4. Observe GCS connection and vehicle process termination
```
Sent all 1 waypoints
no link
link 1 down
```

```
Thread 1 ""arducopter"" received signal SIGFPE, Arithmetic exception.
AP_Mission::mavlink_int_to_mission_cmd (packet=..., cmd=...)
    at ../../libraries/AP_Mission/AP_Mission.cpp:1103
1103            cmd.p1 = packet.param1;
(gdb) print cmd.p1
$1 = 0
(gdb) print packet.param1
$2 = 1e+10
```


**Root cause**

Backtrace of the error shows that insecure type conversion occurs within the **_mavlink_int_to_mission_cmd_**   ([AP_Mission.cpp](https://github.com/ArduPilot/ardupilot/blob/master/libraries/AP_Mission/AP_Mission.cpp)) function, which converts MAVLink message into a **_Mission_Command_** object to be stored in EEPROM vehicle memory. SIGFPE signal is raised at line 1103 due to an arithmetic exception caused by an unsafe assignment between mismatched types with vastly different ranges.
- **_packet.param1_** is a 32-bit float
- **_cmd.p1_** is a 16-bit unsigned integer

The root cause of this vulnerability is the lack of sufficient bounds checking on **_param1_** prior to type conversion, allowing out-of-range values to trigger a hardware exception. Existing sanitization function **_sanity_check_params_** only performs minimal validation by rejecting infinite or selectively NaN values for parameters 1â€“4 and does not enforce any command-specific numeric ranges or limits, making this protection highly insufficient.


**Impact**
The impact of this vulnerability is severe, affecting both operational reliability and flight safety across all affected ArduPilot vehicles including ArduCopter, ArduRover and ArduSub. When an attacker in the network or unintentional operator uploads improperly constructed mission file, the autopilot software immediately crashes due to unhandled exception.  As a direct result, the connection between the GCS and the vehicle is abruptly lost, so any ongoing automated mission is terminated. For vehicle in the air, this loss of control may lead to dangerous situations including uncontrolled flight, failure to perform failsafe procedures, or even a crash.


**Recommendations**
Enforce strict input validation and bounds checking on all externally supplied mission parameters before they are stored or used in arithmetic operations.


**Version**
_4.6.3_

**Platform**
[  ] All
[  ] AntennaTracker
[X] Copter
[X] Plane
[X] Rover
[X] Submarine


**Logs**

Backtrace of the error:
```
(gdb) bt
#0  AP_Mission::mavlink_int_to_mission_cmd (packet=..., cmd=...)
    at ../../libraries/AP_Mission/AP_Mission.cpp:1103
#1  0x000055555595c58b in MissionItemProtocol_Waypoints::replace_item (
    this=0x555555b791e0, mission_item_int=...)
    at ../../libraries/GCS_MAVLink/MissionItemProtocol_Waypoints.cpp:106
#2  0x000055555595ad8b in MissionItemProtocol::handle_mission_item (
    this=0x555555b791e0, msg=..., cmd=...)
    at ../../libraries/GCS_MAVLink/MissionItemProtocol.cpp:291
#3  0x000055555570e1a3 in GCS_MAVLINK::handle_mission_item (
    this=0x555555b84ea0, msg=...)
    at ../../libraries/GCS_MAVLink/GCS_Common.cpp:1028
#4  0x0000555555715f2a in GCS_MAVLINK::handle_common_mission_message (
    this=0x555555b84ea0, msg=...)
    at ../../libraries/GCS_MAVLink/GCS_Common.cpp:4694
#5  0x0000555555715a60 in GCS_MAVLINK::handle_message (this=0x555555b84ea0, 
    msg=...) at ../../libraries/GCS_MAVLink/GCS_Common.cpp:4425
#6  0x00005555555d224e in GCS_MAVLINK_Copter::handle_message (
    this=0x555555b84ea0, msg=...)
    at ../../ArduCopter/GCS_MAVLink_Copter.cpp:1250
#7  0x000055555570f96b in GCS_MAVLINK::raw_packetReceived (
    this=0x555555b84ea0, framing_status=1 '\001', status=..., msg=...)
    at ../../libraries/GCS_MAVLink/GCS_Common.cpp:1924
#8  0x000055555570fbcb in GCS_MAVLINK::update_receive (this=0x555555b84ea0, 
    max_time_us=1000) at ../../libraries/GCS_MAVLink/GCS_Common.cpp:1976
#9  0x0000555555711aaa in GCS::update_receive (
    this=0x555555ae4030 <copter+40240>)
    at ../../libraries/GCS_MAVLink/GCS_Common.cpp:2797
#10 0x00005555555cd930 in Functor<void>::method_wrapper<GCS, &GCS::update_receive> (obj=0x555555ae4030 <copter+40240>)
    at ../../libraries/AP_HAL/utility/functor.h:88
#11 0x0000555555684774 in Functor<void>::operator() (
    this=0x555555aa6980 <Copter::scheduler_tasks+1344>)
    at ../../libraries/AP_HAL/utility/functor.h:54
#12 0x00005555556e72dd in AP_Scheduler::run (this=0x555555ada458 <copter+344>, 
    time_available=2500) at ../../libraries/AP_Scheduler/AP_Scheduler.cpp:270
#13 0x00005555556e778e in AP_Scheduler::loop (this=0x555555ada458 <copter+344>)
    at ../../libraries/AP_Scheduler/AP_Scheduler.cpp:399
#14 0x00005555556ed841 in AP_Vehicle::loop (this=0x555555ada300 <copter>)
    at ../../libraries/AP_Vehicle/AP_Vehicle.cpp:559
#15 0x000055555583d4b8 in HAL_SITL::run (this=0x555555af6860 <hal_sitl_inst>, 
    argc=12, argv=0x7fffffffdb68, callbacks=0x555555ada300 <copter>)
    at ../../libraries/AP_HAL_SITL/HAL_SITL_Class.cpp:298
#16 0x00005555555cce9f in main (argc=12, argv=0x7fffffffdb68)
    at ../../ArduCopter/Copter.cpp:997
```",[],0
ArduPilot/ardupilot,Provide a method to get a binary log of all serial data,"## Feature request

**Is your feature request related to a problem? Please describe.**

I have a serial port, currently configured for a mavlink connection. The autopilot is not accessible when the vehicle is assembled. I want to have an easy way to decode traffic on the port. If it was, I'd use a Saleae logic analyzer, decode as async serial, and then run use pymavlink to decode the stream.

I cant use the mavlink inspector in mission planner because this is an internal connection in the vehicle, and some packets on this mavlink interface are not also repeated on the GCS link.

**Describe the solution you'd like**

Provide an option to enable logging for the serial port, which will log all data to a binary file on the SD card.
I can then use mavftp to transfer this to my computer and interpret the contents.

**Describe alternatives you've considered**

Turning on LOG_REPLAY does not log serial port traffic.

**Platform**
[  ] All
[  ] AntennaTracker
[  ] Copter
[  ] Plane
[  ] Rover
[  ] Submarine


",['FeatureRequest'],0
ArduPilot/ardupilot,scripting SRV_Channels:get_output_scaled nonzero values when disarmed,"## Bug report

**Issue details**

When reading SRV_Channels:get_output_scaled it does not update when disarmed. It can produce nonzero values when disarmed.

To get this issue:
- run the lua script
- arm, manual or something, see values change for SRV_Channels:get_output_scaled(73)
- while the value is nonzero, disarm
- the value will not go to zero or update.


**Version**
master branch as of a few months ago, so no specific version.

**Platform**
[  ] All
[  ] AntennaTracker
[  ] Copter
[  ] Plane
[X] Rover
[  ] Submarine

at least rover, probably all platforms.

**Hardware type**
RPi, HAL Linux

Here's my minimal test
```lua


-- =====================
-- MAIN LOOP
-- =====================
function update()
    local l_sig = SRV_Channels:get_output_scaled(73)
    local r_sig = SRV_Channels:get_output_scaled(74)
    if not arming:is_armed() then
        l_sig = 0
        r_sig = 0
    end

    gcs:send_text(6, ""direct measurement: "" .. SRV_Channels:get_output_scaled(73) .. "" with disarm: "" .. l_sig)
    return update, 1000
end

return update()
```

**Logs**
_Please provide a link to any relevant logs that show the issue_
",[],0
ArduPilot/ardupilot,Drone fails to disarm in BRAKE mode while landed with low throttle (1050us),"## Bug report

**Issue details**
According to the [Brake mode documentation](https://ardupilot.org/copter/docs/brake-mode.html), ""If the vehicle is landed in Brake mode it will immediately disarm."" 
During testing, when the vehicle is on the ground with the throttle held at a low level (above minimum but insufficient for takeoff), switching to Brake mode does not trigger an immediate disarm. Instead, the motors continue to spin at an idle or low speed indefinitely.

Reproduce the steps:
1. mode acro
2. arm throttle
3. rc 3 1050
4. mode brake


**Platform**
[  ] All
[  ] AntennaTracker
[x] Copter
[  ] Plane
[  ] Rover
[  ] Submarine


**Logs**

[log_0_2026-1-8-15-31-14.bin.zip](https://github.com/user-attachments/files/24490502/log_0_2026-1-8-15-31-14.bin.zip)
",['NeedAnswer'],0
ArduPilot/ardupilot,Python 3.9 EOL while search for python.exe if sources cloned in WSL2,"## Bug report

**Issue details**

My initial report was here: https://github.com/ArduPilot/ardupilot_vscode_devenv/issues/42

If ardupilot code was cloned in WSL2, then ""upload firmware to the controller"" task won't work. It generates an error:

```
WSL2 firmware uploads use the host's Windows Python.exe so it has access to the COM ports.

        Windows python.exe not found
        Please download Windows Installer 3.9.x (not 3.10) from https://www.python.org/downloads/
        and make sure to add it to your path during the installation. Once installed, run this
        command in Powershell or Command Prompt to install some packages:
        
        pip.exe install empy==3.3.4 pyserial
```

1) Python 3.9 is deprecated, and no installers are available on the python.org for it https://www.python.org/downloads/release/python-3925/. So maybe that error message should be removed/updated.

2) pyserial works fine in Python 3.12


I have fixed that this way:
`Tools/ardupilotwaf/chibios.py` on line 102 change `where.exe python.exe` to `which python.exe` and on line 106 change `\Python\Python` to `/Python/Python`.



**Version**
4.7.0-dev

**Platform**
N/A

**Airframe type**
n/a

**Hardware type**
n/a

**Logs**
n/a
",['NeedAnswer'],0
ArduPilot/ardupilot,DevEBox and WeAct Studio boards support request,"
## Feature request

DevEBox  stm32H743 and stm32H750 with 25mhz  crystal frequency

Issues is currently having difficulty building from the latest Ardupilot firmware not completing with memory errors
apart from which if it does build it could not connect 

WeAct Studio stm32H743  with 25mhz crystal frequency

Id need help for this board as well into the supported List
nice to add dual imu feature

**Describe alternatives you've considered**
_A clear and concise description of any alternative solutions or features you've considered._

**Platform**
[v  ] All
[  v] AntennaTracker
[ v ] Copter
[ v ] Plane
[ v ] Rover
[v  ] Submarine

**Additional context**

these are development board nice to have for users who wish to experiments and would be helpful with a sample  hwdef in the list as a guide as a working flight controller 
",[],0
ArduPilot/ardupilot,FPE in SITL,"## Bug report

Getting the following FPE in SITL quite regularly:

<img width=""1573"" height=""1382"" alt=""Image"" src=""https://github.com/user-attachments/assets/5115fd11-2981-471b-a465-760618e1d446"" />

Apologies for the screenshot, can't seem to cut and paste

**Issue details**

Crash in SITL

**Version**
master

**Platform**
[  ] All
[  ] AntennaTracker
[X] Copter
[  ] Plane
[  ] Rover
[  ] Submarine

",[],0
ArduPilot/ardupilot,Unexpected Disarm in ACRO mode when Throttle at zero during GCS/RC Failsafe while airborne,"## Bug report

**Issue details**

In ACRO mode, if the vehicle is airborne and the throttle is at the minimum position, triggering an GCS/RC Failsafe causes the copter to immediately disarm in mid-air.

While the ""Disarm on low throttle"" logic is expected on the ground, it should not be triggered when the flight controller detects the vehicle is still in flight (Airborne), especially during a failsafe event where a controlled RTL or Land is expected.

See line 472:
https://github.com/ArduPilot/ardupilot/blob/cef6eb780255cc398657a97aa4ef13e438296490/ArduCopter/events.cpp#L463-L482

Steps to Reproduce:

1. Take off and switch to ACRO mode.
2. Lower the throttle stick to the minimum position.
3. param set FS_THR_VALUE -1 (trigger an RC Failsafe)/param set FS_GCS_TIMEOUT 0 (trigger a GCS Failsafe).
4. The copter disarms immediately and falls.
5. Expected Result: The copter should trigger the configured Failsafe action (e.g., RTL or Land) regardless of the throttle position, as long as it is airborne.

**Version**
Copter 4.4 -> main

**Platform**
[  ] All
[  ] AntennaTracker
[x] Copter
[  ] Plane
[  ] Rover
[  ] Submarine

**Airframe type**
sitl

**Logs**
RC Failsafe:
[log_0_2025-12-27-12-45-46.zip](https://github.com/user-attachments/files/24352570/log_0_2025-12-27-12-45-46.zip)

GCS Failsafe:
[log_0_2026-1-7-16-26-53.bin.zip](https://github.com/user-attachments/files/24468172/log_0_2026-1-7-16-26-53.bin.zip)
",[],0
ArduPilot/ardupilot,Buffer overflow in ESP32 WiFi drivers and XPlane SITL connector,"
## Bug report

I found a buffer overflow vulnerability in ArduPilot:

1. ESP32 WiFi drivers:
   - In libraries/AP_HAL_ESP32/WiFiDriver.cpp and WiFiUdpDriver.cpp, strcpy is used to copy SSID and password into fixed-size buffers.
2. SITL XPlane connector:
   - In libraries/SITL/SIM_XPlane.cpp, strcpy is used to copy dataref names into fixed-size buffers.

Using strings longer than the buffer size can cause stack-buffer-overflow and program crash. 
I verified this using a minimal test case compiled with AddressSanitizer.


**Version**
Branch: master
Commit: ed2c34310cd7771bf920ee855a3d22dab72f3f37

**Platform**
[X] All
[X] Copter
[X] Plane
[X] Rover
[X] Submarine


**Airframe type**
Not applicable (HAL/SITL issue)

**Hardware type**
- ESP32 (WiFi drivers)
- SITL (XPlane connector)


**Logs**

https://github.com/user-attachments/assets/880f2e3f-6190-452b-8c0e-fc96087965c8
",[],0
ArduPilot/ardupilot,[SITL + ardupilot_gazebo] Iris with 3-DOF gimbal: gimbal jumps to an unexpected pose during SITL startup initialization,"## Bug report

**Issue details**
**Steps to reproduce**

(1)Start the Gazebo world and model (the gimbal is initially aligned with the vehicle heading in Gazebo):
gz sim -v4 -r ~/ardupilot/ardupilot_gazebo/worlds/iris_runway.sdf
(2)Start SITL and load the gimbal parameter file:
./Tools/autotest/sim_vehicle.py -D -v ArduCopter -f JSON \
  --add-param-file=$HOME/work/ardupilot/ardupilot_gazebo/config/gazebo-iris-gimbal.parm \
  --console --map

**Expected behavior**

After starting SITL, the gimbal should not suddenly jump to an extreme pose during initialization. At minimum, it should remain aligned with the vehicle heading, or first stabilize to a predictable neutral/retract pose. If RC Targeting is the default mode, a more reasonable initial RC input is expected.

<img width=""261"" height=""175"" alt=""Image"" src=""https://github.com/user-attachments/assets/4d239d0f-26f0-48b4-af37-4c872d763676"" />

**Actual behavior**

After starting SITL, the gimbal in Gazebo quickly twists to a strange orientation, and the initial pose aligned with the vehicle heading is broken.
Early RC inputs captured from RC_CHANNELS are:
chan6_raw=1000
chan7_raw=1000
chan8_raw=1800
Meanwhile, the parameter file configures the calibration range for RC6/7/8 as RCx_MIN=1100, RCx_MAX=1900, so the raw values at startup are below the configured lower bound (at least CH6/7: 1000 < 1100).
Early gimbal attitude captured from GIMBAL_DEVICE_ATTITUDE_STATUS:
q = [0.3921, 0.7231, -0.5320, 0.2010] (w, x, y, z)
The Euler angles converted from this quaternion show a large-angle combination.

<img width=""238"" height=""177"" alt=""Image"" src=""https://github.com/user-attachments/assets/f6d60834-a67b-451a-84e2-288e35f24781"" />

**Version**

ArduPilot: SITL (ArduCopter), started via sim_vehicle.py
Simulation: Gazebo (gz sim) + ardupilot_gazebo model (iris + gimbal)
OS: Ubuntu 22.04
ArduPilot: ArduPilot-4.6.0-beta1-5055-gebe4fc2074
ardupilot_gazebo commit: git rev-parse HEAD
Gazebo / gz-sim version: Gazebo Sim, version 8.10.0

**Platform**
[  ] All
[  ] AntennaTracker
[  ] Copter
[  ] Plane
[  ] Rover
[  ] Submarine

**Airframe type**

**Hardware type**

**Logs**

",[],0
ArduPilot/ardupilot,ESC to RPM sensor processing,"## Feature request

**Is your feature request related to a problem? Please describe.**

* AM32 ESCs sometimes report RPM values that are significantly out of range (50k+ RPM on a motor that spins at 4k RPM). Thist mostly happens during spool up causing heli governor using ESC RPM telemetry to fail to engage)
* BDShot RPM telemetry (on AM32 ESCs) is jittery Â±10RPM jitter at 4k RPM which probably messes with the Ardupilot governor, causing throttle jitter.

**Describe the solution you'd like**
Adding lowpass filter to RPM library and respecting RPMx_MAX when taking data from ESC telemetry would alleviate these issues. At the cost of slightly decreased governor tracking accuracy

**Describe alternatives you've considered**

* Checking that the governor is engaged prior to takeoff
* Kickstarting the motor with the interlock to avoid the issue
* Cycling the interlock to re-engage the governor 
* Using throttle curve (not feasible due to large voltage range and apparent lack of voltage compensation)


**Platform**
[  ] All
[  ] AntennaTracker
[ X ] Copter (heli)
[  ] Plane
[  ] Rover
[  ] Submarine

**Additional context**
Will add screenshots soon.
",[],0
ArduPilot/ardupilot,Standard Modes Support problems,"With the new Standard Modes support in ArduCopter on master QGC now shows those modes in it's ui. There are a number of problems from a usability standpoint with this new functionality:
* The flight mode names are in ALL CAPS. And include things like ADSB_AVOID. From an end user perspective all caps and using underscores in flight mode names is quite jarring in the QGC UI. It's not technically a problem, just not very user friendly from a good user interface standpoint. It's also pretty jarring for existing QGC+ArduPilot users coming from older firmwares where QGC was in control of flight mode names and used Mixed Case everywhere with no underscores.
* Also the list of flight modes is massively long. I don't know if some of these flight modes returned fall into the category of not-selectable by a user. If so, they should be marked as such in the protocol. Also would be nice if there was use of the ""advanced"" setting in the protocol. Then QGC could filter the list so that it wasn't so formidable at first glance.",[],0
ArduPilot/ardupilot,Add Units to PID parameters,By adding units to PID parameters we can help analysis and understanding of the controllers.,"['Copter', 'FeatureRequest']",0
ArduPilot/ardupilot,Ensure Parameter metadata @Range and @Increment display correctly in Mission Planner,Mission planner uses the @Increment value to define the number of zero's displayed. This means that Mission planner may not display the best resolution if the @Increment has been set based on a larger tuning step.,"['Copter', 'FeatureRequest']",0
ArduPilot/ardupilot,Support MAVLink COMPONENT_METADATA,"## Feature request

**Is your feature request related to a problem? Please describe.**
QGC folks, in particular @DonLakeFlyer have to manually update the build each new AP version for the link to the new parameters.

**Describe the solution you'd like**
If we respond to COMPONENT_METADATA then we can give QGC the correct URL for the version, and eventually embed the data on the board with enough space.

**Describe alternatives you've considered**
We could update QGC to calculate the URL itself, but that would be additional complication.

**Platform**
[X] All
[  ] AntennaTracker
[  ] Copter
[  ] Plane
[  ] Rover
[  ] Submarine

**Additional context**

Needed for QGC to update to ARMING_SKIPCHK: https://github.com/ArduPilot/ardupilot/pull/31568

Or some other method of putting 4.7 dev metadata into https://github.com/arduPilot/parameterrepository
",[],0
ArduPilot/ardupilot,Sub: 4.5 forward-port list,"This is a list of anything in an ArduSub release branch that needs equivalent behaviour implemented in master. Some PRs/commits may be able to be directly forward ported, while others will require additional considerations and discussion for their relevance and implementation with respect to the other ArduPilot firmware variants. If new missing items/commits are identified they will be added.

## Parameter migration / bad configuration detection
- [x] Automatic transition from old `BRD_PWM_COUNT` GPIO determination functionality, to new `GPIO (-1)` `SERVOx_FUNCTION` values and/or using the `SERVO_GPIO_MASK`
    - #31685
    - #31702
    - Without this the leak sensor may be silently broken, which is a critical warning system in Sub vehicles
    - It also affects relays, which could benefit from some user warnings if the autopilot can tell they're misconfigured
- [ ] Automatic transition from `RCIN9 (59)`/`RCIN10 (60)` channel passthrough piggy-backing to new dedicated `Lights1 (181)`/`Lights2 (182)` `SERVOx_FUNCTION` outputs
    - #31109
    - This parameter change also broke programmatic control of lights outputs, which has not yet been resolved
        - The illuminator protocol [is being considered](https://github.com/ArduPilot/ardupilot/issues/28814#issuecomment-3617607624) as a replacement for the old [`DO_SET_SERVO`](https://mavlink.io/en/messages/common.html#MAV_CMD_DO_SET_SERVO) commands (which required knowing the channel number of the output), although it is a fair amount more complex
- [ ] Automatic transition from old `Disabled (0)` to new `Actuator1 (184)`/`Actuator2 (185)`/`Actuator3 (186)` `SERVOx_FUNCTION` outputs
    - #31109 
    - This parameter change also broke programmatic control of actuator outputs, which has not yet been resolved
        - [`DO_SET_ACTUATOR`](https://mavlink.io/en/messages/common.html#MAV_CMD_DO_SET_ACTUATOR) MAVLink commands [are planned](https://github.com/ArduPilot/ardupilot/issues/28814) as a replacement for the old [`DO_SET_SERVO`](https://mavlink.io/en/messages/common.html#MAV_CMD_DO_SET_SERVO) commands (which required knowing the channel number of the output)

## Release notes
- [ ] Update master to include [the release notes](https://github.com/ArduPilot/ardupilot/blob/Sub-4.5/ArduSub/ReleaseNotes.txt) from the various 4.5 versions that have been released
    - Some of these are related to the features that need forward-porting, so it likely doesn't make sense to bring them through until those features are also available in master (and there'll likely be one for 4.5.7 with the leak/relay details)","['Sub', 'ReleaseAdmin']",0
ArduPilot/ardupilot,Copter SITL Webots-Python: iris.parm incompatible with current pre-arm validation,"## Bug report

**Issue details**

Please take these findings with a grain of salt.

When using ArduCopter SITL with the Webots `webots-python` model and the provided `iris.parm`, I am seeing inconsistent and sometimes blocking pre-arm behavior on newer commits. The same workflow and Webots world arm and fly reliably when rolling back to the previous commit.

On the newer commit, arming is frequently blocked by pre-arm checks and parameter validation errors, such as:

* `Bad parameter: ATC_RAT_RLL_I must be > 0`
* `PreArm: Check ACRO_BAL_ROLL/PITCH`
* `Arm: Accels inconsistent`

Arming behavior is inconsistent between runs. Sometimes the vehicle arms, sometimes it does not, even with the same setup. Once armed, the vehicle flies normally in Webots.

Rolling back to the immediately previous commit resolves all issues without any manual parameter changes, which suggests a regression or a parameter compatibility issue rather than a Webots model problem.

**Version**

ArduCopter V4.7.0-dev
Issue observed on the latest commit at time of testing.
Rolling back to the previous commit restores reliable arming behavior.

**Platform**

[  ] All
[  ] AntennaTracker
[x] Copter
[  ] Plane
[  ] Rover
[  ] Submarine

**Airframe type**

Quad / X (Webots Iris)

**Hardware type**

SITL using Webots (`webots-python` model)

**Logs**

Console and QGroundControl messages observed on the affected commit include:

* `Bad parameter: ATC_RAT_RLL_I must be > 0`
* `PreArm: Check ACRO_BAL_ROLL/PITCH`
* `Arm: Accels inconsistent`

No crashes observed. Once armed, flight behavior appears normal.
",[],0
ArduPilot/ardupilot,"Copter: Dijkastras may get stuck if ""Allow fast waypoints"" options is enabled and WPNAV_RADIUS > 2 and","If users set the OA_OPTIONS bit ""Allow fast waypoints"" and set WPNAV_RADIUS > 2 the vehicle may get stuck as it flies a path provided by Dijkstras (e.g. a path around a fence)

The core problem is that [OA_PathPlanner](https://github.com/ArduPilot/ardupilot/blob/master/libraries/AC_Avoidance/AP_OAPathPlanner.h) is responsible for returning the next two waypoints that the vehicle should fly to but it doesn't know when AC_WPNav has actually completed these waypoints.  [The code here](https://github.com/ArduPilot/ardupilot/blob/master/libraries/AC_Avoidance/AP_OADijkstra.cpp#L224) tries to figure this out using the vehicle's current location but it assumes the vehicle will come within 2m of the point which it may actually not if ""allow fast waypoints"" is enabled.

The problematic [line is here](https://github.com/ArduPilot/ardupilot/blob/master/libraries/AC_Avoidance/AP_OADijkstra.cpp#L224) and copied below

`const bool near_oa_wp = current_loc.get_distance(destination_new) <= 2.0f;`

Here's an example mission and fence that exhibits the problem.  When the vehicle RTLs at the end
<img width=""813"" height=""688"" alt=""Image"" src=""https://github.com/user-attachments/assets/22935e39-b5e8-4a8d-968b-dd5edb6bd7db"" />

The proper solution is not as simple as just passing in the WPNAV_RADIUS to Dijkstras though because while this parameter sets an **upper limit** on how far the vehicle may fly from the waypoint, the vehicle normally flies closer than this.  In fact a user could set the WPNAV_RADIUS = 100m and still the vehicle would fly quite close to the waypoints.  In this case if Dijkstras used 100m it would consider intermediate points complete far too early and would likely cut corners which could lead to breaching the fence.

I think the proper solution is to improve the interface between [AP_OAPathPlanner::mission_avoidance()](https://github.com/ArduPilot/ardupilot/blob/master/libraries/AC_Avoidance/AP_OAPathPlanner.cpp#L196) and [AP_OADijkstra::update()](https://github.com/ArduPilot/ardupilot/blob/master/libraries/AC_Avoidance/AP_OADijkstra.cpp#L50) (which is currently passing back and forth [avoidance_info stuctures](https://github.com/ArduPilot/ardupilot/blob/master/libraries/AC_Avoidance/AP_OAPathPlanner.h#L98)) so that Dijkstras provides not just the next two intermediate points but instead the entire path around the obstacle.

Then [AC_WPNav_OA](https://github.com/ArduPilot/ardupilot/blob/master/libraries/AC_WPNav/AC_WPNav_OA.h) should hold onto this path and take responsibility for feeding the next two points into the [AC_WPNav base class](https://github.com/ArduPilot/ardupilot/blob/master/libraries/AC_WPNav/AC_WPNav.h).  AC_WPNav_OA can do this better than Dijkstras because it can keep track of where the vehicle is along the path by calling the base classes's [reached_wp_destination()](https://github.com/ArduPilot/ardupilot/blob/master/libraries/AC_WPNav/AC_WPNav.h#L240C18-L240C40) method

BTW, I've done some work in [this PR to restructure Dijkstras](https://github.com/ArduPilot/ardupilot/pull/28542).  We should consider merging this restructure to master and converting AC_Avoidance to always use meters before trying to resolve this issue.","['BUG', 'Safety', 'Library']",0
ArduPilot/ardupilot,AC_PrecLand LOCAL_FRD support yaw rotation sign error,"## Bug report

**Issue details**

It appears that there is a sign error on the recently introduced support for LOCAL_FRD MAV_FRAME in the AC_PrecLand module. From my perspective it seems that ""_inertial_data_delayed->Tbn"" describes the rotation quaternion from the Body Frame to the NED frame. To rotate BODY_FRD (the previous, well tested MAV_FRAME used in AC_PrecLand) we rotate the FRD unit vector by the _intertial_data_delayed->Tbn quaternion to get NED Frame.

When doing this for LOCAL_FRD (which is yaw only, gravity aligned so no roll/pitch) we extract the Euler rotations from the BODY->NED quaternion and rotate by the Yaw only. This is correct, except that we are using rotate_xy( **-** yaw) which I believe to be incorrect.

I have flight-tested and proved this issue by changing my precision landing sensor from BODY_FRD (working) to LOCAL_FRD. Changing to this MAV_FRAME within the LANDING_TARGET message causes divergent behaviour any time the Copter is not aligned with North. If I remove the negative sign from the line below, I get comparable behaviour with BODY_FRD which is expected, as these are both body-yaw aligned frames.

Line in question:
https://github.com/ArduPilot/ardupilot/blob/master/libraries/AC_PrecLand/AC_PrecLand.cpp#L674

The functionality for LOCAL_FRD support was added in this PR: https://github.com/ArduPilot/ardupilot/pull/30446

I believe there also must be an issue in the Simulation environment somewhere as this issue was clearly not picked up during the pull request completion.

**Version**
V4.7.0-dev | any version past the above linked PR.

**Platform**
Observed on Copter, assumed present on all.
[ X ] All
[  ] AntennaTracker
[  ] Copter
[  ] Plane
[  ] Rover
[  ] Submarine

**Airframe type**
X4 Quad - Hexsoon 450 EDU.

**Hardware type**
Cube Orange Plus

**Logs**
N/A currently.
",[],0
ArduPilot/ardupilot,sanity check for MAV_CMD_NAV_RETURN_TO_LAUNCH in AP_Mission,"## Feature request

Add sanity check case for the `MAV_CMD_NAV_RETURN_TO_LAUNCH` command.

**Is your feature request related to a problem? Please describe.**
Sanity check of a `MAV_CMD_NAV_RETURN_TO_LAUNCH`  command parameters sent from `mavsdk` produces an `INVALID_PARAM1`.

https://github.com/ArduPilot/ardupilot/blame/a3f5193c751452d03a3d8783c67c57d985db639f/libraries/AP_Mission/AP_Mission.cpp#L1013

**Describe the solution you'd like**
I'm not sure which implementation of the standard is correct; however, if the [parameteres in the structure are supposed to be empty](https://mavlink.io/en/messages/common.html#MAV_CMD_NAV_RETURN_TO_LAUNCH) then is it necessary to check the parameters are not NaN?

I think adding a case in 

https://github.com/ArduPilot/ardupilot/blob/c40bd1e8d5e1f0ba692d7069d83264a575d402d3/libraries/AP_Mission/AP_Mission.cpp#L1016-L1041

for MAV_CMD_NAV_RETURN_TO_LAUNCH and assigning `nan_mask = 0xf0` would be an ok compromise?

**Describe alternatives you've considered**
I've tried from the other side, i.e. MAVSDK, but the values are hard-coded in the implementation.

https://github.com/mavlink/MAVSDK/blob/de36b0d7d21c2db85f7daf7423f32840b4b31a57/src/mavsdk/plugins/mission/mission_impl.cpp#L612-L630

**Platform**
[ x ] All
[  ] AntennaTracker
[  ] Copter
[  ] Plane
[  ] Rover
[  ] Submarine

",[],0
ArduPilot/ardupilot,ALT_OFFSET error when commanding waypoint/flight mode,"## Bug report

**Issue details**

In summary, in ArduPlane, if you have the parameter ALT_OFFSET set (to say climb 300 meters), when you go to command the aircraft to a waypoint, TECS will somehow double the ALT_OFFSET leading to risk of an altitude excursion. Was able to replicate in SITL. I don't know if this is due to misuse of ALT_OFFSET, or if it's a bug with how it's interpretted

Additional information: When in an autonomous mission using ALT_OFFSET to change a mission altitude, vehicle climbs to target altitude without issue. However, when commanding the vehicle to a waypoint, or a flight mode change (circle/loiter), vehicle will enter a target altitude double ALT_OFFSET until vehicle reaches said waypoint, and then returns to the ALT_OFFSET. Risks causing altitude limit breach unless safeguard system used. Further testing will be done to determine if geofence can trip to RTL and return vehicle to ALT_OFFSET target altitude, or if commanding RTL will return vehicle to ALT_OFFSET target altitude. 

Video of SITL preliminary demo: https://www.youtube.com/watch?v=IEYJDMH2tpw

Can hop on a dev call to discuss further
 

**Version**
4.2.3 live vehicle, 4.7.0 MissionPlanner SITL Plane, presumed affecting all fixed-wing (and rotary wing?) from 4.2.X to latest 

**Platform**
[  ] All
[  ] AntennaTracker
[ ?] Copter
[ X] Plane
[  ] Rover
[  ] Submarine

**Airframe type**
Conventional Pusher Fixed-Wing, SITL Plane vehicle 

**Hardware type**
CubeOrange, MissionPlanner SITL on Windows 11 

**Logs**
Edit: log post attempt: 

<!-- Failed to upload ""00000025.log"" -->

","['BUG', 'Plane']",0
ArduPilot/ardupilot,"AP_TemperatureSensor_MLX90614: only reading ambient temp, not object temp - no indication in Wiki","While using the MLX90614 Sensor i noticed the same issue that @peterbarker noted in his merging comment: a bit low, and very long time constants. Only after holding hot soldering iron directly in front of the sensor, but the readings only changing slightly I figured there must be something seriously wrong. 

so i did some digging and sure enough, the driver only reads the internal (ambient) temp of the sensor, not the temp of the object https://github.com/ArduPilot/ardupilot/blob/a8b57dceadca306aabb3fcc351e3be4a411e5f49/libraries/AP_TemperatureSensor/AP_TemperatureSensor_MLX90614.cpp#L42C45-L42C56

Which is fine, but not what i would expect when using an infrared temp sensor. Also this is not noted in the wiki or elsewhere. 

My suggestion would be to add a config param to switch between the ambient temp and object temp - or to just use the object temp as default.

What are your thoughts? I can make a PR after some discussion.


> I've tested this rebased against master (https://github.com/ArduPilot/ardupilot/pull/28143) and it seems to work as expected.
> 
> Maybe reading a bit low?
> 
> Very long time constants?
> 
> Merging, thanks!
>_Originally posted by @peterbarker in https://github.com/ArduPilot/ardupilot/pull/26886#pullrequestreview-2312088218_
            ",['BUG'],0
ArduPilot/ardupilot,Feature request: Add Linux Mint â€œzaraâ€ codename mapping to install-prereqs.sh,"## **Is your feature request related to a problem? Please describe.**

The prerequisites installation script (`install-prereqs-ubuntu.sh`) does not currently recognize the Linux Mint codename **â€œzaraâ€**, which corresponds to Linux Mint 22.2. When running the script on Mint â€œzaraâ€, it fails during release detection with:

```
Unable to map zara to an Ubuntu release.
```

Because the script cannot translate the Mint codename to the correct Ubuntu base, installation halts prematurely.

## **Describe the solution you'd like**

Add a translation entry for Linux Mint **â€œzaraâ€ â†’ Ubuntu â€œnobleâ€ (24.04)** within the existing Mint-to-Ubuntu mapping block:

```
linuxmint)
    case ${RELEASE_CODENAME} in
        wilma | xia | zara)
            RELEASE_CODENAME='noble'
            ;;
```

This ensures the script correctly detects the system and proceeds with the expected package installation.

## **Describe alternatives you've considered**

* Editing the script locally (works, but not ideal for other Mint users).
* Allowing users to manually override the Ubuntu codename (would require additional script logic).
* Failing early with a more descriptive message (does not solve the underlying mapping problem).

Adding the proper Mint â†’ Ubuntu mapping is consistent with existing behavior and previous merged PRs.

## **Platform**

[ x ] All
[  ] AntennaTracker
[  ] Copter
[  ] Plane
[  ] Rover
[  ] Submarine

## **Additional context**

Linux Mint â€œzaraâ€ (22.2) is based on Ubuntu 24.04 (â€œnobleâ€), as documented on the Mint website:
[https://www.linuxmint.com/download_all.php](https://www.linuxmint.com/download_all.php)

I have tested the fix locally and confirmed that installation completes successfully after adding the mapping. A pull request with the change is ready to be submitted.
",[],0
ArduPilot/ardupilot,AP_SCRIPTING: mavlink:send_chan() returns false intermittently,"## Bug report

**Issue details**

Hi, I'm trying to get send a command_long to another component (winch) on my aircraft. My ""winch"" is a nucleo32 that is listening for commands addressed to it. I have a lua script set up so that upon flipping a switch on my RC the script will see the change and send a command to the winch.
The problem I am having is that the mavlink:send_chan() function doesn't always send the message (and returns false). My nucleo can see every gcs:send_text() and can see that the lua script just doesn't send the command.

I have currently implemented a small workaround that just continually calls the send_chan function with the same parameters until it returns true (or reaches a stopping point) (And this still fails sometimes and no command is sent).  I'm pretty new to coding using the ardupilot scripting and I assume this isn't normal behavior for send_chan().  

**Version**
V4.6.2

**Platform**
[  ] All
[  ] AntennaTracker
[  ] Copter
[ x] Plane
[  ] Rover
[  ] Submarine

**Airframe type**
Fixed-Wing

**Hardware type**
CubeOrangePlus

**Logs**
Here is my function responsible for sending the command:
```function send_command(target_system, target_component, cmd, ...)
    -- Unpack parameters
    local params = {}
    local vargs = {...}
    for _, i in ipairs(vargs) do
        table.insert(params, i)
    end

    local msgid, message = mavlink_msgs.encode(""COMMAND_LONG"", {
    param1 = params[1] or 0,
    param2 = params[2] or 0,
    param3 = params[3] or 0,
    param4 = params[4] or 0,
    param5 = params[5] or 0,
    param6 = params[6] or 0,
    param7 = params[7] or 0,
    command = cmd,        -- MAVLink command ID
    target_system = target_system,        -- Target system
    target_component = target_component,     -- Target component
    confirmation = 0         -- 0 for no confirmation
    })

    -- Send the message
    gcs:send_text(MAV_SEVERITY_INFO, ""CommandL msg: "" .. tostring(message))
    if not message then
        gcs:send_text(MAV_SEVERITY_ERROR, ""Failed to encode COMMAND_LONG message."")
        return
    end
    gcs:send_text(MAV_SEVERITY_INFO, string.format(""Sending command on channel %d (msgid=%d)"", MAVLINK_CHANNEL, COMMAND_LONG_ID))
    local ok = false
    local attempt = 0
    while not ok do
        ok = mavlink:send_chan(MAVLINK_CHANNEL, COMMAND_LONG_ID, message) -- Sometimes returns false for whatever reason

        if ok then
            return
        else
            gcs:send_text(MAV_SEVERITY_ERROR, ""Failed to send command, retrying..."")
            attempt = attempt + 1
            if attempt >= 100 then
                gcs:send_text(MAV_SEVERITY_ERROR, string.format(""Failed to send command after %d attempts."", attempt))
                break
            end
        end
    end

    gcs:send_text(MAV_SEVERITY_INFO, ""mavlink:send_chan returned: "" .. tostring(ok))
end
```
My logs I see back through my nucleo via my serial port look like this:
```
15:27:17.330 -> I: Status severity: 6, Text: Sending command to retract winch.
15:27:17.376 -> I: Recieved a status_text msg
15:27:17.376 -> I: Status severity: 6, Text: Sending command on channel 2 (msgid=76)
15:27:22.921 -> I: Recieved a status_text msg
15:27:22.967 -> I: Status severity: 3, Text: Failed to send command, retrying...
...
15:28:06.980 -> I: Status severity: 3, Text: Failed to send command, retrying...
15:28:06.980 -> I: Recieved a status_text msg
15:28:07.012 -> I: Status severity: 3, Text: Failed to send command after 100 attempts.
15:28:07.012 -> I: Recieved a status_text msg
15:28:07.012 -> I: Status severity: 6, Text: mavlink:send_chan returned: false
15:28:09.638 -> I: Recieved a status_text msg
15:28:09.676 -> I: Status severity: 6, Text: Sending command to extend winch.
15:28:09.676 -> I: Recieved a Command Long
15:28:09.676 -> I: CommandLong: trgt sys: 1
15:28:09.676 -> I: CommandLong: trgt comp: 169
15:28:09.676 -> I: Command recieved: 42600
15:28:09.676 -> I: Recieved command to move winch, passing to winch...
15:28:09.676 -> I: Winch action: RATE_CONTROL
15:28:09.796 -> I: Recieved a status_text msg
15:28:09.829 -> I: Status severity: 6, Text: CommandL msg: 
15:28:09.829 -> I: Recieved a status_text msg
15:28:09.863 -> I: Status severity: 6, Text: Sending command on channel 2 (msgid=76)
15:28:09.909 -> I: Recieved a status_text msg
```
",['Scripting'],0
ArduPilot/ardupilot,Support RTL_ALTITUDE when using RTL_AUTOLAND,"## Feature request

We have a fixed-wing plane which has a complicated landing sequence, which forces us to always land in AUTO mode using `RTL_AUTOLAND` parameter. However, this means that `RTL_ALTITUDE` is ignored, since the vehicle always proceeds to the first waypoint after `MAV_CMD_DO_LAND_START`. I also need a way to support ""return at current altitude"", set when parameter is equal to -1.
The landing sequence contains `MAV_CMD_NAV_LOITER_TO_ALT` at 50m AGL which is required for final approach, but the plane glides to it in a straight descending line, potentially crashing into terrain.

*I would like the vehicle to complete the RTL route on altitude set in `RTL_ALTITUDE` parameter, also when landing in AUTO mode, and only descend when close to landing sequence.*

**Workaround**
By adding a `MAV_CMD_NAV_WAYPOINT` after `MAV_CMD_DO_LAND_START` and before `MAV_CMD_NAV_LOITER_TO_ALT`, I can adjust the return altitude on this mission item, and have the plane climb to it immediately by setting bit 14 of `FLIGHT_OPTIONS`. I am including the Lua function below.

The problem is, that **this script needs to reliably detect when RTL mode is triggered** to update the waypoint. When using `RTL_AUTOLAND` the vehicle switches to AUTO immediately, and I might miss it between the calls of the script. When changing the mode from GCS, I can listen to `MAV_CMD_DO_SET_MODE`, but can't detect RTL triggered by another script or RC transmitter. Finally, I can't simply look at the index of current waypoint, because I don't want to climb to RTL altitude when mission proceeds to landing sequence as planned.

**Platform**
[  ] All
[  ] AntennaTracker
[  ] Copter
[x ] Plane
[  ] Rover
[  ] Submarine

**Additional context**
I'd be happy to contribute either a reliable way to detect RTL in Lua when using Automatic landing or some more involved behaviour for RTL, as preferred by the dev team.

I searched the issues for `RTL_AUTOLAND` and `RTL_ALTITUDE` but didn't find anything with both for my use case. There's a thread [Altitude behaviour on RTL using DO_LAND_START with RTL_AUTOLAND=2](https://discuss.ardupilot.org/t/altitude-behaviour-on-rtl-using-do-land-start-with-rtl-autoland-2/83922) which confirms that adding a waypoint is the way to do it.

```lua
-- Update first waypoint of landing sequence to recreate RTL_ALTITUDE behavior in AUTO
-- 
-- This must be called every time the vehicle enters RTL. Because the altitude target is specified
-- as a regular waypoint with large acceptance radius, the vehicle might not reach it if it's already close.
-- Note that if RTL is switched into AUTO and vehicle continues the mission it will fly to RTL altitude,
-- this can be reset to original value by reuploading the mission plan from GCS.
local function update_rtl_altitude()
    local doland_idx = find_do_land_start_index()
    if not doland_idx then
        return
    end

    local rtl_idx = doland_idx + 1
    local rtl_item = mission:get_item(rtl_idx)
    if (not rtl_item) or rtl_item:command() ~= MAV_CMD_NAV_WAYPOINT then
        return
    end

    local rtl_altitude = RTL_ALTITUDE:get()
    if not rtl_altitude then
        return
    end

    if rtl_altitude == -1 then
        local current_rel_alt = -ahrs:get_relative_position_D_home()
        rtl_item:z(current_rel_alt)
    else
        rtl_item:z(rtl_altitude)
    end

    -- Never write altitude smaller than 50 m
    if rtl_item:z() < 50 then
        rtl_item:z(50)
    end
    -- Ensure the altitude is relative to home like the parameter
    rtl_item:frame(MAV_FRAME_GLOBAL_RELATIVE_ALT)

    mission:set_item(rtl_idx, rtl_item)
    gcs:send_text(MAV_SEVERITY.NOTICE, string.format(""Doing RTL at %.0f m (REL)"", rtl_item:z()))

    -- If already flying there, update the navigation target after mission was modified
    if mission:get_current_nav_index() == rtl_idx then
        mission:set_current_cmd(rtl_idx)
    end
end
```

*Contribution on behalf of @FlyfocusUAV*",[],0
ArduPilot/ardupilot,Copter: Ch6 tuning of WP Speed overshoots during the beginning of the first leg,"Copter supports transmitter based tuning ([see wiki here](https://ardupilot.org/copter/docs/common-transmitter-tuning.html)) including the ability to adjust the speed during auto missions.  Unfortunately it seems this doesn't take effect immediately when auto starts so the vehicle will overspeed in the first segment of the mission.

The issue can be reproduced in SITL with AP-4.7.0-dev (or higher) by doing the following:

1. param set RC6_OPTION 219 (Transmitter Tuning)
2. param set TUNE 10 (WP Speed)
3. param set TUNE_MIN 100 (1m/s)
4. param set TUNE_MAX 1000 (10m/s)
5. param set AUTO_OPTIONS 3 (allow arming and takeoff in Auto mode)
6. create an auto mission with long segments (e.g. a big square)
7. auto
8. arm throttle
9. rc 6 1000

The vehicle should fly at 1m/s but instead it will fly at up to 6m/s for the first segment before slowing to 1m/s

This issue was found as part of the investigation of [this user report](https://discuss.ardupilot.org/t/copter-4-6-3-released/140596/29).","['BUG', 'Copter']",0
ArduPilot/ardupilot,"Copter parameter conversion from cm, cd to m and degrees","ANGLE_MAX
  -> PILOT_ANGLE_MAX
  -> ATC_ANGLE_MAX

ATC_ACCEL_P_MAX
-> ATC_CMD_ACCEL_P

ATC_ACCEL_R_MAX 
-> ATC_CMD_ACCEL_R

ATC_ACCEL_Y_MAX 
-> ATC_CMD_ACCEL_Y

Future PR correction acceleration limits (currently hard-coded to half the command model):
ATC_CORR_A_P_MAX
ATC_CORR_A_R_MAX
ATC_CORR_A_Y_MAX
ATC_CORR_ACCEL_P
ATC_CORR_ACCEL_R
ATC_CORR_ACCEL_Y

ATC_SLEW_YAW
  -> WP_Y_RATE_MAX
  -> ATC_AUTO_YAW_MAX
  -> ATC_SLEW_YAW_MAX

AVOID_ANGLE_MAX -> AVOID_ANGLE_LIM

BARO_ALTERR_MAX

CIRCLE_RADIUS
  -> CIRCLE_RAD
  -> CIRCLE_RAD_M
  -> CIRCLE_RADIUS_M

LAND_ALT_LOW
  -> LAND_ALT_LOW_M
  -> LAND_ALT_FINAL

LAND_SPEED
  -> LAND_SPEED_M
  -> LAND_SPEED_FINAL
  -> LAND_SPD
  -> LAND_SPD_LOW
  -> LAND_SPD_M
  -> LAND_SPD_FINAL

LAND_SPEED_HIGH
  -> LAND_SPD_HIGH
  -> LAND_SPD_HIGH_M
  -> LAND_SPD_HI

LOIT_ACC_MAX
  -> LOIT_ACC_MAX_M
  -> LOIT_ACC_M
  -> LOIT_ACC

LOIT_BRK_ACCEL
  -> LOIT_BRK_ACCEL_M
  -> LOIT_ACCEL_BRK

LOIT_BRK_JERK
  -> LOIT_BRK_JERK_M
  -> LOIT_JERK_BRK

LOIT_SPEED
  -> LOIT_SPEED_M
  -> LOIT_SPD
  -> LOIT_SPD_M

MOT_YAW_HEADROOM
  -> MOT_YAW_RESERVE
  -> MOT_YAW_RES
  -> MOT_YAW_HEAD
(note: currently PWM-based, needs normalisation)

PHLD_BRAKE_ANGLE -> PHLD_BRK_ANGLE
PHLD_BRAKE_RATE  -> PHLD_BRK_RATE

Possible prefix change: PILOT -> PLT

PILOT_ACCEL_Z
  -> PILOT_ACC_Z
  -> PILOT_ACCEL_Z_M

PILOT_SPEED_DN
  -> PILOT_SPD_DN
  -> PILOT_SPD_DN_M
  -> PILOT_SPEED_DN_M

PILOT_SPEED_UP
  -> PILOT_SPD_UP
  -> PILOT_SPD_UP_M
  -> PILOT_SPEED_UP_M

PILOT_TKOFF_ALT
  -> PILOT_TKOFF_ALTM
  -> PILOT_TKO_ALT_M

PILOT_Y_RATE

RTL_ALT
  -> RTL_ALT_MIN
  -> RTL_ALT_M

RTL_ALT_FINAL
  -> RTL_ALT_STOP
  -> RTL_ALT_HOVER
  -> RTL_ALT_FINAL_M

RTL_CLIMB_MIN
  -> RTL_CLIMB_MIN_M
  -> RTL_CLMB_MIN

RTL_SPEED -> RTL_SPD

WPNAV -> WP

WPNAV_ACCEL
WPNAV_ACCEL_C
WPNAV_ACCEL_Z
WPNAV_RADIUS
WPNAV_SPEED
WPNAV_SPEED_DN
WPNAV_SPEED_UP
","['Copter', 'FeatureRequest']",0
ArduPilot/ardupilot,BDShot FW on IOMCU (F103) of newer Pixhawk 2.4.8 doesn't work,"## Bug report

**Issue details**

Setting BRD_IO_DSHOT on a Pixhawk 2.4.8 with F103 co-processor fails to load the dshot firmware onto it and soft-bricks (solid amber iomcu indicator, no ACT indicator, IOMCU unhealthy message)
Normally, the Messages tab shows iomcu: 0 0 411FC231
 but when bricked, it shoes iomcu: 0 0 0

**Version**
4.6.2, 4.6.3
stock Pixhawk-bdshot

**Platform**
[ x ] Copter

**Airframe type**
Quad

**Hardware type**
Pixhawk 2.4.8

**Logs**
N/A",[],0
ArduPilot/ardupilot,ArduPilot doesn't start on some boards with PX4 bootloader,"## Bug report

**Issue details**

ArduPilot doesn't start (immediately crashes) on STM32H7 boards where a PX4 bootloader is used. You can see the bootloader appear on USB and then disappear after a few seconds, and nothing happens after that.

Also see proposed (likely wrong) fix and discussion in https://github.com/ArduPilot/ChibiOS/pull/98.

FYI: @tpwrules, @andyp1per, @bugobliterator 

**To reproduce:**

1. Backup AP parameters in case you don't want to lose them!
2. Flash PX4 (e.g use QGroundControl and flash PX4 firmware. (It says stable 1.16 but in fact installs 1.17alpha1, which works to install the bootloader anyway)
3. In QGC, go to Analyze Tools -> MAVLink Console and run `bl_update /etc/extras/cubepilot_cubeorange_bootloader.bin` for CubeOrange or `bl_update /etc/extras/cubepilot_cubeorangeplus_bootloader.bin` for CubeOrange+.
(Alternatively, you can set param `SYS_BL_UPDATE` to `1` (Enabled) and reboot the board, however, this only works with 1.17/main but not 1.16 or 1.15 where it is broken.)
4. Flash ArduPilot

**Version**
ArduCopter 4.6.3 (and previous)

**Platform**
[ x] All
[  ] AntennaTracker
[  ] Copter
[  ] Plane
[  ] Rover
[  ] Submarine

**Airframe type**
N/A

**Hardware type**
Likely all STM32H7, e.g. CubeOrange, CubeOrange+ 

**Logs**
N/A
",[],0
ArduPilot/ardupilot,STM32F412CGU6  bootloader USB no recognition,"Recently, while testing bootloaders and applications for the F412 series, I found that the USB port was not working properly.",[],0
ArduPilot/ardupilot,No Parameter Length Validation at build,"## Bug report

**Issue details**

When you define a parameter with a name (including the full prefix chain) that's longer than 16 characters, everything compiles fine and the firmware uploads without any warnings. But then the autopilot just crashes silently during startup when it tries to initialize the parameters. There's no error message, no indication of what went wrong - it just fails back to boot loader.

The AP_MAX_NAME_SIZE limit of 16 chars makes sense, but the build system should catch this before you even get to the upload stage.

**Version**

Master branch (current as of 13NOV25)

**Platform**

[X] All

**Airframe type**

Affects all airframe types - this is a build/parameter system issue

**Hardware type**

Tested on various STM32 builds (ie Cube Orange, custom Periph Hardware etc)

**Logs**

No useful logs which was a part of the difficulty in finding this bug - Could pull GDB etc, but the issue is pretty clearly that the user defined Parameter name out of bounds is triggering watchdog loop back to boot loader.",[],0
ArduPilot/ardupilot,Leverage pending Ardupilot gimbal features to gimbal manager V2 mavlink sub-protocol,"Following the conversation https://github.com/ArduPilot/ardupilot/pull/23092, It seems there are some Ardupilot gimbal features that do not have any way to be triggered and monitored using gimbal manager v2 sub-protocol.

Adding them would allow for very nice GCS integrations like we already have for the rest of gimbal features. So far I think the only feature remaining would be:

- Track with gimbal another vehicle system ID.
- Track with gimbal home position.

If there is any other feature not supported please let me know and we can add it to the list. Whenever we all are happy with the pending features to work over, I will work on them in all fronts, Ardupilot, mavlink and QGC.","['Enhancement', 'Gimbal', 'Camera']",0
ArduPilot/ardupilot,Valgrind error in AP_Mission_ChangeDetector,"During the implementation of ARC Waypoints (PR: https://github.com/ArduPilot/ardupilot/pull/31139) we found that Valgrind complains about [AP_Mission_ChangeDetector](https://github.com/ArduPilot/ardupilot/blob/master/libraries/AP_Mission/AP_Mission_ChangeDetector.h) using uninitialised variables.

We have a PR here that includes an autotest that fails due to the issue https://github.com/ArduPilot/ardupilot/pull/31492

I suspect the issue is actually very rare and innocuous but we should still investigate and fix it",[],0
ArduPilot/ardupilot,After upgrading to arduplane 4.6.x GPS bitrate 115k can't be enforced by option.,"## Bug report

**Issue details**

In Arduplane 4.6.x stopped working option GPS_DRV_OPTION=4 (bitmask xxxxx1xx) to enforce GPS bitrate 115200.
This can be because parameter GPS_TYPE was removed in 4.6.3
In versions 4.5 and lower it was necessary to set GPS_TYPE=2 (ublox) to make GPS_DRV_OPTION=4 work (even it's not logical, it didn't work with autodetection, only with fixed GPS type.)
I'm using a GPS beacon ""sniffing"" on the GPS data which can't  work with bitrate higher than 115k.

Long time ago discussed here: https://discuss.ardupilot.org/t/is-it-possible-to-limit-maximal-negotiated-gps-serial-bitrate-tbeacon-stopped-working-after-upgrading-to-plane-4-2/88011
Then I found that GPS_TYPE=2 is needed so I set it like this and it worked ... until 4.6.

**Version**
Arduplane 4.6.3 bdshot

**Platform**
[  ] All
[  ] AntennaTracker
[  ] Copter
[ X] Plane
[  ] Rover
[  ] Submarine

**Airframe type**
V-tail aiplane

**Hardware type**
Matek F405 Wing bdshot

**Logs**

",[],0
ArduPilot/ardupilot,AP_DDS: add vicon pose information input,"## Feature request

**Is your feature request related to a problem? Please describe.**
Motion Capture System is always used to provide pose information for copters in indoors environment. Even mavproxy offers a module to get pose information form Vicon and send to ardupilot, the ROS2 support is still needed as users can do the close-loop completely in ROS2. 

**Describe the solution you'd like**
ardupilot is subscribed to the topic `/vision/pose` and takes pose information from that to update.

**Describe alternatives you've considered**
_A clear and concise description of any alternative solutions or features you've considered._

**Platform**
[  ] All
[  ] AntennaTracker
[X] Copter
[  ] Plane
[  ] Rover
[  ] Submarine

**Additional context**
_Add any other context or screenshots about the feature request here._
",[],0
ArduPilot/ardupilot,DSHOT issue with Ardu using 4in1 BLS ESC,"Hey! Having issues getting motors to run on a quad build with the SIYI N7 + 4-in-1 BLS ESC.

Video, gimbal control, telemetry, and RC link work fine, only motors donâ€™t spin.

Hardware: N7 FC, SIYI GPS A8 Mini, HGLC BLS 4-in-1 ESC 60A 

Firmware: ArduCopter 4.5.7 / Quadcopter 

What happens:
â€¢ When I move motor outputs from MAIN 1â€“4 â†’ AUX 1â€“4, ESCs initialise
â€¢ After reboot, servo outputs reset to 0 and motors stop responding. (photo below)
â€¢ ESC does not initialise, no beep tone. 

Tried:
â€¢ AUX1â€“4 for motors, MAIN disabled/GPIO
â€¢ DShot600, BLHeli autodetect, BLHeli32 mode
â€¢ Reset params â†’ only set PWM=DShot
â€¢ Set servo outputs 9â€“13 as motors
â€¢ New N7, and HolyBro 6C FC + ArduCopter 4.5.7 (same problem)
â€¢ Changed ESC from BL32 to BLS (same problem)
â€¢ ESC/motors confirmed working on Betaflight
â€¢ Spent a week going through threads, chatted with SIYI directly, discord channel and have been referred to submit here, still no progress. Maybe a bug??

Anyone run DShot and had this happen? Any tips appreciated! :-) 

Param file attached. 

<img width=""675"" height=""537"" alt=""Image"" src=""https://github.com/user-attachments/assets/e1ade8f5-8b63-4711-ad79-04c643066fb1"" />

[ArduPilot DSHOT Issue.param.zip](https://github.com/user-attachments/files/23431042/ArduPilot.DSHOT.Issue.param.zip)

",[],0
ArduPilot/ardupilot,DroneCAN Deadlock in dronecan_0 Thread on CubeRed Hardware,"<img width=""470"" height=""464"" alt=""Image"" src=""https://github.com/user-attachments/assets/33d31672-fdb3-4f64-9e56-342adb5253f5"" />

## Bug report

**Setup**
[1] Mavproxy connecting to Cube Red via TCP connection and forwarding via out=udp:127.0.0.1:14550 onto my local machine
[2] DroneCan GUI Tool connected via MavCAN connection
[3] Cube Red configured as TCP server on LWIP stack

**Issue details**
Critical deadlock detected in the dronecan_0 thread (error code 83, 0x20000010) causing internal errors (0x8000) on CubeRed hardware. The deadlock appears to be related to a potential lock ordering issue in the DroneCAN pipeline. Issue is repeatable and occurs when using DroneCAN GUI Tool connected via MavCAN with MAVProxy forwarding TCP connections from a Cube Red configured as TCP server on LWIP stack.

**Version**
master

**Platform**
Discovered on Rover/Boat, but likely on All
[?] All
[  ] AntennaTracker
[  ] Copter
[  ] Plane
[X] Rover
[  ] Submarine

**Airframe type**
Boat

**Hardware type**
CubeRed

**Logs**
AP: CRITICAL Deadlock 83 dronecan_0 0x20000010
AP: Internal Errors 0x8000
Critical failure 0x8000 sysid=1 compid=1
AP: CRITICAL Deadlock 83 dronecan_0 0x20000010
Critical failure 0x8000 sysid=1 compid=1
AP: CRITICAL Deadlock 83 dronecan_0 0x20000010
Critical failure 0x8000 sysid=1 compid=1
","['BUG', 'CAN']",0
ArduPilot/ardupilot,AP_DDS: Servo Control,"It would be great to receive all the servo output telemetry and send servo commands through DDS.

- `/ap/servo_out` and `/ap/servo_in` both of type `ardupilot_msgs/msg/Servo` which reports 32 channels.
- Any channel commanded that is between 800 and 2200 is considered a valid value to command.
- Maybe a protection in the config file to protect the first channels.

**Platform**
[ x ] All
[  ] AntennaTracker
[  ] Copter
[  ] Plane
[  ] Rover
[  ] Submarine


",[],0
pytorch/pytorch,Use typing_extensions.TypeAliasType for better reexport of `__module__`,"### ðŸš€ The feature, motivation and pitch

There is a common paradigm in PyTorch of modifying imported modules __module__ attribute to reexpose them in the proper new location after a refactor. However, these do not play nicely with TypeCheckers / Linters. Recently, we TypeAliasType which can automatically create type aliases in new locations using the new 'type' keyword in Python 3.12. We can also instatiate them manually using typing_extensions.TypeAliasType. We should to remove the need to set __module__ to attribute to __module__

Example below that could benefit from this paradigm:
https://github.com/pytorch/pytorch/blob/3fd1ebd04b3573a96fb7ef3a36da1e43b95681a3/torch/distributed/tensor/experimental/__init__.py#L31

### Alternatives

_No response_

### Additional context

_No response_

cc @lolpack @maggiemoss @ndmitchell @kinto0 @samwgoldman","['good first issue', 'module: typing', 'module: lint', 'triaged', 'actionable']",1
pytorch/pytorch,Compilation events should emit profiler events too,"We can turn the compile events into a perfetto trace via tlparse but it would be nice if they showed up in stock profiler too. Just need to add RecordFunction

cc @chauhang @penguinwu","['good first issue', 'triaged', 'oncall: pt2']",1
pytorch/pytorch,torch.compile regression 2.9.1: Triton kernel name '_Runner__kernel_name_0' is not defined,"### ðŸ› Describe the bug

The following works with torch 2.8.0 and triton 3.4.0, but not with torch 2.9.1 and triton 3.5.1

```
Error:
  File ""/tmp/torchinductor_.../6e/c6ei2zticnswb7dd3gff5kipeuaiygggpdapxwg6t5jqpkf6lxx6.py"", line 1882, in call
    __mm_kernel_0.run(buf4, primals_42, buf3, 1101, 15360, 3072, 3072, 1, 15360, 1, 15360, 1, 17, stream=stream0)
    ^^^^^^^^^^^^^
NameError: name '_Runner__mm_kernel_0' is not defined
```

Full error log below

The name __mm_kernel_0 is defined as follows in the compiled code:
```
__mm_kernel_0 = async_compile.triton('__mm_kernel', '''

import triton
import triton.language as tl
```

The same compiled code can be found using 2.8.0, so my guess is that the behaviour of async_compile.triton has changed.

If you do confirm that this is a bug, a workaround would be appreciated because it prevents us from upgrading.
If it isn't clear immediately how and why this happens, let me know and I can try to create minimal reproduction code.

### Error logs

```
Traceback (most recent call last):
  File ""/home/.../.../OneTrainer/modules/ui/TrainUI.py"", line 755, in __training_thread_function
    trainer.train()
  File ""/home/.../.../OneTrainer/modules/trainer/GenericTrainer.py"", line 750, in train
    loss.backward()
  File ""/home/.../.../OneTrainer/venv/lib/python3.12/site-packages/torch/_tensor.py"", line 625, in backward
    torch.autograd.backward(
  File ""/home/.../.../OneTrainer/venv/lib/python3.12/site-packages/torch/autograd/__init__.py"", line 354, in backward
    _engine_run_backward(
  File ""/home/.../.../OneTrainer/venv/lib/python3.12/site-packages/torch/autograd/graph.py"", line 841, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/.../.../OneTrainer/venv/lib/python3.12/site-packages/torch/autograd/function.py"", line 315, in apply
    return user_fn(self, *args)
           ^^^^^^^^^^^^^^^^^^^^
  File ""/home/.../.../OneTrainer/venv/lib/python3.12/site-packages/torch/utils/checkpoint.py"", line 319, in backward
    torch.autograd.backward(outputs_with_grad, args_with_grad)
  File ""/home/.../.../OneTrainer/venv/lib/python3.12/site-packages/torch/autograd/__init__.py"", line 354, in backward
    _engine_run_backward(
  File ""/home/.../.../OneTrainer/venv/lib/python3.12/site-packages/torch/autograd/graph.py"", line 841, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/.../.../OneTrainer/venv/lib/python3.12/site-packages/torch/autograd/function.py"", line 315, in apply
    return user_fn(self, *args)
           ^^^^^^^^^^^^^^^^^^^^
  File ""/home/.../.../OneTrainer/venv/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py"", line 2303, in backward
    return impl_fn()
           ^^^^^^^^^
  File ""/home/.../.../OneTrainer/venv/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py"", line 2289, in impl_fn
    out = CompiledFunction._backward_impl(ctx, all_args)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/.../.../OneTrainer/venv/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py"", line 2461, in _backward_impl
    out = call_func_at_runtime_with_args(
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/.../.../OneTrainer/venv/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/utils.py"", line 129, in call_func_at_runtime_with_args
    out = normalize_as_list(f(args))
                            ^^^^^^^
  File ""/home/.../.../OneTrainer/venv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py"", line 1044, in _fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File ""/home/.../.../OneTrainer/venv/lib/python3.12/site-packages/torch/_inductor/output_code.py"", line 613, in __call__
    return self.current_callable(inputs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/.../.../OneTrainer/venv/lib/python3.12/site-packages/torch/_inductor/utils.py"", line 3017, in run
    out = model(new_inputs)
          ^^^^^^^^^^^^^^^^^
  File ""/tmp/torchinductor_.../6e/c6ei2zticnswb7dd3gff5kipeuaiygggpdapxwg6t5jqpkf6lxx6.py"", line 1882, in call
    __mm_kernel_0.run(buf4, primals_42, buf3, 1101, 15360, 3072, 3072, 1, 15360, 1, 15360, 1, 17, stream=stream0)
    ^^^^^^^^^^^^^
NameError: name '_Runner__mm_kernel_0' is not defined
```

### Versions

torch 2.8.0 and triton 3.4.0 vs. torch 2.9.1 and triton 3.5.1

cc @chauhang @penguinwu @voznesenskym @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @kadeng @muchulee8 @amjames @aakhundov @coconutruben @jataylo @oulgen @davidberard98 @bertmaher @int3 @nmacchioni @embg @peterbell10 @iupaikov-amd","['good first issue', 'triaged', 'actionable', 'oncall: pt2', 'module: inductor', 'module: user triton']",1
pytorch/pytorch,[auto functionalize][partitioner] ones_like is not getting recomputed,"### ðŸ› Describe the bug

The issue is described at https://github.com/pytorch/pytorch/pull/166788#issuecomment-3640350852



### Error logs

_No response_

### Versions

NA

cc @chauhang @penguinwu @avikchaudhuri @gmagogsfm @zhxchen17 @tugsbayasgalan @angelayi @suo @ydwu4 @desertfire @yushangdi @benjaminglass1 @jataylo @iupaikov-amd","['good first issue', 'oncall: export', 'module: aotinductor']",1
pytorch/pytorch,"[dynamo] [dynamic] `torch.combinations` throws RuntimeError when `(backend=aot_eager, dynamic=True)`","### ðŸ› Describe the bug

`torch.combinations` throws RuntimeError when compiled by `aot_eager` when `dynamic=True`
```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch._inductor import config

config.fallback_random = True
torch.set_grad_enabled(False)


class Model(torch.nn.Module):

    def __init__(self):
        super(Model, self).__init__()

    def forward(self, x):
        x = torch.combinations(x.flatten(), r=2)
        return x


model = Model()


x = torch.randn(3, 1, 1)

inputs = [x]

device = ""cpu""
def run_test(model, inputs, device, backend):
    torch.manual_seed(0)
    model.to(device)
    inputs = [input.to(device) for input in inputs]
    if backend != ""eager"":
        model = torch.compile(model, backend=backend, dynamic=True)
    try:
        output = model(*inputs)
        print(f""succeed on {backend}: {output}"")
    except Exception as e:
        print(e)


run_test(model, inputs, device, 'eager')
run_test(model, inputs, device, 'aot_eager')
```


### Error logs

```
succeed on eager: tensor([[-0.9697, -0.2773],
        [-0.9697,  0.3364],
        [-0.2773,  0.3364]])
got RuntimeError('Cannot call numel() on tensor with symbolic sizes/strides\nException raised from throw_cannot_call_with_symbolic at /pytorch/c10/core/TensorImpl.cpp:291 (most recent call first):\
```




### Versions

2.10.0.dev20250923+cu126

cc @chauhang @penguinwu @ezyang @bobrenjc93","['good first issue', 'triaged', 'oncall: pt2', 'module: dynamic shapes']",1
pytorch/pytorch,foreach_map enhancements,"### ðŸš€ The feature, motivation and pitch

These enhancements are to have a better UX when using foreach_map, suggested in a few places, but most recently, https://github.com/pytorch/pytorch/issues/158371#issuecomment-3088757068

These enhancements should allow easier compiler-first custom optimizer implementations. 
### Alternatives

_No response_

### Additional context

_No response_

cc @chauhang @penguinwu @voznesenskym @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @aakhundov @coconutruben @Lucaskabela","['feature', 'good first issue', 'triaged', 'oncall: pt2', 'module: inductor', 'module: dynamo', 'internal ramp-up task']",1
pytorch/pytorch,get_ema_multi_avg_fn() equation is a little confused,"### ðŸ“š The doc issue

![Image](https://github.com/user-attachments/assets/1f890354-2825-4ba4-bc2d-519448660634)

### Suggest a potential alternative/fix

I think that above equation would be $W_{0}^{\text{EMA}} = W_{0}^{\text{model}}$, and $W_{t+1}^{\text{EMA}} = \alpha W_{t}^{\text{EMA}} + (1 - \alpha) W_{t+1}^{\text{model}}$ or $W_{t}^{\text{EMA}} = \alpha W_{t-1}^{\text{EMA}} + (1 - \alpha) W_{t}^{\text{model}}$ to serve more clear meaning.

cc @svekars @sekyondaMeta @AlannaBurke @vincentqb @jbschlosser @albanD @janeyx99 @crcrpar","['module: docs', 'module: optimizer', 'good first issue', 'triaged', 'easy']",1
pytorch/pytorch,Make tlparse able to show a summary of distinct graph breaks,"### ðŸš€ The feature, motivation and pitch

Right now we have this in tlparse:
<img width=""1296"" alt=""Image"" src=""https://github.com/user-attachments/assets/57506e4c-760a-496c-88c7-67fe1ed2f5b0"" />

which often contains duplicated graph breaks:
<img width=""1571"" alt=""Image"" src=""https://github.com/user-attachments/assets/a6a317b5-23a3-4f12-8191-d8ec93c15b6c"" />

The goal is to have a summary of _distinct_ graph breaks for end users who just want to fix their graph breaks.

### Alternatives

_No response_

### Additional context

_No response_

cc @chauhang @penguinwu @voznesenskym @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @amjames @Lucaskabela","['good first issue', 'triaged', 'oncall: pt2', 'module: dynamo', 'dynamo-triage-jan2025', 'module: compile ux']",1
pytorch/pytorch,[Feature request] `torch.export` .save/.load could support `safetensors` and/or `weights_only=True`,"### ðŸš€ The feature, motivation and pitch

Today, torch.export will load tensors via
```
def deserialize_torch_artifact(serialized: Union[Dict[str, Any], Tuple[Any, ...], bytes]):
    if isinstance(serialized, (dict, tuple)):
        return serialized
    if len(serialized) == 0:
        return {}
    buffer = io.BytesIO(serialized)
    buffer.seek(0)
    # weights_only=False as we want to load custom objects here (e.g. ScriptObject)
    artifact = torch.load(buffer, weights_only=False)
    assert isinstance(artifact, (tuple, dict))
    return artifact
```
Security teams really don't like this I have learned ðŸ™‚. In the interest of leveraging the awesome benefits of export more safely, it would be amazing to add optional support for any or all of `safetensors` and/or `weights_only=True`!

Thank you for all the work on `torch.export`, it has a very promising future!

### Alternatives

_No response_

### Additional context

_No response_

cc @chauhang @penguinwu @avikchaudhuri @gmagogsfm @zhxchen17 @tugsbayasgalan @angelayi @suo @ydwu4","['good first issue', 'oncall: pt2', 'export-triaged', 'oncall: export']",1
pytorch/pytorch,"[inductor] nan_asserts doesn't work for FP8, ""RuntimeError: ""isinf"" not implemented for 'Float8_e4m3fn'""","### ðŸ› Describe the bug

repro:
```
import torch
import torch._inductor.config as config

config.nan_asserts = True


class Model(torch.nn.Module):
    def forward(self, x):
        return x.half() + 1

model = Model().cuda()
x = torch.randn(10).cuda().to(torch.float8_e4m3fn)
_ = torch.compile(model, fullgraph=True)(x)

print(""done"")
```

logs:
```
  File ""/home/henrylhtsang/pytorch/torch/_functorch/_aot_autograd/runtime_wrappers.py"", line 495, in wrapper
    return compiled_fn(runtime_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/henrylhtsang/pytorch/torch/_inductor/output_code.py"", line 460, in __call__
    return self.current_callable(inputs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/henrylhtsang/pytorch/torch/_inductor/utils.py"", line 2397, in run
    return model(new_inputs)
           ^^^^^^^^^^^^^^^^^
  File ""/tmp/torchinductor_henrylhtsang/6t/c6tjaevpzkaafbwl4rv7kbghqfncxogc5dkfbssmyqsp6nh7saot.py"", line 92, in call
    assert not arg0_1.isinf().any().item()
               ^^^^^^^^^^^^^^
RuntimeError: ""isinf"" not implemented for 'Float8_e4m3fn'
```

### Versions

trunk

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @yanbing-j @vkuzo @albanD","['good first issue', 'triaged', 'module: inductor', 'module: floatx (formerly float8)']",1
pytorch/pytorch,[Inductor] Inference failed with the compiled model with aminmax operator,"### ðŸ› Describe the bug

```python

import torch


class SimpleModel(torch.nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.linear = torch.nn.Linear(10, 10)

    def forward(self, x):
        x = self.linear(x)
        min_val, max_val = torch.aminmax(x)
        x_normalized = (x - min_val) / (max_val - min_val)
        return x_normalized

model = SimpleModel()


inputs = torch.randn(1, 10)
with torch.no_grad():
    compiled_model = torch.compile(model, backend='inductor') 
    compiled_out = compiled_model(*inputs)   
```


### StackTrace

```
/data/qshenaf/miniconda3/envs/vllm/lib/python3.11/site-packages/torch/autograd/graph.py:825: UserWarning: Error detected in torch::autograd::NotImplemented. Traceback of forward call that caused the error:
  File ""/data/qshenaf/remote_pc/LLM4Converter/bugs/torch.aminmax.py"", line 11, in forward
    min_val, max_val = torch.aminmax(x)
 (Triggered internally at ../torch/csrc/autograd/python_anomaly_mode.cpp:110.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Traceback (most recent call last):
  File ""/data/qshenaf/miniconda3/envs/vllm/lib/python3.11/site-packages/torch/_dynamo/output_graph.py"", line 1446, in _call_user_compiler
    compiled_fn = compiler_fn(gm, self.example_inputs())
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/qshenaf/miniconda3/envs/vllm/lib/python3.11/site-packages/torch/_dynamo/repro/after_dynamo.py"", line 129, in __call__
    compiled_gm = compiler_fn(gm, example_inputs)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/qshenaf/miniconda3/envs/vllm/lib/python3.11/site-packages/torch/__init__.py"", line 2234, in __call__
    return compile_fx(model_, inputs_, config_patches=self.config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/qshenaf/miniconda3/envs/vllm/lib/python3.11/site-packages/torch/_inductor/compile_fx.py"", line 1521, in compile_fx
    return aot_autograd(
           ^^^^^^^^^^^^^
  File ""/data/qshenaf/miniconda3/envs/vllm/lib/python3.11/site-packages/torch/_dynamo/backends/common.py"", line 72, in __call__
    cg = aot_module_simplified(gm, example_inputs, **self.kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/qshenaf/miniconda3/envs/vllm/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py"", line 1071, in aot_module_simplified
    compiled_fn = dispatch_and_compile()
                  ^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/qshenaf/miniconda3/envs/vllm/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py"", line 1056, in dispatch_and_compile
    compiled_fn, _ = create_aot_dispatcher_function(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/qshenaf/miniconda3/envs/vllm/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py"", line 522, in create_aot_dispatcher_function
    return _create_aot_dispatcher_function(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/qshenaf/miniconda3/envs/vllm/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py"", line 759, in _create_aot_dispatcher_function
    compiled_fn, fw_metadata = compiler_fn(
                               ^^^^^^^^^^^^
  File ""/data/qshenaf/miniconda3/envs/vllm/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py"", line 333, in aot_dispatch_autograd
    fx_g, joint_inputs, maybe_subclass_meta = aot_dispatch_autograd_graph(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/qshenaf/miniconda3/envs/vllm/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/dispatch_and_compile_graph.py"", line 294, in aot_dispatch_autograd_graph
    fx_g = _create_graph(joint_fn_to_trace, updated_joint_inputs, aot_config=aot_config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/qshenaf/miniconda3/envs/vllm/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/dispatch_and_compile_graph.py"", line 54, in _create_graph
    fx_g = make_fx(
           ^^^^^^^^
  File ""/data/qshenaf/miniconda3/envs/vllm/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py"", line 2110, in wrapped
    return make_fx_tracer.trace(f, *args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/qshenaf/miniconda3/envs/vllm/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py"", line 2048, in trace
    return self._trace_inner(f, *args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/qshenaf/miniconda3/envs/vllm/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py"", line 2034, in _trace_inner
    t = dispatch_trace(
        ^^^^^^^^^^^^^^^
  File ""/data/qshenaf/miniconda3/envs/vllm/lib/python3.11/site-packages/torch/_compile.py"", line 32, in inner
    return disable_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/qshenaf/miniconda3/envs/vllm/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py"", line 632, in _fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File ""/data/qshenaf/miniconda3/envs/vllm/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py"", line 1127, in dispatch_trace
    graph = tracer.trace(root, concrete_args)  # type: ignore[arg-type]
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/qshenaf/miniconda3/envs/vllm/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py"", line 632, in _fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File ""/data/qshenaf/miniconda3/envs/vllm/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py"", line 823, in trace
    (self.create_arg(fn(*args)),),
                     ^^^^^^^^^
  File ""/data/qshenaf/miniconda3/envs/vllm/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py"", line 676, in flatten_fn
    tree_out = root_fn(*tree_args)
               ^^^^^^^^^^^^^^^^^^^
  File ""/data/qshenaf/miniconda3/envs/vllm/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py"", line 1182, in wrapped
    out = f(*tensors)
          ^^^^^^^^^^^
  File ""/data/qshenaf/miniconda3/envs/vllm/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py"", line 693, in inner_fn
    outs = fn(*args)
           ^^^^^^^^^
  File ""/data/qshenaf/miniconda3/envs/vllm/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py"", line 644, in joint_helper
    return _functionalized_f_helper(primals, tangents)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/qshenaf/miniconda3/envs/vllm/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py"", line 413, in _functionalized_f_helper
    f_outs = fn(*f_args)
             ^^^^^^^^^^^
  File ""/data/qshenaf/miniconda3/envs/vllm/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py"", line 280, in inner_fn_with_anomaly
    return inner_fn(*args)
           ^^^^^^^^^^^^^^^
  File ""/data/qshenaf/miniconda3/envs/vllm/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py"", line 265, in inner_fn
    backward_out = torch.autograd.grad(
                   ^^^^^^^^^^^^^^^^^^^^
  File ""/data/qshenaf/miniconda3/envs/vllm/lib/python3.11/site-packages/torch/autograd/__init__.py"", line 445, in grad
    return handle_torch_function(
           ^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/qshenaf/miniconda3/envs/vllm/lib/python3.11/site-packages/torch/overrides.py"", line 1717, in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/qshenaf/miniconda3/envs/vllm/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py"", line 1230, in __torch_function__
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/data/qshenaf/miniconda3/envs/vllm/lib/python3.11/site-packages/torch/autograd/__init__.py"", line 496, in grad
    result = _engine_run_backward(
             ^^^^^^^^^^^^^^^^^^^^^
  File ""/data/qshenaf/miniconda3/envs/vllm/lib/python3.11/site-packages/torch/autograd/graph.py"", line 825, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: derivative for aten::aminmax is not implemented

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/data/qshenaf/remote_pc/LLM4Converter/bugs/torch.aminmax.py"", line 22, in <module>
    compiled_out = compiled_model(*inputs)
                   ^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/qshenaf/miniconda3/envs/vllm/lib/python3.11/site-packages/torch/nn/modules/module.py"", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/qshenaf/miniconda3/envs/vllm/lib/python3.11/site-packages/torch/nn/modules/module.py"", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/qshenaf/miniconda3/envs/vllm/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py"", line 465, in _fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File ""/data/qshenaf/miniconda3/envs/vllm/lib/python3.11/site-packages/torch/nn/modules/module.py"", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/qshenaf/miniconda3/envs/vllm/lib/python3.11/site-packages/torch/nn/modules/module.py"", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/qshenaf/miniconda3/envs/vllm/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py"", line 1269, in __call__
    return self._torchdynamo_orig_callable(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/qshenaf/miniconda3/envs/vllm/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py"", line 1064, in __call__
    result = self._inner_convert(
             ^^^^^^^^^^^^^^^^^^^^
  File ""/data/qshenaf/miniconda3/envs/vllm/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py"", line 526, in __call__
    return _compile(
           ^^^^^^^^^
  File ""/data/qshenaf/miniconda3/envs/vllm/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py"", line 924, in _compile
    guarded_code = compile_inner(code, one_graph, hooks, transform)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/qshenaf/miniconda3/envs/vllm/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py"", line 666, in compile_inner
    return _compile_inner(code, one_graph, hooks, transform)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/qshenaf/miniconda3/envs/vllm/lib/python3.11/site-packages/torch/_utils_internal.py"", line 87, in wrapper_function
    return function(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/qshenaf/miniconda3/envs/vllm/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py"", line 699, in _compile_inner
    out_code = transform_code_object(code, transform)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/qshenaf/miniconda3/envs/vllm/lib/python3.11/site-packages/torch/_dynamo/bytecode_transformation.py"", line 1322, in transform_code_object
    transformations(instructions, code_options)
  File ""/data/qshenaf/miniconda3/envs/vllm/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py"", line 219, in _fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File ""/data/qshenaf/miniconda3/envs/vllm/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py"", line 634, in transform
    tracer.run()
  File ""/data/qshenaf/miniconda3/envs/vllm/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py"", line 2796, in run
    super().run()
  File ""/data/qshenaf/miniconda3/envs/vllm/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py"", line 983, in run
    while self.step():
          ^^^^^^^^^^^
  File ""/data/qshenaf/miniconda3/envs/vllm/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py"", line 895, in step
    self.dispatch_table[inst.opcode](self, inst)
  File ""/data/qshenaf/miniconda3/envs/vllm/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py"", line 2987, in RETURN_VALUE
    self._return(inst)
  File ""/data/qshenaf/miniconda3/envs/vllm/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py"", line 2972, in _return
    self.output.compile_subgraph(
  File ""/data/qshenaf/miniconda3/envs/vllm/lib/python3.11/site-packages/torch/_dynamo/output_graph.py"", line 1117, in compile_subgraph
    self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)
  File ""/data/qshenaf/miniconda3/envs/vllm/lib/python3.11/site-packages/torch/_dynamo/output_graph.py"", line 1369, in compile_and_call_fx_graph
    compiled_fn = self.call_user_compiler(gm)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/qshenaf/miniconda3/envs/vllm/lib/python3.11/site-packages/torch/_dynamo/output_graph.py"", line 1416, in call_user_compiler
    return self._call_user_compiler(gm)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/qshenaf/miniconda3/envs/vllm/lib/python3.11/site-packages/torch/_dynamo/output_graph.py"", line 1465, in _call_user_compiler
    raise BackendCompilerFailed(self.compiler_fn, e) from e
torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:
RuntimeError: derivative for aten::aminmax is not implemented

Set TORCH_LOGS=""+dynamo"" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True


Process finished with exit code 1
```

### Versions

PyTorch version: 2.5.1+cu124
Is debug build: False
CUDA used to build PyTorch: 12.4
ROCM used to build PyTorch: N/A

OS: AlmaLinux 9.4 (Seafoam Ocelot) (x86_64)
GCC version: (GCC) 11.4.1 20231218 (Red Hat 11.4.1-3)
Clang version: 17.0.6 (AlmaLinux OS Foundation 17.0.6-5.el9)
CMake version: version 3.26.5
Libc version: glibc-2.34

Python version: 3.11.0 (main, Mar 1 2023, 18:26:19) [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.14.0-427.37.1.el9_4.x86_64-x86_64-with-glibc2.34
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration:
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090

Nvidia driver version: 560.35.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture: x86_64
CPU op-mode(s): 32-bit, 64-bit
Address sizes: 43 bits physical, 48 bits virtual
Byte Order: Little Endian
CPU(s): 64
On-line CPU(s) list: 0-63
Vendor ID: AuthenticAMD
Model name: AMD Ryzen Threadripper PRO 3975WX 32-Cores
CPU family: 23
Model: 49
Thread(s) per core: 2
Core(s) per socket: 32
Socket(s): 1
Stepping: 0
Frequency boost: enabled
CPU(s) scaling MHz: 81%
CPU max MHz: 4368.1641
CPU min MHz: 2200.0000
BogoMIPS: 7000.23
Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sev sev_es
Virtualization: AMD-V
L1d cache: 1 MiB (32 instances)
L1i cache: 1 MiB (32 instances)
L2 cache: 16 MiB (32 instances)
L3 cache: 128 MiB (8 instances)
NUMA node(s): 1
NUMA node0 CPU(s): 0-63
Vulnerability Gather data sampling: Not affected
Vulnerability Itlb multihit: Not affected
Vulnerability L1tf: Not affected
Vulnerability Mds: Not affected
Vulnerability Meltdown: Not affected
Vulnerability Mmio stale data: Not affected
Vulnerability Retbleed: Mitigation; untrained return thunk; SMT enabled with STIBP protection
Vulnerability Spec rstack overflow: Mitigation; Safe RET
Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl
Vulnerability Spectre v1: Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2: Mitigation; Retpolines, IBPB conditional, STIBP always-on, RSB filling, PBRSB-eIBRS Notaffected
Vulnerability Srbds: Not affected
Vulnerability Tsx async abort: Not affected

Versions of relevant libraries:
[pip3] mypy-extensions==1.0.0
[pip3] numpy==1.26.4
[pip3] nvidia-cublas-cu12==12.4.5.8
[pip3] nvidia-cuda-cupti-cu12==12.4.127
[pip3] nvidia-cuda-nvrtc-cu12==12.4.127
[pip3] nvidia-cuda-runtime-cu12==12.4.127
[pip3] nvidia-cudnn-cu12==9.1.0.70
[pip3] nvidia-cufft-cu12==11.2.1.3
[pip3] nvidia-curand-cu12==10.3.5.147
[pip3] nvidia-cusolver-cu12==11.6.1.9
[pip3] nvidia-cusparse-cu12==12.3.1.170
[pip3] nvidia-cusparselt-cu12==0.6.2
[pip3] nvidia-nccl-cu12==2.21.5
[pip3] nvidia-nvjitlink-cu12==12.4.127
[pip3] nvidia-nvtx-cu12==12.4.127
[pip3] onnx==1.17.0
[pip3] onnxruntime==1.20.1
[pip3] optree==0.14.0
[pip3] torch==2.5.1
[pip3] torchaudio==2.5.1
[pip3] torchvision==0.20.1
[pip3] triton==3.1.0
[conda] numpy 1.26.4 pypi_0 pypi
[conda] nvidia-cublas-cu12 12.4.5.8 pypi_0 pypi
[conda] nvidia-cuda-cupti-cu12 12.4.127 pypi_0 pypi
[conda] nvidia-cuda-nvrtc-cu12 12.4.127 pypi_0 pypi
[conda] nvidia-cuda-runtime-cu12 12.4.127 pypi_0 pypi
[conda] nvidia-cudnn-cu12 9.1.0.70 pypi_0 pypi
[conda] nvidia-cufft-cu12 11.2.1.3 pypi_0 pypi
[conda] nvidia-curand-cu12 10.3.5.147 pypi_0 pypi
[conda] nvidia-cusolver-cu12 11.6.1.9 pypi_0 pypi
[conda] nvidia-cusparse-cu12 12.3.1.170 pypi_0 pypi
[conda] nvidia-cusparselt-cu12 0.6.2 pypi_0 pypi
[conda] nvidia-nccl-cu12 2.21.5 pypi_0 pypi
[conda] nvidia-nvjitlink-cu12 12.4.127 pypi_0 pypi
[conda] nvidia-nvtx-cu12 12.4.127 pypi_0 pypi
[conda] optree 0.14.0 pypi_0 pypi
[conda] torch 2.5.1 pypi_0 pypi
[conda] torchaudio 2.5.1 pypi_0 pypi
[conda] torchvision 0.20.1 pypi_0 pypi
[conda] triton 3.1.0 pypi_0 pypi

cc @ezyang @albanD @gqchen @pearu @nikitaved @soulitzer @Varal7 @xmfan @chauhang @penguinwu","['module: autograd', 'good first issue', 'triaged', 'oncall: pt2']",1
pytorch/pytorch,"aten.grid_sampler_3d.default is missing a c-shim implementation, using proxy executor as fallback","### ðŸ› Describe the bug

Do we need any action item here?

### Error logs
```python
site-packages/torch/_inductor/ir.py:6638] [0/0] aten.grid_sampler_3d.default is missing a c-shim implementation, using proxy executor as fallback
```
### Versions

nightly

cc @chauhang @penguinwu @voznesenskym @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @aakhundov @avikchaudhuri @gmagogsfm @zhxchen17 @tugsbayasgalan @angelayi @suo @ydwu4 @desertfire @yushangdi @benjaminglass1 @yf225","['good first issue', 'triaged', 'oncall: pt2', 'module: inductor', 'oncall: export', 'module: aotinductor']",1
pytorch/pytorch,[BUG][PyTorch 2.0 Export][quant]:get_source_partitions() may return different matches with same input graph,"### ðŸ› Describe the bug

I am attempting to extend the quantization backend based on PyTorch 2.0 export. The operator I have chosen is `torch.gather` . 
The input code I tested is as follows
```python
    class GatherLayer(nn.Module):
        def forward(self, x):
            assert x.shape == (2,2)
            x = torch.gather(x, dim=0, index=torch.tensor([[0, 0], [1, 0]]))
            return x

    example_inputs = (torch.tensor([[0,1],[0,1]]),)

    model = GatherLayer()
    model.eval()
    exported_model = torch.export.export(model, example_inputs).module()
    # print(exported_model.graph)

    prepared_model = prepare_pt2e(exported_model, quantizer)
    prepared_model(*example_inputs)
    # print(prepared_model.graph)
    quantized_model = convert_pt2e(prepared_model)
```

. The part of the code I constructed for the quantization backend is as follows: 
```python
def _annotate_gather(
    gm: torch.fx.GraphModule,
    quantization_config: Optional[QuantizationConfig],
    filter_fn: Optional[Callable[[Node], bool]] = None,
) -> Optional[List[List[Node]]]:
    print(gm.graph)
    partitions = get_source_partitions(gm.graph, [torch.gather], filter_fn)
    matches = list(itertools.chain.from_iterable(partitions.values()))
    annotated_partitions = []

    for matche in matches:
        output_nodes = matche.output_nodes
        input_nodes = matche.input_nodes
        gather_node = output_nodes[0]
        input_qspec_map = {}
        partition = []
        input_node = input_nodes[1] # sometimes input_nodes[1] is input, while sometimes input_nodes[0] is input 
        input_qspec_map[input_node] = get_input_act_qspec(quantization_config)
        partition.append(input_node)

        gather_node.meta[""quantization_annotation""] = QuantizationAnnotation(
            input_qspec_map=input_qspec_map,
            output_qspec=get_output_act_qspec(quantization_config),
            _annotated=True,
        )
        _mark_nodes_as_annotated(partition)
        annotated_partitions.append(partition)
    return annotated_partitions
```

 I found that with the same input, the matching results can differ, **_specifically in the order of `input_nodes`_** . Input graph is:
```
graph():
    %lifted_tensor_0 : [num_users=1] = get_attr[target=lifted_tensor_0]
    %x : [num_users=1] = placeholder[target=x]
    %lift_fresh_copy : [num_users=1] = call_function[target=torch.ops.aten.lift_fresh_copy.default](args = (%lifted_tensor_0,), kwargs = {})
    %detach : [num_users=1] = call_function[target=torch.ops.aten.detach.default](args = (%lift_fresh_copy,), kwargs = {})
    %gather : [num_users=1] = call_function[target=torch.ops.aten.gather.default](args = (%x, 0, %detach), kwargs = {})
    return (gather,)
```
 after matching with  `get_source_partitions` , the order of  node `x` and node `detach` in `input_nodes`  is not same all the time, which causes errors in my subsequent code.
**I totally run 100 times, and In 25 of these cases node `x` appears in `input_nodes[1]` and the rest in `input_nodes[0]`** 

Notice:
I tried the `SubgraphMatcherWithNameNodeMap` , but the pattern of `torch.gather` is not easy to describe due to 'index`, so I turned to this api.

cc @jerryzh168 @jianyuh @raghuramank100 @jamesr66a @vkuzo @jgong5 @Xia-Weiwen @leslie-fang-intel @msaroufim @chauhang @penguinwu @avikchaudhuri @gmagogsfm @zhxchen17 @tugsbayasgalan @angelayi @suo @ydwu4



### Versions

Collecting environment information...
PyTorch version: 2.5.1+cpu
Is debug build: False
CUDA used to build PyTorch: None
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.28.4
Libc version: glibc-2.31

Python version: 3.11.11 (main, Dec 11 2024, 16:28:39) [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.4.0-200-generic-x86_64-with-glibc2.31
Is CUDA available: False
CUDA runtime version: No CUDA
CUDA_MODULE_LOADING set to: N/A
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                       x86_64
CPU op-mode(s):                     32-bit, 64-bit
Byte Order:                         Little Endian
Address sizes:                      46 bits physical, 48 bits virtual
CPU(s):                             96
On-line CPU(s) list:                0-95
Thread(s) per core:                 2
Core(s) per socket:                 24
Socket(s):                          2
NUMA node(s):                       2
Vendor ID:                          GenuineIntel
CPU family:                         6
Model:                              85
Model name:                         Intel(R) Xeon(R) Platinum 8369HB CPU @ 3.30GHz
Stepping:                           11
CPU MHz:                            3800.047
CPU max MHz:                        4200.0000
CPU min MHz:                        1200.0000
BogoMIPS:                           6600.06
Hypervisor vendor:                  KVM
Virtualization type:                full
L1d cache:                          1.5 MiB
L1i cache:                          1.5 MiB
L2 cache:                           48 MiB
L3 cache:                           66 MiB
NUMA node0 CPU(s):                  0-47
NUMA node1 CPU(s):                  48-95
Vulnerability Gather data sampling: Unknown: Dependent on hypervisor status
Vulnerability Itlb multihit:        KVM: Vulnerable
Vulnerability L1tf:                 Mitigation; PTE Inversion
Vulnerability Mds:                  Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown
Vulnerability Meltdown:             Mitigation; PTI
Vulnerability Mmio stale data:      Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown
Vulnerability Retbleed:             Vulnerable
Vulnerability Spec store bypass:    Vulnerable
Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:           Mitigation; Retpolines; STIBP disabled; RSB filling; PBRSB-eIBRS Not affected; BHI Retpoline
Vulnerability Srbds:                Not affected
Vulnerability Tsx async abort:      Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown
Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves avx512_bf16 ida arat avx512_vnni

Versions of relevant libraries:
[pip3] intel_extension_for_pytorch==2.5.0
[pip3] numpy==1.26.3
[pip3] onnx==1.17.0
[pip3] torch==2.5.1+cpu
[pip3] torchaudio==2.5.1+cpu
[pip3] torchvision==0.20.1+cpu
[conda] intel-extension-for-pytorch 2.5.0                    pypi_0    pypi
[conda] mkl-include               2025.0.1                 pypi_0    pypi
[conda] mkl-static                2025.0.1                 pypi_0    pypi
[conda] numpy                     1.26.3                   pypi_0    pypi
[conda] torch                     2.5.1+cpu                pypi_0    pypi
[conda] torchaudio                2.5.1+cpu                pypi_0    pypi
[conda] torchvision               0.20.1+cpu               pypi_0    pypi","['oncall: quantization', 'good first issue', 'oncall: pt2', 'oncall: export']",1
pytorch/pytorch,Improve error message for wrong number of arguments in CachingAutotuner,"### ðŸ› Describe the bug

In triton_heuristics.py, the launcher call looks like:
```python
            launcher(
                *args_with_constexprs,
                **cloned_kwargs,
                grid=grid,
                stream=stream,
            )
```
If the kernel has fewer arguments than was passed in, the error looks like this because the `args_with_constexprs` gets splatted and overwrites `grid`:
```
Traceback (most recent call last):
  File ""/home/gabeferns/org/debug/cat-125075/new-cat-code.py"", line 302, in <module>
    compiled_module_main('None', functools.partial(benchmark_compiled_module2, arg0_1))
  File ""/home/gabeferns/pt-envs/cat/torch/_inductor/wrapper_benchmark.py"", line 402, in compiled_module_main
    wall_time_ms = benchmark_compiled_module_fn(times=times, repeat=repeat) * 1000
  File ""/home/gabeferns/org/debug/cat-125075/new-cat-code.py"", line 290, in benchmark_compiled_module2
    return print_performance(fn, times=times, repeat=repeat)
  File ""/home/gabeferns/pt-envs/cat/torch/_inductor/utils.py"", line 422, in print_performance
    timings = torch.tensor([timed(fn, args, times, device) for _ in range(repeat)])
  File ""/home/gabeferns/pt-envs/cat/torch/_inductor/utils.py"", line 422, in <listcomp>
    timings = torch.tensor([timed(fn, args, times, device) for _ in range(repeat)])
  File ""/home/gabeferns/pt-envs/cat/torch/_inductor/utils.py"", line 411, in timed
    result = model(*example_inputs)
  File ""/home/gabeferns/org/debug/cat-125075/new-cat-code.py"", line 289, in <lambda>
    fn = lambda: call2([arg0_1])
  File ""/home/gabeferns/org/debug/cat-125075/new-cat-code.py"", line 278, in call2
    combined_kernel.run(arg0_1, buf0, buf1, 23445504, grid=grid(23445504), stream=stream0)
  File ""/home/gabeferns/pt-envs/cat/torch/_inductor/runtime/triton_heuristics.py"", line 860, in run
    self.autotune_to_one_config(*args, grid=grid, **kwargs)
  File ""/home/gabeferns/pt-envs/cat/torch/_inductor/runtime/triton_heuristics.py"", line 737, in autotune_to_one_config
    timings = self.benchmark_all_configs(*args, **kwargs)
  File ""/home/gabeferns/pt-envs/cat/torch/_inductor/runtime/triton_heuristics.py"", line 711, in benchmark_all_configs
    timings = {
  File ""/home/gabeferns/pt-envs/cat/torch/_inductor/runtime/triton_heuristics.py"", line 712, in <dictcomp>
    launcher: self.bench(launcher, *args, **kwargs)
  File ""/home/gabeferns/pt-envs/cat/torch/_inductor/runtime/triton_heuristics.py"", line 594, in bench
    return benchmarker.benchmark_gpu(kernel_call, rep=40)
  File ""/home/gabeferns/pt-envs/cat/torch/_inductor/runtime/benchmarking.py"", line 39, in wrapper
    return fn(self, *args, **kwargs)
  File ""/home/gabeferns/pt-envs/cat/torch/_inductor/runtime/benchmarking.py"", line 243, in benchmark_gpu
    _callable()
  File ""/home/gabeferns/pt-envs/cat/torch/_inductor/runtime/triton_heuristics.py"", line 578, in kernel_call
    launcher(
TypeError: launcher() got multiple values for argument 'grid'
```
A check to see if the number of arguments is the same as what's expected, or a better error msg would be good.

### Error logs

_No response_

### Versions

Current main

cc @chauhang @penguinwu @voznesenskym @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @aakhundov","['good first issue', 'triaged', 'better-engineering', 'actionable', 'ciflow/trunk', 'oncall: pt2', 'module: inductor', 'module: compile ux']",1
pytorch/pytorch,Device check missing in torch.linalg.solve_triangular leading to hard crash,"### ðŸ› Describe the bug

It seems there is a device check missing in `torch.linalg.solve_triangular`.
When I run 

```
import torch

sq_shape = (3, 3)
device = torch.device('mps:0')
A = torch.normal(torch.zeros(sq_shape), torch.ones(sq_shape)).to(device=device)
eye = torch.eye(A.shape[0], device=torch.device('cpu'))
torch.linalg.solve_triangular(A, eye, upper=True)
```
it crashes with the following error.
```
Process finished with exit code 139 (interrupted by signal 11:SIGSEGV)
```

Running all on `cpu` or `mps` works fine.

### Versions

Collecting environment information...
PyTorch version: 2.5.1
Is debug build: False
CUDA used to build PyTorch: None
ROCM used to build PyTorch: N/A

OS: macOS 14.5 (arm64)
GCC version: Could not collect
Clang version: 16.0.0 (clang-1600.0.26.4)
CMake version: Could not collect
Libc version: N/A

Python version: 3.12.6 (main, Sep  6 2024, 19:03:47) [Clang 15.0.0 (clang-1500.3.9.4)] (64-bit runtime)
Python platform: macOS-14.5-arm64-arm-64bit
Is CUDA available: False
CUDA runtime version: No CUDA
CUDA_MODULE_LOADING set to: N/A
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Apple M1 Max

Versions of relevant libraries:
[pip3] mypy==1.13.0
[pip3] mypy-extensions==1.0.0
[pip3] numpy==2.1.3
[pip3] pytorch-lightning==2.4.0
[pip3] torch==2.5.1
[pip3] torchmetrics==1.6.0
[pip3] torchvision==0.20.1
[conda] libtorch                  2.4.0           cpu_generic_hf1facdc_0    conda-forge
[conda] nomkl                     1.0                  h5ca1d4c_0    conda-forge
[conda] numpy                     1.26.4          py310hd45542a_0    conda-forge
[conda] pytorch                   2.4.0           cpu_generic_py310hb190f2a_0    conda-forge


cc @jianyuh @nikitaved @pearu @mruberry @walterddr @xwang233 @Lezcano @kulinseth @albanD @malfet @DenisVieriu97 @jhavukainen","['module: crash', 'good first issue', 'triaged', 'module: linear algebra', 'module: mps']",1
pytorch/pytorch,Remove tensor variable default method fallthrough,"### ðŸ› Describe the bug

Today at https://github.com/pytorch/pytorch/blob/51e0996d58e6fa40a8d255a26b767c3f3e035943/torch/_dynamo/variables/tensor.py#L589C1-L596C10

we fallthrough and put an arbitrary method in the graph. This is not desired behavior because if this method returns a non-tensor value we will likely hit issues in torch variable that the output generated from a graph node was not the expected type. We should only put methods in the graph which we know we can handle.

To resolve this we should error in this case, which will likely cause many current tests to fail. We should go through these tests and add explicit handlers in tensor variable where they are expected to resolve these failures. 




### Versions

N/A

cc @ezyang @chauhang @penguinwu @voznesenskym @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @amjames","['good first issue', 'triaged', 'enhancement', 'oncall: pt2', 'module: dynamo']",1
pytorch/pytorch,Report WHY a symbol was created dynamically in symbolic_shapes logs,"### ðŸ› Describe the bug

Mainly, want to know if it was due to (1) automatic dynamic, or (2) assume_static_by_default = False, or (3) mark_dynamic.

Internal xref: https://fb.workplace.com/groups/3095840833991792/permalink/3848647042044497/

### Versions

main

cc @chauhang @penguinwu @bobrenjc93","['good first issue', 'triaged', 'oncall: pt2', 'module: dynamic shapes']",1
pytorch/pytorch,Docs are little bit outdated for torch logs,"### ðŸ“š The doc issue

At the end of setup here I assume the output should be some log describing the program rather than saying that the device does not support torch.compile.

https://pytorch.org/tutorials/recipes/torch_logs.html#setup

### Suggest a potential alternative/fix

Run the program in the tutorial and update the doc :) 

cc @ezyang @chauhang @penguinwu","['good first issue', 'module: logging', 'triaged', 'better-engineering', 'oncall: pt2']",1
pytorch/pytorch,Dump bytecode of resumption frames in tlparse,"### ðŸ› Describe the bug

Since resumption frames don't exist in the source code, having access to their bytecode can be useful for diagnosing Dynamo bytecode problems.

### Versions

main

cc @chauhang @penguinwu @voznesenskym @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @amjames @rec","['good first issue', 'module: logging', 'triaged', 'oncall: pt2', 'module: dynamo']",1
pytorch/pytorch,"bmm, topk, cholesky, linalg.norm, max with out variants set causing recompilations in torch.compile","### ðŸ› Describe the bug

Out variants of following ops are causing extra recompilations (in 3rd iteration) in torch.compile when compared to not using out variant,
torch.bmm
torch.topk
torch.cholesky
torch.linalg.norm
torch.max

### Error logs

```
I0912 10:49:41.653000 29510 torch/_dynamo/logging.py:57] [0/0] Step 1: torchdynamo start tracing topk_func <ipython-input-11-cf34f6cf9be7>:9
I0912 10:49:41.684000 29510 torch/_dynamo/logging.py:57] [0/0] Step 1: torchdynamo done tracing topk_func (RETURN_VALUE)
I0912 10:49:41.692000 29510 torch/_dynamo/logging.py:57] [0/0] Step 2: calling compiler function inductor
I0912 10:49:41.787000 29510 torch/_dynamo/logging.py:57] [0/0] Step 2: done compiler function inductor
I0912 10:49:41.812000 29510 torch/fx/experimental/symbolic_shapes.py:3646] [0/0] produce_guards
I0912 10:49:41.833000 29510 torch/_dynamo/logging.py:57] [0/1] Step 1: torchdynamo start tracing topk_func <ipython-input-11-cf34f6cf9be7>:9
I0912 10:49:41.872000 29510 torch/fx/experimental/symbolic_shapes.py:3557] [0/1] create_symbol s0 = 7 for L['input'].size()[0] [2, int_oo] at <ipython-input-11-cf34f6cf9be7>:10 in topk_func (_dynamo/variables/builder.py:2711 in <lambda>), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=""s0""
I0912 10:49:41.895000 29510 torch/fx/experimental/symbolic_shapes.py:4857] [0/1] set_replacement s0 = 7 (range_refined_to_singleton) VR[7, 7]
I0912 10:49:41.902000 29510 torch/fx/experimental/symbolic_shapes.py:5106] [0/1] eval Eq(s0, 7) [guard added] at <ipython-input-11-cf34f6cf9be7>:10 in topk_func (utils/_stats.py:21 in wrapper), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=""Eq(s0, 7)""
I0912 10:49:41.908000 29510 torch/_dynamo/logging.py:57] [0/1] Step 1: torchdynamo done tracing topk_func (RETURN_VALUE)
I0912 10:49:41.915000 29510 torch/_dynamo/logging.py:57] [0/1] Step 2: calling compiler function inductor
Iter 1: No of recompiles: 0
I0912 10:49:42.168000 29510 torch/_dynamo/logging.py:57] [0/1] Step 2: done compiler function inductor
I0912 10:49:42.199000 29510 torch/fx/experimental/symbolic_shapes.py:3646] [0/1] produce_guards
I0912 10:49:42.234000 29510 torch/_dynamo/logging.py:57] [0/2] Step 1: torchdynamo start tracing topk_func <ipython-input-11-cf34f6cf9be7>:9
I0912 10:49:42.277000 29510 torch/fx/experimental/symbolic_shapes.py:3557] [0/2] create_symbol s0 = 9 for L['input'].size()[0] [2, int_oo] at <ipython-input-11-cf34f6cf9be7>:10 in topk_func (_dynamo/variables/builder.py:2711 in <lambda>), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=""s0""
I0912 10:49:42.295000 29510 torch/fx/experimental/symbolic_shapes.py:4857] [0/2] set_replacement s0 = 9 (range_refined_to_singleton) VR[9, 9]
I0912 10:49:42.297000 29510 torch/fx/experimental/symbolic_shapes.py:5106] [0/2] eval Eq(s0, 9) [guard added] at <ipython-input-11-cf34f6cf9be7>:10 in topk_func (utils/_stats.py:21 in wrapper), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=""Eq(s0, 9)""
I0912 10:49:42.308000 29510 torch/_dynamo/logging.py:57] [0/2] Step 1: torchdynamo done tracing topk_func (RETURN_VALUE)
I0912 10:49:42.328000 29510 torch/_dynamo/logging.py:57] [0/2] Step 2: calling compiler function inductor
Iter 2: No of recompiles: 1
I0912 10:49:42.508000 29510 torch/_dynamo/logging.py:57] [0/2] Step 2: done compiler function inductor
I0912 10:49:42.522000 29510 torch/fx/experimental/symbolic_shapes.py:3646] [0/2] produce_guards
Iter 3: No of recompiles: 2
```

### Minified repro

torch.topk
```
import torch

def get_num_torch_recompiles():
    guard_failures = torch._dynamo.utils.guard_failures
    num_recompiles = [len(guard_failures[code]) for code in guard_failures]
    return 0 if len(num_recompiles) == 0 else max(num_recompiles)

total_comps = 0
def topk_func(input, k, out):
  torch.topk(input, k, out=out)

torch._dynamo.reset()
opt_model = torch.compile(topk_func)

values = torch.empty(3)
indices = torch.empty(3, dtype=torch.long)
x = torch.arange(1., 6.)
opt_model(x, 3, out=(values, indices))
print(f""Iter 1: No of recompiles: {get_num_torch_recompiles() - total_comps}"")
total_comps = get_num_torch_recompiles()

x = torch.arange(1., 8.)
opt_model(x, 3, out=(values, indices))
print(f""Iter 2: No of recompiles: {get_num_torch_recompiles() - total_comps}"")
total_comps = get_num_torch_recompiles()

x = torch.arange(1., 10.)
opt_model(x, 3, out=(values, indices))
print(f""Iter 3: No of recompiles: {get_num_torch_recompiles() - total_comps}"")
total_comps = get_num_torch_recompiles()
```
torch.bmm
```
import torch

def get_num_torch_recompiles():
    guard_failures = torch._dynamo.utils.guard_failures
    num_recompiles = [len(guard_failures[code]) for code in guard_failures]
    return 0 if len(num_recompiles) == 0 else max(num_recompiles)

total_comps = 0
def bmm_func(input, mat, out):
  torch.bmm(input, mat, out=out)

torch._dynamo.reset()
opt_model = torch.compile(bmm_func)

input1 = torch.randn(10, 3, 4)
mat1 = torch.randn(10, 4, 5)
out1 = torch.empty(10, 3, 5)
opt_model(input1, mat1, out=out1)
print(f""Iter 1: No of recompiles: {get_num_torch_recompiles() - total_comps}"")
total_comps = get_num_torch_recompiles()

input2 = torch.randn(12, 5, 6)
mat2 = torch.randn(12, 6, 7)
out2 = torch.empty(12, 5, 7)
opt_model(input2, mat2, out=out2)
print(f""Iter 2: No of recompiles: {get_num_torch_recompiles() - total_comps}"")
total_comps = get_num_torch_recompiles()

input3 = torch.randn(14, 7, 8)
mat3 = torch.randn(14, 8, 9)
out3 = torch.empty(14, 7, 9)
opt_model(input3, mat3, out=out3)
print(f""Iter 3: No of recompiles: {get_num_torch_recompiles() - total_comps}"")
total_comps = get_num_torch_recompiles()
```
torch.cholesky
```
import torch

def get_num_torch_recompiles():
    guard_failures = torch._dynamo.utils.guard_failures
    num_recompiles = [len(guard_failures[code]) for code in guard_failures]
    return 0 if len(num_recompiles) == 0 else max(num_recompiles)

total_comps = 0

def cholesky_func(input, out):
    torch.linalg.cholesky(input, out=out)

torch._dynamo.reset()
opt_model = torch.compile(cholesky_func)

values = torch.randn(8, 32, 32)
ifm_positive_definite = values @ values.mT + torch.eye(values.shape[-1])
opt_model(ifm_positive_definite, out=ifm_positive_definite)
print(f""Iter 1: No of recompiles: {get_num_torch_recompiles() - total_comps}"")
total_comps = get_num_torch_recompiles()

values = torch.randn(9, 32, 32)
ifm_positive_definite = values @ values.mT + torch.eye(values.shape[-1])
opt_model(ifm_positive_definite, out=ifm_positive_definite)
print(f""Iter 2: No of recompiles: {get_num_torch_recompiles() - total_comps}"")
total_comps = get_num_torch_recompiles()

values = torch.randn(10, 32, 32)
ifm_positive_definite = values @ values.mT + torch.eye(values.shape[-1])
opt_model(ifm_positive_definite, out=ifm_positive_definite)
print(f""Iter 3: No of recompiles: {get_num_torch_recompiles() - total_comps}"")
total_comps = get_num_torch_recompiles()
```
torch.linalg.norm
```
import torch

def get_num_torch_recompiles():
    guard_failures = torch._dynamo.utils.guard_failures
    num_recompiles = [len(guard_failures[code]) for code in guard_failures]
    return 0 if len(num_recompiles) == 0 else max(num_recompiles)

total_comps = 0

def model_norm(inputs, out):
  return torch.linalg.norm(inputs, ord=-1, dim=(0, 1), out=out)

torch._dynamo.reset()
opt_model = torch.compile(model_norm)

a = torch.rand((16, 16, 550), dtype=torch.bfloat16)
out = torch.empty(550, dtype=torch.bfloat16)
opt_model(a, out)
print(f""Iter 1: No of recompiles: {get_num_torch_recompiles() - total_comps}"")
total_comps = get_num_torch_recompiles()

a = torch.rand((16, 16, 32), dtype=torch.bfloat16)
out = torch.empty(32, dtype=torch.bfloat16)
opt_model(a, out)
print(f""Iter 2: No of recompiles: {get_num_torch_recompiles() - total_comps}"")
total_comps = get_num_torch_recompiles()

a = torch.rand((16, 16, 341), dtype=torch.bfloat16)
out = torch.empty(341, dtype=torch.bfloat16)
opt_model(a, out)
print(f""Iter 3: No of recompiles: {get_num_torch_recompiles() - total_comps}"")
total_comps = get_num_torch_recompiles()

a = torch.rand((16, 16, 402), dtype=torch.bfloat16)
out = torch.empty(402, dtype=torch.bfloat16)
opt_model(a, out)
print(f""Iter 4: No of recompiles: {get_num_torch_recompiles() - total_comps}"")
total_comps = get_num_torch_recompiles()
```
### Versions

```
Collecting environment information...
PyTorch version: 2.5.0.dev20240911+cpu
Is debug build: False
CUDA used to build PyTorch: Could not collect
ROCM used to build PyTorch: N/A

OS: Ubuntu 22.04.3 LTS (x86_64)
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
Clang version: 14.0.0-1ubuntu1.1
CMake version: version 3.30.3
Libc version: glibc-2.35

Python version: 3.10.12 (main, Jul 29 2024, 16:56:48) [GCC 11.4.0] (64-bit runtime)
Python platform: Linux-6.1.85+-x86_64-with-glibc2.35
Is CUDA available: False
CUDA runtime version: 12.2.140
CUDA_MODULE_LOADING set to: N/A
GPU models and configuration: Could not collect
Nvidia driver version: Could not collect
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                         x86_64
CPU op-mode(s):                       32-bit, 64-bit
Address sizes:                        46 bits physical, 48 bits virtual
Byte Order:                           Little Endian
CPU(s):                               2
On-line CPU(s) list:                  0,1
Vendor ID:                            GenuineIntel
Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz
CPU family:                           6
Model:                                79
Thread(s) per core:                   2
Core(s) per socket:                   1
Socket(s):                            1
Stepping:                             0
BogoMIPS:                             4399.99
Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities
Hypervisor vendor:                    KVM
Virtualization type:                  full
L1d cache:                            32 KiB (1 instance)
L1i cache:                            32 KiB (1 instance)
L2 cache:                             256 KiB (1 instance)
L3 cache:                             55 MiB (1 instance)
NUMA node(s):                         1
NUMA node0 CPU(s):                    0,1
Vulnerability Gather data sampling:   Not affected
Vulnerability Itlb multihit:          Not affected
Vulnerability L1tf:                   Mitigation; PTE Inversion
Vulnerability Mds:                    Vulnerable; SMT Host state unknown
Vulnerability Meltdown:               Vulnerable
Vulnerability Mmio stale data:        Vulnerable
Vulnerability Reg file data sampling: Not affected
Vulnerability Retbleed:               Vulnerable
Vulnerability Spec rstack overflow:   Not affected
Vulnerability Spec store bypass:      Vulnerable
Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers
Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)
Vulnerability Srbds:                  Not affected
Vulnerability Tsx async abort:        Vulnerable

Versions of relevant libraries:
[pip3] numpy==1.26.4
[pip3] optree==0.12.1
[pip3] torch==2.5.0.dev20240911+cpu
[pip3] torchaudio==2.4.0+cu121
[pip3] torchsummary==1.5.1
[pip3] torchvision==0.19.0+cu121
[conda] Could not collect
```

cc @ezyang @chauhang @penguinwu","['good first issue', 'triaged', 'oncall: pt2', 'module: dynamic shapes']",1
pytorch/pytorch,reduce torch.compile default logs,"### ðŸ› Describe the bug

Got these logs by default while running torch.compile: https://gist.github.com/jerryzh168/68f3a0b53908df9ae7e767cb20734eec while running this test: https://gist.github.com/jerryzh168/c2d4ce9c95d25b037a4c636a05f84fb7

### Versions

main

cc @chauhang @penguinwu @voznesenskym @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @aakhundov @ezyang @yf225 @ColinPeppler @desertfire","['good first issue', 'module: logging', 'triaged', 'better-engineering', 'oncall: pt2', 'module: inductor']",1
pytorch/pytorch,"dynamo (re)compilation issues: shape (1,1), nn.Parameter, mark_dynamic","### ðŸ› Describe the bug

There are following issues:

1. passing `Parameter` as argument instead of `Tensor` causes recompilation
2. setting `torch._dynamo.mark_dynamic` on `Parameter` has no effect
3. passing tensor of size `(1,1)` causes recompilation with a strange guard failure `- 2 <= L['x'].size()[0]`
4. using `torch._dynamo.mark_dynamic(x, 0, min=0)` causes error `not simple sympy type <class 'NoneType'>`
5. using `torch._dynamo.mark_dynamic(x, 0, min=0, max=65536)` causes error `CONSTRAINTS_VIOLATED`

I've checked on 2.4.0 and on nightly

```python
import torch
import torch.nn as nn
from torch import Tensor

import logging
torch._logging.set_logs(recompiles=True, recompiles_verbose = True)


def f(x):
    return x.sin() + 1.0

# def g(x):
#     return x**2

compile_args = dict(fullgraph=True, dynamic = True, backend = ""inductor"", options={'force_same_precision':True, 'disable_cpp_codegen':False, 'trace.graph_diagram':True, ""triton.cudagraphs"": False})

@torch.compile(**compile_args)
def foo(x):
    y = f(x)
    # z = g(y)
    return y

shapes = [(2,2), (3,3), (1,1)]

for P in [False, True]:
    for k, shape in enumerate(shapes):
        print(f'Shape {k}: {shape}')
        x = torch.ones(shape)
        if P:
            x = torch.nn.Parameter(x)
        for d in range(x.dim()):
            torch._dynamo.mark_dynamic(x, d) # setting min=0 errors
        z = foo(x)
```

```
Shape 0: (2, 2)
Shape 1: (3, 3)
Shape 2: (1, 1)
V0903 12:36:59.429000 140550974269248 torch/_dynamo/guards.py:2609] [0/1] [__recompiles_verbose] Recompiling function foo in /mnt/datagrid/personal/shekhovt/quant/debug/test_compile1.py:17
V0903 12:36:59.429000 140550974269248 torch/_dynamo/guards.py:2609] [0/1] [__recompiles_verbose]     triggered by the following guard failure(s):
V0903 12:36:59.429000 140550974269248 torch/_dynamo/guards.py:2609] [0/1] [__recompiles_verbose]     guard 0 failures:
V0903 12:36:59.429000 140550974269248 torch/_dynamo/guards.py:2609] [0/1] [__recompiles_verbose]     - 2 <= L['x'].size()[0]                                         # _dynamo/output_graph.py:452 in init_ambient_guards
V0903 12:36:59.429000 140550974269248 torch/_dynamo/guards.py:2609] [0/1] [__recompiles_verbose]     - 2 <= L['x'].size()[1]                                         # _dynamo/output_graph.py:452 in init_ambient_guards
Shape 0: (2, 2)
V0903 12:37:02.287000 140550974269248 torch/_dynamo/guards.py:2609] [0/2] [__recompiles_verbose] Recompiling function foo in /mnt/datagrid/personal/shekhovt/quant/debug/test_compile1.py:17
V0903 12:37:02.287000 140550974269248 torch/_dynamo/guards.py:2609] [0/2] [__recompiles_verbose]     triggered by the following guard failure(s):
V0903 12:37:02.287000 140550974269248 torch/_dynamo/guards.py:2609] [0/2] [__recompiles_verbose]     guard 0 failures:
V0903 12:37:02.287000 140550974269248 torch/_dynamo/guards.py:2609] [0/2] [__recompiles_verbose]     - expected type of 'L['x']' to be a tensor type, ' but found <class 'torch.nn.parameter.Parameter'>
V0903 12:37:02.287000 140550974269248 torch/_dynamo/guards.py:2609] [0/2] [__recompiles_verbose] 
V0903 12:37:02.287000 140550974269248 torch/_dynamo/guards.py:2609] [0/2] [__recompiles_verbose]     guard 1 failures:
V0903 12:37:02.287000 140550974269248 torch/_dynamo/guards.py:2609] [0/2] [__recompiles_verbose]     - expected type of 'L['x']' to be a tensor type, ' but found <class 'torch.nn.parameter.Parameter'>
Shape 1: (3, 3)
V0903 12:37:05.634000 140550974269248 torch/_dynamo/guards.py:2609] [0/3] [__recompiles_verbose] Recompiling function foo in /mnt/datagrid/personal/shekhovt/quant/debug/test_compile1.py:17
V0903 12:37:05.634000 140550974269248 torch/_dynamo/guards.py:2609] [0/3] [__recompiles_verbose]     triggered by the following guard failure(s):
V0903 12:37:05.634000 140550974269248 torch/_dynamo/guards.py:2609] [0/3] [__recompiles_verbose]     guard 0 failures:
V0903 12:37:05.634000 140550974269248 torch/_dynamo/guards.py:2609] [0/3] [__recompiles_verbose]     - tensor 'L['x']' size mismatch at index 0. expected 2, actual 3
V0903 12:37:05.634000 140550974269248 torch/_dynamo/guards.py:2609] [0/3] [__recompiles_verbose] 
V0903 12:37:05.634000 140550974269248 torch/_dynamo/guards.py:2609] [0/3] [__recompiles_verbose]     guard 1 failures:
V0903 12:37:05.634000 140550974269248 torch/_dynamo/guards.py:2609] [0/3] [__recompiles_verbose]     - expected type of 'L['x']' to be a tensor type, ' but found <class 'torch.nn.parameter.Parameter'>
V0903 12:37:05.634000 140550974269248 torch/_dynamo/guards.py:2609] [0/3] [__recompiles_verbose] 
V0903 12:37:05.634000 140550974269248 torch/_dynamo/guards.py:2609] [0/3] [__recompiles_verbose]     guard 2 failures:
V0903 12:37:05.634000 140550974269248 torch/_dynamo/guards.py:2609] [0/3] [__recompiles_verbose]     - expected type of 'L['x']' to be a tensor type, ' but found <class 'torch.nn.parameter.Parameter'>
Shape 2: (1, 1)
V0903 12:37:08.674000 140550974269248 torch/_dynamo/guards.py:2609] [0/4] [__recompiles_verbose] Recompiling function foo in /mnt/datagrid/personal/shekhovt/quant/debug/test_compile1.py:17
V0903 12:37:08.674000 140550974269248 torch/_dynamo/guards.py:2609] [0/4] [__recompiles_verbose]     triggered by the following guard failure(s):
V0903 12:37:08.674000 140550974269248 torch/_dynamo/guards.py:2609] [0/4] [__recompiles_verbose]     guard 0 failures:
V0903 12:37:08.674000 140550974269248 torch/_dynamo/guards.py:2609] [0/4] [__recompiles_verbose]     - tensor 'L['x']' size mismatch at index 0. expected 3, actual 1
V0903 12:37:08.674000 140550974269248 torch/_dynamo/guards.py:2609] [0/4] [__recompiles_verbose] 
V0903 12:37:08.674000 140550974269248 torch/_dynamo/guards.py:2609] [0/4] [__recompiles_verbose]     guard 1 failures:
V0903 12:37:08.674000 140550974269248 torch/_dynamo/guards.py:2609] [0/4] [__recompiles_verbose]     - tensor 'L['x']' size mismatch at index 0. expected 2, actual 1
V0903 12:37:08.674000 140550974269248 torch/_dynamo/guards.py:2609] [0/4] [__recompiles_verbose] 
V0903 12:37:08.674000 140550974269248 torch/_dynamo/guards.py:2609] [0/4] [__recompiles_verbose]     guard 2 failures:
V0903 12:37:08.674000 140550974269248 torch/_dynamo/guards.py:2609] [0/4] [__recompiles_verbose]     - expected type of 'L['x']' to be a tensor type, ' but found <class 'torch.nn.parameter.Parameter'>
V0903 12:37:08.674000 140550974269248 torch/_dynamo/guards.py:2609] [0/4] [__recompiles_verbose] 
V0903 12:37:08.674000 140550974269248 torch/_dynamo/guards.py:2609] [0/4] [__recompiles_verbose]     guard 3 failures:
V0903 12:37:08.674000 140550974269248 torch/_dynamo/guards.py:2609] [0/4] [__recompiles_verbose]     - expected type of 'L['x']' to be a tensor type, ' but found <class 'torch.nn.parameter.Parameter'>
```

### Versions

Collecting environment information...
PyTorch version: 2.5.0.dev20240902+cu118
Is debug build: False
CUDA used to build PyTorch: 11.8
ROCM used to build PyTorch: N/A

OS: Ubuntu 22.04.4 LTS (x86_64)
GCC version: (GCC) 13.2.0
Clang version: Could not collect
CMake version: version 3.22.1
Libc version: glibc-2.35

Python version: 3.11.5 (main, Oct  2 2023, 09:22:39) [GCC 13.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-1062-nvidia-x86_64-with-glibc2.35
Is CUDA available: True
CUDA runtime version: 12.2.140
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: 
GPU 0: NVIDIA A100-SXM4-40GB
GPU 1: NVIDIA A100-SXM4-40GB
GPU 2: NVIDIA A100-SXM4-40GB
GPU 3: NVIDIA DGX Display
GPU 4: NVIDIA A100-SXM4-40GB

Nvidia driver version: 535.183.01
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                         x86_64
CPU op-mode(s):                       32-bit, 64-bit
Address sizes:                        43 bits physical, 48 bits virtual
Byte Order:                           Little Endian
CPU(s):                               128
On-line CPU(s) list:                  0-127
Vendor ID:                            AuthenticAMD
Model name:                           AMD EPYC 7742 64-Core Processor
CPU family:                           23
Model:                                49
Thread(s) per core:                   2
Core(s) per socket:                   64
Socket(s):                            1
Stepping:                             0
Frequency boost:                      enabled
CPU max MHz:                          2250,0000
CPU min MHz:                          1500,0000
BogoMIPS:                             4491.73
Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es
Virtualization:                       AMD-V
L1d cache:                            2 MiB (64 instances)
L1i cache:                            2 MiB (64 instances)
L2 cache:                             32 MiB (64 instances)
L3 cache:                             256 MiB (16 instances)
NUMA node(s):                         1
NUMA node0 CPU(s):                    0-127
Vulnerability Gather data sampling:   Not affected
Vulnerability Itlb multihit:          Not affected
Vulnerability L1tf:                   Not affected
Vulnerability Mds:                    Not affected
Vulnerability Meltdown:               Not affected
Vulnerability Mmio stale data:        Not affected
Vulnerability Reg file data sampling: Not affected
Vulnerability Retbleed:               Mitigation; untrained return thunk; SMT enabled with STIBP protection
Vulnerability Spec rstack overflow:   Mitigation; safe RET
Vulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp
Vulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:             Mitigation; Retpolines; IBPB conditional; STIBP always-on; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected
Vulnerability Srbds:                  Not affected
Vulnerability Tsx async abort:        Not affected

Versions of relevant libraries:
[pip3] numpy==1.26.4
[pip3] pytorch-triton==3.0.0+dedb7bdf33
[pip3] torch==2.5.0.dev20240902+cu118
[pip3] torchaudio==2.5.0.dev20240902+cu118
[pip3] torchvision==0.20.0.dev20240902+cu118
[conda] Could not collect


cc @chauhang @penguinwu @voznesenskym @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @amjames @Lucaskabela @ezyang @bobrenjc93","['good first issue', 'triaged', 'oncall: pt2', 'module: dynamo', 'recompilations']",1
pytorch/pytorch,Cannot Convert Pytorch model with fft_rfftn layers  to ONNX using latest torch.onnx.dynamo_export,"### ðŸ› Describe the bug

I am trying to convert the well renowned [LAMA Inpainting](https://github.com/advimman/lama) model to ONNX via the new dynamo_export, as earlier these fft_rfftn were not supported by onnx there wasn't a way to export this model to ONNX, although there was a workaround by adding a custom [FourierUnitJIT](https://github.com/Carve-Photos/lama/blob/f5fb39a18022c34a71bf9a47a6ec393c804b49ca/saicinpainting/training/modules/ffc.py#L153) class which made it compatible with older torch.onnx.export itself but has accuracy losses.
Conversion [Notebook ](https://github.com/Carve-Photos/lama/blob/main/export_LaMa_to_onnx.ipynb )of older approach.

As now we do have the support of these layers in onnx and dynamo_export allows us to export these, there is still some issue 

Minimum Reproducible Code:
```bash
!curl -LJO ""https://github.com/Sanster/models/releases/download/add_big_lama/big-lama.pt""
```
```py
import torch
model = torch.jit.load(""big-lama.pt"", map_location=""cpu"")

dummy_image = torch.randn(1, 3, 512, 512)
dummy_mask = torch.randint(0, 2, (1, 1, 512, 512)).float()

onnx_program = torch.onnx.dynamo_export(model, (dummy_image, dummy_mask))
```
Torch ONNX Dynamo gives an error stating 
OnnxExporterError: Failed to export the model to ONNX. Generating SARIF report at 'report_dynamo_export.sarif'. SARIF is a standard format for the output of static analysis tools
[SARIF REPORT](https://drive.google.com/file/d/1uF9JO-Kn83qIhEYe2ggXNb7V_q08jMoa/view?usp=sharing)

```py
torch.onnx.export(model, (dummy_image, dummy_mask), ""lama_inpainting.onnx"", 
                    opset_version = 18,
                    input_names=[""image"", ""mask""], 
                    output_names=[""output""],
                    dynamic_axes={'image': {0: 'batch_size'},
                                'mask': {0: 'batch_size'},
                                'output': {0: 'batch_size'}})
```
Tried using the old onnx exporter but obviously it wont work as stated in #107588 and the code errors out stating 'aten::fft_rfftn' is not supported as expected 

### Versions

Collecting environment information...
PyTorch version: 2.4.0+cu121
Is debug build: False
CUDA used to build PyTorch: 12.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 22.04.4 LTS (x86_64)
GCC version: Could not collect
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.35

Python version: 3.11.9 | packaged by conda-forge | (main, Apr 19 2024, 18:36:13) [GCC 12.3.0] (64-bit runtime)
Python platform: Linux-6.8.0-40-generic-x86_64-with-glibc2.35
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA L4
Nvidia driver version: 535.183.01
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                         x86_64
CPU op-mode(s):                       32-bit, 64-bit
Address sizes:                        43 bits physical, 48 bits virtual
Byte Order:                           Little Endian
CPU(s):                               128
On-line CPU(s) list:                  0-127
Vendor ID:                            AuthenticAMD
Model name:                           AMD EPYC 7742 64-Core Processor
CPU family:                           23
Model:                                49
Thread(s) per core:                   2
Core(s) per socket:                   64
Socket(s):                            1
Stepping:                             0
Frequency boost:                      enabled
CPU max MHz:                          2250.0000
CPU min MHz:                          1500.0000
BogoMIPS:                             4491.48
Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sev sev_es
Virtualization:                       AMD-V
L1d cache:                            2 MiB (64 instances)
L1i cache:                            2 MiB (64 instances)
L2 cache:                             32 MiB (64 instances)
L3 cache:                             256 MiB (16 instances)
NUMA node(s):                         1
NUMA node0 CPU(s):                    0-127
Vulnerability Gather data sampling:   Not affected
Vulnerability Itlb multihit:          Not affected
Vulnerability L1tf:                   Not affected
Vulnerability Mds:                    Not affected
Vulnerability Meltdown:               Not affected
Vulnerability Mmio stale data:        Not affected
Vulnerability Reg file data sampling: Not affected
Vulnerability Retbleed:               Mitigation; untrained return thunk; SMT enabled with STIBP protection
Vulnerability Spec rstack overflow:   Mitigation; Safe RET
Vulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl
Vulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:             Mitigation; Retpolines; IBPB conditional; STIBP always-on; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected
Vulnerability Srbds:                  Not affected
Vulnerability Tsx async abort:        Not affected

Versions of relevant libraries:
[pip3] numpy==1.26.4
[pip3] onnx==1.16.2
[pip3] onnxscript==0.1.0.dev20240817
[pip3] torch==2.4.0
[pip3] torchvision==0.19.0
[pip3] triton==3.0.0
[conda] numpy                     1.26.4                   pypi_0    pypi
[conda] torchvision               0.19.0                   pypi_0    pypi

cc @justinchuby @titaiwangms","['module: onnx', 'good first issue', 'triaged', 'OSS contribution wanted']",1
pytorch/pytorch,[BE] Deduplicate auto_functionalized and triton_kernel_wrapper_functional,"These do the same thing - they're a functional wrapper around something that is mutable.

cc @chauhang @penguinwu @bdhirsh @oulgen @aakhundov @davidberard98 @ezyang","['good first issue', 'triaged', 'actionable', 'oncall: pt2', 'module: pt2-dispatcher', 'module: user triton']",1
pytorch/pytorch,[export] Schematize nn_module_stack serialization,"### ðŸ› Describe the bug

We should schematize nn_module_stack serialization so that we're not [splitting based on commas / characters](https://github.com/pytorch/pytorch/blob/f901b020666a3cf239fbb9070e3317d453256df7/torch/_export/serde/serialize.py#L570-L585) as this is error prone.

An example is https://fb.workplace.com/groups/1075192433118967/permalink/1471290750175798/ which fails during deserialization [here](https://github.com/pytorch/pytorch/blob/f901b020666a3cf239fbb9070e3317d453256df7/torch/_export/serde/serialize.py#L2154-L2158) due to nn_module_stack containing commas:
```
{L__self__,,torch.nn.modules.container.Sequential;
fn,L['fn'],torch.nn.modules.container.Sequential;
fn_0,getattr(L['fn'], '0'),torch.nn.modules.container.Sequential;
fn_0_0,getattr(getattr(L['fn'], '0'), '0'),on_device_ai.application_profile_runs.sweeper_run.Conv2dBNReLU;
getattr_getattr_L__fn_____0_____0___core,getattr(getattr(L['fn'], '0'), '0').core,torch.nn.modules.container.Sequential;
getattr_getattr_L__fn_____0_____0___core_0,getattr(getattr(getattr(L['fn'], '0'), '0').core, '0'),torch.nn.modules.conv.Conv2d}
```

### Versions

main

cc @avikchaudhuri @gmagogsfm @zhxchen17 @tugsbayasgalan @suo @ydwu4 @penguinwu","['good first issue', 'export-triaged', 'oncall: export']",1
pytorch/pytorch,Using an empty tensor and `torch.int64` or `torch.bool` for `dtype` of `nanmean()` works while a non-empty tensor doesn't work,"### ðŸ› Describe the bug

Using a non-empty tensor and `torch.int64` or `torch.bool` for `dtype` of [nanmean()](https://pytorch.org/docs/stable/generated/torch.nanmean.html) gets the errors as shown below:

```python
import torch

my_tensor = torch.tensor([0., 1., 2.])

torch.nanmean(input=my_tensor, dtype=torch.int64) # Error

torch.nanmean(input=my_tensor, dtype=torch.bool) # Error
```

> RuntimeError: ""nansum_cpu"" not implemented for 'Long'

> RuntimeError: ""nansum_cpu"" not implemented for 'Bool'

But, using an empty tensor and `torch.int64` or `torch.bool` for `dtype` of `nanmean()` works even though the results are `tensor(nan)` which are not converted to `torch.int64` or `torch.bool` as shown below:

```python
import torch

my_tensor = torch.tensor([])
my_tensor = torch.tensor([[]])
my_tensor = torch.tensor([[[]]])

torch.nanmean(input=my_tensor, dtype=torch.int64)
torch.nanmean(input=my_tensor, dtype=torch.bool)
# tensor(nan) 
```

### Versions

```python
import torch

torch.__version__ # 2.3.1+cu121
```

cc @malfet","['good first issue', 'module: error checking', 'triaged', 'module: boolean tensor']",1
pytorch/pytorch,torch.fx.Tracer.record_stack_traces is broken in torch 2.4.0,"### ðŸ› Describe the bug

`torch.fx.Tracer` has a `record_stack_traces` member. It does not give the correct stack trace, starting with torch  2.4 RC.

I am not sure it is part of the external API, but it was nice to have. The following is working in 2.3, but broken in 2.4RC


```python
import torch.fx
class M1(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.linear = torch.nn.Linear(1, 1)
    def forward(self, x):
        return x + self.linear(x)
class M2(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.m1 = M1()
    def forward(self, x):
        return x + self.m1(x)
m = M2()
tracer = torch.fx.Tracer()
tracer.record_stack_traces=True
graph = tracer.trace(m)
sym=torch.fx.GraphModule(m,graph)
print(sym.print_readable())
assert 'code: return x + self.linear(x)' in sym.print_readable()
assert 'code: return x + self.m1(x)' in sym.print_readable()
```

It was broken in this PR: https://github.com/pytorch/pytorch/pull/121449/ where you stopped using the find_user_frame (or an equivalent function) to filter non user stack traces.

Something along these lines can be probably used as a fix:

```python
    def create_proxy(self, kind, target,args, kwargs, name, type_expr = None, proxy_factory_fn = None):
....
     if self.record_stack_traces and not proxy.node.stack_trace:
            proxy.node.stack_trace = _find_user_frame2_4(''.join(CapturedTraceback.extract().format()))


    def _find_user_frame2_4(self, stack_trace):
        stack_trace=stack_trace.split('\n')
        if 'torch/fx/proxy.py' in stack_trace[-3]:
            stack_trace=stack_trace[:-1]
            while len(stack_trace)>2 and '/fx/' in stack_trace[-2]:
                stack_trace=stack_trace[:-2]
        return '\n'.join(stack_trace)
```

Notes:
- I am not sure why you don't see the effect in `torch.export.export`.
- `torch.export.unflatten` looses stack trace for collapsed modules.

### Versions

PyTorch version: 2.4.0+cu121
Is debug build: False
CUDA used to build PyTorch: 12.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 22.04.3 LTS (x86_64)
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
Clang version: Could not collect
CMake version: version 3.29.2
Libc version: glibc-2.35

Python version: 3.10.6 (main, Aug 30 2022, 16:00:07) [GCC 7.5.0] (64-bit runtime)
Python platform: Linux-5.15.0-101-generic-x86_64-with-glibc2.35
Is CUDA available: False
CUDA runtime version: 12.2.91
CUDA_MODULE_LOADING set to: N/A
GPU models and configuration: 
GPU 0: NVIDIA A100-PCIE-40GB
GPU 1: NVIDIA A100-PCIE-40GB

Nvidia driver version: 550.54.14
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                       x86_64
CPU op-mode(s):                     32-bit, 64-bit
Address sizes:                      43 bits physical, 48 bits virtual
Byte Order:                         Little Endian
CPU(s):                             64
On-line CPU(s) list:                0-63
Vendor ID:                          AuthenticAMD
Model name:                         AMD EPYC 7502 32-Core Processor
CPU family:                         23
Model:                              49
Thread(s) per core:                 2
Core(s) per socket:                 32
Socket(s):                          1
Stepping:                           0
BogoMIPS:                           4999.98
Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es
Virtualization:                     AMD-V
L1d cache:                          1 MiB (32 instances)
L1i cache:                          1 MiB (32 instances)
L2 cache:                           16 MiB (32 instances)
L3 cache:                           128 MiB (8 instances)
NUMA node(s):                       1
NUMA node0 CPU(s):                  0-63
Vulnerability Gather data sampling: Not affected
Vulnerability Itlb multihit:        Not affected
Vulnerability L1tf:                 Not affected
Vulnerability Mds:                  Not affected
Vulnerability Meltdown:             Not affected
Vulnerability Mmio stale data:      Not affected
Vulnerability Retbleed:             Mitigation; untrained return thunk; SMT enabled with STIBP protection
Vulnerability Spec rstack overflow: Mitigation; safe RET
Vulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp
Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:           Mitigation; Retpolines, IBPB conditional, STIBP always-on, RSB filling, PBRSB-eIBRS Not affected
Vulnerability Srbds:                Not affected
Vulnerability Tsx async abort:      Not affected

Versions of relevant libraries:
[pip3] mypy==1.10.0
[pip3] mypy-extensions==1.0.0
[pip3] numpy==1.26.4
[pip3] pytorch-lightning==2.2.4
[pip3] torch==2.4.0+cu121
[pip3] torchaudio==2.4.0+cu121
[pip3] torchmetrics==1.3.2
[pip3] torchvision==0.19.0+cu121
[pip3] triton==3.0.0
[conda] No relevant packages

cc @avikchaudhuri @gmagogsfm @zhxchen17 @tugsbayasgalan @angelayi @suo @ydwu4 @penguinwu","['good first issue', 'oncall: export']",1
pytorch/pytorch,Expand Tag Set: views & reductions,"### ðŸš€ The feature, motivation and pitch

`pointwise` tag has found use in multiple places. It would also be useful to have `reduction` and `view` tags. Especially the latter is often re-created adhoc.

cc @ezyang @bhosmer @smessmer @ljk53 @bdhirsh @SherlockNoMad

### Alternatives

_No response_

### Additional context

_No response_","['module: internals', 'good first issue', 'triaged']",1
pytorch/pytorch,[Dynamo] Support tracing through _get_current_dispatch_mode_stack,"### ðŸ› Describe the bug

Internal xref: https://fb.workplace.com/groups/1075192433118967/posts/1425474381424102/

Unsupported call is

```
â€œUnsupported: 'skip function _get_current_dispatch_mode_stack in file /mnt/xarfuse/uid-630643/89abe7ff-seed-nspid4026531836_cgpid307940642-ns-4026531840/torch/utils/_python_dispatch.pyâ€™
```

Since we don't really support dispatch modes in torch compiles, it should be fine to return [] here. Easy starter bug.

High priority due to internal user request

### Versions

main

cc @gchanan @zou3519 @kadeng @msaroufim @bdhirsh @anijain2305 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78","['good first issue', 'triaged', 'oncall: pt2', 'module: dynamo', 'dynamo-tensor-subclasses']",1
pytorch/pytorch,Obscure error: Expected a value of type 'List[int]' for argument 'sizes' but instead found type 'immutable_list',"### ðŸ› Describe the bug

Repro:

```
import torch
import torch._dynamo

torch._dynamo.config.capture_scalar_outputs = True

torch.library.define(""ezyang::split_with_sizes_and_clone"", ""(Tensor input, int[] sizes) -> Tensor[]"")

def split_with_sizes_and_clone(input, sizes):
    return [t.clone() for t in torch.ops.aten.split_with_sizes.default(input, sizes)]

torch.library.impl(""ezyang::split_with_sizes_and_clone"", ""default"", split_with_sizes_and_clone)

@torch.library.impl_abstract(""ezyang::split_with_sizes_and_clone"")
def split_with_sizes_and_clone_abstract(input, sizes):
    # TODO: I'm lazy
    rs = torch.ops.aten.split_with_sizes.default(input, sizes)
    return [input.new_empty(r.size()) for r in rs]

@torch.compile()
def f(sz, x):
    s0, s1 = sz.tolist()
    r0, r1 = torch.ops.ezyang.split_with_sizes_and_clone.default(x, [s0, s1])
    return torch.ops.aten.sort.default(r1)

N = 7312
S0 = 420
S1 = N - S0

f(torch.tensor([S0, S1]), torch.randn(N))
```

fails with

```
torch._dynamo.exc.TorchRuntimeError: Failed running call_function ezyang.split_with_sizes_and_clone.default(*(FakeTensor(..., size=(7312,)), [u0, u1]), **{}):
ezyang::split_with_sizes_and_clone() Expected a value of type 'List[int]' for argument 'sizes' but instead found type 'immutable_list'.
Position: 1
Value: [u0, u1]
Declaration: ezyang::split_with_sizes_and_clone(Tensor input, int[] sizes) -> Tensor[]
Cast error details: Unable to cast Python instance of type <class 'torch.fx.immutable_collections.immutable_list'> to C++ type '?' (#define PYBIND11_DETAILED_ERROR_MESSAGES or compile in debug mode for details)
```

Actually this has nothing to do with immutable_list though; I should have put SymInt in my schema rather than int. Would be good to have a better error in this case

### Versions

main

cc @chauhang @penguinwu @bobrenjc93 @zou3519 @bdhirsh @msaroufim @anijain2305","['good first issue', 'triaged', 'oncall: pt2', 'module: dynamic shapes', 'module: pt2-dispatcher', 'llm-amenable']",1
pytorch/pytorch,AOTAutograd: functionalization should emit foreach_copy_() instead of copy_() when possible,"Consider this code:
```
import torch

@torch.compile
def f(args):
    torch._foreach_mul_(args, 2)


inps = [torch.ones(10) for _ in range(10)]
f(inps)
```

Which generates this AOT graph:
```
    def forward(self, arg0_1: ""f32[10]"", arg1_1: ""f32[10]"", arg2_1: ""f32[10]"", arg3_1: ""f32[10]"", arg4_1: ""f32[10]"", arg5_1: ""f32[10]"", arg6_1: ""f32[10]"", arg7_1: ""f32[10]"", arg8_1: ""f32[10]"", arg9_1: ""f32[10]""):
        # File: /data/users/hirsheybar/a/pytorch/tmp2.py:5 in f, code: torch._foreach_mul_(args, 2)
        _foreach_mul = torch.ops.aten._foreach_mul.Scalar([arg0_1, arg1_1, arg2_1, arg3_1, arg4_1, arg5_1, arg6_1, arg7_1, arg8_1, arg9_1], 2)
        getitem: ""f32[10]"" = _foreach_mul[0]
        getitem_1: ""f32[10]"" = _foreach_mul[1]
        getitem_2: ""f32[10]"" = _foreach_mul[2]
        getitem_3: ""f32[10]"" = _foreach_mul[3]
        getitem_4: ""f32[10]"" = _foreach_mul[4]
        getitem_5: ""f32[10]"" = _foreach_mul[5]
        getitem_6: ""f32[10]"" = _foreach_mul[6]
        getitem_7: ""f32[10]"" = _foreach_mul[7]
        getitem_8: ""f32[10]"" = _foreach_mul[8]
        getitem_9: ""f32[10]"" = _foreach_mul[9];  _foreach_mul = None
        copy_: ""f32[10]"" = torch.ops.aten.copy_.default(arg0_1, getitem);  arg0_1 = getitem = None
        copy__1: ""f32[10]"" = torch.ops.aten.copy_.default(arg1_1, getitem_1);  arg1_1 = getitem_1 = None
        copy__2: ""f32[10]"" = torch.ops.aten.copy_.default(arg2_1, getitem_2);  arg2_1 = getitem_2 = None
        copy__3: ""f32[10]"" = torch.ops.aten.copy_.default(arg3_1, getitem_3);  arg3_1 = getitem_3 = None
        copy__4: ""f32[10]"" = torch.ops.aten.copy_.default(arg4_1, getitem_4);  arg4_1 = getitem_4 = None
        copy__5: ""f32[10]"" = torch.ops.aten.copy_.default(arg5_1, getitem_5);  arg5_1 = getitem_5 = None
        copy__6: ""f32[10]"" = torch.ops.aten.copy_.default(arg6_1, getitem_6);  arg6_1 = getitem_6 = None
        copy__7: ""f32[10]"" = torch.ops.aten.copy_.default(arg7_1, getitem_7);  arg7_1 = getitem_7 = None
        copy__8: ""f32[10]"" = torch.ops.aten.copy_.default(arg8_1, getitem_8);  arg8_1 = getitem_8 = None
        copy__9: ""f32[10]"" = torch.ops.aten.copy_.default(arg9_1, getitem_9);  arg9_1 = getitem_9 = None
        return ()
```

Functionalization sees that `foreach_mul_()` is an input mutation, so it adds `copy_()` nodes to the graph to apply the input mutations properly.

But if we have 100 tensors in the `TensorList` input to `foreach_mul_`, then we will end up with 100 individual `copy_()`  nodes in the graph. A larger graph generally means worse compile times for inductor, unless inductor does extra work to avoid it.

Instead, one thing we can do is recognize when a bunch of inputs were mutated by a single `foreach_*` op, and emit a single `foreach_copy_()` node in the graph in their place. We need to take care though that the inputs to the `foreach` op are preserved in the right order

cc @crcrpar @mcarilli @janeyx99 @ezyang @msaroufim @anijain2305 @zou3519","['good first issue', 'triaged', 'module: mta', 'module: functionalization', 'oncall: pt2', 'module: aotdispatch', 'module: pt2-dispatcher', 'internal ramp-up task']",1
pytorch/pytorch,Supporting custom attributes with `__torch_function__` tensor subclasses,"### ðŸš€ The feature, motivation and pitch

Currently we only support accessing attributes that are defined on the base `torch._tensor` implementation. Since some torch function subclasses define custom attributes it's a good idea to support accessing these as well. 

See https://github.com/pytorch/pytorch/blob/a468b9fbdf75b2da128e5fac4284ebbb981cd31a/torch/_dynamo/variables/torch_function.py#L180C1-L215C1 for the current implementation. 

Ideally we should support two cases 1) if a user defines a custom attribute that does *not* invoke `torch_function` on access (this is the default for an arbitrary attribute) and 2) if the user explicitly adds [this decorator](https://github.com/pytorch/pytorch/blob/a468b9fbdf75b2da128e5fac4284ebbb981cd31a/torch/_tensor.py#L31) to an attribute, we should support tracing through it and dispatching appropriately or shortcutting to the dynamo handling of it to be consistent. 

### Alternatives

_No response_

### Additional context

_No response_

cc @hameerabbasi @rgommers @peterbell10 @ezyang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng","['good first issue', 'triaged', 'enhancement', 'module: __torch_function__', 'module: dynamo']",1
pytorch/pytorch,LoweringException does not report associated user stack for code that failed,"### ðŸ› Describe the bug

When you get a lowering exception, the user stack is not reported. For example, applying this patch to force an exception:
```
diff --git a/torch/_inductor/lowering.py b/torch/_inductor/lowering.py
index 7eccd596953..ac2bf366d88 100644
--- a/torch/_inductor/lowering.py
+++ b/torch/_inductor/lowering.py
@@ -808,6 +808,7 @@ def isnan(x):

 @register_lowering(aten.ceil)
 def ceil(x):
+    assert False
     if is_integer_type(x):
         return clone(x)
     fn = ops_wrapper(""ceil"")
```

And running this script:
```
import torch

def a(x):
  return torch.ceil(x)

@torch.compile
def b(x):
  return a(x)

b(torch.tensor([1.1]))
```

It would be helpful if we used the stack information on the aten node to print out the user stack that produced this aten node. This can assist users in modifying their model to avoid whatever bug they hit.

### Versions

main

cc @chauhang @penguinwu @voznesenskym @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @aakhundov @coconutruben @msaroufim @bdhirsh @anijain2305 @zou3519 @peterbell10 @yf225 @ColinPeppler @desertfire @wconstab","['good first issue', 'module: logging', 'triaged', 'oncall: pt2', 'module: inductor']",1
pytorch/pytorch,Logging when executing fx.Interpreter,"### ðŸ› Describe the bug

Something like this:

```
diff --git a/torch/fx/interpreter.py b/torch/fx/interpreter.py
index c36c5c1004b..afc963ed545 100644
--- a/torch/fx/interpreter.py
+++ b/torch/fx/interpreter.py
@@ -7,10 +7,14 @@ from ._compatibility import compatibility
 from . import config
 import torch.fx.traceback as fx_traceback
 import torch
+from torch._logging import LazyString
 from typing import Any, Dict, Iterator, List, Optional, Tuple, Union
 import inspect
 from contextlib import contextmanager
 from torch.hub import tqdm
+import logging
+
+log = logging.getLogger(__name__)
 
 __all__ = ['Interpreter', 'Transformer']
 
@@ -194,6 +198,7 @@ class Interpreter:
         Returns:
             Any: The result of executing ``n``
         """"""
+        log.debug(""run_node %s"", LazyString(lambda: n.format_node()))
         with self._set_current_node(n):
             args, kwargs = self.fetch_args_kwargs_from_env(n)
             assert isinstance(args, tuple)
```

### Versions

main

cc @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519","['good first issue', 'module: logging', 'triaged', 'oncall: pt2']",1
pytorch/pytorch,[dynamo] Missing support for many trivial builtin functions in operator,"### ðŸ› Describe the bug

We are missing support for a few operators, operators.iconcat and operator.concat 
```python
import functools
import operator
import torch

def fn_works() -> None:
    a = list(range(1, 3))
    b = list(range(3, 6))
    c = [a, b]
    d = functools.reduce(operator.add, c, [])
    e = functools.reduce(operator.iadd, c, [])

def fn() -> None:
    a = list(range(1, 3))
    b = list(range(3, 6))
    c = [a, b]
    d = functools.reduce(operator.concat, c, [])
    e = functools.reduce(operator.iconcat, c, [])

comp_out_works = torch._dynamo.optimize(nopython=True)(fn_works)()
comp_out = torch._dynamo.optimize(nopython=True)(fn)()
```

Just glancing at operators documentation and the current builtin dynamo code: https://github.com/pytorch/pytorch/blob/c7e9c1510274184b41e408e6409f252bb1717085/torch/_dynamo/variables/builtin.py#L92

Things that work that I did not expect:
* operator.is
* operator.is_not
* `operator.__add__` # hurray for aliasing

I think we are also missing support for:
* operator.concat
* operator.iconcat
* operator.contains # almost certainly broken. Be careful this when has reversed operands
* operator.countOf
* operator.delitem # might make sense not to fold?
* operator.setitem # might make sense not to fold?
* operator.abs # surprisingly broken given builtins.abs works
* operator.length_hint # confirmed broken 
* operator.call # python 3.11+ only
* operator.attrgetter
* operator.itemgetter
* operator.methodcaller 


```python
def fn_abs() -> None:
    a = range(-10, 10)
    b = map(operator.abs, a)

comp_out = torch._dynamo.optimize(nopython=True)(fn_abs)()
```

@XuehaiPan want to take a look at this? You did a great job fixing some of the already broken ones.

Should probably done as we can just alias it to other builtin calls.
* operator.concat
* operator.iconcat
* operator.abs

### Versions

Tried on latest stable, but I am sure it's on master too from looking at the source.

cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng","['good first issue', 'triaged', 'oncall: pt2', 'module: dynamo']",1
pytorch/pytorch,Enable ruff rule PLW1510 codebase wide,"### ðŸš€ The feature, motivation and pitch

Follow up to #111682 and enable it codebase wide in pyproject.toml. Needs a lot of manual fixes to bring the benchmarking and other scripts up to standard so we can enable it. Probably a good starter task.

To complete this task,
add the error code the ruff section of the pyproject.toml
run `lintrunner -a --all-files --take RUFF` to show all the instances where it needs to be fixed.
when you fix all the instances by either adding a check=True or check=False, open a PR.

### Alternatives

_No response_

### Additional context

_No response_

cc @seemethere @malfet @pytorch/pytorch-dev-infra","['good first issue', 'module: lint', 'triaged']",1
pytorch/pytorch,refactor TracingContext to take a more limited subset of ViewAndMutationMeta,"Today, we add a `ViewAndMutationMeta` object to the TracingContext [here](https://github.com/pytorch/pytorch/blob/main/torch/_functorch/aot_autograd.py#L2122), that backends can use to ask for certain information about the graph, including information about how many / which inputs were mutated.

This technically exposes the `ViewAndMutationMeta` object to out-of-tree backends, which is bad because:
* we want to be able to add/remove fields to this object over time, and exposing this object to backends makes BC harder
* it probably exposes a lot more information than a backend actually needs

We should create a new object to represent the subset of `ViewAndMutationMeta` that inductor actually uses today, and expose that through the TracingContext instead.

cc @ezyang @msaroufim @wconstab @anijain2305 @zou3519","['good first issue', 'triaged', 'oncall: pt2', 'module: aotdispatch', 'module: pt2-dispatcher']",1
pytorch/pytorch,[PT2] [Hardening] Track recompiles alongside graph breaks in our actual/expected comparison CI runs,"### ðŸš€ The feature, motivation and pitch

An example where this was a little annoying was in debugging the problem as described in https://github.com/pytorch/pytorch/pull/111717#issuecomment-1795715999 

Being able to see both graph breaks and recompiles will give us a clearer picture.

Today, the problem is that changing PT2 to recompile more looks like a graph break increase if there are graph breaks in the frame, or is totally opaque if there are not!

**Example 1: frame a has 3 graph breaks.**

1) compile a [3 breaks]
2) recompile a [6 breaks]

6 total - what looks like a graph break regression is actually a recompile regression

**Example 2: frame b has 0 graph breaks**

1) compile b [0 breaks]
2) recompile b [0 breaks]

0 total - what was a recompile regression is missed

**Example 3 (probably rare)**

Before: 

1) compile a once [4 breaks]

After

1) compile a [2 breaks]
2) recompile a [2 breaks]

4 breaks - looks neutral, when actually both recompiles and graph breaks changed. 
 
### Alternatives

_No response_

### Additional context

_No response_

cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519 @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng","['feature', 'good first issue', 'triaged', 'oncall: pt2', 'module: dynamo']",1
pytorch/pytorch,Export swallows exception,"### ðŸ› Describe the bug

torch export on except raises another exception without including original exception.
That  hides failure information.

code point:

https://github.com/pytorch/pytorch/blob/main/torch/_dynamo/utils.py#L1412



### Versions

master

cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519 @avikchaudhuri @gmagogsfm @zhxchen17 @tugsbayasgalan @angelayi","['good first issue', 'triaged', 'oncall: pt2', 'export-triaged', 'oncall: export']",1
pytorch/pytorch,Enable more flake8-pyi ruff checks,"### ðŸš€ The feature, motivation and pitch

https://github.com/pytorch/pytorch/pull/110830 enabled a bunch of use flake8-PYI checks to improve the quality of type hints on the PyTorch codebase. While this was a good first pass, many rules need to removed from the ignore list in the pyproject.toml. Fixing these rules an enabling the lints would be a good starter task.

For other rules to enable, see this starter task: https://github.com/pytorch/pytorch/issues/106571

### Alternatives

_No response_

### Additional context

_No response_","['good first issue', 'module: lint', 'triaged', 'actionable']",1
pytorch/pytorch,switch more test cases to use MultithreadTestCase,"MultithreadTestCase allow us to run less resouce by spawning threads instead of processes, which could make distributed tests run faster. We have the following test files still not using MultithreadTestCase, and we should switch those test case to use it.

[ ] https://github.com/pytorch/pytorch/blob/main/test/distributed/_tensor/test_math_ops.py
[ ] https://github.com/pytorch/pytorch/blob/main/test/distributed/_tensor/test_matrix_ops.py
[ ] https://github.com/pytorch/pytorch/blob/main/test/distributed/_tensor/test_tensor_ops.py
[ ] https://github.com/pytorch/pytorch/blob/main/test/distributed/_tensor/test_embedding_ops.py

Example test case that already uses multithreaded test case, see https://github.com/pytorch/pytorch/blob/main/test/distributed/_tensor/test_pointwise_ops.py#L75

one just need to extend the `DTensorOpTestBase` for the above test files, should be relatively simple","['good first issue', 'triaged', 'module: dtensor']",1
pytorch/pytorch,Enable more flake8-bugbear lints,"### ðŸš€ The feature, motivation and pitch

* There are a lot of flake8-bugbear code reported by flake8-bugbear and ruff that need to be fixed. These would be a good starting issue for a new contributor.

This can be found be removing the codes from the ignore list in the .flake8 file and the .pyproject file for ruff.

- [ ] ""B007"", 
- [ ] ""B008"" 
- [ ] ""B017"",
- [ ] ""B018"", # Useless expression
- [ ] ""B019"", 
- [ ] ""B020"",
- [ ] ""B023"", 
- [ ] ""B024"", 
- [ ] ""B026"",
- [ ] ""B028"", # No explicit `stacklevel` keyword argument found
- [ ] ""B904"",

### Alternatives

_No response_

### Additional context

_No response_","['good first issue', 'module: lint', 'triaged', 'enhancement', 'better-engineering']",1
pytorch/pytorch,Command to reproduce error is incorrect,"### ðŸ› Describe the bug


```
To execute this test, run the following from the base repo dir: TEST_WITH_TORCHINDUCTOR=1 python test/test_torch.py -k test_nondeterministic_alert_NLLLoss_cuda
```

But I had to run it with 
```
PYTORCH_TEST_WITH_INDUCTOR=1 python test/test_torch.py -k test_nondeterministic_alert_NLLLoss_cuda
```

Ref CI failure: https://github.com/pytorch/pytorch/actions/runs/5678841216/job/15391208767?pr=106118
Ref PR: https://github.com/pytorch/pytorch/pull/106118

### Versions

master

cc @mruberry @ZainRizvi","['needs reproduction', 'good first issue', 'module: tests', 'triaged', 'module: infra', 'module: testing']",1
pytorch/pytorch, Improve Error Message in MultiMarginLoss for Inconsistent Target Size,"### ðŸ› Describe the bug

While using the MultiMarginLoss function in PyTorch, I encountered a RuntimeError that could be more informative. The error message received was:
```
RuntimeError: inconsistent target size, got: [7]
```
This error occurred with the following code:

```python
import torch
import torch.nn as nn

ip_size = [8,8]
target_size = [7]
input_tensor = torch.rand(ip_size)
target_tensor = torch.rand(target_size).type(torch.LongTensor)

loss_function = nn.MultiMarginLoss()

# Compute loss
loss = loss_function(input_tensor, target_tensor)
```
For a more effective debugging experience, I suggest revising the error message to provide more specifics about the size and dimension mismatches, similar to other loss functions error messages. A proposed change could be:
```python
""MultiMarginLoss: The size of input tensor ({input size}) must match the size of target tensor ({target size}) at non-singleton dimension {dimension}""
```
This message would not only communicate the problem but also provide the specific mismatched sizes and dimension, which would assist users in correcting their code.

Please let me know if there's any additional information you need or any other ways I can assist. If this behavior is expected and known, feel free to close this issue.
While this message indicates that there is an inconsistency with the target size, it does not provide specific details about the expected size or the mismatched dimensions.





### Versions

PyTorch version: 2.1.0.dev20230622+cu118
Is debug build: False
CUDA used to build PyTorch: 11.8
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0
Clang version: 12.0.0-3ubuntu1~20.04.5
CMake version: version 3.26.1
Libc version: glibc-2.31

Python version: 3.9.16 (main, Mar  8 2023, 14:00:05)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.4.0-152-generic-x86_64-with-glibc2.31
Is CUDA available: True
CUDA runtime version: 12.1.105
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration:
GPU 0: NVIDIA GeForce RTX 2070
GPU 1: NVIDIA GeForce RTX 2070
GPU 2: NVIDIA GeForce RTX 2070
GPU 3: NVIDIA GeForce RTX 2070

Nvidia driver version: 530.30.02
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.2
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.2
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.2
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.2
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.2
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.2
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.2
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                    x86_64
CPU op-mode(s):                  32-bit, 64-bit
Byte Order:                      Little Endian
Address sizes:                   46 bits physical, 48 bits virtual
CPU(s):                          32
On-line CPU(s) list:             0-31
Thread(s) per core:              2
Core(s) per socket:              8
Socket(s):                       2
NUMA node(s):                    2
Vendor ID:                       GenuineIntel
CPU family:                      6
Model:                           63
Model name:                      Intel(R) Xeon(R) CPU E5-2630 v3 @ 2.40GHz
Stepping:                        2
CPU MHz:                         1224.656
CPU max MHz:                     3200.0000
CPU min MHz:                     1200.0000
BogoMIPS:                        4794.39
Virtualization:                  VT-x
L1d cache:                       512 KiB
L1i cache:                       512 KiB
L2 cache:                        4 MiB
L3 cache:                        40 MiB
NUMA node0 CPU(s):               0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30
NUMA node1 CPU(s):               1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31
Vulnerability Itlb multihit:     KVM: Mitigation: Split huge pages
Vulnerability L1tf:              Mitigation; PTE Inversion; VMX conditional cache flushes, SMT vulnerable
Vulnerability Mds:               Mitigation; Clear CPU buffers; SMT vulnerable
Vulnerability Meltdown:          Mitigation; PTI
Vulnerability Mmio stale data:   Mitigation; Clear CPU buffers; SMT vulnerable
Vulnerability Retbleed:          Not affected
Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp
Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:        Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIBP conditional, RSB filling, PBRSB-eIBRS Not affected
Vulnerability Srbds:             Not affected
Vulnerability Tsx async abort:   Not affected
Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm cpuid_fault epb invpcid_single pti ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear flush_l1d

Versions of relevant libraries:
[pip3] numpy==1.24.1
[pip3] pytorch-triton==2.1.0+440fd1bf20
[pip3] torch==2.1.0.dev20230622+cu118
[pip3] torchaudio==2.1.0.dev20230622+cu118
[pip3] torchvision==0.16.0.dev20230622+cu118
[conda] blas                      1.0                         mkl

[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2023.1.0         h6d00ec8_46342

[conda] mkl-service               2.4.0            py39h5eee18b_1

[conda] mkl_fft                   1.3.6            py39h417a72b_1

[conda] mkl_random                1.2.2            py39h417a72b_1

[conda] numpy                     1.24.1                   pypi_0    pypi
[conda] pytorch-mutex             1.0                         cpu    pytorch
[conda] pytorch-triton            2.1.0+440fd1bf20          pypi_0    pypi
[conda] torch                     2.1.0.dev20230622+cu118          pypi_0    pypi
[conda] torchaudio                2.1.0.dev20230622+cu118          pypi_0    pypi
[conda] torchvision               0.16.0.dev20230622+cu118          pypi_0    pypi

cc @albanD @mruberry @jbschlosser @walterddr @mikaylagawarecki @malfet","['module: nn', 'good first issue', 'module: error checking', 'triaged', 'actionable']",1
pytorch/pytorch,Inductor: delete code that extracts out sizevars by inspecting tensor inputs to find a size that handled it,"### ðŸ› Describe the bug

This is no longer needed since we now pass in all free symbols as explicit arguments

### Versions

main

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @Xia-Weiwen @ngimel","['good first issue', 'triaged', 'better-engineering', 'module: inductor']",1
pytorch/pytorch,skipIfTorchInductor Tracking Issue ,"### ðŸ› Describe the bug

We have a long list of known failures in test_torch.py.

./test/test_torch.py:
- [x] 'test_scalar_check' - ""segfaults""
- [x] 'test_conv_transposed_backward_agnostic_to_memory_format' - ""Please convert all Tensors to FakeTensors""
- [ ] 'test_sync_warning' - ""FIXME""
- [x] 'test_copy_transpose_math_view' - https://github.com/pytorch/pytorch/issues/110291
- [x] 'test_errors_index_copy' - ""FIXME""
- [x] 'test_scatter_reduce_non_unique_index' - ""FIXME""
- [x] 'test_dim_function_empty' - ""RuntimeError: Trying to create tensor with negative dimension -1: [-1]"" - https://github.com/pytorch/pytorch/pull/106994
- [x] 'test_ternary_op_mem_overlap' - ""FIXME""
- [x] 'test_multinomial_device_constrain' - ""out_wrapper does not check devices correctly""
- [x] 'test_multinomial_gpu_device_constrain' - ""out_wrapper does not check devices correctly""
- [x] 'test_memory_format_propagation_rules' - ""To be supported""
- [x] 'test_memory_format_empty_like' - ""To be supported""
- [x] 'test_memory_format_operators' - ""To be supported""
- [x] 'test_pin_memory_from_constructor' - ""pin_memory isn't yet supported in TorchInductor""
- [x] 'test_memory_format_to' - ""To be supported""
- [x] 'test_memory_format_type' - ""To be supported""
- [x] 'test_memory_format_clone' - ""To be supported""
- [x] 'test_memory_format_factory_like_functions_preserve' - ""To be supported"" (@masnesral)
- [x] 'test_memory_format_type_shortcuts' - ""To be supported""
- [x] 'test_memory_format_cpu_and_cuda_ops' - ""To be supported""
- [ ] 'test_hook_remove' - ""FIXME""
- [x] 'test_assertRaisesRegex_ignore_msg_non_native_device' - ""random_.from needs to be renamed""
- [x] 'test_advancedindex_mixed_cpu_devices' - ""FIXME""
- [x] 'test_advancedindex_mixed_devices_error' - ""FIXME""
- [x] #108181 and https://github.com/pytorch/pytorch/issues/108798
- [ ] 'test_typed_storage_deprecation_warning' - ""FIXME""
- [x] 'test_pin_memory' - ""pin_memory isn't yet supported in TorchInductor""
- [x] 'test_memory_format' - ""To be supported""
- [x] 'test_upsample_nearest2d_meta' - ""https://github.com/pytorch/pytorch/issues/97414""
- [x] 'test_copy_broadcast' - ""FIXME""
- [x] 'test_copy_many_to_one' - https://github.com/pytorch/pytorch/pull/108989
- [x] 'test_copy_float16' - ""FIXME""
- [x] 'test_to' - ""FIXME""
- [x] 'test_slice' - ""FIXME""


All the non deterministic tests are low pri, but it might be easy enough to fix
- [ ] (one issue)
'test_nondeterministic_alert_AvgPool3d' - ""aot-autograd issue""
'test_nondeterministic_alert_AdaptiveAvgPool2d' - ""aot-autograd issue""
'test_nondeterministic_alert_AdaptiveAvgPool3d' - ""aot-autograd issue""
'test_nondeterministic_alert_MaxPool3d' - ""aot-autograd issue""
'test_nondeterministic_alert_AdaptiveMaxPool2d' - ""aot-autograd issue""
'test_nondeterministic_alert_FractionalMaxPool2d' - ""aot-autograd issue""
'test_nondeterministic_alert_FractionalMaxPool3d' - ""aot-autograd issue""
'test_nondeterministic_alert_interpolate_linear' - ""aot-autograd issue""
'test_nondeterministic_alert_interpolate_bilinear' - ""aot-autograd issue""
'test_deterministic_interpolate_bilinear' - ""aot-autograd issue""
'test_nondeterministic_alert_interpolate_bicubic' - ""aot-autograd issue""
'test_nondeterministic_alert_interpolate_trilinear' - ""aot-autograd issue""
'test_nondeterministic_alert_ReflectionPad1d' - ""aot-autograd issue""
'test_nondeterministic_alert_ReflectionPad2d' - ""aot-autograd issue""
'test_nondeterministic_alert_ReflectionPad3d' - ""aot-autograd issue""
'test_nondeterministic_alert_ReplicationPad1d' - ""aot-autograd issue""
'test_nondeterministic_alert_ReplicationPad2d' - ""aot-autograd issue""
'test_nondeterministic_alert_ReplicationPad3d' - ""aot-autograd issue""
'test_nondeterministic_alert_CTCLoss' - ""aot-autograd issue""
'test_nondeterministic_alert_EmbeddingBag_max' - ""aot-autograd issue""
'test_nondeterministic_alert_cumsum' - ""aot-autograd issue""
'test_nondeterministic_alert_put_accumulate' - ""warning is logged from the FallbackKernel: torch.ops.aten.put_.default when warn_only=True""
'test_nondeterministic_alert_grid_sample_2d' - ""aot-autograd issue""
'test_nondeterministic_alert_grid_sample_3d' - ""aot-autograd issue""

./test/test_modules.py:
- [x] 'test_cpu_gpu_parity' - ""to be fixed""
- [x] 'test_memory_format' - ""to be fixed""
- [x] 'test_non_contiguous_tensors - ""to be fixed""

These are lower priority than 

./test/test_ops.py
- [x] 'test_out_warning' - ""rng mismatch""
- [x] 'test_out' - ""rng mismatch""
- [x] 'test_variant_consistency_eager' - ""complex error""
- [x] 'test_complex_half_reference_testing' - ""complex error""
- [x] 'test_non_standard_bool_values' - ""Inductor does not support view with dtype yet""

Following root cause is https://github.com/pytorch/pytorch/issues/107861#issuecomment-1708955469
- [x] 'test_conj_view' - ""complex error"" (fails with aot_eager) (@int3)
- [x] 'test_neg_view' - ""grad error "" (fails with aot_eager)
- [x] 'test_neg_conj_view' - ""grad error"" (fails with aot_eager



Test that take too long (defer for now, eventually find way to run periodically)

Covered by torchinductor opinfo tests, nongoal 
- test_python_ref_meta' - ""Takes too long for inductor"" (skip or find way to run periodically)
- 'test_python_ref' - ""Takes too long for inductor"" (skip or find way to run periodically)
- 'test_python_ref_torch_fallback' - ""Takes too long for inductor"" (skip or find way to run periodically)
- 'test_python_ref_executor' - ""Takes too long for inductor"" (skip or find way to run periodically)

Would be good to enable
- [ ] 'test_python_ref_errors' - ""Takes too long for inductor"" (skip or find way to run periodically)

./test/test_ops_fwd_gradients.py:
- [ ] 'test_inplace_forward_mode_AD' - ""to be fixed"" (lower pri)

### Versions

master

cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @ngimel @yf225 @chenyang78","['good first issue', 'triaged', 'oncall: pt2', 'module: inductor']",1
pytorch/pytorch,Dynamo infers different return type vs. eager for `torch.ops.aten`,"### ðŸ› Describe the bug

```python
import torch
from torch._dynamo.backends.common import aot_autograd
from torch._functorch.aot_autograd import make_boxed_compiler
import torch._dynamo as dynamo


@make_boxed_compiler
def my_aot_autograd_backend(gm: torch.fx.GraphModule, _example_inputs):
    last_node = list(gm.graph.nodes)[-2]
    print(""dynamo return dtype"", last_node.meta[""val""].dtype)
    return gm


my_backend = aot_autograd(fw_compiler=my_aot_autograd_backend)


def compile(model, example_args):
    with torch.no_grad():
        torch._dynamo.reset()
        dynamo.optimize(my_backend, nopython=True)(model)(*example_args)


def run(model, example_args):
    print(model.__class__.__name__)
    print(
        f""eager return dtype"",
        model.forward(*example_args).dtype,
    )
    compile(model, example_args)
    print()


class BaddbmmDifferentDtypesModule(torch.nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, input, batch1, batch2):
        return torch.ops.aten.baddbmm(input, batch1, batch2)


run(
    BaddbmmDifferentDtypesModule(),
    (
        torch.randint(size=(3, 4, 5), low=0, high=10),
        torch.rand(3, 4, 6),
        torch.rand(3, 6, 5),
    ),
)


class FullModuleInt3D(torch.nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self):
        return torch.ops.aten.full([2, 3, 4], 5)


run(FullModuleInt3D(), ())


class ThresholdBackward1dIntModule(torch.nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, grad, input):
        return torch.ops.aten.threshold_backward(grad, input, 1)


run(
    ThresholdBackward1dIntModule(),
    (torch.randint(0, 10, (4,)), torch.randint(0, 8, (4,))),
)


class ThresholdBackward2dIntModule(torch.nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, grad, input):
        return torch.ops.aten.threshold_backward(grad, input, 0.5)


run(
    ThresholdBackward2dIntModule(),
    (torch.randint(0, 10, (4, 5)), torch.randint(0, 8, (4, 5))),
)


class ThresholdBackward3dIntModule(torch.nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, grad, input):
        return torch.ops.aten.threshold_backward(grad, input, 1)


run(
    ThresholdBackward2dIntModule(),
    (torch.randint(0, 10, (4, 5, 6)), torch.randint(0, 8, (4, 5, 6))),
)
```

cc @ezyang @soumith @msaroufim @wconstab @ngimel @bdhirsh @anijain2305 @SherlockNoMad @powderluv, @ramiro050

### Versions

```
Collecting environment information...
PyTorch version: 2.1.0.dev20230428+cu117
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 22.04.2 LTS (x86_64)
GCC version: (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0
Clang version: 14.0.6
CMake version: version 3.26.3
Libc version: glibc-2.35

Python version: 3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:08:06) [GCC 11.3.0] (64-bit runtime)
Python platform: Linux-5.19.0-41-generic-x86_64-with-glibc2.35
Is CUDA available: True
CUDA runtime version: 11.7.99
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3080
Nvidia driver version: 515.65.01
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                    x86_64
CPU op-mode(s):                  32-bit, 64-bit
Address sizes:                   48 bits physical, 48 bits virtual
Byte Order:                      Little Endian
CPU(s):                          32
On-line CPU(s) list:             0-31
Vendor ID:                       AuthenticAMD
Model name:                      AMD Ryzen 9 5950X 16-Core Processor
CPU family:                      25
Model:                           33
Thread(s) per core:              2
Core(s) per socket:              16
Socket(s):                       1
Stepping:                        0
Frequency boost:                 enabled
CPU max MHz:                     5083.3979
CPU min MHz:                     2200.0000
BogoMIPS:                        6787.52
Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 invpcid cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip pku ospke vaes vpclmulqdq rdpid overflow_recov succor smca
Virtualization:                  AMD-V
L1d cache:                       512 KiB (16 instances)
L1i cache:                       512 KiB (16 instances)
L2 cache:                        8 MiB (16 instances)
L3 cache:                        64 MiB (2 instances)
NUMA node(s):                    1
NUMA node0 CPU(s):               0-31
Vulnerability Itlb multihit:     Not affected
Vulnerability L1tf:              Not affected
Vulnerability Mds:               Not affected
Vulnerability Meltdown:          Not affected
Vulnerability Mmio stale data:   Not affected
Vulnerability Retbleed:          Not affected
Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl
Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:        Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIBP always-on, RSB filling, PBRSB-eIBRS Not affected
Vulnerability Srbds:             Not affected
Vulnerability Tsx async abort:   Not affected

Versions of relevant libraries:
[pip3] numpy==1.24.3
[pip3] pytorch-triton==2.1.0+7d1a95b046
[pip3] torch==2.1.0.dev20230428+cu117
[pip3] torchaudio==2.0.0.dev20230313+cu117
[pip3] torchvision==0.16.0.dev20230428+cu117
[pip3] triton==2.0.0
[conda] numpy                     1.24.3                   pypi_0    pypi
[conda] pytorch-triton            2.1.0+7d1a95b046          pypi_0    pypi
[conda] torch                     2.1.0.dev20230428+cu117          pypi_0    pypi
[conda] torchaudio                2.0.0.dev20230313+cu117          pypi_0    pypi
[conda] torchvision               0.16.0.dev20230428+cu117          pypi_0    pypi
[conda] triton                    2.0.0                    pypi_0    pypi
```","['good first issue', 'triaged', 'oncall: pt2', 'module: decompositions']",1
pytorch/pytorch,Build error on libstc++ header stl_alogbase.h on riscv,"### ðŸ› Describe the bug

I am currently trying to build PyTorch using the latest master commit (fdbc8625a1a255cc7156cbbe54f681851de96c5f) on riscv. While so far I encountered another issue that was clearly caused by compiling for riscv, this error might be a general issue:

```c++
[ 78%] Building CXX object test_api/CMakeFiles/test_api.dir/init.cpp.o
In file included from /usr/include/c++/12/memory:63,
                 from /home/user/git/pytorch/third_party/googletest/googletest/include/gtest/gtest.h:57,                                                                                                           
                 from /home/user/git/pytorch/test/cpp/api/dataloader.cpp:1:
In static member function _static _Tp* std::__copy_move<_IsMove, true, std::random_access_iterator_tag>::__copy_m(const _Tp*, const _Tp*, _Tp*) [with _Tp = long unsigned int; bool _IsMove = false]_,
    inlined from __OI std::__copy_move_a2(_II, _II, _OI) [with bool _IsMove = false; _II = const long unsigned int*; _OI = long unsigned int*]_ at /usr/include/c++/12/bits/stl_algobase.h:495:30,
    inlined from __OI std::__copy_move_a1(_II, _II, _OI) [with bool _IsMove = false; _II = const long unsigned int*; _OI = long unsigned int*]_ at /usr/include/c++/12/bits/stl_algobase.h:522:42,
    inlined from __OI std::__copy_move_a(_II, _II, _OI) [with bool _IsMove = false; _II = __gnu_cxx::__normal_iterator<const long unsigned int*, vector<long unsigned int> >; _OI = __gnu_cxx::__normal_iterator<lo
ng unsigned int*, vector<long unsigned int> >]_ at /usr/include/c++/12/bits/stl_algobase.h:529:31,
    inlined from __OI std::copy(_II, _II, _OI) [with _II = __gnu_cxx::__normal_iterator<const long unsigned int*, vector<long unsigned int> >; _OI = __gnu_cxx::__normal_iterator<long unsigned int*, vector<long u
nsigned int> >]_ at /usr/include/c++/12/bits/stl_algobase.h:620:7,
    inlined from _std::vector<_Tp, _Alloc>& std::vector<_Tp, _Alloc>::operator=(const std::vector<_Tp, _Alloc>&) [with _Tp = long unsigned int; _Alloc = std::allocator<long unsigned int>]_ at /usr/include/c++/12
/bits/vector.tcc:244:21:
/usr/include/c++/12/bits/stl_algobase.h:431:30: error: argument 1 null where non-null expected [-Werror=nonnull]
  431 |             __builtin_memmove(__result, __first, sizeof(_Tp) * _Num);
      |             ~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
/usr/include/c++/12/bits/stl_algobase.h:431:30: note: in a call to built-in function _void* __builtin_memmove(void*, const void*, long unsigned int)_
[ 78%] Building CXX object test_api/CMakeFiles/test_api.dir/jit.cpp.o
At global scope:
cc1plus: note: unrecognized command-line option _-Wno-aligned-allocation-unavailable_ may have been intended to silence earlier diagnostics
cc1plus: note: unrecognized command-line option _-Wno-unused-private-field_ may have been intended to silence earlier diagnostics
cc1plus: note: unrecognized command-line option _-Wno-invalid-partial-specialization_ may have been intended to silence earlier diagnostics
cc1plus: some warnings being treated as errors
gmake[2]: *** [test_api/CMakeFiles/test_api.dir/build.make:118: test_api/CMakeFiles/test_api.dir/dataloader.cpp.o] Error 1
gmake[2]: *** Waiting for unfinished jobs....
```

My first guess is that this is an unlucky combination of the libstc++ version, compiler version and the `-Werror=nonnull` flag. I will try to set up the same build on an x86 machine but this might take some time.  I will also try to compile using the tagged 2.0.0 version and report if this changes something.

Note for riscv compilation (In case someone wants to reproduce this exactly):
The third-party lib SLEEF (https://github.com/shibatch/sleef) will only compile with the small fix from https://github.com/shibatch/sleef/pull/448. It is e.g. possible to compile SLEEF separately with the fix included and use `USE_SYSTEM_SLEEF=ON` for compiling PyTorch.

Addition: Kineto also does not build for now, and can be disabled with `USE_KINETO=0`.


### Versions

PyTorch version: N/A
Is debug build: N/A
CUDA used to build PyTorch: N/A
ROCM used to build PyTorch: N/A

OS: Debian GNU/Linux bookworm/sid (riscv64)
GCC version: (Debian 12.2.0-10) 12.2.0
Clang version: 14.0.6
CMake version: version 3.25.1
Libc version: glibc-2.36

Python version: 3.10.9 (main, Dec  7 2022, 13:47:07) [GCC 12.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-starfive-riscv64-with-glibc2.36
Is CUDA available: N/A
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: N/A
GPU models and configuration: Could not collect
Nvidia driver version: Could not collect
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: N/A

CPU:
Architecture:        riscv64
Byte Order:          Little Endian
CPU(s):              4
On-line CPU(s) list: 0-3

Versions of relevant libraries:
[pip3] numpy==1.24.2
[conda] Could not collect


cc @malfet @seemethere","['module: build', 'good first issue', 'triaged']",1
pytorch/pytorch,Improve collectives fingerprinting,"### ðŸš€ The feature, motivation and pitch

When using `TORCH_DISTRIBUTED_DEBUG=DETAIL` we collect collectives fingerprints and those are quite helpful when troubleshooting issues like stragglers.

One recurring problem in distributed jobs are stragglers and, in special, those triggered by python GC activity. We should extend CollectiveFingerPrint to include two pieces of information: python gc counts (for all 3 gens) and some monotonic clock source.

Those would enable us us to detect such issues as part of `TORCH_DISTRIBUTED_DEBUG=DETAIL`.

One complication of this idea is that we currently compare fingerprints in a bitwise fashion, which won't work since some of this information is just advisory.



### Alternatives

_No response_

### Additional context

_No response_","['good first issue', 'triaged', 'module: c10d']",1
pytorch/pytorch,Better error message when trying to run fp16 weights on CPU,"### ðŸš€ The feature, motivation and pitch

Hey :wave: from the Hugging Face Open-Source team,

We're seeing the following issue over and over again across libraries
```
RuntimeError: ""LayerNormKernelImpl"" not implemented for 'Half'
```

or:

```
RuntimeError: ""addmm_impl_cpu_"" not implemented for 'Half'
```

E.g.: https://github.com/runwayml/stable-diffusion/issues/23

The problem here is that a PyTorch model has been converted to fp16 and the user tried to run it on CPU, e.g. the following:

```py
from torch import nn
import torch

linear = nn.Linear(2,2, dtype=torch.float16)

tensor = torch.ones((2,), dtype=torch.float16)

linear(tensor)
```

yields:

```
""addmm_impl_cpu_"" not implemented for 'Half'
```

Could we maybe catch such errors in the forward of https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module

and return a simpler error message that just says ""Float16 cannot be run on CPU""?

### Alternatives

_No response_

### Additional context

_No response_

cc @malfet","['good first issue', 'module: error checking', 'triaged']",1
pytorch/pytorch,[Inductor] [CUDA] torch.bucketize produces inconsistent results for nan values between Eager and Inductor modes on CUDA,"### ðŸ› Describe the bug

torch.bucketize exhibits inconsistent behavior when handling nanvalues between Eager mode and Inductor compiled mode on CUDA. When mathematical operations like torch.rsqrt produce nan values, torch.bucketize returns different results in compiled vs. non-compiled execution, leading to silent numerical errors.

```
import torch

# åœ¨ CUDA ä¸Šè¿è¡Œ
x = torch.tensor([-1.0], device=""cuda"")  # è´Ÿæ•°ä¼šäº§ç”Ÿ nan
thresholds = torch.tensor([0.2, 0.5, 0.8], device=""cuda"")

# Eager æ¨¡å¼
result_eager = torch.bucketize(torch.rsqrt(x), thresholds, right=True)
# è¾“å‡º: tensor([3], device='cuda:0')
print(result_eager)
# Inductor æ¨¡å¼
@torch.compile(backend=""inductor"")
def compiled_func(x, thr):
    return torch.bucketize(torch.rsqrt(x), thr, right=True)

result_inductor = compiled_func(x, thresholds)
print(result_inductor)
# è¾“å‡º: tensor([0], device='cuda:0')
# âŒ ç»“æžœä¸ä¸€è‡´ï¼
```

### Error logs

tensor([3], device='cuda:0')
tensor([0], device='cuda:0')

### Versions


PyTorch version: 2.11.0.dev20260118+cu126
Is debug build: False
CUDA used to build PyTorch: 12.6
ROCM used to build PyTorch: N/A

OS: Ubuntu 22.04.5 LTS (x86_64)
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04.2) 11.4.0
Clang version: Could not collect
CMake version: version 3.22.1
Libc version: glibc-2.35

Python version: 3.14.2 (main, Jan 14 2026, 19:38:07) [Clang 21.1.4 ] (64-bit runtime)
Python platform: Linux-5.15.0-122-generic-x86_64-with-glibc2.35
Is CUDA available: True
CUDA runtime version: 11.5.119
CUDA_MODULE_LOADING set to: 
GPU models and configuration: 
GPU 0: NVIDIA RTX A6000

Nvidia driver version: 535.183.01
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                         x86_64
CPU op-mode(s):                       32-bit, 64-bit
Address sizes:                        43 bits physical, 48 bits virtual
Byte Order:                           Little Endian
CPU(s):                               64
On-line CPU(s) list:                  0-63
Vendor ID:                            AuthenticAMD
Model name:                           AMD EPYC 7282 16-Core Processor
CPU family:                           23
Model:                                49
Thread(s) per core:                   2
Core(s) per socket:                   16
Socket(s):                            2
Stepping:                             0
Frequency boost:                      enabled
CPU max MHz:                          2800.0000
CPU min MHz:                          1500.0000
BogoMIPS:                             5600.23
Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es
Virtualization:                       AMD-V
L1d cache:                            1 MiB (32 instances)
L1i cache:                            1 MiB (32 instances)
L2 cache:                             16 MiB (32 instances)
L3 cache:                             128 MiB (8 instances)
NUMA node(s):                         2
NUMA node0 CPU(s):                    0-15,32-47
NUMA node1 CPU(s):                    16-31,48-63
Vulnerability Gather data sampling:   Not affected
Vulnerability Itlb multihit:          Not affected
Vulnerability L1tf:                   Not affected
Vulnerability Mds:                    Not affected
Vulnerability Meltdown:               Not affected
Vulnerability Mmio stale data:        Not affected
Vulnerability Reg file data sampling: Not affected
Vulnerability Retbleed:               Mitigation; untrained return thunk; SMT enabled with STIBP protection
Vulnerability Spec rstack overflow:   Mitigation; safe RET
Vulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp
Vulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:             Mitigation; Retpolines; IBPB conditional; STIBP always-on; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected
Vulnerability Srbds:                  Not affected
Vulnerability Tsx async abort:        Not affected

Versions of relevant libraries:
[pip3] No relevant packages
[conda] _anaconda_depends         2024.02             py311_mkl_1  
[conda] blas                      1.0                         mkl  
[conda] mkl                       2023.1.0         h213fc3f_46344  
[conda] mkl-service               2.4.0           py311h5eee18b_1  
[conda] mkl_fft                   1.3.8           py311h5eee18b_0  
[conda] mkl_random                1.2.4           py311hdb19cb5_0  
[conda] numpy                     2.4.0                    pypi_0    pypi
[conda] numpydoc                  1.5.0           py311h06a4308_0  
[conda] torch                     2.9.1                    pypi_0    pypi
[conda] torchaudio                2.3.1                    pypi_0    pypi
[conda] torchvision               0.18.1                   pypi_0    pypi
[conda] triton                    3.5.1                    pypi_0    pypi

cc @chauhang @penguinwu",['oncall: pt2'],0
pytorch/pytorch,Seq Faults after update,"### ðŸ› Describe the bug

About two weeks ago I updated pytorch with the nightly build that I usually do every week or two. My system was working fine, not a single issue besides the one-off quirks you usually see. But after the update, I found that ComfyUI was failing to load any models. At first I thought it was the ComfyUI update that broke it, so I started there, but couldn't find any issue. The only message it was giving me was a seg fault when trying to load the model. I then turned to PyTorch and found that it would seg fault during a basic torch command.

```
import torch
torch.rand(50000,30000).cuda() @ torch.rand(30000,20000).cuda()
Segmentation fault         (core dumped) python
```


I've provided the valgrind, rocminfo, and boot entries.

Another odd thing is that rewinding the version, doesn't seem to fix the issue. I went back as far as a month and I got the exact same seg fault.

Another thing I should mention is that I did do a system update just before updating pytorch, so it could be possible that this could have happened from an AMD driver update and looking in the wrong spot. If I was smart, I would have tested torch after doing the system update before updating torch, but I have done this so many times, that I got used to it just working.

### Versions

[collected_env.txt](https://github.com/user-attachments/files/24810701/collected_env.txt)

[rocminfo.txt](https://github.com/user-attachments/files/24810749/rocminfo.txt)

[valgrind_results.txt](https://github.com/user-attachments/files/24810769/valgrind_results.txt)

[boot_options.txt](https://github.com/user-attachments/files/24810784/boot_options.txt)

cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @jataylo @hongxiayang @naromero77amd @pragupta @jerrymannil @xinyazhang","['module: rocm', 'triaged']",0
pytorch/pytorch,"[DTensor] Gradient of unused output becomes plain torch.Tensor in backward, causing type mismatch with torch.compile","### ðŸ› Describe the bug

**Summary**

When an op with DTensor inputs returns multiple DTensor outputs and one output is unused in the downstream computation, the gradient for that unused output is returned as a plain torch.Tensor of zeros instead of a DTensor. This causes a type mismatch during the backward pass when combined with torch.compile.

**Reproduce**

Mini repro: https://gist.github.com/fegin/ac5ded1a7bdb8686a94357043ef74b50

Command: `torchrun --nproc-per-node 4 dtensor_unused_output.py`

Error message:
```
[rank0]:   File ""/data/users/chienchin/mywork3/pytorch/torch/_functorch/_aot_autograd/runtime_wrappers.py"", line 2526, in backward
[rank0]:     all_args = _backward_prologue_functional(
[rank0]:   File ""/data/users/chienchin/mywork3/pytorch/torch/_functorch/_aot_autograd/runtime_wrappers.py"", line 1972, in _backward_prologue_functional
[rank0]:     flat_processed_tangents = list(
[rank0]:   File ""/data/users/chienchin/mywork3/pytorch/torch/_functorch/_aot_autograd/runtime_wrappers.py"", line 1975, in <genexpr>
[rank0]:     AOTDispatchAutograd.process_runtime_tangent(
[rank0]:   File ""/data/users/chienchin/mywork3/pytorch/torch/_functorch/_aot_autograd/runtime_wrappers.py"", line 2238, in process_runtime_tangent
[rank0]:     raise RuntimeError(
[rank0]: RuntimeError:
[rank0]: During the backward, we encountered a tensor subclass where we guessed its
[rank0]: metadata incorrectly.

[rank0]: Expected metadata: (DTensorSpec(mesh=DeviceMesh((dp=4), 'cuda', stride=(1,)), placements=(Shard(dim=0),), tensor_meta=TensorMeta(shape=torch.Size([16, 16]), stride=(16, 1), dtype=torch.float32), shard_order=(ShardOrderEntry(tensor_dim=0, mesh_dims=(0,)),)), False), expected type: <class 'torch.distributed.tensor.DTensor'>

[rank0]: Runtime metadata: None, runtime type: <class 'torch.Tensor'>

[rank0]: shape: torch.Size([16, 16])
[rank0]: To fix this, your tensor subclass must implement the dunder method __force_to_same_metadata__.
```

### Versions

nightly

cc @H-Huang @awgu @wanchaol @fduwjj @wz337 @wconstab @d4l3k @pragupta @msaroufim @dcci @tianyu-l @XilunWu @SherlockNoMad @ppwwyyxx","['oncall: distributed', 'module: dtensor']",0
pytorch/pytorch,"Variable ""small"" in c10/cuda /CUDACachingAllocator.h collides with windows.h defining ""small"" as a macro","### ðŸ› Describe the bug

Unfortunately ""small"" is a macro in `C:\Program Files (x86)\Windows Kits\10\Include\10.0.26100.0\shared\rpcndr.h:#define small char`. `rpcndr.h` is included indirectly via `windows.h` so it's not possible to not include this on Windows. Can we rename ""small"" to something else at https://github.com/pytorch/pytorch/blob/main/c10/cuda/CUDACachingAllocator.h#L210-L215 to avoid the collision?

This problem was discovered when I updated libtorch from 2.9.1 to 2.10.0 for my codebase.

### Versions

libtorch 2.10.0

cc @malfet @seemethere @peterjc123 @mszhanyi @skyline75489 @nbcsm @iremyux @Blackhex","['module: build', 'module: windows', 'triaged', 'module: regression', 'needs research']",0
pytorch/pytorch,`maximum()`: out-of-place returns incorrect unsigned output on type promotion.,"### ðŸ› Describe the bug

Given the following CUDA tensors:

```python
# Byte tensor.
b0 = torch.tensor([1], dtype=torch.uint8, device=""cuda"")
b1 = torch.zeros(1, dtype=torch.uint8, device=""cuda"")
# Long tensor.
l0 = torch.tensor([-9], dtype=torch.int64, device=""cuda"")
```

I am getting the following outputs:

```python
>>> torch.maximum(b0, l0)
tensor([1], device='cuda:0')

>>> torch.maximum(b0, l0, out=b1)
tensor([247], device='cuda:0', dtype=torch.uint8)

>>> torch.maximum(b0.cpu(), l0.cpu(), out=b1.cpu())
tensor([1], dtype=torch.uint8)
```

The output of the `maximum(out=)` on CUDA is clearly incorrect.

**The Problem:** it seems like CUDA is not correctly promoting the inputs to the common dtype. My guess is that it should be using `iter.common_dtype()` as its `scalar_t`, instead of `iter.dtype()`.

### Versions

```python
>>> torch.__version__
'2.10.0+cu126'
```

cc @ezyang @gchanan @kadeng @msaroufim @ptrblck @eqy @jerryzh168 @tinglvv @nWEIdia","['high priority', 'triage review', 'module: cuda', 'triaged', 'actionable', 'module: correctness (silent)']",0
pytorch/pytorch,Crash with `CUDACachingAllocator.cpp` when running Nvidia `nemotron-parse` at load,"### ðŸ› Describe the bug

I am using `vllm==0.14.0` with their new `nemotron-parse` VLM ([link](https://github.com/vllm-project/vllm/pull/30864)) with `torch==2.9.1`. Under load on an [Nvidia H100 served via Modal](https://modal.com/docs/reference/modal.gpu#modalgpu), eventually `torch` hits a crash. This is not a one-off, it happens every time with heavy load (80 concurrent requests on the H100). I believe it's a symptom of an OOM, but reporting nonetheless:

```none
(EngineCore_DP0 pid=61) ERROR 01-22 20:08:25 [dump_input.py:72] Dumping input data for V1 LLM engine (v0.14.0) with config: model='nvidia/NVIDIA-Nemotron-Parse-v1.1', speculative_config=None, tokenizer='nvidia/NVIDIA-Nemotron-Parse-v1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=9000, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=nvidia/NVIDIA-Nemotron-Parse-v1.1, enable_prefix_caching=False, enable_chunked_prefill=False, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [9000], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_DECODE_ONLY: (2, 0)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 160, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None},
...
(APIServer pid=6) ERROR 01-22 20:08:25 [async_llm.py:546] vllm.v1.engine.exceptions.EngineDeadError: EngineCore encountered an issue. See stack trace (above) for the root cause.
(EngineCore_DP0 pid=61) Process EngineCore_DP0:
(EngineCore_DP0 pid=61) Traceback (most recent call last):
(EngineCore_DP0 pid=61)   File ""/usr/local/lib/python3.13/multiprocessing/process.py"", line 313, in _bootstrap
(EngineCore_DP0 pid=61)     self.run()
(EngineCore_DP0 pid=61)     ~~~~~~~~^^
...
(EngineCore_DP0 pid=61)     curr_group_outputs = model.embed_multimodal(**mm_kwargs_group)
(EngineCore_DP0 pid=61)   File ""/usr/local/lib/python3.13/site-packages/vllm/model_executor/models/nemotron_parse.py"", line 893, in embed_multimodal
(EngineCore_DP0 pid=61)     vision_embeddings = self._process_image_input(image_input)
(EngineCore_DP0 pid=61)   File ""/usr/local/lib/python3.13/site-packages/vllm/model_executor/models/nemotron_parse.py"", line 884, in _process_image_input
(EngineCore_DP0 pid=61)     return self.encoder(pixel_values)
(EngineCore_DP0 pid=61)            ~~~~~~~~~~~~^^^^^^^^^^^^^^
(EngineCore_DP0 pid=61)   File ""/usr/local/lib/python3.13/site-packages/torch/nn/modules/module.py"", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=61)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=61)            ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=61)   File ""/usr/local/lib/python3.13/site-packages/torch/nn/modules/module.py"", line 1786, in _call_impl
(EngineCore_DP0 pid=61)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=61)   File ""/usr/local/lib/python3.13/site-packages/vllm/model_executor/models/nemotron_parse.py"", line 769, in forward
(EngineCore_DP0 pid=61)     summary, feature = self.model_encoder(pixel_values)
(EngineCore_DP0 pid=61)                        ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^
(EngineCore_DP0 pid=61)   File ""/usr/local/lib/python3.13/site-packages/torch/nn/modules/module.py"", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=61)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=61)            ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=61)   File ""/usr/local/lib/python3.13/site-packages/torch/nn/modules/module.py"", line 1786, in _call_impl
(EngineCore_DP0 pid=61)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=61)   File ""/usr/local/lib/python3.13/site-packages/vllm/model_executor/models/radio.py"", line 508, in forward
(EngineCore_DP0 pid=61)     y = self.model(pixel_values)
(EngineCore_DP0 pid=61)   File ""/usr/local/lib/python3.13/site-packages/torch/nn/modules/module.py"", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=61)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=61)            ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=61)   File ""/usr/local/lib/python3.13/site-packages/torch/nn/modules/module.py"", line 1786, in _call_impl
(EngineCore_DP0 pid=61)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=61)   File ""/usr/local/lib/python3.13/site-packages/vllm/model_executor/models/radio.py"", line 465, in forward
(EngineCore_DP0 pid=61)     encoder_outputs = self.encoder(inputs_embeds=hidden_states)
(EngineCore_DP0 pid=61)   File ""/usr/local/lib/python3.13/site-packages/torch/nn/modules/module.py"", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=61)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=61)            ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=61)   File ""/usr/local/lib/python3.13/site-packages/torch/nn/modules/module.py"", line 1786, in _call_impl
(EngineCore_DP0 pid=61)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=61)   File ""/usr/local/lib/python3.13/site-packages/vllm/model_executor/models/intern_vit.py"", line 385, in forward
(EngineCore_DP0 pid=61)     hidden_states = encoder_layer(hidden_states)
(EngineCore_DP0 pid=61)   File ""/usr/local/lib/python3.13/site-packages/torch/nn/modules/module.py"", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=61)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=61)            ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=61)   File ""/usr/local/lib/python3.13/site-packages/torch/nn/modules/module.py"", line 1786, in _call_impl
(EngineCore_DP0 pid=61)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=61)   File ""/usr/local/lib/python3.13/site-packages/vllm/model_executor/models/intern_vit.py"", line 344, in forward
(EngineCore_DP0 pid=61)     hidden_states = hidden_states + self.mlp(self.norm2(hidden_states)) * self.ls2
(EngineCore_DP0 pid=61)                                     ~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=61)   File ""/usr/local/lib/python3.13/site-packages/torch/nn/modules/module.py"", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=61)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=61)            ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=61)   File ""/usr/local/lib/python3.13/site-packages/torch/nn/modules/module.py"", line 1786, in _call_impl
(EngineCore_DP0 pid=61)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=61)   File ""/usr/local/lib/python3.13/site-packages/vllm/model_executor/models/intern_vit.py"", line 270, in forward
(EngineCore_DP0 pid=61)     hidden_states = self.activation_fn(hidden_states)
(EngineCore_DP0 pid=61)   File ""/usr/local/lib/python3.13/site-packages/torch/nn/modules/module.py"", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=61)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=61)            ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=61)   File ""/usr/local/lib/python3.13/site-packages/torch/nn/modules/module.py"", line 1786, in _call_impl
(EngineCore_DP0 pid=61)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=61)   File ""/usr/local/lib/python3.13/site-packages/torch/nn/modules/activation.py"", line 816, in forward
(EngineCore_DP0 pid=61)     return F.gelu(input, approximate=self.approximate)
(EngineCore_DP0 pid=61)            ~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=61) RuntimeError: NVML_SUCCESS == DriverAPI::get()->nvmlDeviceGetHandleByPciBusId_v2_( pci_id, &nvml_device) INTERNAL ASSERT FAILED at ""/pytorch/c10/cuda/CUDACachingAllocator.cpp"":1108, please report a bug to PyTorch.
```

From then on out vLLM never recovers and all logs are just:

```none
(APIServer pid=6) INFO:     127.0.0.1:59638 - ""POST /v1/chat/completions HTTP/1.1"" 500 Internal Server Error
```

The full logs: [container_logs.txt](https://github.com/user-attachments/files/24806781/container_logs.txt)

It looks like https://github.com/pytorch/pytorch/issues/112950 was similar, but it's way old now and was closed for just needing a server reboot.

### Versions

```none
PyTorch version: 2.9.1+cu128
Is debug build: False
CUDA used to build PyTorch: 12.8
ROCM used to build PyTorch: N/A

OS: Ubuntu 24.04.2 LTS (x86_64)
GCC version: (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.39

Python version: 3.13.0 (main, Oct  8 2024, 00:56:54) [Clang 18.1.8 ] (64-bit runtime)
Python platform: Linux-4.4.0-x86_64-with-glibc2.39
Is CUDA available: True
CUDA runtime version: 12.9.41
CUDA_MODULE_LOADING set to:
GPU models and configuration: GPU 0: NVIDIA H100 80GB HBM3
Nvidia driver version: 580.95.05
cuDNN version: Could not collect
Is XPU available: False
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True
Caching allocator config: N/A

CPU:
Architecture:        x86_64
CPU op-mode(s):      32-bit, 64-bit
Address sizes:       46 bits physical, 48 bits virtual
Byte Order:          Little Endian
CPU(s):              32
On-line CPU(s) list: 0-31
Vendor ID:           AuthenticAMD
Model name:          unknown
CPU family:          175
Model:               1
Thread(s) per core:  1
Core(s) per socket:  32
Socket(s):           1
Stepping:            unknown
BogoMIPS:            2650.00
Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext perfctr_core fsgsbase bmi1 avx2 smep bmi2 invpcid rdseed adx smap clwb sha_ni xsaveopt xsavec xgetbv1 vaes vpclmulqdq rdpid
Hypervisor vendor:   KVM
Virtualization type: full

Versions of relevant libraries:
[pip3] numpy==2.2.6
[pip3] nvidia-cublas-cu12==12.8.4.1
[pip3] nvidia-cuda-cupti-cu12==12.8.90
[pip3] nvidia-cuda-nvrtc-cu12==12.8.93
[pip3] nvidia-cuda-runtime-cu12==12.8.90
[pip3] nvidia-cudnn-cu12==9.10.2.21
[pip3] nvidia-cudnn-frontend==1.17.0
[pip3] nvidia-cufft-cu12==11.3.3.83
[pip3] nvidia-curand-cu12==10.3.9.90
[pip3] nvidia-cusolver-cu12==11.7.3.90
[pip3] nvidia-cusparse-cu12==12.5.8.93
[pip3] nvidia-cusparselt-cu12==0.7.1
[pip3] nvidia-nccl-cu12==2.27.5
[pip3] nvidia-nvjitlink-cu12==12.8.93
[pip3] nvidia-nvtx-cu12==12.8.90
[pip3] open_clip_torch==3.2.0
[pip3] torch==2.9.1
[pip3] torchaudio==2.9.1
[pip3] torchvision==0.24.1
[pip3] triton==3.5.1
[conda] Could not collect
```

Here's how I set up the container on Modal:

```python
vllm_image = (
    modal.Image.from_registry(""nvidia/cuda:12.9.0-devel-ubuntu24.04"", add_python=""3.13"")
    .apt_install(
        ""libxcb1""  # Without this, vLLM inspecting ""NemotronParseForConditionalGeneration"" during `import vc` hits ""ImportError: libxcb.so.1: cannot open shared object file: No such file or directory""  # noqa: E501
    )
    .uv_pip_install(
        ""vllm==0.14.0"",  # Pin for nemotron-parse 1.1
        ""aiohttp"",
        ""albumentations"",
        ""fastapi"",
        ""open-clip-torch"",
        ""starlette"",
        ""transformers[timm]"",
        # SEE: https://docs.vllm.ai/en/latest/contributing/ci/nightly_builds/
        extra_index_url=""https://wheels.vllm.ai/0.14.0/cu129/"",
    )
    .env({
        ""HF_XET_HIGH_PERFORMANCE"": ""1"",  # Faster model transfers
        ""HF_HOME"": hf_cache_path,
        ""VLLM_CACHE_PATH"": vllm_cache_path,
        ""TORCH_CPP_LOG_LEVEL"": ""FATAL"",
    })
    .entrypoint([])  # Let this Modal script control the container startup
)

...

@app.cls(
    image=vllm_image,
    gpu=""H100"",
    cpu=16,
    memory=12 * 2**10,  # MiB (= 12 GiB)
    volumes={hf_cache_path: hf_cache_vol, vllm_cache_path: vllm_cache_vol},
    enable_memory_snapshot=ENABLE_GPU_SNAPSHOT,
    experimental_options={""enable_gpu_snapshot"": ENABLE_GPU_SNAPSHOT},
    startup_timeout=10 * 60,  # Cold start timeout should normally be less than 3-min
    scaledown_window=3 * 60,  # Keep container alive to allow for retrying or scattered scrapes
)
@modal.concurrent(max_inputs=80)
...
```",[],0
pytorch/pytorch,Dynamo errors when you `assume_constant_result` on a cached @property,"mini repro:
```
import torch
import torch.nn as nn
class MyModule(nn.Module):
    def __init__(self):
        super().__init__()
        self._cached_id = None
        self.linear = nn.Linear(10, 10)
    @property
    def cached_id(self) -> int:
        if self._cached_id is None:
            self._cached_id = 42
        return self._cached_id
    def forward(self, x):
        # Access the cached_id property - this should be constant-folded
        # We use it to index into a list to prove it's being treated as constant
        scales = [0.5, 1.0, 1.5, 2.0]
        idx = self.cached_id % len(scales)
        return self.linear(x) * scales[idx]
# Mark the property getter as constant for torch.compile
# This should avoid a recompile (where `self._cached_id` is None the first iter)
torch._dynamo.assume_constant_result(MyModule.cached_id.fget)
def main():
    model = MyModule()
    # Compile the model
    compiled_model = torch.compile(model)
    x = torch.randn(2, 10)
    out1 = compiled_model(x)
    out2 = compiled_model(x)
if __name__ == ""__main__"":
    main()
```

This crashes with:
```
  File ""/data/users/hirsheybar/new2/pytorch/torch/_dynamo/variables/user_defined.py"", line 1108, in as_python_constant
    return super().as_python_constant()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/hirsheybar/new2/pytorch/torch/_dynamo/variables/base.py"", line 365, in as_python_constant
    raise AsPythonConstantNotImplementedError(self)
torch._dynamo.exc.InternalTorchDynamoError: AsPythonConstantNotImplementedError: UnspecializedNNModuleVariable(MyModule) is not a constant

from user code:
   File ""/data/users/hirsheybar/new2/pytorch/repro_dynamo.py"", line 40, in forward
    idx = self.cached_id % len(scales)
```

cc @chauhang @penguinwu @voznesenskym @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @kadeng @amjames @Lucaskabela @jataylo","['oncall: pt2', 'module: dynamo']",0
pytorch/pytorch,Typo in torch_compiler export.md,"### ðŸ“š The doc issue

https://github.com/pytorch/pytorch/blob/d056df9f930da7616d06a8a41891c4023c8b72de/docs/source/user_guide/torch_compiler/export.md?plain=1#L19

This should be ""different inputs"" instead of ""different outputs"", to match https://github.com/pytorch/pytorch/blob/d056df9f930da7616d06a8a41891c4023c8b72de/torch/export/__init__.py#L72


### Suggest a potential alternative/fix

_No response_

cc @svekars @sekyondaMeta @AlannaBurke","['module: docs', 'triaged', 'actionable']",0
pytorch/pytorch,vLLM on CPU Crashes on models re-runs with torch 2.10,"### ðŸ› Describe the bug

While updating PyTorch version in vLLM from 2.9.1 to 2.10 we noticed that after the first run, vLLM/PyTorch would try to load the compiled model which does not happen successfully and causes the Engine to crash.

Please see: https://github.com/vllm-project/vllm/issues/32033 for reproducer and stack trace.
We hot-fixed this in vLLM by we temporarily disabling AOT Compile for CPUs in https://github.com/vllm-project/vllm/pull/32037

I'm raising this issue here for us to make sure that root cause is fixed. I'm still not sure if the fix should be in vLLM or in PyTorch


### Versions

torch 2.10, on AArch64/x86 CPUs

cc @ezyang @gchanan @kadeng @msaroufim @snadampal @milpuz01 @aditew01 @nikhil-arm @nWEIdia @chauhang @penguinwu","['high priority', 'triage review', 'needs reproduction', 'module: regression', 'oncall: pt2', 'oncall: cpu inductor']",0
pytorch/pytorch,Dynamo ONNX export of trivial model produces incorrect input name,"### ðŸ› Describe the bug

The following script runs without errors and produces two .onnx files. However the one exported with dynamo (shown to the left) has incorrect input name: ""output_samples"", while the legacy export has the correct input name.

<img width=""800"" height=""400"" alt=""Image"" src=""https://github.com/user-attachments/assets/7429a5ba-53b6-4e36-9cc0-b8c6f05efd05"" />

```python
import torch
import torch.onnx


class TrivialModel(torch.nn.Module):
    def forward(self, input_samples):
        return input_samples


example_inputs = (torch.tensor([-0.5, 0.0, 0.5]),)

# EXPORT WITH DYNAMO
onnx_program = torch.onnx.export(
    TrivialModel(),
    example_inputs,
    input_names=[""input_samples""],
    output_names=[""output_samples""],
    dynamo=True,
)
onnx_program.save(""trivial-dynamo.onnx"")

# EXPORT WITH LEGACY
torch.onnx.export(
    TrivialModel(),
    example_inputs,
    input_names=[""input_samples""],
    output_names=[""output_samples""],
    f=""trivial-legacy.onnx"",
    dynamo=False,
)

```

### Versions

```plaintext
Collecting environment information...
PyTorch version: 2.9.1+cpu
Is debug build: False
CUDA used to build PyTorch: None
ROCM used to build PyTorch: N/A

OS: Ubuntu 24.04.2 LTS (x86_64)
GCC version: (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.39

Python version: 3.12.3 (main, Jan  8 2026, 11:30:50) [GCC 13.3.0] (64-bit runtime)
Python platform: Linux-6.14.0-37-generic-x86_64-with-glibc2.39
Is CUDA available: False
CUDA runtime version: No CUDA
CUDA_MODULE_LOADING set to: N/A
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA
Is XPU available: False
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True
Caching allocator config: N/A

CPU:
Architecture:                            x86_64
CPU op-mode(s):                          32-bit, 64-bit
Address sizes:                           43 bits physical, 48 bits virtual
Byte Order:                              Little Endian
CPU(s):                                  8
On-line CPU(s) list:                     0-7
Vendor ID:                               AuthenticAMD
Model name:                              AMD Ryzen 5 3500U with Radeon Vega Mobile Gfx
CPU family:                              23
Model:                                   24
Thread(s) per core:                      2
Core(s) per socket:                      4
Socket(s):                               1
Stepping:                                1
Frequency boost:                         enabled
CPU(s) scaling MHz:                      129%
CPU max MHz:                             2100.0000
CPU min MHz:                             1400.0000
BogoMIPS:                                4192.15
Flags:                                   fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb hw_pstate ssbd ibpb vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif overflow_recov succor smca sev sev_es
Virtualization:                          AMD-V
L1d cache:                               128 KiB (4 instances)
L1i cache:                               256 KiB (4 instances)
L2 cache:                                2 MiB (4 instances)
L3 cache:                                4 MiB (1 instance)
NUMA node(s):                            1
NUMA node0 CPU(s):                       0-7
Vulnerability Gather data sampling:      Not affected
Vulnerability Ghostwrite:                Not affected
Vulnerability Indirect target selection: Not affected
Vulnerability Itlb multihit:             Not affected
Vulnerability L1tf:                      Not affected
Vulnerability Mds:                       Not affected
Vulnerability Meltdown:                  Not affected
Vulnerability Mmio stale data:           Not affected
Vulnerability Reg file data sampling:    Not affected
Vulnerability Retbleed:                  Mitigation; untrained return thunk; SMT vulnerable
Vulnerability Spec rstack overflow:      Mitigation; Safe RET
Vulnerability Spec store bypass:         Mitigation; Speculative Store Bypass disabled via prctl
Vulnerability Spectre v1:                Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:                Mitigation; Retpolines; IBPB conditional; STIBP disabled; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected
Vulnerability Srbds:                     Not affected
Vulnerability Tsa:                       Not affected
Vulnerability Tsx async abort:           Not affected
Vulnerability Vmscape:                   Mitigation; IBPB before exit to userspace

Versions of relevant libraries:
[pip3] Could not collect
[conda] Could not collect

```

cc @justinchuby @titaiwangms","['module: onnx', 'triaged']",0
pytorch/pytorch,UniformQuantizationObserverBase computes wrong scale and zero_point,"### ðŸ› Describe the bug

https://github.com/pytorch/pytorch/blob/main/torch/ao/quantization/observer.py#L407

`scale = (max_val_pos - min_val_neg) / float(quant_max - quant_min)`

This should really be

`scale = (max_val - min_val) / float(quant_max - quant_min)`

Unless I am missing something big times. We want to map [min_val, max_val] to [quant_min, quant_max].

When the original code is run on an array with large positive numbers, but from a rather narrow range, then `min_val_neg == 0`, but `min_val` is large positive. The `scale` being used is much too large then.

### Versions

Collecting environment information...
PyTorch version: 2.9.1
Is debug build: False
CUDA used to build PyTorch: None
ROCM used to build PyTorch: N/A

OS: macOS 15.7.3 (arm64)
GCC version: Could not collect
Clang version: 17.0.0 (clang-1700.6.3.2)
CMake version: Could not collect
Libc version: N/A

Python version: 3.13.2 (main, Feb  4 2025, 14:51:09) [Clang 16.0.0 (clang-1600.0.26.6)] (64-bit runtime)
Python platform: macOS-15.7.3-arm64-arm-64bit-Mach-O
Is CUDA available: False
CUDA runtime version: No CUDA
CUDA_MODULE_LOADING set to: N/A
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Apple M3 Pro

Versions of relevant libraries:
[pip3] flake8==7.3.0
[pip3] mypy_extensions==1.1.0
[pip3] numpy==2.4.0
[pip3] pytorch-lightning==2.6.0
[pip3] torch==2.9.1
[pip3] torchao==0.15.0
[pip3] torchmetrics==1.8.2
[pip3] torchvision==0.24.1
[conda] Could not collect

cc @jerryzh168 @jianyuh @raghuramank100 @jamesr66a @vkuzo @jgong5 @Xia-Weiwen @leslie-fang-intel",['oncall: quantization'],0
pytorch/pytorch,Allow privateuse1 devices in FlexAttention device validation,"### ðŸš€ The feature, motivation and pitch

FlexAttention currently restricts supported devices via a hard-coded device-type allowlist in `_validate_device`, which excludes `privateuse1` devices.

This prevents FlexAttention from being used on third-party or experimental backends that integrate with PyTorch via the `privateuse1` mechanism, even when the backend provides compatible kernels or relies on Inductor/Triton lowering.

This feature request proposes allowing `privateuse1` devices to pass the device validation step, deferring actual support checks to the backend or kernel implementation.

The intention is not to claim official support for any private backend, but to avoid prematurely blocking valid execution paths at the Python layer and to better align FlexAttention with PyTorchâ€™s extensibility model.


### Alternatives

An alternative would be to keep the current device allowlist and require private backends to modify FlexAttention locally.

However, this approach does not scale well for third-party backends and requires downstream forks or patches, whereas allowing `privateuse1` keeps the extensibility consistent with other PyTorch operators.


### Additional context

Relevant code:

https://github.com/pytorch/pytorch/blob/main/torch/nn/attention/flex_attention.py#L1318

```python
supported_devices = {""cuda"", ""cpu"", ""xpu"", ""hpu""}


cc @chauhang @penguinwu @Chillee @drisspg @yanboliang @BoyuanFeng","['triage review', 'oncall: pt2', 'module: flex attention']",0
pytorch/pytorch,[Inductor] Triton convolution template produces incorrect results for non-1x1 kernels,"The Triton convolution template in TorchInductor produces incorrect results for convolutions with kernel sizes larger than 1x1. This is due to a known Triton loop unrolling issue (triton-lang/triton#1254).

## Error

`test_convolution1` in `test/inductor/test_select_algorithm.py` fails with:

AssertionError: Incorrect result from choice TritonTemplateCaller(
BLOCK_K=16, BLOCK_M=1024, BLOCK_N=16, GROUPS=1,
KERNEL_H=3, KERNEL_W=3, PADDING_H=4, PADDING_W=5,
STRIDE_H=2, STRIDE_W=3, UNROLL=False
)
Mismatched elements: 19584 / 23120 (84.7%)
Greatest absolute difference: 132.32015991210938 at index (0, 22, 4, 13)


## To Reproduce

python test/inductor/test_select_algorithm.py TestSelectAlgorithm.test_convolution1

Root Cause
The Triton conv template (torch/_inductor/kernel/conv.py) uses UNROLL=is_ones(kernel_shape), meaning non-1x1 kernels use a non-unrolled loop pattern. This triggers incorrect results due to triton-lang/triton#1254.



cc @ezyang @gchanan @kadeng @msaroufim @chauhang @penguinwu @voznesenskym @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @muchulee8 @amjames @aakhundov @coconutruben @jataylo","['high priority', 'triage review', 'oncall: pt2', 'module: inductor']",0
pytorch/pytorch,Training accuracy may suddenly fall to near 0 in the early training stage using pytorch 2.9 and AMP (bfloat16),"### ðŸ› Describe the bug

I am training a deep convolutional neural network for a custom image classification task using ConvNext-v2 backbone. In the early training stage, for example in warmup or the first 80 epochs, the training accuracy may suddenly fall to near 0 and then recover to the original accuracy in a few epochs. I use AdamW optimizer with AMP (bfloat16), and learning rate scheduling with linear warmup and Cosine annealing, and gradients accumulation to accelerate training on a H20 gpu. The batch size is 512, and the accumulated batch size is 4096. The pytorch and CUDA version are 2.9.0 and 12.8, respectively. I have tried to extend the warmup epochs or decrease the learning rate but failed.

Curiously if I add the wandb watch function in the training loop, this phenomenon will not occur.

However, this phenomenon has never occurred before when running the same code and the same training strategy with batchsize=128 and accumulated  batchsize=1024, and pytorch+CUDA=2.6.0+12.4 on a 3090 GPU.

What are the possible reasons?

### Versions

Pytorch 2.9.0
CUDA 12.8
Python 3.12

cc @vincentqb @jbschlosser @albanD @janeyx99 @crcrpar","['needs reproduction', 'module: optimizer', 'triaged']",0
pytorch/pytorch,[INFO] Triton XPU Build Failure on SLES 15 SP4 - Blocks PyTorch Testing,"## Purpose
This is an informational issue to notify the PyTorch team about a blocking compatibility issue with Triton XPU on SLES 15 SP4, which prevents PyTorch Upstream validation with XPU support on this platform.

## Summary
Triton XPU (commit `225cdbde3ea155d5ed4c0aad1f2aa4bd2b3c4a3d`, pinned by PyTorch) cannot be built or used on SLES 15 SP4 due to GLIBC incompatibility. The issue has been reported to the Intel Triton XPU team.

## Related Issue
**Intel Triton XPU Issue**: https://github.com/intel/intel-xpu-backend-for-triton/issues/5899

## Problem Overview
- **Root cause**: Triton's `setup.py` auto-downloads pre-built Ubuntu LLVM binaries requiring GLIBC 2.32-2.34
- **SLES 15 SP4**: Ships with GLIBC 2.31
- **Impact**: PyTorch builds succeed, but Triton import fails, blocking XPU-related tests
- **Workarounds attempted**: LLVM source builds, Docker, wheel transfers - all unsuccessful

## Environment
- **OS**: SLES 15 SP4 (GLIBC 2.31)
- **Python**: 3.10.19
- **PyTorch**: Upstream build
- **Triton XPU commit**: `225cdbde3ea155d5ed4c0aad1f2aa4bd2b3c4a3d` (pinned by PyTorch)

## Error During Triton Import
```
ImportError: /lib64/libc.so.6: version `GLIBC_2.32' not found
ImportError: /lib64/libc.so.6: version `GLIBC_2.34' not found
```

## Impact on PyTorch
- **Blocks**: PyTorch Upstream validation with XPU on SLES 15 SP4
- **Scope**: All test pipelines requiring `import triton` on SLES
- **Status**: Tests suspended until Triton XPU provides SLES-compatible solution

## Why This Affects PyTorch
PyTorch depends on Triton XPU for Intel XPU backend support. While PyTorch itself builds successfully on SLES, runtime tests fail when attempting to import Triton, making it impossible to validate XPU functionality on this platform.

## Requested Actions from PyTorch Team

### Awareness
Please be aware that PyTorch testing with XPU on SLES 15 SP4 is currently blocked by this Triton XPU compatibility issue.

### Potential CI Considerations
If PyTorch CI includes SLES testing with XPU:
- Add checks to detect Triton XPU build compatibility before running XPU tests
- Consider documenting SLES as currently unsupported for XPU until Triton issue is resolved
- Monitor Intel Triton XPU repository for compatibility updates

### Documentation
Consider adding a note in PyTorch XPU documentation that:
- SLES 15 SP4 is currently not supported for XPU due to Triton dependency issues
- Ubuntu 22.04+ is recommended for XPU development
- Reference the Intel Triton XPU issue for updates

## Current Status
- Issue reported to Intel Triton XPU team: [Link pending]
- Waiting for upstream fix (SLES-compatible LLVM binaries or LLVM 22 API compatibility fixes)
- PyTorch XPU testing on SLES 15 SP4 suspended

## Notes
- This is **not a PyTorch bug** - the issue originates from Triton XPU dependency
- Ubuntu 22.04/24.04 work fine (GLIBC 2.32+)
- No workaround available without system-level changes

## Contact
- **Reporting Team**: DLPV (Deep Learning Product Validation)
- **Context**: Enterprise PyTorch testing framework
- **Platform requirement**: SLES 15 SP4 is a critical enterprise OS for validation

---

*This issue is for informational purposes. The primary issue tracking is in the Intel Triton XPU repository.*


cc @gujinghui @EikanWang @fengyuan14 @guangyey","['triaged', 'module: xpu']",0
pytorch/pytorch,pandas/numpy binary incompatibility breaks test_dataloader and test_datapipe,"Tests that import pandas fail with `ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject` 

Affected tests:
**1. **Test_dataloader :**** `test_dataloader.py::TestDataLoader::test_multiprocessing_iterdatapipe`, 'test/test_dataloader.py::TestDataLoaderPersistentWorkers::test_multiprocessing_iterdatapipe'

**2. Test_datapipe :**  'test/test_datapipe.py::TestCaptureDataFrame::test_basic_capture', 'test/test_datapipe.py::TestDataFramesPipes::test_batch', 'test/test_datapipe.py::TestDataFramesPipes::test_capture', 'test/test_datapipe.py::TestDataFramesPipes::test_collate', 'test/test_datapipe.py::TestDataFramesPipes::test_filter', 'test/test_datapipe.py::TestDataFramesPipes::test_shuffle', 'test/test_datapipe.py::TestDataFramesPipes::test_unbatch', 'test/test_datapipe.py::TestFunctionalIterDataPipe::test_filter_datapipe', 'test/test_datapipe.py::TestFunctionalIterDataPipe::test_serializable', 'test/test_datapipe.py::TestFunctionalIterDataPipe::test_serializable_with_dill', 'test/test_datapipe.py::TestSharding::test_legacy_custom_sharding', 'test/test_datapipe.py::TestSharding::test_legacy_custom_sharding_with_old_dataloader', 'test/test_datapipe.py::TestSharding::test_old_dataloader', 'test/test_datapipe.py::TestSharding::test_simple_sharding', 'test/test_datapipe.py::TestIterDataPipeSingletonConstraint::test_iterdatapipe_singleton_constraint_multiple_outputs', 'test/test_datapipe.py::TestIterDataPipeSingletonConstraint::test_iterdatapipe_singleton_generator', 'test/test_datapipe.py::TestIterDataPipeSingletonConstraint::test_iterdatapipe_singleton_new_object', 'test/test_datapipe.py::TestIterDataPipeSingletonConstraint::test_iterdatapipe_singleton_self_next'


Versions
- Python: 3.12
- numpy: 1.26.2
- pandas: 2.0.3

cc @andrewkho @divyanshk @ssnl @VitalyFedyunin @dzhulgakov @scotts @mruberry",['triaged'],0
pytorch/pytorch,Broken link to user guide for masked tensors,"### ðŸ“š The doc issue

Tried to navigate to new user guide on the [torch.masked](https://docs.pytorch.org/docs/stable/masked.html) page. Link to ""[Overview â€“ the place to start for new users, discusses how to use MaskedTensors and why theyâ€™re useful](https://pytorch.org/tutorials/prototype/maskedtensor_overview)"" was broken. 

### Suggest a potential alternative/fix

Either put in the correct link or remove the article (I would prefer you link a guide, though :) )

cc @nikitaved @pearu @cpuhrsch @amjames @bhosmer @jcaip @svekars @sekyondaMeta @AlannaBurke @jbschlosser @drisspg @soulitzer @davidberard98 @YuqingJ","['module: sparse', 'module: docs', 'triaged', 'module: nestedtensor']",0
pytorch/pytorch,[Inconsistency] TorchInductor produces incorrect result for aten.hardswish_backward followed by aten.add,"### ðŸ› Describe the bug

When compiling a function with `torch.compile`, `TorchInductor` produces results that differ from the `eager` backend when an `aten.add` operation is applied after `aten.hardswish_backward`. The inconsistency only occurs when the addition is present; removing it restores correct behavior.

### Reproduction Script
```
import torch

def model_func(grad_output, self):
    out = torch.ops.aten.hardswish_backward(grad_output, self=self,)
    out = out + 1  # inconcsistency only when add this line
    return out

op_config = {'grad_output':torch.randn([10, 20],dtype=torch.bfloat16, device='cuda'),'self':torch.randn([10, 20],dtype=torch.bfloat16, device='cuda'),}

compiled_eager = torch.compile(model_func, backend=""eager"")
out1 = compiled_eager(**op_config)
compiled_inductor = torch.compile(model_func, backend=""inductor"")
out_inductor = compiled_inductor(**op_config)
torch.testing.assert_close(out1, out_inductor)
```

### Error logs

```
Traceback (most recent call last):
  File ""test_hardswish_backward_0_1.py"", line 14, in <module>
    torch.testing.assert_close(out1, out_inductor)
  File ""/home/miniconda3/lib/python3.12/site-packages/torch/testing/_comparison.py"", line 1530, in assert_close
    raise error_metas[0].to_error(msg)
AssertionError: Tensor-likes are not close!

Mismatched elements: 2 / 200 (1.0%)
Greatest absolute difference: 0.0010986328125 at index (0, 14) (up to 1e-05 allowed)
Greatest relative difference: 0.06689453125 at index (3, 7) (up to 0.016 allowed)

```

### Versions

```
Versions of relevant libraries:
[pip3] mypy_extensions==1.1.0
[pip3] numpy==1.26.0
[pip3] nvidia-cublas-cu12==12.4.5.8
[pip3] nvidia-cuda-cupti-cu12==12.4.127
[pip3] nvidia-cuda-nvrtc-cu12==12.4.127
[pip3] nvidia-cuda-runtime-cu12==12.4.127
[pip3] nvidia-cudnn-cu12==9.1.0.70
[pip3] nvidia-cufft-cu12==11.2.1.3
[pip3] nvidia-curand-cu12==10.3.5.147
[pip3] nvidia-cusolver-cu12==11.6.1.9
[pip3] nvidia-cusparse-cu12==12.3.1.170
[pip3] nvidia-cusparselt-cu12==0.7.1
[pip3] nvidia-nccl-cu12==2.21.5
[pip3] nvidia-nvjitlink-cu12==12.4.127
[pip3] nvidia-nvtx-cu12==12.4.127
[pip3] torch==2.5.1
[pip3] triton==3.1.0
[conda] numpy                     1.26.0                   pypi_0    pypi
[conda] nvidia-cublas-cu12        12.4.5.8                 pypi_0    pypi
[conda] nvidia-cuda-cupti-cu12    12.4.127                 pypi_0    pypi
[conda] nvidia-cuda-nvrtc-cu12    12.4.127                 pypi_0    pypi
[conda] nvidia-cuda-runtime-cu12  12.4.127                 pypi_0    pypi
[conda] nvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi
[conda] nvidia-cufft-cu12         11.2.1.3                 pypi_0    pypi
[conda] nvidia-curand-cu12        10.3.5.147               pypi_0    pypi
[conda] nvidia-cusolver-cu12      11.6.1.9                 pypi_0    pypi
[conda] nvidia-cusparse-cu12      12.3.1.170               pypi_0    pypi
[conda] nvidia-cusparselt-cu12    0.7.1                    pypi_0    pypi
[conda] nvidia-nccl-cu12          2.21.5                   pypi_0    pypi
[conda] nvidia-nvjitlink-cu12     12.4.127                 pypi_0    pypi
[conda] nvidia-nvtx-cu12          12.4.127                 pypi_0    pypi
[conda] torch                     2.5.1                    pypi_0    pypi
[conda] triton                    3.1.0                    pypi_0    pypi

```

cc @chauhang @penguinwu @voznesenskym @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @kadeng @muchulee8 @amjames @aakhundov @coconutruben @jataylo","['triaged', 'oncall: pt2', 'module: inductor']",0
pytorch/pytorch,[Inconsistency] aten.linalg_pinv returns inconsistent output shape between eager and inductor,"### ðŸ› Describe the bug


When using `torch.compile` with `aten.linalg_pinv`, the output shape differs between the `eager` and `inductor` backends if rcond is provided as a tensor. This causes a shape mismatch and fails correctness checks.

### Reproduction Script


```python
import torch

def model_func(self, rcond):
    out = torch.ops.aten.linalg_pinv(self, rcond=rcond,hermitian=False)
    return out

op_config = {'self':torch.randn([8, 3],dtype=torch.float64, device='cuda'),'rcond':torch.randn([1],dtype=torch.float64, device='cuda')}

compiled_eager = torch.compile(model_func, backend=""eager"")
out1 = compiled_eager(**op_config)
compiled_inductor = torch.compile(model_func, backend=""inductor"")
out_inductor = compiled_inductor(**op_config)
torch.testing.assert_close(out1, out_inductor)

```




### Error logs

```
Traceback (most recent call last):
  File ""/linalg_pinv_3_2.py"", line 13, in <module>
    torch.testing.assert_close(out1, out_inductor)
  File ""/miniconda3/lib/python3.12/site-packages/torch/testing/_comparison.py"", line 1530, in assert_close
    raise error_metas[0].to_error(msg)
AssertionError: The values for attribute 'shape' do not match: torch.Size([3, 8]) != torch.Size([1, 3, 8]).

```

### Versions

```
Versions of relevant libraries:
[pip3] mypy_extensions==1.1.0
[pip3] numpy==1.26.0
[pip3] nvidia-cublas-cu12==12.4.5.8
[pip3] nvidia-cuda-cupti-cu12==12.4.127
[pip3] nvidia-cuda-nvrtc-cu12==12.4.127
[pip3] nvidia-cuda-runtime-cu12==12.4.127
[pip3] nvidia-cudnn-cu12==9.1.0.70
[pip3] nvidia-cufft-cu12==11.2.1.3
[pip3] nvidia-curand-cu12==10.3.5.147
[pip3] nvidia-cusolver-cu12==11.6.1.9
[pip3] nvidia-cusparse-cu12==12.3.1.170
[pip3] nvidia-cusparselt-cu12==0.7.1
[pip3] nvidia-nccl-cu12==2.21.5
[pip3] nvidia-nvjitlink-cu12==12.4.127
[pip3] nvidia-nvtx-cu12==12.4.127
[pip3] torch==2.5.1
[pip3] triton==3.1.0
[conda] numpy                     1.26.0                   pypi_0    pypi
[conda] nvidia-cublas-cu12        12.4.5.8                 pypi_0    pypi
[conda] nvidia-cuda-cupti-cu12    12.4.127                 pypi_0    pypi
[conda] nvidia-cuda-nvrtc-cu12    12.4.127                 pypi_0    pypi
[conda] nvidia-cuda-runtime-cu12  12.4.127                 pypi_0    pypi
[conda] nvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi
[conda] nvidia-cufft-cu12         11.2.1.3                 pypi_0    pypi
[conda] nvidia-curand-cu12        10.3.5.147               pypi_0    pypi
[conda] nvidia-cusolver-cu12      11.6.1.9                 pypi_0    pypi
[conda] nvidia-cusparse-cu12      12.3.1.170               pypi_0    pypi
[conda] nvidia-cusparselt-cu12    0.7.1                    pypi_0    pypi
[conda] nvidia-nccl-cu12          2.21.5                   pypi_0    pypi
[conda] nvidia-nvjitlink-cu12     12.4.127                 pypi_0    pypi
[conda] nvidia-nvtx-cu12          12.4.127                 pypi_0    pypi
[conda] torch                     2.5.1                    pypi_0    pypi
[conda] triton                    3.1.0                    pypi_0    pypi

```

cc @chauhang @penguinwu @eellison @bdhirsh @bobrenjc93","['triaged', 'oncall: pt2', 'module: fakeTensor', 'module: decompositions', 'module: pt2-dispatcher']",0
pytorch/pytorch,OOM error suggests expandable_segments even when enabled,"When a CUDA OOM error occurs, PyTorch suggests trying `PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True` to avoid fragmentation.
However, this message appears currently even if `expandable_segments` is already enabled by the user. This can be confusing.

It would be clearer if we checked `CUDAAllocatorConfig::expandable_segments()` before showing this tip, so it only appears when the feature is actually disabled.

cc @ptrblck @msaroufim @eqy @jerryzh168 @tinglvv @nWEIdia @malfet","['module: cuda', 'module: error checking', 'triaged', 'actionable', 'module: CUDACachingAllocator']",0
pytorch/pytorch,[Segfault] compilion with eager for the rnn_tanh operator leads to a segfault,"### ðŸ› Describe the bug

Executing the script below results in a `Segmentation fault (core dumped)`, leading to a process crash.
Regardless of whether the input is valid or invalid, such behavior is unsafe; the error should be properly detected and handled instead of causing abrupt termination.



```python
import torch

def model_func(data, batch_sizes, hx, params):
    out = torch.ops.aten.rnn_tanh(data, batch_sizes=batch_sizes,hx=hx,params=params,has_biases=True,num_layers=1,dropout=0,train=False,bidirectional=False)
    return out

op_config = {
        'data':torch.randn([10, 3, 8],dtype=torch.float32, device='cuda'),
        'batch_sizes':torch.randint(0, 3, [10],dtype=torch.int64, device='cuda'),
        'hx':torch.randn([1, 10, 20],dtype=torch.float32, device='cuda'),
        'params':[torch.randn([20, 8],dtype=torch.float32, device='cuda'),
            torch.randn([20, 20],dtype=torch.float32, device='cuda'),
            torch.randn([20],dtype=torch.float32, device='cuda'),
            torch.randn([20],dtype=torch.float32, device='cuda')],}
compiled_eager = torch.compile(model_func, backend=""eager"")
out = compiled_eager(**op_config)
```

### Versions

```
[pip3] mypy_extensions==1.1.0
[pip3] numpy==1.26.0
[pip3] nvidia-cublas-cu12==12.4.5.8
[pip3] nvidia-cuda-cupti-cu12==12.4.127
[pip3] nvidia-cuda-nvrtc-cu12==12.4.127
[pip3] nvidia-cuda-runtime-cu12==12.4.127
[pip3] nvidia-cudnn-cu12==9.1.0.70
[pip3] nvidia-cufft-cu12==11.2.1.3
[pip3] nvidia-curand-cu12==10.3.5.147
[pip3] nvidia-cusolver-cu12==11.6.1.9
[pip3] nvidia-cusparse-cu12==12.3.1.170
[pip3] nvidia-cusparselt-cu12==0.7.1
[pip3] nvidia-nccl-cu12==2.21.5
[pip3] nvidia-nvjitlink-cu12==12.4.127
[pip3] nvidia-nvtx-cu12==12.4.127
[pip3] torch==2.5.1
[pip3] triton==3.1.0
[conda] numpy                     1.26.0                   pypi_0    pypi
[conda] nvidia-cublas-cu12        12.4.5.8                 pypi_0    pypi
[conda] nvidia-cuda-cupti-cu12    12.4.127                 pypi_0    pypi
[conda] nvidia-cuda-nvrtc-cu12    12.4.127                 pypi_0    pypi
[conda] nvidia-cuda-runtime-cu12  12.4.127                 pypi_0    pypi
[conda] nvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi
[conda] nvidia-cufft-cu12         11.2.1.3                 pypi_0    pypi
[conda] nvidia-curand-cu12        10.3.5.147               pypi_0    pypi
[conda] nvidia-cusolver-cu12      11.6.1.9                 pypi_0    pypi
[conda] nvidia-cusparse-cu12      12.3.1.170               pypi_0    pypi
[conda] nvidia-cusparselt-cu12    0.7.1                    pypi_0    pypi
[conda] nvidia-nccl-cu12          2.21.5                   pypi_0    pypi
[conda] nvidia-nvjitlink-cu12     12.4.127                 pypi_0    pypi
[conda] nvidia-nvtx-cu12          12.4.127                 pypi_0    pypi
[conda] torch                     2.5.1                    pypi_0    pypi
[conda] triton                    3.1.0                    pypi_0    pypi
```

cc @chauhang @penguinwu","['needs reproduction', 'triaged', 'oncall: pt2']",0
pytorch/pytorch,[DTensor][TP][Distributed] DTensor with Shard(1) placement causes errors in TP Linear layers due to non-contiguous local tensors,"### ðŸ› Describe the bug

## Description

When using DTensor with `Shard(1)` placement in Tensor Parallel (TP) scenarios, several issues occur:

1. **Direct Shard(1) inputs fail**: DTensors created with `Shard(1)` placement fail when passed to parallelized Linear layers, while `Shard(0)` works correctly.

2. **Redistribution doesn't fix non-contiguity**: After redistributing a DTensor from `Shard(1)` to `Replicate()`, the underlying local tensor remains non-contiguous, causing errors in subsequent operations. Notably, `Shard(0)` redistributed to `Replicate()` produces contiguous local tensors.

3. **Inconsistent contiguous checks**: There appears to be inconsistency between `DTensor.is_contiguous()` and `DTensor._local_tensor.is_contiguous()` in some cases.


## Minimal Reproduce Code

```python
# torchrun --nproc_per_node=2 reproduce_non_contiguous_tp.py
import torch
import loguru
from torch import nn
from torch.distributed import init_process_group
from torch.distributed.device_mesh import init_device_mesh
from torch.distributed._tensor import DTensor, distribute_tensor, Shard, Replicate
from torch.distributed.tensor.parallel import parallelize_module, ColwiseParallel

init_process_group()
torch.cuda.set_device(torch.distributed.get_rank())
device_mesh = init_device_mesh(device_type='cuda', mesh_shape=(2,), mesh_dim_names=('tp',))

def rank0_log(message, level):
    if torch.distributed.get_rank() == 0:
        loguru.logger.log(level, message)

def test_case(case_num, shard_dim=1):
    model = nn.Linear(10, 10).cuda()
    bs = 3
    n_token = 31
    
    if case_num == 1:
        # Case 1: Direct Shard(1) input fails
        inp_local = torch.randn(bs, n_token, 10, device='cuda', requires_grad=True)
        inp = DTensor.from_local(inp_local, device_mesh=device_mesh, placements=[Shard(shard_dim)])
    
    elif case_num == 2:
        # Case 2: After redistributing from Shard(1) to Replicate, local tensor is still non-contiguous
        inp_local = torch.randn(bs, n_token, 10, device='cuda', requires_grad=True)
        inp = distribute_tensor(inp_local, device_mesh, [Replicate()])
        inp = inp.redistribute(placements=[Shard(shard_dim)])
        inp = inp.redistribute(placements=[Replicate()])
    
    elif case_num == 3:
        # Case 3: Workaround - manually make local tensor contiguous after redistribution. Fixes Case 2
        inp_local = torch.randn(bs, n_token, 10, device='cuda', requires_grad=True)
        inp = distribute_tensor(inp_local, device_mesh, [Replicate()])
        inp = inp.redistribute(placements=[Shard(shard_dim)])
        inp = inp.redistribute(placements=[Replicate()])
        inp._local_tensor = inp._local_tensor.contiguous()
    
    elif case_num == 4:
        # Case 4: Manually making local tensor contiguous doesn't fix direct test_case 1
        inp_local = torch.randn(bs, n_token, 10, device='cuda', requires_grad=True)
        inp = DTensor.from_local(inp_local, device_mesh=device_mesh, placements=[Shard(shard_dim)])
        inp._local_tensor = inp._local_tensor.contiguous()
    else:
        raise ValueError(f""Invalid case number: {case_num}"")

    parallelize_module(model, device_mesh, ColwiseParallel(use_local_output=False))
    rank0_log(f'[Test {case_num} | Shard({shard_dim})]: {inp.placements=} {inp.is_contiguous()=} {inp._local_tensor.is_contiguous()=}', level='DEBUG')
    
    out = model(inp)

# Shard(1) has issues
for i in range(1, 5):
    try:
        test_case(i, shard_dim=1)
        rank0_log(f'[Test {i} | Shard(1)]: pass', level='INFO')
    except Exception as e:
        rank0_log(f'[Test {i} | Shard(1)]: failed: {e}', level='ERROR')


# For Shard(0), everything is fine
for i in range(1, 5):
    try:
        test_case(i, shard_dim=0)
        rank0_log(f'[Test {i} | Shard(0)]: pass', level='INFO')
    except Exception as e:
        rank0_log(f'[Test {i} | Shard(0)]: failed: {e}', level='ERROR')
```

## Observed Behavior

1. **Case 1**: Direct `Shard(1)` DTensor input fails when passed to parallelized Linear layer.
2. **Case 2**: After redistributing from `Shard(1)` to `Replicate()`, the local tensor remains non-contiguous, causing errors.
3. **Case 3**: Manually making the local tensor contiguous after redistribution fixes Case 2, but this is a workaround.
4. **Case 4**: Manually making the local tensor contiguous doesn't fix Case 1 (direct `Shard(1)` input).

All cases work correctly with `Shard(0)`.

## Error Logs
```
[Test 1 | Shard(1)]: inp.placements=(Shard(dim=1),) inp.is_contiguous()=True inp._local_tensor.is_contiguous()=True
[Test 1 | Shard(1)]: failed: Sharding propagation failed for Op(op=aten.view.default, args_schema=Spec(S(1) on (3, 62, 10)), [186, 10] @ mesh: (2,))
[Test 2 | Shard(1)]: inp.placements=(Replicate(),) inp.is_contiguous()=True inp._local_tensor.is_contiguous()=False
[Test 2 | Shard(1)]: failed: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.
[Test 3 | Shard(1)]: inp.placements=(Replicate(),) inp.is_contiguous()=True inp._local_tensor.is_contiguous()=True
[Test 3 | Shard(1)]: pass
[Test 4 | Shard(1)]: inp.placements=(Shard(dim=1),) inp.is_contiguous()=True inp._local_tensor.is_contiguous()=True
[Test 4 | Shard(1)]: failed: Sharding propagation failed for Op(op=aten.view.default, args_schema=Spec(S(1) on (3, 62, 10)), [186, 10] @ mesh: (2,))
[Test 1 | Shard(0)]: inp.placements=(Shard(dim=0),) inp.is_contiguous()=True inp._local_tensor.is_contiguous()=True
[Test 1 | Shard(0)]: pass
[Test 2 | Shard(0)]: inp.placements=(Replicate(),) inp.is_contiguous()=True inp._local_tensor.is_contiguous()=True
[Test 2 | Shard(0)]: pass
[Test 3 | Shard(0)]: inp.placements=(Replicate(),) inp.is_contiguous()=True inp._local_tensor.is_contiguous()=True
[Test 3 | Shard(0)]: pass
[Test 4 | Shard(0)]: inp.placements=(Shard(dim=0),) inp.is_contiguous()=True inp._local_tensor.is_contiguous()=True
[Test 4 | Shard(0)]: pass
```

<img width=""2328"" height=""492"" alt=""Image"" src=""https://github.com/user-attachments/assets/49938d6b-cf9b-4a6b-a035-a1cb8f3dd8e4"" />

### Versions

2.9.1

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @msaroufim @dcci @tianyu-l @XilunWu @SherlockNoMad @ppwwyyxx",['oncall: distributed'],0
pytorch/pytorch,Regional inductor doesn't support HOP,"### ðŸ› Describe the bug

If there is HOP in the regional inductor annotated region, e.g, I'd like to put a logging with `_register_effectful_op`, it would error out. I think the root cause is we go through aot_autograd twice for regional inductor region: 
* The regional inductor pass is a post grad pass, where we should already go through aot autograd once;
* Then the `torch._inductor.standalone_compile` will call `compile_fx` and go through `aot_autograd` another time. This cause issue for HOPs like `with_effect`.

Repro:
```
import torch
import torch.fx.traceback as fx_traceback
from torch._dynamo.backends.common import aot_autograd
from torch.fx.passes.regional_inductor import regional_inductor


def _record_scalar_tensor(x: torch.Tensor, prefix: str) -> None:
    print(f""{prefix} = {x}"")


@torch.library.custom_op(""mylib::record_scalar_tensor"", mutates_args=())
def record_scalar_tensor(x: torch.Tensor, prefix: str) -> None:
    _record_scalar_tensor(x.clone(), prefix)


@record_scalar_tensor.register_fake
def _(x: torch.Tensor, prefix: str) -> None:
    return

torch._higher_order_ops.effects._register_effectful_op(
    torch.ops.mylib.record_scalar_tensor.default,
    torch._higher_order_ops.effects._EffectType.ORDERED,
)

def aot_eager_regional_inductor():
    kwargs = {}
    def regional_inductor_fn(gm, *args, **kwargs):
        gm = regional_inductor(gm, *args, **kwargs)
        print(""++++++++++"")
        print(gm)
        return gm

    return aot_autograd(
        fw_compiler=regional_inductor_fn,
        bw_compiler=regional_inductor_fn,
        **kwargs,
    )

def fn(x, y):
    sin = torch.sin(x)
    with fx_traceback.annotate({""compile_with_inductor"": 0}):
        torch.ops.mylib.record_scalar_tensor(sin.sum(), ""sin.sum"")
        mul = sin * y
        add = mul + 1

    return torch.sin(add)

opt_fn = torch.compile(
    fn, backend=aot_eager_regional_inductor(), fullgraph=True
)
x = torch.randn(10, requires_grad=True)
y = torch.randn(10, requires_grad=True)
out = opt_fn(x, y)
print(out)
```

Error stack:
```
Traceback (most recent call last):
  File ""/mlx_devbox/users/yanbo.liang/playground/debug/debug2.py"", line 53, in <module>
    out = opt_fn(x, y)
          ^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py"", line 988, in compile_wrapper
    raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/dist-packages/torch/_dynamo/output_graph.py"", line 2455, in _call_user_compiler
    raise BackendCompilerFailed(
  File ""/usr/local/lib/python3.11/dist-packages/torch/_dynamo/output_graph.py"", line 2430, in _call_user_compiler
    compiled_fn = compiler_fn(gm, example_inputs)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/dist-packages/torch/_dynamo/repro/after_dynamo.py"", line 156, in __call__
    compiled_gm = compiler_fn(gm, example_inputs)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/dist-packages/torch/__init__.py"", line 2522, in __call__
    return self.compiler_fn(model_, inputs_, **self.kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/dist-packages/torch/_dynamo/backends/common.py"", line 124, in __call__
    cg = aot_module_simplified(gm, example_inputs, **self.kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/dist-packages/torch/_functorch/aot_autograd.py"", line 1115, in aot_module_simplified
    compiled_fn, _ = aot_stage2_compile(
                     ^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/dist-packages/torch/_functorch/_aot_autograd/graph_compile.py"", line 358, in aot_stage2_compile
    return aot_stage2_autograd(aot_state, aot_graph_capture)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/dist-packages/torch/_functorch/_aot_autograd/graph_compile.py"", line 2065, in aot_stage2_autograd
    fwd_output_strides, compiled_fw_func = _aot_stage2b_fw_compile(
                                           ^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/dist-packages/torch/_functorch/_aot_autograd/graph_compile.py"", line 1856, in _aot_stage2b_fw_compile
    return _aot_stage2b_compile_forward_or_inference(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/dist-packages/torch/_functorch/_aot_autograd/graph_compile.py"", line 2359, in _aot_stage2b_compile_forward_or_inference
    compiled_fw_func = compiler(fw_module, adjusted_flat_args)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/mlx_devbox/users/yanbo.liang/playground/debug/debug2.py"", line 28, in regional_inductor_fn
    gm = regional_inductor(gm, *args, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/dist-packages/torch/fx/passes/regional_inductor.py"", line 239, in regional_inductor
    gm = _compile_inductor_marked_regions(gm)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/dist-packages/torch/fx/passes/regional_inductor.py"", line 217, in _compile_inductor_marked_regions
    return _RegionCompiler()(gm)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/dist-packages/torch/fx/passes/regional_inductor.py"", line 207, in __call__
    return _RegionCompiler.recursively_compile_regions(gm)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/dist-packages/torch/fx/passes/regional_inductor.py"", line 202, in recursively_compile_regions
    return _RegionCompiler.compile_region(gm)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/dist-packages/torch/fx/passes/regional_inductor.py"", line 180, in compile_region
    gm = _compile_submod(gm, ""__marked_inductor_submod"")
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/dist-packages/torch/fx/passes/regional_inductor.py"", line 96, in _compile_submod
    compiled_fn = torch._inductor.standalone_compile(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/dist-packages/torch/_inductor/__init__.py"", line 445, in standalone_compile
    return standalone_compile(
           ^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/dist-packages/torch/_inductor/standalone_compile.py"", line 423, in standalone_compile
    compiled_fn = compile_fx(
                  ^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/dist-packages/torch/_inductor/compile_fx.py"", line 2545, in compile_fx
    return _maybe_wrap_and_compile_fx_main(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/dist-packages/torch/_inductor/compile_fx.py"", line 2622, in _maybe_wrap_and_compile_fx_main
    return _compile_fx_main(
           ^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/dist-packages/torch/_inductor/compile_fx.py"", line 2817, in _compile_fx_main
    return aot_autograd(
           ^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/dist-packages/torch/_dynamo/backends/common.py"", line 124, in __call__
    cg = aot_module_simplified(gm, example_inputs, **self.kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/dist-packages/torch/_functorch/aot_autograd.py"", line 1105, in aot_module_simplified
    aot_state = create_aot_state(
                ^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/dist-packages/torch/_functorch/aot_autograd.py"", line 570, in create_aot_state
    fw_metadata = run_functionalized_fw_and_collect_metadata(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/dist-packages/torch/_functorch/_aot_autograd/collect_metadata_analysis.py"", line 209, in inner
    flat_f_outs = f(*flat_f_args)
                  ^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/dist-packages/torch/_functorch/_aot_autograd/graph_capture_wrappers.py"", line 1376, in functional_call
    out = PropagateUnbackedSymInts(mod).run(*args)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/dist-packages/torch/fx/interpreter.py"", line 200, in run
    self.env[node] = self.run_node(node)
                     ^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/dist-packages/torch/fx/experimental/symbolic_shapes.py"", line 7955, in run_node
    result = super().run_node(n)
             ^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/dist-packages/torch/fx/interpreter.py"", line 295, in run_node
    return getattr(self, n.op)(n.target, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/dist-packages/torch/fx/interpreter.py"", line 375, in call_function
    return target(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/dist-packages/torch/_higher_order_ops/effects.py"", line 95, in __call__
    return super().__call__(token, op, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/dist-packages/torch/_ops.py"", line 518, in __call__
    return self.dispatch(dispatch_key_set.highestPriorityTypeId(), *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/dist-packages/torch/_ops.py"", line 407, in dispatch
    raise NotImplementedError(
torch._dynamo.exc.BackendCompilerFailed: backend='compiler_fn' raised:
NotImplementedError: There was no rule registered for HigherOrderOperator with_effects and mode <torch._subclasses.functional_tensor.FunctionalTensorMode object at 0x7fb3089242d0>.Hint: set <torch._subclasses.functional_tensor.FunctionalTensorMode object at 0x7fb3089242d0>'s supports_higher_order_operators to True. This causes all higher order operators to pass through <torch._subclasses.functional_tensor.FunctionalTensorMode object at 0x7fb3089242d0>'s __torch_dispatch__, so handle them accordingly by adding support for HigerOrderOperators (in this case, with_effects) in <torch._subclasses.functional_tensor.FunctionalTensorMode object at 0x7fb3089242d0>.__torch_dispatch__ or returning NotImplemented when not supported.

While executing %with_effects : [num_users=1] = call_function[target=torch.ops.higher_order.with_effects](args = (%primals_1, mylib.record_scalar_tensor.default, %sum_1, sin.sum), kwargs = {})
Original traceback:
  File ""/mlx_devbox/users/yanbo.liang/playground/debug/debug2.py"", line 42, in fn
    torch.ops.mylib.record_scalar_tensor(sin.sum(), ""sin.sum"")

```

### Versions

torch 2.9 and main

cc @chauhang @penguinwu @ydwu4 @bdhirsh @bobrenjc93","['triaged', 'oncall: pt2', 'module: higher order operators', 'module: pt2-dispatcher']",0
pytorch/pytorch,[PrivateUse1]How to set privateuse1 to cpu tensor.data?,"### ðŸš€ The feature, motivation and pitch

Given following example:
```python
import torch, torch_openreg
cpu_tensor = torch.randn(2, 3, device='cpu')
openreg_tensor = torch.randn(2, 10, device='openreg')

cpu_tensor.data = openreg_tensor  # it routes to cpu tensor impl's set_data -> variable hook's method
```
This can work for openreg if 
1. implement `_has_compatible_shallow_copy_type` for openreg, like we did in this [pr](https://github.com/pytorch/pytorch/pull/172564).
2. openreg's tensor impl only has `_storage` since `set_data` [only set it](https://github.com/pytorch/pytorch/blob/3fd3905ef5700e11d481ab0f9b981b25b3c321c6/c10/core/TensorImpl.cpp#L605-L607).

For step2, if our privateuse1 tensor has slightly difference on tensor storage, it is hard to support `cpu_tensor.data = openreg_tensor` since `set_data` is called from a cpu tensor, since:
1. `set_data` op is already deprecated and was moved to [VariableHooks](https://github.com/pytorch/pytorch/blob/3fd3905ef5700e11d481ab0f9b981b25b3c321c6/torch/csrc/autograd/variable.h#L959) which cannot be inherit again.
2. `set_data` routes to `copy_tensor_metadata_except_version_counter`  where only storage_ is set: `dest_impl->storage_ = src_impl->storage_;`

### Alternatives

Would it be possible to make `VariableHooks` non-final? That way, we could inherit from it and override the `set_data` method.

@fffrog @wschin

### Additional context

_No response_

cc @NmomoN @mengpenghui @fwenguang @cdzhan @1274085042 @PHLens @albanD","['triaged', 'module: PrivateUse1', 'module: openreg']",0
pytorch/pytorch,Leveraging Unified Memory for MPS Tensors on Apple Silicone,"### ðŸš€ The feature, motivation and pitch

### Background

In the current PyTorch MPS backend, tensor backing buffers are allocated in the owner deviceâ€™s private memory. When a tensor is moved between cpu and mps, PyTorch:
1. Allocates a new backing buffer in the destination deviceâ€™s private memory.
2. Encodes a Metal `blit` operation to copy data from source to destination.

This behavior is consistent and correct, but may not fully leverage the unified physical memory architecture of Apple Silicon. Specifically in cases where the source and destination tensors are not modified, this results in two sets of identical buffers representing the same tensor on source and destination device. Our proposal to address this issue is comprised of three parts: 
1. Allocate all tensors in unified memory instead of device's private memory.
2. In case of moving read-only tensors: Since the data cannot be mutated, avoid duplication and `memcpy` of the source backing buffer, and instead reference the same unified memory for the destination tensor.
3. In case of assigning the result of a tensor move to itself: Since source tensor and its backing buffer are discarded after device move, avoid duplication and `memcpy` of the source backing buffer, and instead reference the same unified memory for the destination tensor.


### Proposal Part 1 - Allocate Tensors in Unified Memory
Part 1 of the proposal is implemented and currently going through testing and verification. Should Parts 2 & 3 be rejected, Part 1 can still be merged, as it is independent of the subsequent parts and can still provide performance benefits. 

**Summary**
Instead of allocating tensor backing buffers in device-private memory by default, we propose allocating _all_ tensors in the unified (shared) memory accessible by both CPU and MPS. Under this model:

- Tensor device transfers (cpu â†” mps) would still create a destination tensor with new ownership.
- However, data movement would use a `memcpy` on unified memory plus a memory barrier, rather than a Metal cpu â†” mps `blit` operation where applicable.
- This part of the proposal on its own is **not** intended to eliminate destination allocations for `.to(device=...);` calls rather, it focuses on reducing the cost of data movement, when both source and destination backing buffers reside in unified memory. It also paves the way for the next parts of the proposal to specifically reduce memory duplication. 

**Expected Impact**
- Memory allocation size: No significant reduction expected in the general case.
- Transfer cost: Potential reduction by replacing CPU â†” GPU `blit` operations with lighter-weight `memcpy` calls on unified memory.
- Semantics: No change to PyTorch tensor ownership or device semantics.

**Code Changes**
- Tensor Allocation [[EmptyTensor.cpp](https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/mps/EmptyTensor.cpp#L21)] 
Tensor allocation is performed by fetching an allocator class through the `GetMPSAllocator` call. If device supports unified memory, we propose to pass `useSharedAllocator = true` for allocator creation. As a result, all tensors allocated through this allocator will reside in unified memory.

- Tensor Device Move [[Copy.mm](https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/mps/operations/Copy.mm#L161)]
Currently, during an CPU â†’ MPS copy, an explicit blit (`streamâ†’copy_and_sync()`) is encoded to move bytes from CPU-backed private memory into MPS. This will no longer be needed as both source and destination will reside in shared memory. Instead, we propose to perform a `memcpy` of the backing buffer on cpu-visible unified memory. Since a mere `memcpy` does not carry tensor-specific information, we also manually perform cpu-side negation/conjugation if necessary, as well as to manually change ownership to `device(at::kMPS)`. We also add a memory barrier (`stream->synchronize(SyncType::COMMIT_AND_WAIT);`) to ensure writes to the tensor by source device are visible to the destination device. Please note that at this stage memory duplication still occurs but it is on unified memory rather than device private memory.


### Proposal Part 2 - Avoid Duplication for Read-only Tensors
**Summary**
With Part 1 implemented, all tensors will reside in unified memory and be accessible from both CPU and MPS. However, when a tensor is moved between devices, we cannot enforce source and destination tensors to reference the same backing buffer, as PyTorch semantics require them to behave independently, i.e. write to either tensor should not modify the other. This problem does not however exist in case of read-only tensors, and we can reference the same memory with new device ownership instead of maintaining two copies of the same data.


**Code Changes**
Still in progress and subject to approval of the proposal.

**Expected Impact**
- Memory allocation size: Reduction by 50% for read-only tensors, by maintaining a single copy of the backing buffer in unified memory for both source and destination tensors. 
- Transfer cost: Reduction by avoiding a `memcpy` on unified memory all together, and simply changing the tensor ownership.
- Semantics: Mutable tensors will still maintain two copies for source and destination tensors. Therefore, no change to PyTorch tensor ownership or device semantics. 


### Proposal Part 3 - Avoid Duplication when Assigning a Tensor Device Move to Itself
**Summary**
Currently and with Part 1 of the proposal in place, when the result of a tensor device move is assigned to itself (`a = a.to(device='mps')`), a duplicate destination buffer is allocated and copied from source, and immediately after the source tensor is discarded and its backing buffer freed. This is equivalent to simply moving the backing buffer from one place of the unified memory to another. Thus we propose to eliminate the unnecessary duplication, and simply reference the same source backing buffer with new ownership for the destination tensor. 


**Code Changes**
Still in progress and subject to approval of the proposal. We are also still investigating if it is possible to distinguish when a tensor device move is assigned to a new tensor vs. when it is assigned to itself. 

**Expected Impact**
- Memory allocation size: overall memory consumption should remain unchanged, but peak memory consumption has the potential to be reduced by reusing the source backing buffer. 
- Transfer cost: Reduction by avoiding a `memcpy` on unified memory all together, and simply changing the tensor ownership.
- Semantics: If both source and destination tensors remain alive, they will still maintain two copies for source and destination tensors. Therefore, no change to PyTorch tensor ownership or device semantics. 




### Alternatives

Instead of Parts 2 & 3 of the proposal, a general Copy-On-Write implementation was also considered that would keep both source and destination tensors referencing the same backing buffer in unified memory, until either one was modified. However, as we understand, [the last Copy-On-Write attempt](https://docs.google.com/document/d/10fyjbbpTc9-7T8QHIkPtJxnHyqMzqD4DhG6vppz8ujc/edit?tab=t.0) was ultimately rejected due to the design being too brittle. Thus, we propose to reduce the optimization scope to read-only tensors and self-assigning device moves. 

### Additional context

- Part 1 of the proposal is implemented and currently at a 97% pass rate for `tests_mps.py`. We are tracking down the remaining 3% failures. 
- Parts 2 & 3 are still under investigation and not implemented. We hope to reach an understanding on whether or not these parts of the proposal in theory are acceptable to the maintainers before attempting to implement. 

cc @kulinseth @malfet @DenisVieriu97 @jhavukainen","['module: memory usage', 'triaged', 'enhancement', 'module: mps']",0
pytorch/pytorch,add a support for fp8 in symm_mem,"### New Feature for Release

add fp8 support in symm_memory, please see https://github.com/pytorch/helion/pull/1343

### Point(s) of contact

_No response_

### Release Mode (pytorch/pytorch features only)

In-tree

### Out-Of-Tree Repo

_No response_

### Description and value to the user

Using FP32/FP16 workarounds would temporarily inflate memory because all-gather happens over NVLink, defeating the purpose of FP8.

Ideally, you want FP8 allocation + random initialization directly in symmetric memory, so NVLink transfer stays FP8.


### Link to design doc, GitHub issues, past submissions, etc

https://github.com/pytorch/helion/pull/1343


### What feedback adopters have provided

_No response_

### Plan for documentations / tutorials

Tutorial exists

### Additional context for tutorials

_No response_

### Marketing/Blog Coverage

Yes

### Are you requesting other marketing assistance with this feature?

_No response_

### Release Version

_No response_

### OS / Platform / Compute Coverage

_No response_

### Testing Support (CI, test cases, etc..)

_No response_

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @msaroufim @dcci","['oncall: distributed', 'feature', 'release-feature-request']",0
pytorch/pytorch,Mutation during backward on tensor saved for backward can create invalid graph,"### ðŸ› Describe the bug

Internal xref: https://fb.workplace.com/groups/1075192433118967/permalink/1858383771466492/

Repro:


```python
import torch

class Repro(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x, offsets):
        ctx.save_for_backward(offsets)
        return x * 2

    @staticmethod
    def backward(ctx, grad_out):
        offsets, = ctx.saved_tensors
        offsets.add_(grad_out)
        return grad_out.clone(), None

@torch.compile(backend='aot_eager_decomp_partition', fullgraph=True)
def fn(x):
    offsets = x.clone()
    out = Repro.apply(x, offsets)
    return out.clone(), offsets  # offsets must be returned!

x = torch.randn(1024, device='cuda', requires_grad=True)
fn(x)

```

1. Forward creates a tensor, saves it for backward, and returns it as output
2. Backward mutates the tensor saved for backward (involve the tangents, so that this node is required to be in the backward)

This produces a graph where tensor originally created during forward is now an output of a node that must be in the backward:
```
  File ""/data/users/jw3468/a/pytorch/torch/_functorch/_aot_autograd/graph_compile.py"", line 358, in aot_stage2_compile
    return aot_stage2_autograd(aot_state, aot_graph_capture)
  File ""/data/users/jw3468/a/pytorch/torch/_functorch/_aot_autograd/graph_compile.py"", line 2053, in aot_stage2_autograd
    ) = _aot_stage2a_partition(
  File ""/data/users/jw3468/a/pytorch/torch/_functorch/_aot_autograd/graph_compile.py"", line 1669, in _aot_stage2a_partition
    fw_module, bw_module = aot_config.partition_fn(
  File ""/data/users/jw3468/a/pytorch/torch/_functorch/partitioners.py"", line 2988, in min_cut_rematerialization_partition
    node_info = classify_nodes(
  File ""/data/users/jw3468/a/pytorch/torch/_functorch/partitioners.py"", line 2900, in classify_nodes
    forward_only_graph = _extract_graph_with_inputs_outputs(
  File ""/data/users/jw3468/a/pytorch/torch/_functorch/partitioners.py"", line 263, in _extract_graph_with_inputs_outputs
    assert not isinstance(env[x], InvalidNodeBase), (
torch._dynamo.exc.BackendCompilerFailed: backend='aot_eager_decomp_partition' raised:
AssertionError: Node add was invalid, but is output
```


### Versions

main

cc @chauhang @penguinwu @bdhirsh @bobrenjc93","['triaged', 'oncall: pt2', 'module: pt2-dispatcher']",0
pytorch/pytorch,Support for new Eigen 5.0.X,"### ðŸš€ The feature, motivation and pitch

Eigen 5.0.1 was released about two months ago. This version provides new features, performance improvements, and bug fixes.
https://gitlab.com/libeigen/eigen/-/releases/

One of the release notes says:
""The CMake build system has been modernized and older properties have been removed - projects relying on CMake may need to update their configurations ""
https://gitlab.com/libeigen/eigen/-/merge_requests/485
For example, EIGEN3_* variables were removed which makes it not possible to build PyTorch using system Eigen 5.0.1.

If possible, please consider adding support for the new Eigen version. Thank you in advance!

### Alternatives

_No response_

### Additional context

_No response_

cc @malfet @seemethere","['module: build', 'triaged', 'enhancement']",0
pytorch/pytorch,Tried to access the schema for which doesn't have a schema registered yet,"### ðŸ› Describe the bug

Running a regular LTX-2 workflow in ComfyUI with flash attention.

Requested to load LTXAV
loaded completely; 21625.46 MB usable, 9676.78 MB loaded, full load: True
  0%|                                                                                                                                                        | 0/3 [00:00<?, ?it/s]Flash Attention failed, using default SDPA: schema_.has_value() INTERNAL ASSERT FAILED at ""C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen/core/dispatch/OperatorEntry.h"":84, please report a bug to PyTorch. Tried to access the schema for  which doesn't have a schema registered yet
Flash Attention failed, using default SDPA: schema_.has_value() INTERNAL ASSERT FAILED at ""C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen/core/dispatch/OperatorEntry.h"":84, please report a bug to PyTorch. Tried to access the schema for  which doesn't have a schema registered yet
Flash Attention failed, using default SDPA: schema_.has_value() INTERNAL ASSERT FAILED at ""C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen/core/dispatch/OperatorEntry.h"":84, please report a bug to PyTorch. Tried to access the schema for  which doesn't have a schema registered yet
Flash Attention failed, using default SDPA: schema_.has_value() INTERNAL ASSERT FAILED at ""C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen/core/dispatch/OperatorEntry.h"":84, please report a bug to PyTorch. Tried to access the schema for  which doesn't have a schema registered yet
...
and on and on and on.....

### Versions

script won't run, sorry.","['needs reproduction', 'triaged', 'module: dispatch']",0
pytorch/pytorch,cuda graphs dont play nice with numpy numbers,"### ðŸ› Describe the bug

In the below example the use of np.sqrt (which will produce a np.float64) , causes a cuda graph partition. We have instances in our code where this sort of thing actually breaks the compile, ultimately segfaulting. Can this be handled better.

```python
import numpy as np
import torch


class MyMod(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.linear = torch.nn.Linear(10, 10)
        self._float = np.sqrt(4.0)

    def forward(self, x):
        return self.linear(x) * self._float


if __name__ == ""__main__"":
    model = MyMod()
    model.to(""cuda"")
    model.compile(mode=""max-autotune"", dynamic=False)
    example_input = torch.randn(1, 10, device=""cuda"")

    torch.compiler.reset()
    with torch._inductor.utils.fresh_inductor_cache():
        model(example_input)

```

### Error logs

```
/persist/envs/Fluyt312/lib/python3.12/site-packages/torch/_inductor/compile_fx.py:312: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
cudagraph partition due to non gpu ops
cudagraph partition due to DeviceCopy ops
cudagraph partition into 2 partitions
```

### Versions

torch==2.9.1

cc @mcarilli @ezyang @eellison @penguinwu @BoyuanFeng @chauhang @voznesenskym @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @kadeng @muchulee8 @amjames @aakhundov @coconutruben @jataylo","['triaged', 'module: cuda graphs', 'oncall: pt2', 'module: inductor']",0
pytorch/pytorch,XPU: x.item() fails on Intel Arc Pro B50 with UR_RESULT_ERROR_OUT_OF_DEVICE_MEMORY; UR shows urEnqueueUSMMemcpy(size=4),"### ðŸ› Describe the bug

I am using an Intel Arc Pro B50.
It seems that there is a problem copying from the device to the host. Here is a script that reproduces the problem:
``` python
import torch

print(""torch:"", torch.__version__)
print(""is_available:"", torch.xpu.is_available())
print(""device:"", torch.xpu.get_device_name(0))
print(""mem_get_info:"", torch.xpu.mem_get_info())

x = torch.randn(1, device=""xpu"")
torch.xpu.synchronize()
print(""x.item() ="", x.item())   # fails
```

Output:
```
torch        : 2.10.0+xpu
is_available : True
device       : Intel(R) Graphics [0xe212]
mem_get_info : (15582441472, 16241180672)
Traceback (most recent call last):
  File ""/home/myusername/pytorch_test.py"", line 11, in <module>
    print(""x.item() ="", x.item()) # fails
                        ~~~~~~^^
RuntimeError: level_zero backend failed with error: 39 (UR_RESULT_ERROR_OUT_OF_DEVICE_MEMORY)
```


When running with log env variables:
```
$ UR_LOG_LOADER=""level:debug;flush:debug;output:stderr"" \
      UR_LOG_LEVEL_ZERO=""level:debug;flush:debug;output:stderr"" \
      UR_LOG_TRACING=""level:info;output:stderr"" \
      ZE_ENABLE_LOADER_DEBUG_TRACE=1 \
      ZE_ENABLE_VALIDATION_LAYER=1 \
      ZE_ENABLE_PARAMETER_VALIDATION=1 \
      python ./pytorch_test.py >pytorch_test.out 2>pytorch_test.err
```

I get a lot of data [pytorch_test.err.log](https://github.com/user-attachments/files/24760779/pytorch_test.err.log)

But I think the most interesting are a large number of `urQueueFinish` operations, followed by one  `urEnqueueUSMMemcpy`, and then a large number of `urQueueRelease` operations:
```
... (many more lines)...
   ---> urQueueFinish
   <--- urQueueFinish(.hQueue = 0x5650e6728750) -> UR_RESULT_SUCCESS;
   ---> urQueueFinish
   <--- urQueueFinish(.hQueue = 0x5650e6735220) -> UR_RESULT_SUCCESS;
   ---> urQueueFinish
   <--- urQueueFinish(.hQueue = 0x5650e6741cf0) -> UR_RESULT_SUCCESS;
   ---> urQueueFinish
   <--- urQueueFinish(.hQueue = 0x5650e674e7c0) -> UR_RESULT_SUCCESS;
   ---> urQueueFinish
   <--- urQueueFinish(.hQueue = 0x5650e675b290) -> UR_RESULT_SUCCESS;
   ---> urQueueFinish
   <--- urQueueFinish(.hQueue = 0x5650e6767d60) -> UR_RESULT_SUCCESS;
   ---> urEnqueueUSMMemcpy
   <--- urEnqueueUSMMemcpy(.hQueue = 0x5650e65d1150, .blocking = 0, .pDst = 0x5650e677dc00, .pSrc = 0xffffffffff800000, .size = 4, .numEventsInWaitList = 0, .phEventWaitList = nullptr, .phEvent = 0x7ffd5cf0f070 (0x5650e680c4d8)) -> UR_RESULT_ERROR_OUT_OF_DEVICE_MEMORY;
Traceback (most recent call last):
  File ""/home/myusername/pytorch_test.py"", line 11, in <module>
    print(""x.item() ="", x.item()) # fails
                        ~~~~~~^^
RuntimeError: level_zero backend failed with error: 39 (UR_RESULT_ERROR_OUT_OF_DEVICE_MEMORY)
   ---> urQueueRelease
   <--- urQueueRelease(.hQueue = 0x5650e6767d60) -> UR_RESULT_SUCCESS;
   ---> urQueueRelease
   <--- urQueueRelease(.hQueue = 0x5650e675b290) -> UR_RESULT_SUCCESS;
   ---> urQueueRelease
   <--- urQueueRelease(.hQueue = 0x5650e674e7c0) -> UR_RESULT_SUCCESS;
   ---> urQueueRelease
   <--- urQueueRelease(.hQueue = 0x5650e6741cf0) -> UR_RESULT_SUCCESS;
   ---> urQueueRelease
   <--- urQueueRelease(.hQueue = 0x5650e6735220) -> UR_RESULT_SUCCESS;
   ---> urQueueRelease
   <--- urQueueRelease(.hQueue = 0x5650e6728750) -> UR_RESULT_SUCCESS;
   ---> urQueueRelease
   <--- urQueueRelease(.hQueue = 0x5650e671bc80) -> UR_RESULT_SUCCESS;
   ---> urQueueRelease
   <--- urQueueRelease(.hQueue = 0x5650e670f1b0) -> UR_RESULT_SUCCESS;
   ---> urQueueRelease
   <--- urQueueRelease(.hQueue = 0x5650e67026e0) -> UR_RESULT_SUCCESS;
... (many more lines)...
```

### Versions

```
Collecting environment information...
PyTorch version: 2.10.0+xpu
Is debug build: False
CUDA used to build PyTorch: None
ROCM used to build PyTorch: N/A

OS: Fedora Linux 42 (Adams) (x86_64)
GCC version: (GCC) 15.2.1 20251211 (Red Hat 15.2.1-5)
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.41

Python version: 3.13.11 (main, Jan  6 2026, 00:00:00) [GCC 15.2.1 20251211 (Red Hat 15.2.1-5)] (64-bit runtime)
Python platform: Linux-6.18.5-100.fc42.x86_64-x86_64-with-glibc2.41
Is CUDA available: False
CUDA runtime version: No CUDA
CUDA_MODULE_LOADING set to: N/A
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA
Is XPU available: True
XPU used to build PyTorch: 20250301
Intel GPU driver version:
* intel-opencl:	25.31.34666.3-2.fc42
* level-zero:	25.31.34666.3-2.fc42
Intel GPU models onboard:
N/A
Intel GPU models detected:
* [0] _XpuDeviceProperties(name='Intel(R) Graphics [0xe212]', platform_name='Intel(R) oneAPI Unified Runtime over Level-Zero V2', type='gpu', device_id=0xE212, uuid=868012e2-0000-0000-2300-000000000000, driver_version='1.6.34666+3', total_memory=15488MB, max_compute_units=128, gpu_eu_count=128, gpu_subslice_count=16, max_work_group_size=1024, max_num_sub_groups=64, sub_group_sizes=[16 32], has_fp16=1, has_fp64=1, has_atomic64=1)
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True
Caching allocator config: N/A

CPU:
Architecture:                            x86_64
CPU op-mode(s):                          32-bit, 64-bit
Address sizes:                           43 bits physical, 48 bits virtual
Byte Order:                              Little Endian
CPU(s):                                  48
On-line CPU(s) list:                     0-47
Vendor ID:                               AuthenticAMD
Model name:                              AMD Ryzen Threadripper 3960X 24-Core Processor
CPU family:                              23
Model:                                   49
Thread(s) per core:                      2
Core(s) per socket:                      24
Socket(s):                               1
Stepping:                                0
Frequency boost:                         enabled
CPU(s) scaling MHz:                      57%
CPU max MHz:                             4572.2178
CPU min MHz:                             578.4140
BogoMIPS:                                7600.80
Flags:                                   fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sev sev_es
L1d cache:                               768 KiB (24 instances)
L1i cache:                               768 KiB (24 instances)
L2 cache:                                12 MiB (24 instances)
L3 cache:                                128 MiB (8 instances)
NUMA node(s):                            1
NUMA node0 CPU(s):                       0-47
Vulnerability Gather data sampling:      Not affected
Vulnerability Ghostwrite:                Not affected
Vulnerability Indirect target selection: Not affected
Vulnerability Itlb multihit:             Not affected
Vulnerability L1tf:                      Not affected
Vulnerability Mds:                       Not affected
Vulnerability Meltdown:                  Not affected
Vulnerability Mmio stale data:           Not affected
Vulnerability Old microcode:             Not affected
Vulnerability Reg file data sampling:    Not affected
Vulnerability Retbleed:                  Mitigation; untrained return thunk; SMT enabled with STIBP protection
Vulnerability Spec rstack overflow:      Mitigation; Safe RET
Vulnerability Spec store bypass:         Mitigation; Speculative Store Bypass disabled via prctl
Vulnerability Spectre v1:                Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:                Mitigation; Retpolines; IBPB conditional; STIBP always-on; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected
Vulnerability Srbds:                     Not affected
Vulnerability Tsa:                       Not affected
Vulnerability Tsx async abort:           Not affected
Vulnerability Vmscape:                   Mitigation; IBPB before exit to userspace

Versions of relevant libraries:
[pip3] dpcpp-cpp-rt==2025.3.1
[pip3] impi-rt==2021.17.0
[pip3] intel-cmplr-lib-rt==2025.3.1
[pip3] intel-cmplr-lib-ur==2025.3.1
[pip3] intel-cmplr-lic-rt==2025.3.1
[pip3] intel-opencl-rt==2025.3.1
[pip3] intel-openmp==2025.3.1
[pip3] intel-pti==0.15.0
[pip3] intel-sycl-rt==2025.3.1
[pip3] mkl==2025.3.0
[pip3] numpy==2.3.5
[pip3] oneccl==2021.17.1
[pip3] oneccl-devel==2021.17.1
[pip3] onemkl-license==2025.3.0
[pip3] onemkl-sycl-blas==2025.3.0
[pip3] onemkl-sycl-dft==2025.3.0
[pip3] onemkl-sycl-lapack==2025.3.0
[pip3] onemkl-sycl-rng==2025.3.0
[pip3] onemkl-sycl-sparse==2025.3.0
[pip3] tbb==2022.3.0
[pip3] tcmlib==1.4.1
[pip3] torch==2.10.0+xpu
[pip3] torchaudio==2.10.0+xpu
[pip3] torchvision==0.25.0+xpu
[pip3] triton-xpu==3.6.0
[pip3] umf==1.0.2
[conda] Could not collect
```

cc @gujinghui @EikanWang @fengyuan14 @guangyey","['triaged', 'module: xpu']",0
pytorch/pytorch,[DTensor] `to_local` + `from_local` on `Partial` DTensor produce different gradient,"### ðŸ› Describe the bug

## Description

When using `DTensor.to_local()` followed by `DTensor.from_local()` on a DTensor with `Partial` placement, the forward pass appears to be a no-op (as expected), but the backward pass produces different gradients compared to using the DTensor directly.


Why aren't the gradients identical when using `to_local()` followed by `from_local()` with the same placements? Shouldn't this pattern be a no-op, resulting in the same gradients as using the DTensor directly?

This design is error-prone in some scenarios. Would it be better to add an extra check in `DTensor.to_local` to raise an error if the placements is `Partial` and `grad_placements` is not `Replicate()`?


## Reproduce Code
```py
# torchrun --nproc_per_node=2 test_grad.py
import torch
import loguru
from torch.distributed import init_process_group
from torch.distributed.device_mesh import init_device_mesh
from torch.distributed._tensor import DTensor, distribute_tensor, Shard

init_process_group()
torch.cuda.set_device(torch.distributed.get_rank())
device_mesh = init_device_mesh(device_type='cuda', mesh_shape=(2,), mesh_dim_names=('tp',))

def test_case(plan=1):
    original_tensor = torch.tensor([1., 2.], device='cuda', requires_grad=True)
    original_tensor = distribute_tensor(original_tensor, device_mesh, [Shard(0)])
    out = original_tensor.sum()
    if plan == 1:
        out = DTensor.from_local(
            out.to_local(), out.device_mesh, out.placements, run_check=False
        )
    elif plan == 2:
        pass # do nothing

    loss = out.full_tensor().sum()
    loss.backward()
    grad = original_tensor.grad.full_tensor()
    return loss, grad

loss1, grad1 = test_case(1)
loss2, grad2 = test_case(2)
loguru.logger.info(f""Plan 1 Loss: {loss1} Grad: {grad1}"")
loguru.logger.info(f""Plan 2 Loss: {loss2} Grad: {grad2}"")
```


## Output
```
Plan 1 Loss: 3.0 Grad: tensor([2., 2.], device='cuda:1')
Plan 1 Loss: 3.0 Grad: tensor([2., 2.], device='cuda:0')
Plan 2 Loss: 3.0 Grad: tensor([1., 1.], device='cuda:1')
Plan 2 Loss: 3.0 Grad: tensor([1., 1.], device='cuda:0')
```

### Versions

2.9.1

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @msaroufim @dcci @tianyu-l @XilunWu @SherlockNoMad @ppwwyyxx","['oncall: distributed', 'module: dtensor']",0
pytorch/pytorch,torch.export failed while exporting alpamayo model (Qwen3_vl) with symbolic error,"In export the Alpamayo-R1 model (which internally uses Qwen3-VL) using torch.export and then lower it to torch-mlir (ATen dialect).

The export fails due to torch.linspace being called with a data-dependent length, which becomes symbolic during export.

**Error message :**

During torch.export.export(...), the following error is raised:

torch.fx.experimental.symbolic_shapes.GuardOnDataDependentSymNode: Could not extract specialized integer from data-dependent expression u0

The error originates from a call to torch.linspace.

**Problematic code:**

`def fast_pos_embed_interpolate(self, grid_thw):
grid_ts, grid_hs, grid_ws = grid_thw[:, 0], grid_thw[:, 1], grid_thw[:, 2]

for t, h, w in zip(grid_ts, grid_hs, grid_ws):
    h_idxs = torch.linspace(0, self.num_grid_per_side - 1, h)
    w_idxs = torch.linspace(0, self.num_grid_per_side - 1, w)`
Here, h and w are derived from grid_thw and become symbolic integers during export, which causes torch.linspace(..., steps=h) to fail

**Related issue:**

This appears similar to the following PyTorch issue:

https://github.com/pytorch/pytorch/issues/138116

@chauhang @penguinwu @avikchaudhuri @gmagogsfm @zhxchen17 @tugsbayasgalan
Any guidance on how to resolve this would be very helpful and is the suggestable to modify the code in model

cc @chauhang @penguinwu @avikchaudhuri @zhxchen17 @tugsbayasgalan @angelayi @suo @ydwu4","['oncall: pt2', 'oncall: export']",0
pytorch/pytorch,`test_cutlass_backend_fp8_scaled_mm_fast_accum_filtering` fails on GB200 due to `No CUDATemplateCaller choices`,"### ðŸ› Describe the bug

```
TORCHINDUCTOR_CUTLASS_DIR=/opt/pytorch/pytorch/third_party/cutlass pytest test/inductor/test_cutlass_backend.py::TestCutlassBackend::test_cutlass_backend_fp8_scaled_mm_fast_accum_filtering -v -s
```

fails on GB200 (aarch64) but succeeds on DGX H100 (x86_64).

On GB200:
```
FAILED [6.5956s]

=================================== FAILURES ===================================
__ TestCutlassBackend.test_cutlass_backend_fp8_scaled_mm_fast_accum_filtering __
Traceback (most recent call last):
  File ""/opt/pytorch/pytorch/test/inductor/test_cutlass_backend.py"", line 1354, in test_cutlass_backend_fp8_scaled_mm_fast_accum_filtering
    run_test(True)
  File ""/opt/pytorch/pytorch/test/inductor/test_cutlass_backend.py"", line 1352, in run_test
    assert cuda_template_count > 0, ""No CUDATemplateCaller choices""
           ^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: No CUDATemplateCaller choices
```

On DGX H100:
```
[2026-01-20 22:03:07] test/inductor/test_cutlass_backend.py::TestCutlassBackend::test_cutlass_backend_fp8_scaled_mm_fast_accum_filtering frames [('total', 2)]
stats [('calls_captured', 2)]
inductor [('fxgraph_cache_miss', 2)]
aot_autograd [('total', 2), ('autograd_cache_miss', 2), ('not_ok', 2)]
graph_break []
aten_mm_info [('aten._scaled_mm.default_128_128_128', 2)]
PASSED [3.2382s]

============================================================================================================================================================================================================= 1 passed in 16.87s =============================================================================================================================================================================================================
```

### Versions

- cuda: 13.1
- pytorch: a1560557626275cb66bda37251faabacc5ae3435

GB200:
- aarch64
- GB200

DGXH100:
- x86_64
- H100 80GB HBM3

cc @chauhang @penguinwu @voznesenskym @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @kadeng @muchulee8 @amjames @aakhundov @coconutruben @jataylo","['triaged', 'oncall: pt2', 'module: inductor']",0
pytorch/pytorch,[CI] Changes to install scripts in lintrunner.toml are not tested in CI,"### ðŸ› Describe the bug

Problem is, lint invocation rely on `pytorch-linux-jammy-cuda12.8-cudnn9-py3.10-linter` and `pytorch-linux-jammy-linter` which are rebuild if and only if there are changes in `.ci/docker` folder 
(Even though new builds are scheduled whenever file is touched, see https://github.com/pytorch/pytorch/pull/114190 )

As result PR like https://github.com/pytorch/pytorch/pull/172661 landed, passed all CI checks, but all subsequent attempts to update start failing the linter, for example see https://github.com/pytorch/pytorch/commit/ec5db652693fce0d24734526728c06b967380165 ) and only later (and asynchronously) was fixed by https://github.com/pytorch/pytorch/pull/172897

### Versions

CI

cc @ezyang @gchanan @kadeng @msaroufim @lolpack @maggiemoss @ndmitchell @kinto0 @samwgoldman @seemethere @pytorch/pytorch-dev-infra","['high priority', 'triage review', 'module: typing', 'module: ci']",0
pytorch/pytorch,"Missing semicolon in rrelu function in activation.cpp potentially causing compile error? (Not tested, just from looking at the code)","In aten/src/ATen/native/activation.cpp, line with:
TORCH_CHECK(lower.to<double>() <= upper.to<double>(), ""Lower bound should be less than or equal to the upper bound"")

Missing semicolon at the end of the TORCH_CHECK statement.
	â€¢	potentially causes compiler errors and prevents building the C++ backend?

Iâ€˜m not quite sure, maybe I am mistaken because Iâ€˜ve Not tested this, but just thought this when looking at the code.

cc @malfet @seemethere","['module: build', 'triaged']",0
pytorch/pytorch,_dynamo_graph_capture_for_export is failing while torch.compile(fullgraph=True) doesn't,"### ðŸ› Describe the bug

```python 
import torch
from torch.nn.attention.flex_attention import create_mask


def main():
    device = ""cuda"" if torch.cuda.is_available() else ""cpu""
    B, S = 2, 16

    visibility = torch.randn(B, S, device=device)

    import torch._dynamo as dynamo
        
    # Test 2: _dynamo_graph_capture_for_export (fails)
    dynamo.reset()
    from torch._dynamo.functional_export import _dynamo_graph_capture_for_export

    class Model(torch.nn.Module):
        def __init__(self):
            super().__init__()
            self.v = visibility

        def forward(self):
            v = self.v

            def mask_mod(b_idx, h_idx, q_idx, k_idx):
                del h_idx
                return v[b_idx, q_idx] > 0  # Same tensor indexing

            return create_mask(mask_mod, B=B, H=1, Q_LEN=S, KV_LEN=S, device=device).sum()

    with torch._dynamo.config.patch(install_free_tensors=True):
        torch.compile(Model(), fullgraph=True)()
        print(""HEEEEEEEEEEREEEEE"")
        _dynamo_graph_capture_for_export(Model())()
        


if __name__ == ""__main__"":
    main()
```
Fails with:
```
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/symbolic_convert.py"", line 2673, in CALL_FUNCTION_EX
    self.call_function(fn, argsvars.items, kwargsvars)
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/symbolic_convert.py"", line 1247, in call_function
    self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/variables/lazy.py"", line 433, in realize_and_forward
    return getattr(self.realize(), name)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/variables/higher_order_ops.py"", line 3214, in call_function
    return super().call_function(tx, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/variables/functions.py"", line 694, in call_function
    return super().call_function(tx, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/variables/functions.py"", line 408, in call_function
    return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)  # type: ignore[attr-defined]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/symbolic_convert.py"", line 1268, in inline_user_function_return
    return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/symbolic_convert.py"", line 4843, in inline_call
    return tracer.inline_call_()
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/symbolic_convert.py"", line 5063, in inline_call_
    self.run()
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/symbolic_convert.py"", line 1675, in run
    while self.step():
          ^^^^^^^^^^^
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/symbolic_convert.py"", line 1340, in step
    self.dispatch_table[inst.opcode](self, inst)
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/symbolic_convert.py"", line 888, in wrapper
    return inner_fn(self, inst)
           ^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/symbolic_convert.py"", line 2673, in CALL_FUNCTION_EX
    self.call_function(fn, argsvars.items, kwargsvars)
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/symbolic_convert.py"", line 1247, in call_function
    self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/variables/lazy.py"", line 433, in realize_and_forward
    return getattr(self.realize(), name)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/variables/functions.py"", line 694, in call_function
    return super().call_function(tx, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/variables/functions.py"", line 408, in call_function
    return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)  # type: ignore[attr-defined]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/symbolic_convert.py"", line 1268, in inline_user_function_return
    return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/symbolic_convert.py"", line 4843, in inline_call
    return tracer.inline_call_()
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/symbolic_convert.py"", line 5063, in inline_call_
    self.run()
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/symbolic_convert.py"", line 1675, in run
    while self.step():
          ^^^^^^^^^^^
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/symbolic_convert.py"", line 1340, in step
    self.dispatch_table[inst.opcode](self, inst)
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/symbolic_convert.py"", line 888, in wrapper
    return inner_fn(self, inst)
           ^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/symbolic_convert.py"", line 2673, in CALL_FUNCTION_EX
    self.call_function(fn, argsvars.items, kwargsvars)
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/symbolic_convert.py"", line 1247, in call_function
    self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/variables/functions.py"", line 408, in call_function
    return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)  # type: ignore[attr-defined]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/symbolic_convert.py"", line 1268, in inline_user_function_return
    return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/symbolic_convert.py"", line 4843, in inline_call
    return tracer.inline_call_()
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/symbolic_convert.py"", line 5063, in inline_call_
    self.run()
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/symbolic_convert.py"", line 1675, in run
    while self.step():
          ^^^^^^^^^^^
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/symbolic_convert.py"", line 1340, in step
    self.dispatch_table[inst.opcode](self, inst)
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/symbolic_convert.py"", line 888, in wrapper
    return inner_fn(self, inst)
           ^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/symbolic_convert.py"", line 2673, in CALL_FUNCTION_EX
    self.call_function(fn, argsvars.items, kwargsvars)
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/symbolic_convert.py"", line 1247, in call_function
    self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/variables/lazy.py"", line 433, in realize_and_forward
    return getattr(self.realize(), name)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/variables/higher_order_ops.py"", line 3214, in call_function
    return super().call_function(tx, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/variables/functions.py"", line 694, in call_function
    return super().call_function(tx, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/variables/functions.py"", line 408, in call_function
    return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)  # type: ignore[attr-defined]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/symbolic_convert.py"", line 1268, in inline_user_function_return
    return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/symbolic_convert.py"", line 4843, in inline_call
    return tracer.inline_call_()
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/symbolic_convert.py"", line 5063, in inline_call_
    self.run()
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/symbolic_convert.py"", line 1675, in run
    while self.step():
          ^^^^^^^^^^^
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/symbolic_convert.py"", line 1340, in step
    self.dispatch_table[inst.opcode](self, inst)
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/symbolic_convert.py"", line 888, in wrapper
    return inner_fn(self, inst)
           ^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/symbolic_convert.py"", line 2673, in CALL_FUNCTION_EX
    self.call_function(fn, argsvars.items, kwargsvars)
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/symbolic_convert.py"", line 1247, in call_function
    self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/variables/lazy.py"", line 433, in realize_and_forward
    return getattr(self.realize(), name)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/variables/functions.py"", line 694, in call_function
    return super().call_function(tx, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/variables/functions.py"", line 408, in call_function
    return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)  # type: ignore[attr-defined]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/symbolic_convert.py"", line 1268, in inline_user_function_return
    return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/symbolic_convert.py"", line 4843, in inline_call
    return tracer.inline_call_()
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/symbolic_convert.py"", line 5063, in inline_call_
    self.run()
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/symbolic_convert.py"", line 1675, in run
    while self.step():
          ^^^^^^^^^^^
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/symbolic_convert.py"", line 1340, in step
    self.dispatch_table[inst.opcode](self, inst)
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/symbolic_convert.py"", line 888, in wrapper
    return inner_fn(self, inst)
           ^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/symbolic_convert.py"", line 2673, in CALL_FUNCTION_EX
    self.call_function(fn, argsvars.items, kwargsvars)
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/symbolic_convert.py"", line 1247, in call_function
    self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/variables/functions.py"", line 408, in call_function
    return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)  # type: ignore[attr-defined]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/symbolic_convert.py"", line 1268, in inline_user_function_return
    return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/symbolic_convert.py"", line 4843, in inline_call
    return tracer.inline_call_()
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/symbolic_convert.py"", line 5063, in inline_call_
    self.run()
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/symbolic_convert.py"", line 1675, in run
    while self.step():
          ^^^^^^^^^^^
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/symbolic_convert.py"", line 1340, in step
    self.dispatch_table[inst.opcode](self, inst)
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/symbolic_convert.py"", line 888, in wrapper
    return inner_fn(self, inst)
           ^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/symbolic_convert.py"", line 471, in impl
    self.push(fn_var.call_function(self, self.popn(nargs), {}))
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/variables/builtin.py"", line 1457, in call_function
    return handler(tx, args, kwargs)  # type: ignore[return-value]
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/variables/builtin.py"", line 1035, in <lambda>
    return lambda tx, args, kwargs: obj.call_function(
                                    ^^^^^^^^^^^^^^^^^^
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/variables/builtin.py"", line 1457, in call_function
    return handler(tx, args, kwargs)  # type: ignore[return-value]
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/variables/builtin.py"", line 1319, in _handle_insert_op_in_graph
    return dispatch_torch_function(tx, fn_var, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/variables/torch_function.py"", line 543, in dispatch_torch_function
    res = tx.symbolic_torch_function_state.call_torch_function_mode(
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/variables/torch_function.py"", line 324, in call_torch_function_mode
    return cur_mode.call_torch_function(tx, fn, types, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/variables/torch_function.py"", line 193, in call_torch_function
    return call_torch_function(
           ^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/variables/torch_function.py"", line 498, in call_torch_function
    return torch_function_var.call_function(tx, tf_args, {})
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/variables/functions.py"", line 1471, in call_function
    return super().call_function(tx, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/variables/functions.py"", line 694, in call_function
    return super().call_function(tx, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/variables/functions.py"", line 408, in call_function
    return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)  # type: ignore[attr-defined]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/symbolic_convert.py"", line 1268, in inline_user_function_return
    return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/symbolic_convert.py"", line 4843, in inline_call
    return tracer.inline_call_()
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/symbolic_convert.py"", line 5063, in inline_call_
    self.run()
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/symbolic_convert.py"", line 1675, in run
    while self.step():
          ^^^^^^^^^^^
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/symbolic_convert.py"", line 1340, in step
    self.dispatch_table[inst.opcode](self, inst)
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/symbolic_convert.py"", line 888, in wrapper
    return inner_fn(self, inst)
           ^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/symbolic_convert.py"", line 2673, in CALL_FUNCTION_EX
    self.call_function(fn, argsvars.items, kwargsvars)
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/symbolic_convert.py"", line 1247, in call_function
    self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/variables/builtin.py"", line 1457, in call_function
    return handler(tx, args, kwargs)  # type: ignore[return-value]
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/variables/builtin.py"", line 1413, in _handle_insert_op_in_graph
    return wrap_fx_proxy(tx, proxy)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/variables/builder.py"", line 2993, in wrap_fx_proxy
    return wrap_fx_proxy_cls(target_cls=TensorVariable, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/variables/builder.py"", line 3068, in wrap_fx_proxy_cls
    out: VTTypeAlias = _wrap_fx_proxy(
                       ^^^^^^^^^^^^^^^
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/variables/builder.py"", line 3192, in _wrap_fx_proxy
    example_value = get_fake_value(proxy.node, tx, allow_non_graph_fake=True)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/utils.py"", line 3700, in get_fake_value
    raise TorchRuntimeError(msg).with_traceback(e.__traceback__) from None
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/utils.py"", line 3598, in get_fake_value
    ret_val = wrap_fake_exception(
              ^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/utils.py"", line 3009, in wrap_fake_exception
    return fn()
           ^^^^
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/utils.py"", line 3599, in <lambda>
    lambda: run_node(tx.output, node, args, kwargs, nnmodule)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/utils.py"", line 3809, in run_node
    raise RuntimeError(make_error_message(e)).with_traceback(
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/utils.py"", line 3768, in run_node
    return node.target(*args, **kwargs)  # type: ignore[operator]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch._dynamo.exc.TorchRuntimeError: Dynamo failed to run FX node with fake tensors: call_function <built-in function getitem>(*(FakeTensor(..., device='cuda:0', size=(2, 16)), (BatchedTensor(lvl=1, bdim=0, value=
    FakeTensor(..., device='cuda:0', size=(2,), dtype=torch.int64)
), BatchedTensor(lvl=3, bdim=0, value=
    FakeTensor(..., device='cuda:0', size=(16,), dtype=torch.int64)
))), **{}): got RuntimeError(""vmap: It looks like you're calling .item() on a Tensor. We don't support vmap over calling .item() on a Tensor, please try to rewrite what you're doing with other operations. If error is occurring somewhere inside PyTorch internals, please file a bug report."")
```
This is because: 
https://github.com/pytorch/pytorch/blob/c701f4d781124346092b4a18621bb12c56d72015/torch/_dynamo/variables/builtin.py#L1298 

### Versions

main

cc @chauhang @penguinwu @avikchaudhuri @zhxchen17 @angelayi @suo @ydwu4","['oncall: pt2', 'oncall: export']",0
pytorch/pytorch,libtorch_cpu.so have unresolved symbols,"### ðŸ› Describe the bug

When 2.9.1 is built with MPI and DISTRIBUTED the libtorch-cpu.so (on Linux) has unresolved symbols.
`ldd -r /usr/lib64/libtorch_cpu.so`
the symbols are pointing to httplib::httplib

That does not mean that pytorch does not work. It is just an anomaly. 

See 
https://bugs.gentoo.org/968839


I fixed adding this snippet to caffe2/CMakeLists.txt but I'm not sure it works on all configurations:

```
--- a/caffe2/CMakeLists.txt     2026-01-19 20:28:45.127614622 +0100
+++ b/caffe2/CMakeLists.txt     2026-01-19 20:32:04.509213652 +0100
@@ -1495,6 +1495,12 @@
   endif()
 endif()

+if(USE_DISTRIBUTED)
+  if(USE_MPI)
+    find_package(httplib REQUIRED)
+    target_link_libraries(torch_cpu PRIVATE httplib::httplib)
+  endif()
+endif()
 if(NOT INTERN_BUILD_MOBILE)
   if(${CAFFE2_LINK_LOCAL_PROTOBUF})
     target_link_libraries(torch_cpu INTERFACE protobuf::libprotobuf)
```

I do not have pytorch installed on this machine, so Versions is about the machine where I was building it
Regards

### Versions

Collecting environment information...
PyTorch version: N/A
Is debug build: N/A
CUDA used to build PyTorch: N/A
ROCM used to build PyTorch: N/A

OS: Gentoo Linux (x86_64)
GCC version: (Gentoo 15.2.1_p20251122 p3) 15.2.1 20251122
Clang version: 21.1.8
CMake version: version 4.1.4
Libc version: glibc-2.41

Python version: 3.13.11 (main, Jan 20 2026, 19:20:08) [GCC 15.2.1 20251122] (64-bit runtime)
Python platform: Linux-6.12.41-gentoo-x86_64-x86_64-11th_Gen_Intel-R-_Core-TM-_i9-11900K_@_3.50GHz-with-glibc2.41
Is CUDA available: N/A
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: N/A
GPU models and configuration: Could not collect
Nvidia driver version: Could not collect
cuDNN version: Could not collect
Is XPU available: N/A
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: N/A
Caching allocator config: N/A

CPU:
Architecture:                            x86_64
CPU op-mode(s):                          32-bit, 64-bit
Address sizes:                           39 bits physical, 48 bits virtual
Byte Order:                              Little Endian
CPU(s):                                  16
On-line CPU(s) list:                     0-15
Vendor ID:                               GenuineIntel
Model name:                              11th Gen Intel(R) Core(TM) i9-11900K @ 3.50GHz
CPU family:                              6
Model:                                   167
Thread(s) per core:                      2
Core(s) per socket:                      8
Socket(s):                               1
Stepping:                                1
CPU(s) scaling MHz:                      72%
CPU max MHz:                             5300.0000
CPU min MHz:                             800.0000
BogoMIPS:                                7008.00
Flags:                                   fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid mpx avx512f avx512dq rdseed adx smap avx512ifma clflushopt intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp hwp_pkg_req vnmi avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq rdpid fsrm md_clear flush_l1d arch_capabilities
Virtualization:                          VT-x
L1d cache:                               384 KiB (8 instances)
L1i cache:                               256 KiB (8 instances)
L2 cache:                                4 MiB (8 instances)
L3 cache:                                16 MiB (1 instance)
Vulnerability Gather data sampling:      Mitigation; Microcode
Vulnerability Indirect target selection: Mitigation; Aligned branch/return thunks
Vulnerability Itlb multihit:             Not affected
Vulnerability L1tf:                      Not affected
Vulnerability Mds:                       Not affected
Vulnerability Meltdown:                  Not affected
Vulnerability Mmio stale data:           Mitigation; Clear CPU buffers; SMT vulnerable
Vulnerability Reg file data sampling:    Not affected
Vulnerability Retbleed:                  Mitigation; Enhanced IBRS
Vulnerability Spec rstack overflow:      Not affected
Vulnerability Spec store bypass:         Mitigation; Speculative Store Bypass disabled via prctl
Vulnerability Spectre v1:                Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:                Vulnerable: eIBRS with unprivileged eBPF
Vulnerability Srbds:                     Not affected
Vulnerability Tsa:                       Not affected
Vulnerability Tsx async abort:           Not affected

Versions of relevant libraries:
[pip3] Could not collect
[conda] Could not collect


cc @malfet @seemethere","['module: build', 'triaged', 'module: mpi', 'actionable']",0
pytorch/pytorch,Need for all_reduce to initialize NCCL communicator,"### ðŸ› Describe the bug

I noticed in the [test_nccl.py](https://github.com/pytorch/pytorch/blob/613298f2ce474593ee82023d38774ea5bead51a2/test/distributed/test_nccl.py#L236), it mentions:

```text
# Need this all_reduce to initialize NCCL communicator. Otherwise, the
# test will hang.  TODO: investigate how NCCLSymmetricMemory can
# initialize NCCL communicator.
```

I was built a NCCL extension using the new [device-initiated API](https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/deviceapi.html#gin-device-kernel), and `symm_mem.rendezvous` keeps getting illegal memory access error unless I explicitly call `c10d.all_reduce`, any reason why this happens?

In addition, it seems that currently there's no way to set/initialize the GIN context properly. For example, there's no way to set up `ginSignalCount` inside `try_emplace_devcomm`, so the `ncclDevCommCreate` will not be able to set up the `ginContextCount` and `ginSignalCount` (which are both 0), and doing synchronization inside a CUDA kernel (e.g. use `gin.readSignal()` will just segfault.

Are there plans to fix this issue? Or should I try torchcomms instead since it seems it does have these features on my first look. Thanks!


### Versions

Collecting environment information...
PyTorch version: 2.11.0.dev20260119+cu130
Is debug build: False
CUDA used to build PyTorch: 13.0
ROCM used to build PyTorch: N/A

OS: Ubuntu 22.04.4 LTS (x86_64)
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04.2) 11.4.0
Clang version: Could not collect
CMake version: version 3.22.1
Libc version: glibc-2.35

Python version: 3.12.12 (main, Nov 19 2025, 22:46:53) [Clang 21.1.4 ] (64-bit runtime)
Python platform: Linux-5.15.0-161-generic-x86_64-with-glibc2.35
Is CUDA available: True
CUDA runtime version: 13.1.80
CUDA_MODULE_LOADING set to: 
GPU models and configuration: 
GPU 0: NVIDIA A100 80GB PCIe
GPU 1: NVIDIA A100 80GB PCIe

Nvidia driver version: 580.95.05
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.5
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.5
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.5
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.5
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.5
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.5
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.5
/usr/local/cuda-11.7/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.7/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.7/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.7/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.7/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.7/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.7/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
Is XPU available: False
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True
Caching allocator config: N/A

CPU:
Architecture:                            x86_64
CPU op-mode(s):                          32-bit, 64-bit
Address sizes:                           46 bits physical, 57 bits virtual
Byte Order:                              Little Endian
CPU(s):                                  64
On-line CPU(s) list:                     0-63
Vendor ID:                               GenuineIntel
Model name:                              Intel(R) Xeon(R) Silver 4314 CPU @ 2.40GHz
CPU family:                              6
Model:                                   106
Thread(s) per core:                      2
Core(s) per socket:                      16
Socket(s):                               2
Stepping:                                6
CPU max MHz:                             3400.0000
CPU min MHz:                             800.0000
BogoMIPS:                                4800.00
Flags:                                   fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single intel_ppin ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities
Virtualization:                          VT-x
L1d cache:                               1.5 MiB (32 instances)
L1i cache:                               1 MiB (32 instances)
L2 cache:                                40 MiB (32 instances)
L3 cache:                                48 MiB (2 instances)
NUMA node(s):                            2
NUMA node0 CPU(s):                       0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62
NUMA node1 CPU(s):                       1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63
Vulnerability Gather data sampling:      Mitigation; Microcode
Vulnerability Indirect target selection: Mitigation; Aligned branch/return thunks
Vulnerability Itlb multihit:             Not affected
Vulnerability L1tf:                      Not affected
Vulnerability Mds:                       Not affected
Vulnerability Meltdown:                  Not affected
Vulnerability Mmio stale data:           Mitigation; Clear CPU buffers; SMT vulnerable
Vulnerability Reg file data sampling:    Not affected
Vulnerability Retbleed:                  Not affected
Vulnerability Spec rstack overflow:      Not affected
Vulnerability Spec store bypass:         Mitigation; Speculative Store Bypass disabled via prctl and seccomp
Vulnerability Spectre v1:                Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:                Mitigation; Enhanced / Automatic IBRS; IBPB conditional; PBRSB-eIBRS SW sequence; BHI SW loop, KVM SW loop
Vulnerability Srbds:                     Not affected
Vulnerability Tsa:                       Not affected
Vulnerability Tsx async abort:           Not affected
Vulnerability Vmscape:                   Not affected

Versions of relevant libraries:
[pip3] Could not collect
[conda] No relevant packages

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @msaroufim @dcci","['oncall: distributed', 'module: nccl', 'module: symm_mem']",0
pytorch/pytorch,[opaque obj] Figure out subclassing OpaqueBase in pybinded classes,"### ðŸ› Describe the bug

In https://github.com/pytorch/pytorch/pull/171738 we had to create both OpaqueBase and OpaqueBaseMeta since I kept segfaulting when trying to make pybinded classes like PG subclass OpaqueBase (https://github.com/pytorch/pytorch/pull/172566). Pretty sure it's a skill issue that I should figure out.. 

cc @chauhang @penguinwu @bdhirsh @bobrenjc93 @zou3519 

### Versions

main","['triaged', 'module: custom-operators', 'oncall: pt2', 'module: pt2-dispatcher']",0
pytorch/pytorch,[test] Add unittest for nanmean output dtype validation (follow-up to #131043),"## Summary

This is a follow-up issue to add a unittest for the fix in PR #172809, which addresses issue #131043.

## Background

Issue #131043 reported that `nanmean()` with invalid output dtype (`int64`, `bool`) behaved inconsistently:
- Non-empty tensor + `dtype=int64` â†’ RuntimeError with unclear message
- Empty tensor + `dtype=int64` â†’ âŒ Returns `tensor(nan)` without error (bug!)

PR #172809 fixes this by adding upfront validation of `opt_dtype` in `nanmean()` and `nanmean_out()`.

## This Issue

Per reviewer feedback from @malfet on PR #172809:
> It'll be nice to have a unittest (via OpInfo probably?)

This issue tracks adding a unittest to verify the fix works correctly.

## Test Coverage Needed

1. `nanmean()` should reject invalid output dtypes (`int64`, `bool`) for:
   - Non-empty tensors
   - Empty tensors (the bug case)
   - Scalar tensors

2. `nanmean()` should still work with valid output dtypes:
   - `dtype=torch.float32`
   - `dtype=None` (default)

## Related PRs

- **Fix PR:** #172809 - `[aten] Add opt_dtype validation to nanmean() for consistent error handling`
- **Test PR:** #172848 - `[test] Add unittest for nanmean output dtype validation`

## References

- Original issue: #131043
- Reviewer comment: https://github.com/pytorch/pytorch/pull/172809#issuecomment-3771349517

cc @mruberry @malfet @hyperkai @ezyang","['module: tests', 'triaged', 'module: reductions']",0
pytorch/pytorch,_dynamo_graph_capture_for_export doesn't work on pytree output with tensor closures,"### ðŸ› Describe the bug

```python 
import torch
from torch import Tensor
from torch.nn.attention.flex_attention import create_block_mask, BlockMask
from torch._dynamo.functional_export import _dynamo_graph_capture_for_export, dynamo_graph_capture_for_export


# Register BlockMask as a pytree node (like sixlib does)
def _block_mask_flatten(block_mask: BlockMask):
    """"""Flatten BlockMask for pytree.""""""
    return (
        [block_mask.kv_num_blocks, block_mask.kv_indices, block_mask.full_kv_num_blocks, block_mask.full_kv_indices],
        {
            ""BLOCK_SIZE"": block_mask.BLOCK_SIZE,
            ""mask_mod"": block_mask.mask_mod,
            ""seq_lengths"": block_mask.seq_lengths,
        },
    )


def _block_mask_unflatten(values, context):
    """"""Unflatten BlockMask from pytree.""""""
    kv_num_blocks, kv_indices, full_kv_num_blocks, full_kv_indices = values
    return BlockMask.from_kv_blocks(
        kv_num_blocks,
        kv_indices,
        full_kv_num_blocks,
        full_kv_indices,
        BLOCK_SIZE=context[""BLOCK_SIZE""],
        mask_mod=context[""mask_mod""],
        seq_lengths=context[""seq_lengths""],
    )


torch.utils._pytree.register_pytree_node(
    BlockMask,
    _block_mask_flatten,
    _block_mask_unflatten,
)


# Simplified InvocationContext that can hold step state (like BlockMask)
class InvocationContext:
    def __init__(self):
        self._step_state = {}

    def add_step_state(self, key, value):
        self._step_state[key] = value

    def get_step_state(self, key):
        return self._step_state.get(key)


torch.utils._pytree.register_pytree_node(
    InvocationContext,
    lambda ctx: (list(ctx._step_state.values()), list(ctx._step_state.keys())),
    lambda values, keys: _reconstruct_ctx(values, keys),
)


def _reconstruct_ctx(values, keys):
    ctx = InvocationContext()
    for k, v in zip(keys, values):
        ctx.add_step_state(k, v)
    return ctx


def visibility_mask_fn(*, visibility: Tensor, tokens: Tensor):
    lower = torch.zeros_like(visibility)  # [B, Q]
    upper = visibility  # [B, Q] - upper bound is where visibility ends

    def fn(b_idx: Tensor, h_idx: Tensor, q_idx: Tensor, k_idx: Tensor):
        del h_idx  # unused
        # Use unsqueeze fix to avoid .item() issue in vmap
        b_exp = b_idx.unsqueeze(-1) if b_idx.dim() == 0 else b_idx
        q_exp = q_idx.unsqueeze(-1) if q_idx.dim() == 0 else q_idx
        lower_val = lower[b_exp, q_exp].squeeze(-1)
        upper_val = upper[b_exp, q_exp].squeeze(-1)
        return (lower_val <= k_idx) & (k_idx <= upper_val)

    return fn


def main():
    B, Q = 2, 128
    device = ""cuda"" if torch.cuda.is_available() else ""cpu""

    # Create input tensors - must be on same device
    visibility = torch.randint(0, Q, (B, Q), device=device)
    tokens = torch.randn(B, Q, 64, device=device)

    class TestModel(torch.nn.Module):
        def forward(self, visibility, tokens):
            # Create mask function with tensor closures
            mask_fn = visibility_mask_fn(visibility=visibility, tokens=tokens)

            # Create BlockMask - this stores mask_fn which has tensor closures
            block_mask = create_block_mask(mask_fn, B=B, H=4, Q_LEN=Q, KV_LEN=Q, device=device)

            ctx = InvocationContext()
            ctx.add_step_state(""block_mask"", block_mask)

            # Compute some dummy loss
            dummy_loss = tokens.sum()

            return dummy_loss, ctx

    model = TestModel()

    gm = _dynamo_graph_capture_for_export(model)(visibility, tokens)
    gm2 = dynamo_graph_capture_for_export(model)(visibility, tokens)
    print(gm2)



if __name__ == ""__main__"":
    main()

``` 
This fails with: 
```
(pytorch_v2) [tmanlaibaatar@devvm005.dkl0 /data/users/tmanlaibaatar/pytorch (main)]$ python a.py
/data/users/tmanlaibaatar/pytorch/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
torch._dynamo.variables.base.AsPythonConstantNotImplementedError: TensorVariable() is not a constant

The above exception was the direct cause of the following exception:

torch._dynamo.variables.functions.ClosureConversionError: failed to convert closure cell to Python constant

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/data/users/tmanlaibaatar/pytorch/a.py"", line 128, in <module>
    main()
  File ""/data/users/tmanlaibaatar/pytorch/a.py"", line 121, in main
    gm = _dynamo_graph_capture_for_export(model)(visibility, tokens)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/functional_export.py"", line 796, in inner
    out = fullgraph_capture(
          ^^^^^^^^^^^^^^^^^^
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/convert_frame.py"", line 1220, in fullgraph_capture
    return _fullgraph_capture_frame(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/tmanlaibaatar/pytorch/torch/_dynamo/convert_frame.py"", line 1286, in _fullgraph_capture_frame
    raise e.with_traceback(None) from e.__cause__  # User compiler error
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch._dynamo.exc.Unsupported: nested function with non-constructible closure in output
```
Notice that dynamo_graph_capture_for_export works fine in this case:
```
GraphModule(
  (_in_shuffle_graph): <lambda>()
  (_out_shuffle_graph): <lambda>()
)



def forward(self, args_0, args_1):
    _tree_leaf_0, _tree_leaf_1, _tree_leaf_2, = pytree.tree_leaves((self, args_0, args_1,))
    L_visibility_ , L_tokens_ , = self._in_shuffle_graph(_tree_leaf_0, _tree_leaf_1, _tree_leaf_2)
    l_visibility_ = L_visibility_
    l_tokens_ = L_tokens_
    zeros_like = torch.zeros_like(l_visibility_)
    b = torch.arange(0, 2, device = 'cuda')
    h = torch.arange(0, 4, device = 'cuda')
    m = torch.arange(0, 128, device = 'cuda')
    n = torch.arange(0, 128, device = 'cuda')
    lazy_load_decompositions = torch._functorch.predispatch.lazy_load_decompositions();  lazy_load_decompositions = None
    _vmap_increment_nesting = torch._functorch.predispatch._vmap_increment_nesting(2, 'error');  _vmap_increment_nesting = None
    child = torch._functorch.predispatch._add_batch_dim(b, 0, 1);  b = None
    lazy_load_decompositions_1 = torch._functorch.predispatch.lazy_load_decompositions();  lazy_load_decompositions_1 = None
    _vmap_increment_nesting_1 = torch._functorch.predispatch._vmap_increment_nesting(4, 'error');  _vmap_increment_nesting_1 = None
    child_1 = torch._functorch.predispatch._add_batch_dim(h, 0, 2);  h = child_1 = None
    lazy_load_decompositions_2 = torch._functorch.predispatch.lazy_load_decompositions();  lazy_load_decompositions_2 = None
    _vmap_increment_nesting_2 = torch._functorch.predispatch._vmap_increment_nesting(128, 'error');  _vmap_increment_nesting_2 = None
    child_2 = torch._functorch.predispatch._add_batch_dim(m, 0, 3);  m = None
    lazy_load_decompositions_3 = torch._functorch.predispatch.lazy_load_decompositions();  lazy_load_decompositions_3 = None
    _vmap_increment_nesting_3 = torch._functorch.predispatch._vmap_increment_nesting(128, 'error');  _vmap_increment_nesting_3 = None
    _add_batch_dim_3 = torch._functorch.predispatch._add_batch_dim(n, 0, 4);  n = None
    b_exp = child.unsqueeze(-1);  child = None
    q_exp = child_2.unsqueeze(-1);  child_2 = None
    index = torch.ops.aten.index(zeros_like, [b_exp, q_exp])
    lower_val = index.squeeze(-1);  index = None
    index_1 = torch.ops.aten.index(l_visibility_, [b_exp, q_exp]);  l_visibility_ = b_exp = q_exp = None
    upper_val = index_1.squeeze(-1);  index_1 = None
    le = lower_val.le(_add_batch_dim_3);  lower_val = None
    le_1 = _add_batch_dim_3.le(upper_val);  _add_batch_dim_3 = upper_val = None
    batched_outputs = le.__and__(le_1);  le = le_1 = None
    batched_outputs_1 = torch._functorch.predispatch._remove_batch_dim(batched_outputs, 4, 128, 0);  batched_outputs = None
    _vmap_decrement_nesting = torch._functorch.predispatch._vmap_decrement_nesting();  _vmap_decrement_nesting = None
    batched_outputs_2 = torch._functorch.predispatch._remove_batch_dim(batched_outputs_1, 3, 128, 0);  batched_outputs_1 = None
    _vmap_decrement_nesting_1 = torch._functorch.predispatch._vmap_decrement_nesting();  _vmap_decrement_nesting_1 = None
    batched_outputs_3 = torch._functorch.predispatch._remove_batch_dim(batched_outputs_2, 2, 4, 0);  batched_outputs_2 = None
    _vmap_decrement_nesting_2 = torch._functorch.predispatch._vmap_decrement_nesting();  _vmap_decrement_nesting_2 = None
    mask = torch._functorch.predispatch._remove_batch_dim(batched_outputs_3, 1, 2, 0);  batched_outputs_3 = None
    _vmap_decrement_nesting_3 = torch._functorch.predispatch._vmap_decrement_nesting();  _vmap_decrement_nesting_3 = None
    mask_1 = torch._C._nn.pad(mask, (0, 0, 0, 0), 'constant', None);  mask = None
    mask_2 = mask_1.view(2, 4, 1, 128, 1, 128);  mask_1 = None
    mask_3 = mask_2.permute(0, 1, 2, 4, 3, 5);  mask_2 = None
    mask_block_sum = mask_3.sum(dim = [-2, -1]);  mask_3 = None
    full_blocks = mask_block_sum.__eq__(16384)
    gt = mask_block_sum > 0
    lt = mask_block_sum < 16384;  mask_block_sum = None
    partial_blocks = gt & lt;  gt = lt = None
    partial_blocks_1 = partial_blocks.to(dtype = torch.int8);  partial_blocks = None
    full_blocks_1 = full_blocks.to(dtype = torch.int8);  full_blocks = None
    dense_mask = partial_blocks_1.to(dtype = torch.int32);  partial_blocks_1 = None
    num_blocks_in_row = dense_mask.sum(dim = -1)
    col_indices = torch.argsort(dense_mask, dim = -1, descending = True, stable = True);  dense_mask = None
    child_3 = num_blocks_in_row.to(torch.int32, memory_format = torch.contiguous_format);  num_blocks_in_row = None
    child_4 = col_indices.to(torch.int32, memory_format = torch.contiguous_format);  col_indices = None
    dense_mask_1 = full_blocks_1.to(dtype = torch.int32);  full_blocks_1 = None
    num_blocks_in_row_1 = dense_mask_1.sum(dim = -1)
    col_indices_1 = torch.argsort(dense_mask_1, dim = -1, descending = True, stable = True);  dense_mask_1 = None
    child_7 = num_blocks_in_row_1.to(torch.int32, memory_format = torch.contiguous_format);  num_blocks_in_row_1 = None
    child_8 = col_indices_1.to(torch.int32, memory_format = torch.contiguous_format);  col_indices_1 = None
    lazy_load_decompositions_4 = torch._functorch.predispatch.lazy_load_decompositions();  lazy_load_decompositions_4 = None
    _vmap_increment_nesting_4 = torch._functorch.predispatch._vmap_increment_nesting(2, 'error');  _vmap_increment_nesting_4 = None
    child_5 = torch._functorch.predispatch._add_batch_dim(child_3, 0, 1)
    child_6 = torch._functorch.predispatch._add_batch_dim(child_4, 0, 1)
    lazy_load_decompositions_5 = torch._functorch.predispatch.lazy_load_decompositions();  lazy_load_decompositions_5 = None
    _vmap_increment_nesting_5 = torch._functorch.predispatch._vmap_increment_nesting(4, 'error');  _vmap_increment_nesting_5 = None
    _add_batch_dim_6 = torch._functorch.predispatch._add_batch_dim(child_5, 0, 2);  child_5 = None
    _add_batch_dim_7 = torch._functorch.predispatch._add_batch_dim(child_6, 0, 2);  child_6 = None
    dense_mask_2 = _add_batch_dim_7.new_zeros(1, 2, dtype = torch.int32)
    arange_4 = torch.arange(1, dtype = torch.int32, device = device(type='cuda', index=0))
    row_indices = arange_4.unsqueeze(-1);  arange_4 = None
    col_range = torch.arange(1, dtype = torch.int32, device = device(type='cuda', index=0))
    unsqueeze_3 = _add_batch_dim_6.unsqueeze(-1);  _add_batch_dim_6 = None
    index_mask = col_range < unsqueeze_3;  col_range = unsqueeze_3 = None
    valid_indices = torch.where(index_mask, _add_batch_dim_7, 1);  index_mask = _add_batch_dim_7 = None
    new_ones = dense_mask_2.new_ones(())
    dense_mask_2[(row_indices, valid_indices)] = new_ones;  setitem = dense_mask_2;  row_indices = valid_indices = new_ones = setitem = None
    getitem = dense_mask_2[(slice(None, None, None), slice(None, 1, None))];  dense_mask_2 = None
    batched_outputs_4 = getitem.contiguous();  getitem = None
    batched_outputs_5 = torch._functorch.predispatch._remove_batch_dim(batched_outputs_4, 2, 4, 0);  batched_outputs_4 = None
    _vmap_decrement_nesting_4 = torch._functorch.predispatch._vmap_decrement_nesting();  _vmap_decrement_nesting_4 = None
    out = torch._functorch.predispatch._remove_batch_dim(batched_outputs_5, 1, 2, 0);  batched_outputs_5 = None
    _vmap_decrement_nesting_5 = torch._functorch.predispatch._vmap_decrement_nesting();  _vmap_decrement_nesting_5 = None
    transpose = out.transpose(-2, -1);  out = None
    dense_mask_3 = transpose.to(dtype = torch.int32);  transpose = None
    num_blocks_in_row_2 = dense_mask_3.sum(dim = -1)
    col_indices_2 = torch.argsort(dense_mask_3, dim = -1, descending = True, stable = True);  dense_mask_3 = None
    q_num_blocks = num_blocks_in_row_2.to(torch.int32, memory_format = torch.contiguous_format);  num_blocks_in_row_2 = None
    q_indices = col_indices_2.to(torch.int32, memory_format = torch.contiguous_format);  col_indices_2 = None
    lazy_load_decompositions_6 = torch._functorch.predispatch.lazy_load_decompositions();  lazy_load_decompositions_6 = None
    _vmap_increment_nesting_6 = torch._functorch.predispatch._vmap_increment_nesting(2, 'error');  _vmap_increment_nesting_6 = None
    child_9 = torch._functorch.predispatch._add_batch_dim(child_7, 0, 1)
    child_10 = torch._functorch.predispatch._add_batch_dim(child_8, 0, 1)
    lazy_load_decompositions_7 = torch._functorch.predispatch.lazy_load_decompositions();  lazy_load_decompositions_7 = None
    _vmap_increment_nesting_7 = torch._functorch.predispatch._vmap_increment_nesting(4, 'error');  _vmap_increment_nesting_7 = None
    _add_batch_dim_10 = torch._functorch.predispatch._add_batch_dim(child_9, 0, 2);  child_9 = None
    _add_batch_dim_11 = torch._functorch.predispatch._add_batch_dim(child_10, 0, 2);  child_10 = None
    dense_mask_4 = _add_batch_dim_11.new_zeros(1, 2, dtype = torch.int32)
    arange_6 = torch.arange(1, dtype = torch.int32, device = device(type='cuda', index=0))
    row_indices_1 = arange_6.unsqueeze(-1);  arange_6 = None
    col_range_1 = torch.arange(1, dtype = torch.int32, device = device(type='cuda', index=0))
    unsqueeze_5 = _add_batch_dim_10.unsqueeze(-1);  _add_batch_dim_10 = None
    index_mask_1 = col_range_1 < unsqueeze_5;  col_range_1 = unsqueeze_5 = None
    valid_indices_1 = torch.where(index_mask_1, _add_batch_dim_11, 1);  index_mask_1 = _add_batch_dim_11 = None
    new_ones_1 = dense_mask_4.new_ones(())
    dense_mask_4[(row_indices_1, valid_indices_1)] = new_ones_1;  setitem_1 = dense_mask_4;  row_indices_1 = valid_indices_1 = new_ones_1 = setitem_1 = None
    getitem_1 = dense_mask_4[(slice(None, None, None), slice(None, 1, None))];  dense_mask_4 = None
    batched_outputs_6 = getitem_1.contiguous();  getitem_1 = None
    batched_outputs_7 = torch._functorch.predispatch._remove_batch_dim(batched_outputs_6, 2, 4, 0);  batched_outputs_6 = None
    _vmap_decrement_nesting_6 = torch._functorch.predispatch._vmap_decrement_nesting();  _vmap_decrement_nesting_6 = None
    out_1 = torch._functorch.predispatch._remove_batch_dim(batched_outputs_7, 1, 2, 0);  batched_outputs_7 = None
    _vmap_decrement_nesting_7 = torch._functorch.predispatch._vmap_decrement_nesting();  _vmap_decrement_nesting_7 = None
    transpose_1 = out_1.transpose(-2, -1);  out_1 = None
    dense_mask_5 = transpose_1.to(dtype = torch.int32);  transpose_1 = None
    num_blocks_in_row_3 = dense_mask_5.sum(dim = -1)
    col_indices_3 = torch.argsort(dense_mask_5, dim = -1, descending = True, stable = True);  dense_mask_5 = None
    full_q_num_blocks = num_blocks_in_row_3.to(torch.int32, memory_format = torch.contiguous_format);  num_blocks_in_row_3 = None
    full_q_indices = col_indices_3.to(torch.int32, memory_format = torch.contiguous_format);  col_indices_3 = None
    dummy_loss = l_tokens_.sum();  l_tokens_ = None
    return pytree.tree_unflatten(self._out_shuffle_graph(_tree_leaf_0, _tree_leaf_1, _tree_leaf_2, dummy_loss, zeros_like, full_q_indices, full_q_num_blocks, q_indices, q_num_blocks, child_8, child_7, child_4, child_3), self._out_spec)
```

### Versions

main

cc @chauhang @penguinwu @avikchaudhuri @zhxchen17 @angelayi @suo @ydwu4","['oncall: pt2', 'oncall: export']",0
pytorch/pytorch,importing cudagraph_trees causes segfault,"I noticed that running this locally on a recent main commit:
```
import torch
from torch._inductor import cudagraph_trees
```

is enough to segfault:
```
(pytorch) [hirsheybar@devgpu034.ldc1 ~/local/new2/pytorch/autoparallel (main)]$ python tmp.py
Aborted (core dumped)
```

cc @mcarilli @ezyang @eellison @penguinwu @BoyuanFeng @chauhang","['triaged', 'module: cuda graphs', 'oncall: pt2']",0
pytorch/pytorch,"`torch.compile` fails in FakeTensor meta path: `Cannot view ... strides ... as (1, 2048)` while eager works","### ðŸ› Describe the bug

## **Summary**

A small model using BatchNorm1d, LogSoftmax, MultiheadAttention, reciprocal, NCHW conversion, and bilinear Upsample runs fine in eager mode, but fails under `torch.compile(..., backend=""inductor"")` during FakeTensor meta evaluation. The failure occurs on `view(b, -1)` due to a stride-based constraint inside `_reshape_view_helper_core_alg`.

Eager forward produces the expected `(1, 2048)` tensor. The compiled path fails before kernel generation.

---

## **Minimal Reproduction**

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

def print_env():
    print(""PyTorch version:"", torch.__version__)

class MyModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc = nn.Linear(784, 512)
        self.bn = nn.BatchNorm1d(512)
        self.logsoftmax = nn.LogSoftmax(dim=1)
        self.attn = nn.MultiheadAttention(
            embed_dim=512,
            num_heads=8,
            batch_first=True,
        )
        self.upsample = nn.Upsample(
            scale_factor=2,
            mode=""bilinear"",
            align_corners=False,
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        b = x.shape[0]
        x = x.view(b, -1)
        x = self.fc(x)
        x = self.bn(x)
        x = self.logsoftmax(x)

        attn_in = x.unsqueeze(1)
        attn_out, _ = self.attn(attn_in, attn_in, attn_in)

        eps = 1e-6
        x = 1.0 / (attn_out + eps)

        x = x.permute(0, 2, 1).contiguous()
        x = x.unsqueeze(-1)
        x = self.upsample(x)

        b, c, h, w = x.shape
        x = x.view(b, -1)
        return x

def get_input():
    return torch.randn(1, 28, 28)

def main():
    print_env()
    model = MyModel().eval()
    inp = get_input()

    with torch.no_grad():
        out_eager = model(inp)
    print(""Eager output shape:"", out_eager.shape)

    compiled = torch.compile(model)
    out_compiled = compiled(inp)
    print(""Compiled output shape:"", out_compiled.shape)

if __name__ == ""__main__"":
    main()
```

---

## **Actual Behavior**

Console output (relevant part):

```
PyTorch version: 2.9.1+cu128
Eager output shape: torch.Size([1, 2048])

ValueError: Cannot view a tensor with shape torch.Size([1, 512, 2, 2])
and strides (2048, 1, 1024, 512) as a tensor with shape (1, 2048)!
```

Full trace (shortened for clarity):

```
torch._dynamo.exc.TorchRuntimeError: Dynamo failed to run FX node with fake tensors:
call_method view(..., 1, -1)
...
raise ValueError(msg)
```

The failure comes from `_reshape_view_helper_core_alg` via FakeTensor op implementations inside Dynamo/Inductor tracing.




### Versions

```
PyTorch version:  2.9.1+cu128
Is debug build: False
CUDA used to build PyTorch: 12.6
ROCM used to build PyTorch: N/A

OS: Ubuntu 22.04.4 LTS (x86_64)
GCC version: (Ubuntu 12.3.0-1ubuntu1~22.04) 12.3.0
Clang version: Could not collect
CMake version: version 3.22.1
Libc version: glibc-2.35

Python version: 3.10.16 (main, Dec 11 2024, 16:24:50) [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-6.8.0-59-generic-x86_64-with-glibc2.35
Is CUDA available: True
```

cc @chauhang @penguinwu @eellison @ezyang @bobrenjc93 @bdhirsh","['triaged', 'oncall: pt2', 'module: fakeTensor', 'module: dynamic shapes', 'module: decompositions', 'module: pt2-dispatcher']",0
pytorch/pytorch,[torch.compile] Failing on some complex operations  with torch._inductor.exc.InductorError: AttributeError: 'complex' object has no attribute 'get_name',"### ðŸ› Describe the bug

torch.compile fails to compile some code with complex operations, minimal example:
```python
import torch
from scipy.signal import get_window
import numpy as np

class TorchSTFT(torch.nn.Module):
    def __init__(self, device, filter_length=800, hop_length=200, win_length=800, window='hann'):
        super().__init__()
        self.filter_length = filter_length
        self.hop_length = hop_length
        self.win_length = win_length
        self.device = device
        self.window = torch.from_numpy(get_window(window, win_length, fftbins=True).astype(np.float32)).to(device)

    def transform(self, input_data):
        forward_transform = torch.stft(
            input_data,
            self.filter_length, self.hop_length, self.win_length, window=self.window,
            return_complex=True)

        return torch.abs(forward_transform), torch.angle(forward_transform)

    def inverse(self, magnitude, phase):
        inverse_transform = torch.istft(
            magnitude * torch.exp(phase * 1j),
            self.filter_length, self.hop_length, self.win_length, window=self.window)

        return inverse_transform.unsqueeze(-2)

    def forward(self, input_data):
        self.magnitude, self.phase = self.transform(input_data)
        reconstruction = self.inverse(self.magnitude, self.phase)
        return reconstruction


model = torch.compile(TorchSTFT(device='cuda'))
x = torch.randn(16000, requires_grad=True).to('cuda')
y = model(x)
y.sum().backward()
```

Also attaching tlparse trace: 
[tl_out.zip](https://github.com/user-attachments/files/24737740/tl_out.zip)

### Error logs

```
[redacted]\env\lib\site-packages\torch\_inductor\lowering.py:2161: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
Traceback (most recent call last):
  File ""[redacted]\test_stft_compile.py"", line 48, in <module>
    y.sum().backward()
  File ""[redacted]\env\lib\site-packages\torch\_tensor.py"", line 631, in backward
    torch.autograd.backward(
  File ""[redacted]\env\lib\site-packages\torch\autograd\__init__.py"", line 381, in backward
    _engine_run_backward(
  File ""[redacted]\env\lib\site-packages\torch\autograd\graph.py"", line 865, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File ""[redacted]\env\lib\site-packages\torch\autograd\function.py"", line 317, in apply
    return user_fn(self, *args)
  File ""[redacted]\env\lib\site-packages\torch\_functorch\_aot_autograd\runtime_wrappers.py"", line 2539, in backward
    return impl_fn()
  File ""[redacted]\env\lib\site-packages\torch\_functorch\_aot_autograd\runtime_wrappers.py"", line 2525, in impl_fn
    out = CompiledFunction._backward_impl(ctx, all_args)
  File ""[redacted]\env\lib\site-packages\torch\_functorch\_aot_autograd\runtime_wrappers.py"", line 2666, in _backward_impl
    CompiledFunction.compiled_bw = aot_config.bw_compiler(
  File ""[redacted]\env\lib\site-packages\torch\_functorch\_aot_autograd\schemas.py"", line 1249, in __call__
    return self.compiler_fn(gm, example_inputs)
  File ""[redacted]\env\lib\site-packages\torch\_dynamo\backends\common.py"", line 84, in _wrapped_bw_compiler
    disable(
  File ""[redacted]\env\lib\site-packages\torch\_dynamo\eval_frame.py"", line 1210, in _fn
    return fn(*args, **kwargs)
  File ""[redacted]\env\lib\site-packages\torch\_utils_internal.py"", line 97, in wrapper_function
    return function(*args, **kwargs)
  File ""[redacted]\env\lib\site-packages\torch\_inductor\compile_fx.py"", line 2739, in bw_compiler
    return compile_fx_backward(
  File ""[redacted]\env\lib\site-packages\torch\_inductor\compile_fx.py"", line 2418, in compile_fx_backward
    return inner_compile(
  File ""[redacted]\env\lib\site-packages\torch\_inductor\compile_fx.py"", line 821, in compile_fx_inner
    return wrap_compiler_debug(_compile_fx_inner, compiler_name=""inductor"")(
  File ""[redacted]\env\lib\site-packages\torch\_dynamo\repro\after_aot.py"", line 146, in debug_wrapper
    inner_compiled_fn = compiler_fn(gm, example_inputs)
  File ""[redacted]\env\lib\site-packages\torch\_inductor\compile_fx.py"", line 1041, in _compile_fx_inner
    raise InductorError(e, currentframe()).with_traceback(
  File ""[redacted]\env\lib\site-packages\torch\_inductor\compile_fx.py"", line 1025, in _compile_fx_inner
    mb_compiled_graph = fx_codegen_and_compile(
  File ""[redacted]\env\lib\site-packages\torch\_inductor\compile_fx.py"", line 1788, in fx_codegen_and_compile
    return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
  File ""[redacted]\env\lib\site-packages\torch\_inductor\compile_fx.py"", line 1474, in codegen_and_compile
    graph.run(*example_inputs)
  File ""[redacted]\env\lib\site-packages\torch\_inductor\graph.py"", line 991, in run
    return super().run(*args)
  File ""[redacted]\env\lib\site-packages\torch\fx\interpreter.py"", line 200, in run
    self.env[node] = self.run_node(node)
  File ""[redacted]\env\lib\site-packages\torch\_inductor\graph.py"", line 1669, in run_node
    result = fallback_handler(n.target, add_to_fallback_set=False)(
  File ""[redacted]\env\lib\site-packages\torch\_inductor\lowering.py"", line 2150, in handler
    wrap_tensors, ir.FallbackKernel.create(kernel, *args, **kwargs)
  File ""[redacted]\env\lib\site-packages\torch\_inductor\ir.py"", line 8251, in create
    packed = cls(
  File ""[redacted]\env\lib\site-packages\torch\_inductor\ir.py"", line 7867, in __init__
    handle_aliasing_and_mutation(info, arg)
  File ""[redacted]\env\lib\site-packages\torch\_inductor\ir.py"", line 7864, in handle_aliasing_and_mutation
    add_alias(arg)
  File ""[redacted]\env\lib\site-packages\torch\_inductor\ir.py"", line 7850, in add_alias
    self.alias_names.append(t.get_name())
torch._inductor.exc.InductorError: AttributeError: 'complex' object has no attribute 'get_name'
```

### Versions

```
PyTorch version: 2.11.0a0+gitdac4f45
Is debug build: False
CUDA used to build PyTorch: 12.8
ROCM used to build PyTorch: N/A

OS: Microsoft Windows 11 Pro for Workstations (10.0.26220 64-bit)
GCC version: (Rev2, Built by MSYS2 project) 14.2.0
Clang version: 19.1.5
CMake version: version 3.31.6
Libc version: N/A

Python version: 3.10.18 | packaged by Anaconda, Inc. | (main, Jun  5 2025, 13:08:55) [MSC v.1929 64 bit (AMD64)] (64-bit runtime)
Python platform: Windows-10-10.0.26220-SP0
Is CUDA available: True
CUDA runtime version: 12.8.93
CUDA_MODULE_LOADING set to:
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 5090
Nvidia driver version: 591.74
cuDNN version: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.8\bin\cudnn_ops64_9.dll
Is XPU available: False
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True
Caching allocator config: N/A

CPU:
Name: AMD Ryzen 9 9950X3D 16-Core Processor
Manufacturer: AuthenticAMD
Family: 107
Architecture: 9
ProcessorType: 3
DeviceID: CPU0
CurrentClockSpeed: 4300
MaxClockSpeed: 4300
L2CacheSize: 16384
L2CacheSpeed: None
Revision: 17408

Versions of relevant libraries:
[pip3] numpy==1.26.4
[pip3] pytorch-ranger==0.1.1
[pip3] ring-attention-pytorch==0.5.17
[pip3] torch==2.11.0a0+gitdac4f45
[pip3] torch-dct==0.1.6
[pip3] torch-optimi==0.3.2
[pip3] torch-optimizer==0.3.0
[pip3] torchaudio==2.10.0.dev20260105+cu128
[pip3] torchcrepe==0.0.23
[pip3] torchfcpe==0.0.4
[pip3] torchview==0.2.7
[pip3] torchvision==0.25.0.dev20260105+cu128
[pip3] triton-windows==3.5.1.post24
[conda] No relevant packages
```

cc @chauhang @penguinwu @voznesenskym @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @kadeng @muchulee8 @amjames @aakhundov @coconutruben @jataylo","['triaged', 'oncall: pt2', 'module: inductor']",0
pytorch/pytorch,"torch.compile(dynamic=True) specializes batch dimension for LayerNorm backward, causing recompiles","### ðŸ› Describe the bug

**Summary:**

When using `make_fx` with `tracing_mode='symbolic'` to trace a module containing `nn.LayerNorm` and `torch.autograd.grad(create_graph=True)`, and then compiling the resulting FX graph with `torch.compile(dynamic=True, fullgraph=True)`, the batch dimension (`dim 0`) gets specialized. This causes a new recompilation for every new batch size, triggered by a ""guard size mismatch at index 0"".

This behavior is specific to the presence of the `native_layer_norm_backward` operator in the graph. A forward-only `LayerNorm` graph compiles with a truly dynamic batch dimension as expected.

**Background:**
I am working on a machine learning force field project and aim to accelerate my model's training process using torch.compile. Briefly, the machine learning force field model reads distance vectors between atoms and outputs energy and forces, where the latter are derived by computing the gradient of energy with respect to the distance vectors. In other words, the forward() function of my model contains manually invoked autograd operations. Additionally, the loss function of my model consists of two components (energy and forces), which means that higher-order gradients need to be computed during the backward pass of training.

Meanwhile, the ""batch_size"" varies for each batch (since the number of atoms in each molecule is different), which requires that the code generated by compilation does not statically fix the dimensions of the input tensors.

To address this, I first traced my computation graph using make_fx(mode=""symbolic""), then compiled the model with torch.compile(dynamic=True). However, I found that the compiled output still has statically fixed dimensions, leading to frequent recompilation when feeding inputs of different shapes. After analysis, I identified that the root cause of the recompilation is the backward operator of torch.nn.LayerNorm. A minimal reproducible example is provided below:

**Minimal Reproducer:**

```python
import os
import torch
import torch.nn as nn
from torch.fx import make_fx

# Set logs to observe guard failures and recompilations
os.environ[""TORCH_LOGS""] = ""dynamic,guards,recompiles""

class TinyDemo(nn.Module):
    """"""
    A simple model that applies LayerNorm and computes its gradient w.r.t. input.
    """"""
    def __init__(self, dim=64):
        super().__init__()
        self.dim = dim
        self.layer_norm = nn.LayerNorm(self.dim)

    def forward(self, x: torch.Tensor):
        x.requires_grad_(True)
        y = self.layer_norm(x)
        grad_outputs = [torch.ones_like(y)]
        # create_graph=True is essential to produce native_layer_norm_backward in the graph
        grad = torch.autograd.grad(
            [y],
            [x],
            grad_outputs=grad_outputs,
            create_graph=True,
            allow_unused=True
        )[0]
        return y, grad

# 1. Trace the model with symbolic tracing
model = TinyDemo(dim=64)
example_input = torch.randn(10, 64)
fx_model = make_fx(
    model,
    tracing_mode='symbolic',
    _allow_non_fake_inputs=True,
    _error_on_data_dependent_ops=True,
)(example_input)

print(""--- FX Graph Code ---"")
print(fx_model.code)

# 2. Compile the FX graph with dynamic=True
compiled_model = torch.compile(
    fx_model,
    dynamic=True,
    fullgraph=True
)

# 3. Run with varying batch sizes to trigger recompilations
print(""\n--- Running with different batch sizes ---"")
for n in [10, 20, 30, 40, 50, 60, 70, 80, 90]:
    print(f""Running with batch size N={n}"")
    x = torch.randn(n, 64)
    try:
        y, g = compiled_model(x)
        print(f""  Success: y.shape={y.shape}, g.shape={g.shape}"")
    except Exception as e:
        print(f""  Failed with error: {e}"")

print(""\n--- End of script ---"")
```

[tl_out.zip](https://github.com/user-attachments/files/24736313/tl_out.zip)

### Error logs

**Expected Behavior:**

With `torch.compile(dynamic=True)`, the compiled model should handle a dynamic batch dimension (`N`) without recompiling. The first run (e.g., `N=10`) should produce a compiled artifact where the batch dimension is treated as a `SymInt`. Subsequent runs with different `N` values (20, 30, etc.) should reuse this artifact without triggering any guard failures related to the batch dimension.

**Actual Behavior:**

For each new batch size `N`, a recompilation is triggered. The logs show a series of guard failures indicating that the batch dimension was specialized to the previous value. For example, when running with `N=90` after running with `N=10` through `N=80`, the logs show:

> V0120... [__recompiles] Recompiling function forward...
> V0120... [__recompiles]     triggered by the following guard failure(s):
> V0120... [__recompiles]     - 0/7: tensor 'arg0_1' size mismatch at index 0. expected 80, actual 90
> V0120... [__recompiles]     - 0/6: tensor 'arg0_1' size mismatch at index 0. expected 70, actual 90
> ...
> V0120... [__recompiles]     - 0/0: tensor 'arg0_1' size mismatch at index 0. expected 10, actual 90

### Versions

PyTorch version: 2.9.1
Is debug build: False
CUDA used to build PyTorch: 13.0
ROCM used to build PyTorch: N/A

OS: Debian GNU/Linux 12 (bookworm) (x86_64)
GCC version: (Debian 12.2.0-14+deb12u1) 12.2.0
Clang version: Could not collect
CMake version: version 3.31.6
Libc version: glibc-2.36

Python version: 3.12.4 (main, Dec 15 2025, 11:08:09) [GCC 12.2.0] (64-bit runtime)
Python platform: Linux-5.4.210.bsk.6-amd64-x86_64-with-glibc2.36
Is CUDA available: False
CUDA runtime version: 13.0.88

Nvidia driver version: Could not collect
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.9.16.0
/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.16.0
/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.16.0
/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.16.0
/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.16.0
/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.16.0
/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.16.0
/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.16.0
Is XPU available: False
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True
Caching allocator config: N/A

CPU:
Architecture:                    x86_64
CPU op-mode(s):                  32-bit, 64-bit
Address sizes:                   46 bits physical, 57 bits virtual
Byte Order:                      Little Endian
CPU(s):                          128
On-line CPU(s) list:             0-127
Vendor ID:                       GenuineIntel
Model name:                      Intel(R) Xeon(R) Platinum 8336C CPU @ 2.30GHz
CPU family:                      6
Model:                           106
Thread(s) per core:              2
Core(s) per socket:              32
Socket(s):                       2
Stepping:                        6
CPU(s) scaling MHz:              86%
CPU max MHz:                     3500.0000
CPU min MHz:                     800.0000
BogoMIPS:                        4600.00
Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq rdpid md_clear pconfig flush_l1d arch_capabilities
Virtualization:                  VT-x
L1d cache:                       3 MiB (64 instances)
L1i cache:                       2 MiB (64 instances)
L2 cache:                        80 MiB (64 instances)
L3 cache:                        108 MiB (2 instances)
NUMA node(s):                    4
NUMA node0 CPU(s):               0-15,64-79
NUMA node1 CPU(s):               16-31,80-95
NUMA node2 CPU(s):               32-47,96-111
NUMA node3 CPU(s):               48-63,112-127
Vulnerability Itlb multihit:     Not affected
Vulnerability L1tf:              Not affected
Vulnerability Mds:               Not affected
Vulnerability Meltdown:          Not affected
Vulnerability Mmio stale data:   Mitigation; Clear CPU buffers; SMT vulnerable
Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp
Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence
Vulnerability Srbds:             Not affected
Vulnerability Tsx async abort:   Not affected

Versions of relevant libraries:
[pip3] botorch==0.16.1
[pip3] gpytorch==1.15.1
[pip3] mypy_extensions==1.1.0
[pip3] numpy==2.3.5
[pip3] pytorch-lightning==2.6.0
[pip3] torch==2.9.1
[pip3] torch-runstats==0.2.0
[pip3] torchaudio==2.9.1+cu130
[pip3] torchmetrics==1.8.2
[pip3] torchtnt==0.2.4
[pip3] torchvision==0.24.1+cu130
[pip3] triton==3.5.1
[conda] Could not collect

cc @chauhang @penguinwu @ezyang @bobrenjc93","['triaged', 'oncall: pt2', 'module: dynamic shapes']",0
pytorch/pytorch,coordinate descent tuner incorrectly runs on warm cache,"### ðŸ› Describe the bug

My understanding of how the cache should work is the following, I run compile with max autotune, the best configs are found, saved to the cache, the next time i run the same model, I should be much faster. In particular, I should not see any time spent in `CachingAutotuner.coordinate_descent`. This is not the case in `torch==2.9.1`

If I clear my local inductor cache and run a small demo script twice, I see 2 min compile time on first run, and 15 seconds for the second run. Now digging into the time of the second run, most of it is in the CachingAutotune.coordinate_descent_tuner. **I think this is a bug?** Indeed I can monkey patch a function (included in the script) that reduces this time to zero in my second run. If I monkeypatch the function, then my second compile time goes to 2 secs.

In production runs this effect is much bigger than the 12 seconds seen here!

```python
import logging
import sys
from collections.abc import Iterator
from contextlib import contextmanager
from time import perf_counter

import torch
import torch._inductor.config as inductor_config
import torch._inductor.runtime.triton_heuristics
import torch.nn.functional as F
from torch._inductor.runtime import triton_heuristics
from torch._inductor.runtime.runtime_utils import triton_config_to_hashable

BATCH_SIZE = 1024
INPUT_DIM = 2048
OUTPUT_DIM = 32
HIDDEN_DIM = 512
NUM_LAYERS = 8

torch.set_float32_matmul_precision(""medium"")


class MyMod(torch.nn.Module):
    def __init__(self) -> None:
        super().__init__()

        self._input_layer = torch.nn.Linear(INPUT_DIM, HIDDEN_DIM)
        linear = [torch.nn.Linear(HIDDEN_DIM + k, HIDDEN_DIM + k + 1) for k in range(NUM_LAYERS)]
        relu = [torch.nn.ReLU() for _ in range(NUM_LAYERS)]
        layer_norm = [torch.nn.LayerNorm(HIDDEN_DIM + k + 1) for k in range(NUM_LAYERS)]
        layers = [x for triple in zip(linear, relu, layer_norm) for x in triple]
        self._net = torch.nn.ModuleList(layers)
        self._output_layer = torch.nn.Linear(HIDDEN_DIM + NUM_LAYERS, OUTPUT_DIM)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self._input_layer(x)
        for layer in self._net:
            x = layer(x)
        return self._output_layer(x)


def slow_compile_loss(y: torch.Tensor) -> torch.Tensor:
    """"""
    Intentionally over-complicated ""loss"" that depends only on y.
    Designed to create a large/ugly compiled graph with many ops.
    """"""
    # Ensure float for special funcs; keep device/dtype friendly
    y = y.float()
    x = y

    # Make a ""local context"" term via rolling and nonlinear mixes
    r1 = torch.roll(x, shifts=1, dims=-1)
    r2 = torch.roll(x, shifts=-2, dims=-1)
    x = x + 0.05 * torch.tanh(r1 - r2) + 0.03 * torch.sin(r1 + r2)

    loss = x.new_zeros(())

    # Many loop iterations; each iteration adds diverse ops.
    # Increase ITS / inner sizes to make compile even slower.
    ITS = 1
    for i in range(ITS):
        a = x * (0.97 + 0.001 * i) + (i % 7 - 3) * 0.01
        b = torch.sin(a * (1.0 + i * 0.02)) + torch.cos(a * (0.7 + i * 0.01))
        c = torch.tanh(b) * torch.sigmoid(0.5 * b + 0.1)

        # Branching that depends on i (static) but changes the graph shape path
        if i % 3 == 0:
            d = torch.where(c > 0, c * c, -torch.sqrt(c.abs() + 1e-4))
            e = torch.log1p(d.abs() + 1e-3) + 0.1 * torch.exp(-d.abs())
        elif i % 3 == 1:
            d = torch.where(c < 0, torch.sinh(c), torch.asinh(c))
            e = torch.special.xlog1py(d.abs() + 1e-4, d.abs() + 0.2)
        else:
            d = torch.special.entr(torch.sigmoid(c)) + torch.special.entr(torch.sigmoid(-c))
            e = torch.sqrt(d + 1e-5) + torch.rsqrt(d + 1e-5)

        # Small ""structured"" mixes: reshape to 2D and do a tiny matmul if possible
        z = e.flatten()
        n = z.numel()
        # static-ish inner size chosen by i; stays small to avoid huge runtime
        k = 8 + (i % 8)  # 8..15
        m = (n // k) * k
        z = z[:m].view(-1, k)

        # Create a per-iter matrix with nontrivial values (still only from y via x)
        M = torch.sin(torch.arange(k, device=y.device, dtype=y.dtype) * (0.1 + 0.01 * i))
        M = M.unsqueeze(0).repeat(k, 1)  # (k, k)
        M = M + torch.eye(k, device=y.device, dtype=y.dtype) * (1.0 + 0.001 * i)
        M = torch.tanh(M) + 0.1 * torch.cos(M * (0.5 + 0.02 * i))

        # Mix via matmul + more pointwise ops
        u = z @ M  # (-1, k)
        u = u + 0.05 * torch.sin(u) - 0.03 * torch.cos(1.7 * u)
        u = F.gelu(u) + 0.02 * F.silu(u)

        # Reduce in a slightly odd way to add more ops
        p = u.mean(dim=0)
        q = (u.var(dim=0, unbiased=False) + 1e-4).sqrt()
        t = (p * torch.rsqrt(q + 1e-4)).sum()

        # Couple back into x for next iterations (still only depends on y)
        x = x + (0.001 / (i + 1)) * torch.tanh(t) * torch.sign(x)

        loss = loss + (t * t) * (0.01 + 0.0001 * i)

    # Final regularizers
    loss = loss + 0.001 * (y.pow(2).mean() + y.abs().mean())
    loss = loss + 0.0005 * (torch.sin(y).mean().abs() + torch.cos(y).mean().abs())
    return loss


@contextmanager
def timer(name: str) -> Iterator[None]:
    start = perf_counter()
    yield
    logging.info(f""{name}: took {perf_counter() - start:.2f}"")


def log_compile_times() -> None:
    headers, values = torch._dynamo.utils.compile_times(""csv"", aggregate=True)
    metrics = []
    for metric_name, value in zip(headers, values):
        agg_value = float(value)  # type: ignore
        metrics.append((metric_name, agg_value))
    metrics_sorted = sorted(metrics, key=lambda x: x[1], reverse=True)
    total_compile_time = sum([v for _, v in metrics_sorted])
    metrics_msg = f""total compile time {total_compile_time:.2f}:\n""
    for metric_name, agg_value in metrics_sorted:
        metrics_msg += f""\t{metric_name:30s}: {agg_value:.2f} seconds\n""
    logging.info(metrics_msg)


def train_model() -> None:
    model = MyMod()
    model.to(""cuda"")

    def train(x: torch.Tensor) -> None:
        y = model(x)
        loss = slow_compile_loss(y)
        return loss

    compiled_train = torch.compile(train, mode=""max-autotune"", dynamic=False)
    opt = torch.optim.SGD(model.parameters(), lr=0.01)

    x = torch.randn((BATCH_SIZE, INPUT_DIM), device=""cuda"")
    with timer(""first""):
        torch.compiler.cudagraph_mark_step_begin()
        loss = compiled_train(x)
        loss.backward()
        opt.step()
        opt.zero_grad(set_to_none=True)

    log_compile_times()
    with timer(""compiled""):
        torch.compiler.cudagraph_mark_step_begin()
        loss = compiled_train(x)
        loss.backward()
        opt.step()
        opt.zero_grad(set_to_none=True)


dynamo_timed = torch._dynamo.utils.dynamo_timed  # type: ignore[has-type]


def monkey_patch_recheck_cache() -> None:
    def recheck_autotune_cache(self, reload_kernel_from_src) -> None:
        """"""
        On cache load on static autotuner, we need to recheck the autotune cache, since
        a best config could have been found from a previous run
        """"""
        assert self.is_statically_launchable()

        configs = [result.config for result in self.compile_results]

        (cached_configs, _, autotune_cache_info) = triton_heuristics.check_autotune_cache(
            configs, self.filename, self.inductor_meta
        )
        self.autotune_cache_info = autotune_cache_info
        # I.e. there was an autotune cache hit
        # NOTE in torch this is > 1, but should be >= 1
        # NOTE
        # NOTE
        if len(cached_configs) == 1 and len(configs) >= 1:
            # NOTE
            # NOTE
            # NOTE
            best_config = cached_configs[0]
            # Grab the best compiled config, if it's in the list of available ones
            best_config_hash = triton_config_to_hashable(best_config)

            for compile_result in self.compile_results:
                if triton_config_to_hashable(compile_result.config) == best_config_hash:
                    # NOTE need to set this to true so that later logic knows
                    # NOTE
                    # NOTE
                    compile_result.config.found_by_coordesc = True
                    # NOTE
                    # NOTE
                    # NOTE
                    self.compile_results = [compile_result]
                    return

            # If the best config isn't in our list of compile results,
            # it's likely because it was found by coordesc after the cache
            # already saved
            if best_config.found_by_coordesc:
                with dynamo_timed(""CachingAutotuner.slow_precompile_config""):
                    if self.fn.fn is None:
                        self.fn = reload_kernel_from_src().fn
                    # NOTE need to set this to true so that later logic knows
                    # NOTE
                    # NOTE
                    launcher = self._precompile_config(best_config)
                    launcher.config.found_by_coordesc = True
                    # NOTE
                    # NOTE
                    # NOTE
                    self.compile_results = [launcher]

    torch._inductor.runtime.triton_heuristics.CachingAutotuner.recheck_autotune_cache = recheck_autotune_cache


if __name__ == ""__main__"":
    logging.basicConfig(level=logging.INFO, stream=sys.stderr)
    inductor_config.compile_threads = 0
    monkey_patch_recheck_cache()
    train_model()

```



### Error logs

without the patched function
```bash
# first call with empty cache
INFO:root:first: took 154.99
INFO:root:total compile time 885.31:
        compile_fx_inner              : 88.55 seconds
        fx_codegen_and_compile        : 88.53 seconds
        CachingAutotuner.coordinate_descent_tuning: 66.19 seconds
        GraphLowering.compile_to_fn   : 63.45 seconds
        GraphLowering.compile_to_module: 63.45 seconds
        backward._backward_impl       : 53.99 seconds
        compile_fx.<locals>.bw_compiler: 53.94 seconds
        _compile.compile_inner        : 38.51 seconds
        compile_attempt_0             : 38.45 seconds
        OutputGraph.call_user_compiler: 38.06 seconds
        create_aot_dispatcher_function: 37.56 seconds
        PyCodeCache.load_by_key_path  : 35.58 seconds
        async_compile.precompile      : 35.55 seconds
        compile_fx.<locals>.fw_compiler_base: 34.60 seconds
        GraphLowering.codegen         : 27.86 seconds
        Scheduler.__init__            : 25.74 seconds
        GraphLowering.run             : 24.07 seconds
        Scheduler.fused_nodes         : 20.32 seconds
        benchmark_fused_nodes         : 18.89 seconds
        InductorBenchmarker.benchmark_gpu: 14.05 seconds
        mm_template_autotuning        : 3.38 seconds
        CachingAutotuner.synchronize  : 2.71 seconds
        CachingAutotuner.benchmark_all_configs: 2.32 seconds
        Scheduler.codegen             : 2.07 seconds
        _recursive_joint_graph_passes : 1.59 seconds
        addmm_template_autotuning     : 1.55 seconds
        pad_mm_benchmark              : 1.23 seconds
        aot_trace_joint_graph         : 0.75 seconds
        InductorBenchmarker.benchmark : 0.50 seconds
        bytecode_tracing              : 0.37 seconds
        min_cut_rematerialization_partition: 0.35 seconds
        TritonBundler.collect         : 0.24 seconds
        aot_collect_metadata          : 0.22 seconds
        additional_fake_tensor_prop   : 0.19 seconds
        inductor_codecache_torch_key  : 0.13 seconds
        _recursive_post_grad_passes   : 0.10 seconds
        PythonWrapperCodegen.generate : 0.07 seconds
        mm_template_precompiling      : 0.06 seconds
        build_guards                  : 0.06 seconds
        cudagraphify.get_container    : 0.04 seconds
        addmm_template_precompiling   : 0.02 seconds
        pad_mm_benchmark_get_do_bench : 0.01 seconds
        _recursive_pre_grad_passes    : 0.01 seconds
        gc                            : 0.00 seconds

# second with warm cache

INFO:root:first: took 14.49
INFO:root:total compile time 21.78:
        CachingAutotuner.coordinate_descent_tuning: 12.50 seconds
        InductorBenchmarker.benchmark_gpu: 3.88 seconds
        _compile.compile_inner        : 1.31 seconds
        compile_attempt_0             : 1.25 seconds
        OutputGraph.call_user_compiler: 0.90 seconds
        AOTAutogradCache.inductor_load: 0.41 seconds
        bytecode_tracing              : 0.32 seconds
        PyCodeCache.load_by_key_path  : 0.20 seconds
        reload_kernel_in_parent       : 0.19 seconds
        cudagraphify.get_container    : 0.18 seconds
        StaticAutotunerFuture.warm_precompile: 0.17 seconds
        CachingAutotuner.benchmark_all_configs: 0.15 seconds
        inductor_codecache_torch_key  : 0.14 seconds
        CachingAutotuner.slow_precompile_config: 0.09 seconds
        build_guards                  : 0.06 seconds
        TritonBundler.read_and_emit   : 0.02 seconds
        _recursive_pre_grad_passes    : 0.01 seconds
        CachingAutotuner.synchronize  : 0.01 seconds
        TritonBundler.load_cached_static_autotuners: 0.01 seconds
        gc                            : 0.00 seconds

```
If I then clear the cache and repeat the above but with the monkeypatched function
```bash
# first
same as above

# warm cache
INFO:root:first: took 1.73
INFO:root:total compile time 6.21:
        _compile.compile_inner        : 1.43 seconds
        compile_attempt_0             : 1.37 seconds
        OutputGraph.call_user_compiler: 1.03 seconds
        AOTAutogradCache.inductor_load: 0.54 seconds
        PyCodeCache.load_by_key_path  : 0.32 seconds
        bytecode_tracing              : 0.32 seconds
        StaticAutotunerFuture.warm_precompile: 0.29 seconds
        CachingAutotuner.slow_precompile_config: 0.22 seconds
        CachingAutotuner.benchmark_all_configs: 0.15 seconds
        InductorBenchmarker.benchmark_gpu: 0.14 seconds
        inductor_codecache_torch_key  : 0.14 seconds
        reload_kernel_in_parent       : 0.13 seconds
        build_guards                  : 0.06 seconds
        cudagraphify.get_container    : 0.03 seconds
        TritonBundler.read_and_emit   : 0.02 seconds
        CachingAutotuner.synchronize  : 0.01 seconds
        _recursive_pre_grad_passes    : 0.01 seconds
        TritonBundler.load_cached_static_autotuners: 0.01 seconds
        gc                            : 0.00 seconds
```

### Versions

torch==2.9.1

cc @chauhang @penguinwu @voznesenskym @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @kadeng @muchulee8 @amjames @aakhundov @coconutruben @jataylo","['triaged', 'oncall: pt2', 'module: inductor']",0
pytorch/pytorch,ðŸ› Bug: `local_map` produces incorrect results with uneven sharding,"### ðŸ› Describe the bug

`torch.distributed.tensor.experimental.local_map` doesn't handle unevenly sharded DTensors correctly. When local shards have different sizes across ranks (common in real training scenarios), it creates DTensors with wrong global shapes and values.

```
import torch
import torch.distributed as dist
from torch.distributed.device_mesh import init_device_mesh
from torch.distributed.tensor import distribute_tensor, Shard
from torch.distributed.tensor.experimental import local_map

# run with: torchrun --nproc_per_node=2 repro.py

dist.init_process_group(backend=""gloo"")
device_mesh = init_device_mesh(""cpu"", mesh_shape=(2,))

# 5 elements across 2 workers -> rank 0 gets [3], rank 1 gets [2]
data = torch.arange(20).reshape(5, 4).float()
dt = distribute_tensor(data, device_mesh, [Shard(0)])

def add_one(x):
    return x + 1

local_add = local_map(
    add_one,
    out_placements=[Shard(0)],
    in_placements=([Shard(0)],),
    device_mesh=device_mesh,
)

result = local_add(dt)
print(f""Expected shape: {data.shape}"")
print(f""Got shape: {result.full_tensor().shape}"")  # WRONG!

dist.destroy_process_group()
```

**Expected**: output shape `torch.Size([5, 4])` with correct values
**Actual**: output shape `torch.Size([3, 4])` (uses rank 0's local size as global)

this is common in production when 
- last batch in training has 1000 samples, batch_size=32, 3 workers â†’ final batch has 8 samples split [3,3,2]
- tensor parallelism:  hidden dimension 2049 across 8 workers
- any non-divisible dimension: size N where `N % world_size != 0`

### Versions

```
PyTorch version: 2.11.0a0+git0f84569
Is debug build: False
CUDA used to build PyTorch: 13.0
ROCM used to build PyTorch: N/A

OS: Fedora Linux 42 (Cloud Edition) (x86_64)
GCC version: (GCC) 15.2.1 20250808 (Red Hat 15.2.1-1)
Clang version: Could not collect
CMake version: version 4.2.1
Libc version: glibc-2.41

Python version: 3.10.19 | packaged by conda-forge | (main, Oct 22 2025, 22:29:10) [GCC 14.3.0] (64-bit runtime)
Python platform: Linux-6.15.7-200.fc42.x86_64-x86_64-with-glibc2.41
Is CUDA available: True
CUDA runtime version: 13.0.88
CUDA_MODULE_LOADING set to: 
GPU models and configuration: 
GPU 0: Tesla T4
GPU 1: Tesla T4
GPU 2: Tesla T4
GPU 3: Tesla T4

Nvidia driver version: 580.82.09
cuDNN version: Could not collect
Is XPU available: False
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True
Caching allocator config: N/A

CPU:
Architecture:                            x86_64
CPU op-mode(s):                          32-bit, 64-bit
Address sizes:                           46 bits physical, 48 bits virtual
Byte Order:                              Little Endian
CPU(s):                                  48
On-line CPU(s) list:                     0-47
Vendor ID:                               GenuineIntel
Model name:                              Intel(R) Xeon(R) Platinum 8259CL CPU @ 2.50GHz
CPU family:                              6
Model:                                   85
Thread(s) per core:                      2
Core(s) per socket:                      24
Socket(s):                               1
Stepping:                                7
BogoMIPS:                                5000.00
Flags:                                   fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch cpuid_fault pti fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke avx512_vnni
Hypervisor vendor:                       KVM
Virtualization type:                     full
L1d cache:                               768 KiB (24 instances)
L1i cache:                               768 KiB (24 instances)
L2 cache:                                24 MiB (24 instances)
L3 cache:                                35.8 MiB (1 instance)
NUMA node(s):                            1
NUMA node0 CPU(s):                       0-47
Vulnerability Gather data sampling:      Unknown: Dependent on hypervisor status
Vulnerability Ghostwrite:                Not affected
Vulnerability Indirect target selection: Mitigation; Aligned branch/return thunks
Vulnerability Itlb multihit:             KVM: Mitigation: VMX unsupported
Vulnerability L1tf:                      Mitigation; PTE Inversion
Vulnerability Mds:                       Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown
Vulnerability Meltdown:                  Mitigation; PTI
Vulnerability Mmio stale data:           Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown
Vulnerability Reg file data sampling:    Not affected
Vulnerability Retbleed:                  Vulnerable
Vulnerability Spec rstack overflow:      Not affected
Vulnerability Spec store bypass:         Vulnerable
Vulnerability Spectre v1:                Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:                Mitigation; Retpolines; STIBP disabled; RSB filling; PBRSB-eIBRS Not affected; BHI Retpoline
Vulnerability Srbds:                     Not affected
Vulnerability Tsa:                       Not affected
Vulnerability Tsx async abort:           Not affected

Versions of relevant libraries:
[pip3] intel-cmplr-lib-ur==2025.3.1
[pip3] intel-openmp==2025.3.1
[pip3] numpy==2.2.6
[pip3] optree==0.18.0
[pip3] tcmlib==1.4.1
[pip3] torch==2.11.0a0+git0b2b744
[pip3] umf==1.0.2
[conda] intel-cmplr-lib-ur        2025.3.1                 pypi_0    pypi
[conda] intel-openmp              2025.3.1                 pypi_0    pypi
[conda] nccl                      2.29.2.1             h8340e53_1    conda-forge
[conda] numpy                     2.2.6                    pypi_0    pypi
[conda] optree                    0.18.0                   pypi_0    pypi
[conda] tcmlib                    1.4.1                    pypi_0    pypi
[conda] torch                     2.11.0a0+git0b2b744          pypi_0    pypi
[conda] umf                       1.0.2                    pypi_0    pypi
```

cc @ezyang @gchanan @kadeng @msaroufim @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @dcci @tianyu-l @XilunWu @SherlockNoMad @ppwwyyxx","['high priority', 'triage review', 'oncall: distributed', 'module: dtensor']",0
pytorch/pytorch,Flight Recoder default buffer size is not consistent,"### ðŸ› Describe the bug

Currently flight recoder is opened by default ,  but the buffer size default value in here: https://github.com/pytorch/pytorch/blob/release/2.9/torch/csrc/distributed/c10d/FlightRecorder.hpp#L100 is not consistent with the value in ProcessGroupNCCL:  https://github.com/pytorch/pytorch/blob/release/2.9/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp#L908

### Versions

PyTorch version: 2.9.0+cu128
Is debug build: False
CUDA used to build PyTorch: 12.8
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: 10.0.0-4ubuntu1
CMake version: version 3.16.3
Libc version: glibc-2.31

Python version: 3.10.18 (main, Jun  5 2025, 13:14:17) [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.13.0-52-generic-x86_64-with-glibc2.31
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration:
GPU 0: NVIDIA A100-SXM4-80GB
GPU 1: NVIDIA A100-SXM4-80GB
GPU 2: NVIDIA A100-SXM4-80GB
GPU 3: NVIDIA A100-SXM4-80GB
GPU 4: NVIDIA A100-SXM4-80GB
GPU 5: NVIDIA A100-SXM4-80GB
GPU 6: NVIDIA A100-SXM4-80GB
GPU 7: NVIDIA A100-SXM4-80GB

Nvidia driver version: 570.172.08
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] mypy_extensions==1.1.0
[pip3] numpy==2.1.2
[pip3] pytorch-triton==3.5.0+git7416ffcb
[pip3] torch==2.9.0+cu128
[pip3] torchaudio==2.8.0+cu128
[pip3] torchdata==0.11.0
[pip3] torchvision==0.23.0+cu128
[conda] numpy                       2.1.2              pypi_0           pypi
[conda] pytorch-triton              3.5.0+git7416ffcb  pypi_0           pypi
[conda] torch                       2.9.0+cu128        pypi_0           pypi
[conda] torchaudio                  2.8.0+cu128        pypi_0           pypi
[conda] torchdata                   0.11.0             pypi_0           pypi
[conda] torchvision                 0.23.0+cu128       pypi_0           pypi

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @msaroufim @dcci",['oncall: distributed'],0
pytorch/pytorch,"[CUDA] Auto-detected arch flags omit accelerated suffix (sm_120a â†’ sm_120), breaking Blackwell NVFP4/block-scaled MMA","When `TORCH_CUDA_ARCH_LIST` is unset, `_get_cuda_arch_flags()` auto-detects GPU architecture but strips `a`/`f` suffixes because the code only uses the major.minor capability tuple. This causes `sm_120a` builds to generate `-gencode=...sm_120` instead of `sm_120a`, breaking CUTLASS block_scale MMA and NVFP4 on Blackwell.

Downstream impact:
- CUTLASS: https://github.com/NVIDIA/cutlass/issues/2800
- CUTLASS: https://github.com/NVIDIA/cutlass/issues/2820
- Transformer Engine: https://github.com/NVIDIA/TransformerEngine/issues/2255

Fix submitted: PR #172721

cc @malfet @seemethere @ptrblck @msaroufim @eqy @jerryzh168 @tinglvv @nWEIdia","['module: build', 'module: cuda', 'triaged']",0
scikit-learn/scikit-learn,"âš ï¸ CI failed on Linux_Nightly.pylatest_pip_scipy_dev (last failure: Jan 22, 2026) âš ï¸","**CI failed on [Linux_Nightly.pylatest_pip_scipy_dev](https://dev.azure.com/scikit-learn/scikit-learn/_build/results?buildId=84468&view=logs&j=dfe99b15-50db-5d7b-b1e9-4105c42527cf)** (Jan 22, 2026)
Unable to find junit file. Please see link for details.",['Needs Triage'],0
scikit-learn/scikit-learn,Support clusterers in `DecisionBoundaryDisplay`,"### Describe the workflow you want to enable

So far, `response_method=""auto""` does not resolve correctly (list of options as for classifiers instead of `predict`).
The approach I implemented is: using PCA, followed by DBSCAN and Logistic Regression as a final step.
Properly showing the relevant datapoints, categorised/colored by their identified clusters, would be great without an extra call to `scatter`.

### Describe your proposed solution

As per discussion linked below, proper support for clusterers should be added to `DecisionBoundaryDisplay`.

### Describe alternatives you've considered, if relevant

Simply plotting a scatterplot on top does not necessarily yield the same class colors, because the clusters are not necessarily ordered in the same way as the `y` values.

### Additional context

https://github.com/scikit-learn/scikit-learn/pull/33015#issuecomment-3738401403",['New Feature'],0
scikit-learn/scikit-learn,fetch_kddcup99 cache written uncompressed due to legacy decompression workaround,"The `fetch_kddcup99` dataset loader currently forces `joblib.dump(..., compress=0)` when writing its local cache. This appears to be a historical workaround for an old decompression bug referenced in an inline comment, but the workaround is still present. As a result, the cache is stored uncompressed even though the dataset is very large, leading to unnecessary disk usage and extra IO.

<img width=""689"" height=""323"" alt=""Image"" src=""https://github.com/user-attachments/assets/8da31fff-e2d9-45e4-beab-3bc88e5b088c"" />

## Impact
- Significantly larger local cache footprint for the KDDCup99 dataset
- Increased disk usage and slower cache read/write operations
- Inconsistent with other dataset fetchers that use compressed joblib caches by default

## Steps/Code to Reproduce
```python
from sklearn.datasets import fetch_kddcup99

# Download and cache the dataset
fetch_kddcup99(download_if_missing=True)

# Inspect the cache directory (default):
# ~/scikit_learn_data/kddcup99_10-py3/
# Files: samples, targets
# Observe these files are uncompressed on disk.
```

## Expected Results
Cache files should be written with compression (e.g., compress=3) to reduce disk usage and IO.

## Actual Results
Cache files are always written uncompressed due to compress=0.

",[],0
scikit-learn/scikit-learn,Fix and refactor color handling in `DecisionBoundaryDisplay`,"This is to keep track and address several things we noticed in #33015

I will address them in follow-up PRs.

- [ ] check number of (distinct) `colors == n_classes`
- [ ] use same selection mechanism for all qualitative cmaps (that have `colors`)
- [ ] refactor and unify checks (some done in `from_estimator`, some in `plot`)
- [ ] use colors for binary problems as well (but keep `viridis` as default)
- [ ] maybe even move color selection into a separate function?

Please add, if I missed anything @lucyleeow @ogrisel ",[],0
scikit-learn/scikit-learn,BUG: Trees: minor bug in `NaN` detection leading to incorrect split.,"### Describe the bug

The `break` [here](https://github.com/scikit-learn/scikit-learn/blob/afc095a7b17d5528d62f55f07cab39f963299c78/sklearn/tree/_utils.pyx#L85) is wrong, and in some edge cases it will prevent `_any_isnan_axis0` detecting NaNs for some features. Then because of that, the splitter won't considers splits with NaNs on the left (because it's not aware this feature has NaNs).

### Steps/Code to Reproduce

```Python
import numpy as np
from numpy import nan
from sklearn.tree import DecisionTreeRegressor

X = np.array([
    [nan, nan],
    [1, 0],
    [1, 1]
])

y = np.array([0, 0, 1])

tree = DecisionTreeRegressor(max_depth=1).fit(X, y)
assert (tree.tree_.impurity[1:] == 0).all()
```

### Expected Results

The best split is found => No assertion error

The best split: `X[:, 1] < 0.5` and missing values to the left node.

### Actual Results

Assertion error.

### Versions

```shell
System:
    python: 3.12.11 (main, Aug 18 2025, 19:19:11) [Clang 20.1.4 ]
executable: /home/arthur/dev-perso/scikit-learn/sklearn-env/bin/python
   machine: Linux-6.14.0-37-generic-x86_64-with-glibc2.39

Python dependencies:
      sklearn: 1.9.dev0
          pip: None
   setuptools: 80.9.0
        numpy: 2.3.5
        scipy: 1.16.3
       Cython: 3.2.1
       pandas: 2.3.3
   matplotlib: 3.10.7
       joblib: 1.5.2
threadpoolctl: 3.6.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: openblas
    num_threads: 16
         prefix: libscipy_openblas
       filepath: /home/arthur/dev-perso/scikit-learn/sklearn-env/lib/python3.12/site-packages/numpy.libs/libscipy_openblas64_-fdde5778.so
        version: 0.3.30
threading_layer: pthreads
   architecture: Haswell

       user_api: blas
   internal_api: openblas
    num_threads: 16
         prefix: libscipy_openblas
       filepath: /home/arthur/dev-perso/scikit-learn/sklearn-env/lib/python3.12/site-packages/scipy.libs/libscipy_openblas-b75cc656.so
        version: 0.3.29.dev
threading_layer: pthreads
   architecture: Haswell

       user_api: openmp
   internal_api: openmp
    num_threads: 16
         prefix: libgomp
       filepath: /usr/lib/x86_64-linux-gnu/libgomp.so.1.0.0
        version: None
```",['Bug'],0
scikit-learn/scikit-learn,`cross_validate` slows down with `n_jobs > 1`,"### Describe the bug

The time needed to`cross_validate` can have a big (x100) slow down when using parallel jobs (`n_jobs > 1`).
Note that the effect is difficult to reproduce as it happens only on the first few calls to `cross_validate`, perhaps some weird caching mechanism is at play ? 

### Steps/Code to Reproduce

```python
from sklearn.datasets import make_regression
from sklearn.linear_model import Ridge
from sklearn.model_selection import cross_validate
import time

est = Ridge()
X, y = make_regression(100, 5, random_state=22)

for idx in range(5):
    start_time = time.time()
    scores = cross_validate(est, X, y, n_jobs=2)
    elapsed_time = time.time() - start_time
    print(f""{idx=} {elapsed_time=:.2f}"")
```

### Expected Results

About the same time for each call.

### Actual Results

```
idx=0 elapsed_time=1.15 # about 100x slower
idx=1 elapsed_time=0.01
idx=2 elapsed_time=0.01
idx=3 elapsed_time=0.01
idx=4 elapsed_time=0.01
```

### Versions

```shell
System:
    python: 3.13.5 | packaged by conda-forge | (main, Jun 16 2025, 08:24:05) [Clang 18.1.8 ]
executable: /Users/abaker/miniforge3/envs/sklearn-dev/bin/python
   machine: macOS-14.5-arm64-arm-64bit-Mach-O

Python dependencies:
      sklearn: 1.9.dev0
          pip: 25.2
   setuptools: 80.9.0
        numpy: 2.3.2
        scipy: 1.16.1
       Cython: 3.1.3
       pandas: 2.3.2
   matplotlib: 3.10.5
       joblib: 1.5.1
threadpoolctl: 3.6.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: openblas
    num_threads: 8
         prefix: libopenblas
       filepath: /Users/abaker/miniforge3/envs/sklearn-dev/lib/libopenblas.0.dylib
        version: 0.3.30
threading_layer: openmp
   architecture: VORTEX

       user_api: openmp
   internal_api: openmp
    num_threads: 8
         prefix: libomp
       filepath: /Users/abaker/miniforge3/envs/sklearn-dev/lib/libomp.dylib
        version: None
```","['Bug', 'Documentation', 'Performance']",0
scikit-learn/scikit-learn,RFC `DecisionBoundaryDisplay` levels overlap with 'contour',"> [!WARNING]
> This is a request for comment (RFC), please do not open a pull request for this issue. If you wish to contribute to scikit-learn please have a look at our [contributing doc](https://scikit-learn.org/dev/developers/contributing.html) and in particular the section [New contributors](https://scikit-learn.org/dev/developers/contributing.html#new-contributors).

Noticed while reviewing #33015. Also related to #32866

#31553 reverted the plotting behaviour when `plot_method='contour'` such that only thresholded integers are used plotting, for all plot methods, not continuous proba/decision function values.

This means that the contour lines show where the predicted class changes from one to the other:

<img width=""1600"" height=""896"" alt=""Image"" src=""https://github.com/user-attachments/assets/d583b915-d44a-4071-98fd-e68deb41bd7b"" />

instead of contour lines showing how continuous predicted values change, over the region where that class dominates:

<img width=""1600"" height=""896"" alt=""Image"" src=""https://github.com/user-attachments/assets/62babf2a-83f0-42c7-8f6a-e14971373d0a"" />

(figures taken from #31546) 

The problem is that without setting `levels` mpl infers number of levels, 9 seems to be the default, resulting in a lot of overlapping lines.

<details open>
<summary>Details</summary>

The [contourf](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.contourf.html#matplotlib.pyplot.contourf) docs aren't very helpful - it says `levels` is optional but does not tell you what the default is. Digging into the code, it seems that [`_process_contour_level_args`](https://github.com/matplotlib/matplotlib/blob/ebf9b14f7f41029e2c61dd09cc37dae98a9bc10d/lib/matplotlib/contour.py#L1011C9-L1011C36) is responsible. It calls [`_ensure_locator_exists`](https://github.com/matplotlib/matplotlib/blob/ebf9b14f7f41029e2c61dd09cc37dae98a9bc10d/lib/matplotlib/contour.py#L957) which does:

```python
        if self.locator is None:
            if self.logscale:
                self.locator = ticker.LogLocator(numticks=N)
            else:
                if N is None:
                    N = 7  # Hard coded default
                self.locator = ticker.MaxNLocator(N + 1, min_n_ticks=1)
```

which seems to call [`MaxNLocator`](https://matplotlib.org/stable/api/ticker_api.html#matplotlib.ticker.MaxNLocator), with `n_bins=7+1`, which gives max 9 (n_bins + 1) ticks. This does seem to be what happens:

```python
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.inspection import DecisionBoundaryDisplay
from sklearn.linear_model import LogisticRegression

iris = load_iris()
X = iris.data[:, :2]
classifier = LogisticRegression().fit(X, iris.target)

# Create the contour plot
disp = DecisionBoundaryDisplay.from_estimator(
    classifier,
    X,
    plot_method='contour',
    xlabel=iris.feature_names[0],
    ylabel=iris.feature_names[1],
)

# Access the matplotlib axes and contour object
ax = disp.ax_
contour = disp.surface_

print(f""Number of classes: {len(iris.target_names)}"")
print(f""Number of contour levels: {len(contour.levels)}"")
print(f""Contour levels (decision values): {contour.levels}"")
```

```
Number of classes: 3
Number of contour levels: 9
Contour levels (decision values): [0.   0.25 0.5  0.75 1.   1.25 1.5  1.75 2.  ]
```

</details>

The other problem is that we pass `n_classes` colors, but this does not match the number of levels (the passed colours are used to colour each level). In the case above, we pass 3 colours but there are 9 levels and from the docs:

> The sequence is cycled for the levels in ascending order. If the sequence is shorter than the number of levels, it's repeated.

Thus the colours don't make sense and don't really correspond to any individual class.

---

Not sure what the best solution would be. We could specify `levels` such that we get `n_classes` levels, which would be a great improvement already. The question of what colour(s) the lines should be is more difficult. e.g., for the case above with 3 classes and levels at [1,2,3], 2 lines are drawn: 


<img width=""567"" height=""432"" alt=""Image"" src=""https://github.com/user-attachments/assets/101f6e59-565f-4d41-85bd-9ac932e02085"" />

<details open>
<summary>Code</summary>

```python
from sklearn.datasets import load_iris
from sklearn.inspection import DecisionBoundaryDisplay
from sklearn.linear_model import LogisticRegression

iris = load_iris()
X = iris.data[:, :2]
classifier = LogisticRegression().fit(X, iris.target)

# Create the contour plot
disp = DecisionBoundaryDisplay.from_estimator(
    classifier,
    X,
    plot_method='contour',
    xlabel=iris.feature_names[0],
    ylabel=iris.feature_names[1],
    levels=[0,1,2],
)

```

</details>


one for level 0: the transition from 0 -> 1 / 0 -> 2 and one for level 1: the transition from 1 -> 2 (the level 1 line also draws a line from 0 -> 2, but I think that is due to interpolation of the 0 -> 2 transition being 1 ? - this is the part where there are both blue and orange lines). The line colours don't really match to a single class, mostly because we have 2 lines for 3 classes.

Maybe it would be best to just draw `contour` lines in just one colour?

Related: https://github.com/scikit-learn/scikit-learn/pull/33015#discussion_r2703246240

cc @AnneBeyer @ogrisel ",['RFC'],0
scikit-learn/scikit-learn,CI Automatic update of CUDA CI lock-file fails,"[January 19 build log](https://github.com/scikit-learn/scikit-learn/actions/runs/21126061615/job/60747475675)

[January 12 build log](https://github.com/scikit-learn/scikit-learn/actions/runs/20908719794/job/60067290832)

The symptom is that it fails after a very long time (more than 45 minutes in both cases) and exits with code 143, which means that it got a `SIGTERM` signal (143=128 + 15, 15=`SIGTERM`).

```
Preparing transaction: - done
Verifying transaction: | / done
Executing transaction: \ | / - \ | / - \ | / - \ | / - \ | / - \ | / - \ | / - done
# 1 selected builds
  - pylatest_conda_forge_cuda_array-api_linux-64, type: conda, tag: cuda
# Writing conda environments
# Writing conda lock files
# Locking dependencies for pylatest_conda_forge_cuda_array-api_linux-64
Error: Process completed with exit code 143.
```

I guess one of the first step would be to try to reproduce locally.",['Build / CI'],0
scikit-learn/scikit-learn,"âš ï¸ CI failed on Linux_Runs.pylatest_conda_forge_mkl (last failure: Jan 20, 2026) âš ï¸","**CI is still failing on [Linux_Runs.pylatest_conda_forge_mkl](https://dev.azure.com/scikit-learn/scikit-learn/_build/results?buildId=84414&view=logs&j=dde5042c-7464-5d47-9507-31bdd2ee0a3a)** (Jan 20, 2026)
- Test Collection Failure",[],0
scikit-learn/scikit-learn,Add `show_data=False` option to `DecisionBoundaryDisplay.from_estimator`,"### Describe the workflow you want to enable

While reviewing @AnneBeyer's #33015 PR, I thought it would be nice to be able to display the decision boundaries of a classifier along with the data points with consistent color maps (at least for classifiers, outlier detectors and regressors) instead of having to a manually scatter plot on the same `ax` instance.


### Describe your proposed solution

Add a new `show_data` boolean kwarg to the `from_estimator` method that already accepts the `X` values as argument. The `from_estimator` method would then call `estimator.predict(X)` to find the values of the target to be used to set the colors of the dots of the scatter plot.


The `from_estimator` could also take an optional `y` argument with true class labels. When provided, the scatter plot enabled from the `show_data=True` param of `from_estimator` would use the provided `y` values instead of using `estimator.predict(X)`.

The `__init__` of the display would then also accept the `X` and `y` values.

Ideally, the legend would make it explicit whether the scatter dots represent predictions or true labels, and could be overridden by a `data_label` kwarg passed to `from_estimator`.

Maybe `show_data=True` could even be the default.

### Describe alternatives you've considered, if relevant

While it's possible to do scatter plot manually, it's quite verbose and not necessarily trivial to understand if the colors used for the two plots are expected to be consistent or not.

I think people often want to overlay the training set with the decision boundary when generating such plots for educational purposes.

### Additional context

@AnneBeyer if you think this is a good feature to add, would be interested in contributing it (as a follow-up to #33015 and related PRs)?

Also cc @lucyleeow @glemaitre in case you already discussed this in the past.",['New Feature'],0
scikit-learn/scikit-learn,DOC error message when checking response values of an estimator not clear,"### Describe the issue linked to the documentation

Current function that checks response values of an estimator
assumes that if the estimator is not a classifier or an outlier detector, then it is a regressor.

In the latter case, if it doesn't find the `predict` function, 
it raises an error stating that the estimator is a regressor: `ValueError: ... Got a regressor...`.

However, the estimator might be something other than a regressor.

For example, it can be a classification `Pipeline` with a MRO issue, see https://github.com/NeuroTechX/moabb/issues/815
Many other GitHub repositories have reported this ambiguous error, see list in https://github.com/scikit-learn/scikit-learn/pull/33084#issue-3814820493

The goal of this issue is to discuss how to correct the message by making it clearer,
specifically by no longer stating that it is a regressor.

### Suggest a potential alternative/fix

Current function `_get_response_values.py` in `scikit-learn/sklearn/utils/_response.py`
```
    else:  # estimator is a regressor
        if response_method != ""predict"":
            raise ValueError(
                f""{estimator.__class__.__name__} should either be a classifier to be ""
                f""used with response_method={response_method} or the response_method ""
                ""should be 'predict'. Got a regressor with response_method=""
                f""{response_method} instead.""
            )
        prediction_method = estimator.predict
        y_pred, pos_label = prediction_method(X), None
```
could be corrected into
```
    elif is_regressor(estimator):
        prediction_method = estimator.predict
        y_pred, pos_label = prediction_method(X), None
    else:
        raise ValueError(
            f""{estimator.__class__.__name__} is not recognized as a classifier, ""
            ""an outlier detector or a regressor.""
        )
```
and, if necessary, we could add before `else`

```
    elif is_clusterer(estimator):
        prediction_method = ...
        y_pred, pos_label = ...
```","['Documentation', 'Needs Triage']",0
scikit-learn/scikit-learn,StratifiedGroupKFold produces empty / degenerate splits,"### Describe the bug

StratifiedGroupKFold seems to be broken (Python 3.13, scikit-learn 1.8.0) for some fairly simple grouped classification datasets, quite frequently producing empty training or test sets. 

While I am aware of the many issues covering poor stratification and the workarounds suggested (https://github.com/scikit-learn/scikit-learn/issues/24656, https://github.com/scikit-learn/scikit-learn/issues/12076), this is a much more annoying problem in automated pipelines, as we can't even assume that e.g. 5-fold actually produces 5 valid folds. 

This seems not to be some rare example, but occurs very often with some pretty simple grouped classification datasets, even when the number of samples is quite large (see attached script `demo.py`).

I really hope I am not doing something dumb here, because this seems to be a pretty big issue otherwise. I think the issue is that, obviously, when the number of groups and/or labels is small, and when these are not both distributed uniformly and independently over the samples, then it can just be the case that you simply cannot satisfy all three goals of balanced folds, groups, and target / y labels simultaneously in a satisfactory way. However, in such cases, there should surely be a warning and some kind of natural fallback (perhaps not stratifying, or ignoring grouping, depending on an option), or perhaps an error. 

But producing a degenerate fold definitely does not feel right. 

[demo.py](https://github.com/user-attachments/files/24630346/demo.py)

### Steps/Code to Reproduce

```python
from sklearn.model_selection import StratifiedGroupKFold

y = [0, 2, 2, 2, 1, 2, 1, 1, 0, 0, 0, 0]
g = [0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1]
skf = StratifiedGroupKFold(n_splits=3, shuffle=False)

for fold, (ix_train, ix_test) in enumerate(skf.split(X=y, y=y, groups=g)):
    if len(ix_train) == 0:
        assert len(ix_test) == len(y)
        degen = ""training""
    elif len(ix_test) == 0:
        assert len(ix_train) == len(y)
        degen = ""test""
    else:
        continue

    print(f""Degenerate (empty) {degen} set on fold {fold + 1} for data:"")
    print(f""y = {y}"")
    print(f""g = {g}"")
```

### Expected Results

We should not be getting empty training or test sets with any k-fold method. This will almost always cause errors downstream. 

### Actual Results

If you run `demo.py`, you will see a large number of combinations of grouping variables and classification targets cause degenerate splits.

### Versions

```shell
>>> import sklearn
>>> sklearn.show_versions()

System:
    python: 3.13.9 (main, Oct 14 2025, 21:10:40) [Clang 20.1.4 ]
executable: /Users/me/Documents/groupkfold/.venv/bin/python
   machine: macOS-15.5-arm64-arm-64bit-Mach-O

Python dependencies:
      sklearn: 1.8.0
          pip: None
   setuptools: None
        numpy: 2.4.1
        scipy: 1.17.0
       Cython: None
       pandas: None
   matplotlib: None
       joblib: 1.5.3
threadpoolctl: 3.6.0

Built with OpenMP: True

threadpoolctl info:
       user_api: openmp
   internal_api: openmp
    num_threads: 8
         prefix: libomp
       filepath: /Users/me/Documents/groupkfold/.venv/lib/python3.13/site-packages/sklearn/.dylibs/libomp.dylib
        version: None
```","['Bug', 'Needs Triage']",0
scikit-learn/scikit-learn,`benchmarks/bench_multilabel_metrics.py` uses removed `jaccard_similarity_score`,"### Describe the bug

I noticed that `benchmarks/bench_multilabel_metrics.py` is currently using `metrics.jaccard_similarity_score`. This function was deprecated and subsequently removed in **scikit-learn 0.23**. Consequently, this benchmark script fails to run on current versions of scikit-learn.

### Steps/Code to Reproduce

Run the benchmark script:

```bash
python benchmarks/bench_multilabel_metrics.py
```

### Expected Results

The script should run successfully, calculating performance metrics for the specified multilabel formats and printing the results table (and plots if requested).

### Actual Results

The script crashes immediately upon import with an `ImportError` because the function no longer exists in `sklearn.metrics`.

```
Traceback (most recent call last):
  File ""/workspaces/scikit-learn/benchmarks/bench_multilabel_metrics.py"", line 17, in <module>
    from sklearn.metrics import (
ImportError: cannot import name 'jaccard_similarity_score' from 'sklearn.metrics' (/workspaces/scikit-learn/.venv/lib/python3.12/site-packages/sklearn/metrics/__init__.py)
```

### Versions

```shell
System:
    python: 3.12.1 (main, Nov 27 2025, 10:47:52) [GCC 13.3.0]
executable: /usr/local/python/3.12.1/bin/python3
   machine: Linux-6.8.0-1030-azure-x86_64-with-glibc2.39

Python dependencies:
      sklearn: 1.8.0
          pip: 25.3
   setuptools: 80.9.0
        numpy: 2.4.1
        scipy: 1.17.0
       Cython: None
       pandas: None
   matplotlib: 3.10.8
       joblib: 1.5.3
threadpoolctl: 3.6.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: openblas
    num_threads: 2
         prefix: libscipy_openblas
       filepath: /workspaces/scikit-learn/.venv/lib/python3.12/site-packages/numpy.libs/libscipy_openblas64_-fdde5778.so
        version: 0.3.30
threading_layer: pthreads
   architecture: Haswell

       user_api: blas
   internal_api: openblas
    num_threads: 2
         prefix: libscipy_openblas
       filepath: /workspaces/scikit-learn/.venv/lib/python3.12/site-packages/scipy.libs/libscipy_openblas-6cdc3b4a.so
        version: 0.3.30
threading_layer: pthreads
   architecture: Haswell

       user_api: openmp
   internal_api: openmp
    num_threads: 2
         prefix: libgomp
       filepath: /workspaces/scikit-learn/.venv/lib/python3.12/site-packages/scikit_learn.libs/libgomp-e985bcbb.so.1.0.0
        version: None
```","['Bug', 'Needs Triage']",0
scikit-learn/scikit-learn,Mixed string/numeric when input is list for classification metrics,"> [!WARNING]
> This is not a good first (or second) issue to contribute. If you are interested in contributing to scikit-learn please have a look at our [contributing doc](https://scikit-learn.org/dev/developers/contributing.html) and in particular the section [Issues for new contributors](https://scikit-learn.org/dev/developers/contributing.html#new-contributors). 

Noticed while working on #32755

Classification metrics (`CLASSIFICATION_METRICS` from `test_common.py`, they are all defined in `sklearn/metrics/_classification.py`, but not all metrics in `_classification.py` fall in this list) vary in how they handle mixed string and numeric inputs (e.g., `y_true` is string and `y_pred` is numeric).

* `precision_recall_fscore_support` and friends (`precision_score`, `recall_score`, `f1_score`, `fbeta_score` and related `jaccard_score`) - raises `ValueError`:
   * via `_check_set_wise_labels`, which calls [`unique_labels`](https://github.com/scikit-learn/scikit-learn/blob/66200f149ba6c64a3a93dd73e37bdeea87bb5db8/sklearn/utils/multiclass.py#L41), which does not allow ""mix of string and integer labelsâ€œ
   * note that these metrics do take a `pos_label` parameter, so theoretically when `y_true` is string, we could use `y_true == pos_label` to convert to [1,0]. This would not make sense if `y_pred` was string.
* `confusion_matrix` (and metrics that use it - `balanced_accuracy_score`, `cohen_kappa_score`) - raises `ValueError` ONLY when `labels=None`:
   * `labels=None`: calls [`unique_labels`](https://github.com/scikit-learn/scikit-learn/blob/66200f149ba6c64a3a93dd73e37bdeea87bb5db8/sklearn/utils/multiclass.py#L41) like above
   * `labels` is set - AFAICT no error but the confusion matrix would only contain 0. e.g., if `y_true` is string and `labels` is set to all possible unique values in `y_true`, we would do the following conversion:
  
https://github.com/scikit-learn/scikit-learn/blob/66200f149ba6c64a3a93dd73e37bdeea87bb5db8/sklearn/metrics/_classification.py#L584-L587

Since none of the string `labels` are present in numeric `y_pred`, `y_pred` would be converted to array that consists only of `n_labels + 1` values

* `multilabel_confusion_matrix` - always raises raises `ValueError` (again via `unique_labels`):
   * unlike `confusion_matrix`, even when `labels` is given, we call `unique_labels(y_true, y_pred)` to get all labels present, and if any are not provided in `labels` they are added.

The following all DO NOT error:

* `accuracy_score` - no error but result would always be 0 (as `score = y_true == y_pred` would always be 0). Note not relevant for multilabel cases as input needs to be label indicator matrix.
* `hamming_loss` - no error but result would always be 1. Again not relevant for multilabel.
* `zero_one_loss` - no error but result would always be 1 (when `normalize=False`). Again not relevant for multilabel.
* `matthews_corrcoef` - no error but result would always be 0. Note that we transform `y_true` and `y_pred` using `LabelEncoder` fit on `y_true` and `y_pred` concat (meaning `y_true` and `y_pred` will be numbers, but not contain any numbers in common), before we pass to `confusion_matrix`.

For these classification metrics which require `y_pred` to be thresholded predictions, I don't think it makes sense to have mixed string and numeri (e.g., how would you match ['apple', 'orange', 'apple] to [2,3,2] ?). Indeed, for the metrics that do not error, the result is the 'worst' value.
For metrics that accept `pos_label` the way forward is less clear. Note that for several continuous classification metrics we do use something like `y_true == pos_label`, thus allowing `y_true` to be string.

Note that I will open a separate issue around this for continuous classification metrics (i.e. those in `_ranking.py`).

cc @ogrisel 

","['RFC', 'Needs Triage']",0
scikit-learn/scikit-learn,Online Quantile Approximation,"### Describe the workflow you want to enable

`partial_fit` for `RobustScaler`, `QuantileTransformer`, and likely others


### Describe your proposed solution

It seems like functionality for approximating quantiles using t-digest or something will do most of the heavy lifting.

### Describe alternatives you've considered, if relevant

Stuck with sampling from the dataset, fitting to it, and hoping it's sufficiently representative of the whole dataset.
","['New Feature', 'Needs Decision - Include Feature']",0
scikit-learn/scikit-learn,`KNeighborsClassifier` fails on non-numeric labels with `ValueError` on certain parameters combination,"### Describe the bug

When used with `algorithm=""brute""` (or `algorithm=""auto""` and the heuristic suggests brute force) and `p = 1`, `KNeighborsClassifier.predict` crashes when y labels are non-integers.

Note that this only crashes for this specific param combination, and `p=2`, for example, works fine with non-integer labels.

This happens because `self.classes_` is passed to `unique_Y_labels` here:

https://github.com/scikit-learn/scikit-learn/blob/66200f149ba6c64a3a93dd73e37bdeea87bb5db8/sklearn/neighbors/_classification.py#L358

And the [docs](https://github.com/scikit-learn/scikit-learn/blob/66200f149ba6c64a3a93dd73e37bdeea87bb5db8/sklearn/metrics/_pairwise_distances_reduction/_dispatcher.py#L514-L516) for `ArgKminClassMode.compute` say:

```
        unique_Y_labels : ndarray
            An array containing all unique indices contained in the
            corresponding `Y_labels` array.
```

while `self.classes_` contains actual values, not indices.

### Steps/Code to Reproduce

```python
import random

import numpy as np
from sklearn.neighbors import KNeighborsClassifier

# generate random data
rng = np.random.default_rng()
x = rng.normal(size=(5, 3))
y = random.choices(['foo', 'bar'], k=5)

kn = KNeighborsClassifier(algorithm='brute', p=1)
kn = kn.fit(x, y)
kn.predict(rng.normal(size=(2, 3)))
```

### Expected Results

No error.

### Actual Results

```python-traceback
Traceback (most recent call last):
  File ""<string>"", line 16, in <module>
  File ""/home/stas/Documents/Work/polymerize/ml_dev/.venv/lib/python3.11/site-packages/sklearn/neighbors/_classification.py"", line 266, in predict
    probabilities = self.predict_proba(X)
                    ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/stas/Documents/Work/polymerize/ml_dev/.venv/lib/python3.11/site-packages/sklearn/neighbors/_classification.py"", line 352, in predict_proba
    probabilities = ArgKminClassMode.compute(
                    ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/stas/Documents/Work/polymerize/ml_dev/.venv/lib/python3.11/site-packages/sklearn/metrics/_pairwise_distances_reduction/_dispatcher.py"", line 589, in compute
    unique_Y_labels=np.array(unique_Y_labels, dtype=np.intp),
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: invalid literal for int() with base 10: np.str_('bar')
```

### Versions

```shell
System:
    python: 3.11.14 (main, Dec  9 2025, 19:02:23) [Clang 21.1.4 ]
executable: /home/stas/Documents/Work/polymerize/ml_dev/.venv/bin/python
   machine: Linux-6.14.0-37-generic-x86_64-with-glibc2.39

Python dependencies:
      sklearn: 1.8.0
          pip: None
   setuptools: None
        numpy: 2.4.0
        scipy: 1.16.3
       Cython: None
       pandas: 2.3.1
   matplotlib: 3.10.8
       joblib: 1.5.3
threadpoolctl: 3.6.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: openblas
    num_threads: 4
         prefix: libscipy_openblas
       filepath: /home/stas/Documents/Work/polymerize/ml_dev/.venv/lib/python3.11/site-packages/numpy.libs/libscipy_openblas64_-fdde5778.so
        version: 0.3.30
threading_layer: pthreads
   architecture: Haswell

       user_api: blas
   internal_api: openblas
    num_threads: 4
         prefix: libscipy_openblas
       filepath: /home/stas/Documents/Work/polymerize/ml_dev/.venv/lib/python3.11/site-packages/scipy.libs/libscipy_openblas-b75cc656.so
        version: 0.3.29.dev
threading_layer: pthreads
   architecture: Haswell

       user_api: openmp
   internal_api: openmp
    num_threads: 4
         prefix: libgomp
       filepath: /home/stas/Documents/Work/polymerize/ml_dev/.venv/lib/python3.11/site-packages/scikit_learn.libs/libgomp-e985bcbb.so.1.0.0
        version: None
```",['Bug'],0
scikit-learn/scikit-learn,RFC Question global random seed testing,"As a developer, I more and more doubt the added value of our global random seed test setup. Ideally, I do not want to bear the mental load of:
- running tests locally with all random seeds (`SKLEARN_TESTS_GLOBAL_RANDOM_SEED=""all""`)
- add a commit with message `[all random seeds]` just to trigger CI/CD to run with all seeds.

Recently, CI/CD often fails because of runs with all random seeds. This not only annoys but actually slows down development. It seems to be mainly @lesteve how takes care of those.

**What is the actual benefit of it? Which bugs have we found this way?**

On top, we have the open issue #22827, which seems like a never ending story. I mostly don't see the added value of the changed tests.

See discussion in #33021:
> by @lorentzenchr 
> General comment: As a developer, I find our test setup too complicated. Even when running tests with all rng seeds locally, one still needs a commit to trigger the CD pipeline with all seeds.
I would prefer to not need to think about test random seeds at all.

> by @lesteve 
> Personally I am less and less convinced that our global_random_seed setup is worth its maintenance cost. I do understand why it exists, i.e. catch numerical instability or avoid that a test pass only because we chose a lucky random seed. At the same time this setup has a maintenance cost and I am not sure we have actually seen any of the benefit. Maybe I don't remember the few cases where it allowed us to notice a numerical instability or maybe it's the kind of ""silent"" usefulness that makes you write a more robust test in the first place and/or catch numerical instability bugs when you write the test.
>
> To be explicit about the maintenance costs:
>
>    intermittent CI error tracking. I am probably the main person that seems to care about [sporadic automatically opened issues](https://github.com/scikit-learn/scikit-learn/issues?q=sort%3Aupdated-desc%20is%3Aissue%20is%3Aopen%20author%3Ascikit-learn-bot). This does take a bit of time (like a few hours a week maybe) and adds some noise in the CI. I would say global_random_seed are the main source of intermittent errors even if there are others, like conda-forge download error, or intermittent segmentation fault on free-threaded.
    ad-hoc PR raising atol and/or rtol where nobody really has a super clear idea whether this is OK or not to raise it or it would need a deeper look and generally we merge anyway.
    new global_random_seed issues keep appearing, the PR CI only checks a single random seed, and it's only afterwards that you notice issues in automatically opened issues where the global random seed is random on each daily schedule run.
>
> @lorentzenchr to answer your point I don't know how to make our test setup simpler with our current set of constraints so I am proposing to remove one constraint (without too much hope to be honest).
>
> In an ideal infinite-resources world (which we don't live in, remember ðŸ˜‰), I think we should have some kind of fuzzing-like tests that lives outside the main CI workflow, tweaks the random seeds, and reports the most problematic tests (maybe they fail for a high number of seeds, maybe the difference is big for values not near 0, or some better criterion). Just to be explicit, this ideal scenario is super unlikely to happen.","['RFC', 'module:test-suite']",0
scikit-learn/scikit-learn,BUG: LinearRegression is wrong on float32 and large n_samples (> 500k rows) due to `cond` param to scipy.linalg.lstsq,"### Describe the bug

`LinearRegression` is wrong on `float32` with a large sample size (e.g. 500k rows) since sklearn 1.6.0. This is still the case on 1.8.0. See minimal repro below. The residual sum of squares is much larger than the solution found by `{scipy,numpy}.linalg.lstsq` on `float32`.

Using the minimal repro below, `git bisect` points to this commit from https://github.com/scikit-learn/scikit-learn/pull/30040: https://github.com/scikit-learn/scikit-learn/commit/bef9d1803d1f243e4d3f9e845fac356a4fc555a8

https://github.com/scikit-learn/scikit-learn/blob/66200f149ba6c64a3a93dd73e37bdeea87bb5db8/sklearn/linear_model/_base.py#L702-L703

- Indeed, changing L702 to `cond = None` fixes the bug.
    - Doing so will revive issue #26164, so someone needs to make a call.
    - I think issue 26164 is strictly less important than the correctness of linear regression (minimize residue).
- `cond` at L702 scales up linearly with the number of rows and eventually zeros out large enough singular values to throw off the solution completely.
- As a result, sklearn `LinearRegression` fails to scale to even 500k rows at `float32` precision below.
- Meanwhile, `scipy.linalg.lstsq` (the underlying solver) and `numpy.linalg.lstsq` don't have this issue with the internally defaulted `cond` / `rcond` values (see inline comments below).

### Steps/Code to Reproduce

```python3
import numpy as np
from scipy import linalg
from sklearn.linear_model import LinearRegression

np.random.seed(1337)
n_samples = 500_000

x1 = np.random.rand(n_samples)
x2 = 0.3 * x1 + 0.1 * np.random.rand(n_samples)
X = np.column_stack([x1, x2])
y = X @ [0.5, 2.0] + 0.1 * np.random.rand(n_samples)
assert X.dtype == np.float64 and y.dtype == np.float64

X32, y32 = X.astype(np.float32), y.astype(np.float32)

scipy_f64 = linalg.lstsq(X, y)[0]
scipy_f32 = linalg.lstsq(X32, y32)[0]  # pdb -> cond defaults to np.float32(1.1920929e-07)
numpy_f64 = np.linalg.lstsq(X, y)[0]
numpy_f32 = np.linalg.lstsq(X32, y32)[0]  # pdb -> rcond defaults to np.float64(1.1102230246251565e-10)
sklearn_f64 = LinearRegression(fit_intercept=False).fit(X, y).coef_
sklearn_f32 = LinearRegression(fit_intercept=False).fit(X32, y32).coef_  # pdb -> sklearn sets cond to np.float32(0.059604645)

print(f""scipy float64:    {scipy_f64}, residue: {linalg.norm(y - X @ scipy_f64):.2f}"")
print(f""scipy float32:    {scipy_f32}, residue: {linalg.norm(y32 - X32 @ scipy_f32):.2f}"")
print(f""numpy float64:    {numpy_f64}, residue: {linalg.norm(y - X @ numpy_f64):.2f}"")
print(f""numpy float32:    {numpy_f32}, residue: {linalg.norm(y32 - X32 @ numpy_f32):.2f}"")
print(f""sklearn float64:  {sklearn_f64}, residue: {linalg.norm(y - X @ sklearn_f64):.2f}"")
print(f""sklearn float32:  {sklearn_f32}, residue: {linalg.norm(y32 - X32 @ sklearn_f32):.2f}  <--------"")
```

### Expected Results

```python3
scipy float64:    [0.41391995 2.42901607], residue: 24.40
scipy float32:    [0.4139197 2.429016 ], residue: 24.40
numpy float64:    [0.41391995 2.42901607], residue: 24.40
numpy float32:    [0.41391996 2.429016  ], residue: 24.40
sklearn float64:  [0.41391995 2.42901607], residue: 24.40
sklearn float32:  [0.4139197 2.429016 ], residue: 24.40  <--------
```

### Actual Results

```python3
scipy float64:    [0.41391995 2.42901607], residue: 24.40
scipy float32:    [0.4139197 2.429016 ], residue: 24.40
numpy float64:    [0.41391995 2.42901607], residue: 24.40
numpy float32:    [0.41391996 2.429016  ], residue: 24.40
sklearn float64:  [0.41391995 2.42901607], residue: 24.40
sklearn float32:  [1.1635013  0.43804753], residue: 59.06  <--------
```


### Root cause analysis

Why does sklearn set `cond` this way in the first place? [#30040](https://github.com/scikit-learn/scikit-learn/pull/30040) explains:
> The `scipy.linalg.lstsq` solver can fail the sample weight consistency test, especially for wide dataset (`n_features > n_samples`) after centering `X,y` (as done when `fit_intercept=True`).
> 
> Setting the `cond` parameter (cut-off ratio on singular values) to the value recommended by `numpy.linalg.lstsq` documentation seems to fix the bug.
- Indeed, `numpy.linalg.lstsq` defaults to [`rcond = finfo(t).eps * max(n, m)`](https://github.com/numpy/numpy/blob/v2.4.0/numpy/linalg/_linalg.py#L2528-L2529)
    - However, for float32 input, numpy sets `t` (calculation precision) to float64 and `result_t` (result precision) to float32 ([two  lines above](https://github.com/numpy/numpy/blob/v2.4.0/numpy/linalg/_linalg.py#L2525)), so numpy does not have the bug described by the current issue.
        - Calc type is always float64: [here](https://github.com/numpy/numpy/blob/v2.4.1/numpy/linalg/_linalg.py#L206-L210) and [there (`signature = 'ddd->ddid'`)](https://github.com/numpy/numpy/blob/v2.4.1/numpy/linalg/_linalg.py#L2525-L2531).
        - This comes at a cost. float32 X will be cast into a float64 copy by numpy's [ufunc `signature` mechanism](https://numpy.org/doc/stable/reference/ufuncs.html):
            - > Casting will be used as necessary. The actual DType of the input arrays is not considered unless signature is None for that array.
    - sklearn's underlying solver is `scipy.linalg.lstsq`. For float32 input, it sets calculation precision to `float32` (no casting / copy), unlike numpy's `float64`.
        - This does not interact well with `numpy's` default `rcond`, resulting in this bug.
    - The above can be validated by pdb-ing into the corresponding lines in the minimal repro above.

### Suggested fix

First add the above minimal repro as a new test case.

Then add `float32` to #30040's test case `test_linear_regression_sample_weight_consistency` with this patch:
<details>

<summary>Click to expand patch!</summary>

```diff
diff --git a/sklearn/linear_model/tests/test_base.py b/sklearn/linear_model/tests/test_base.py
index a1b7c79e6f..9d4982f52e 100644
--- a/sklearn/linear_model/tests/test_base.py
+++ b/sklearn/linear_model/tests/test_base.py
@@ -762,8 +762,9 @@ def test_fused_types_make_dataset(csr_container):
     ],
 )
 @pytest.mark.parametrize(""fit_intercept"", [False, True])
+@pytest.mark.parametrize(""dtype"", [np.float32, np.float64])
 def test_linear_regression_sample_weight_consistency(
-    X_shape, sparse_container, fit_intercept, global_random_seed
+    X_shape, sparse_container, fit_intercept, dtype, global_random_seed
 ):
     """"""Test that the impact of sample_weight is consistent.
 
@@ -774,8 +775,8 @@ def test_linear_regression_sample_weight_consistency(
     rng = np.random.RandomState(global_random_seed)
     n_samples, n_features = X_shape
 
-    X = rng.rand(n_samples, n_features)
-    y = rng.rand(n_samples)
+    X = rng.rand(n_samples, n_features).astype(dtype)
+    y = rng.rand(n_samples).astype(dtype)
     if sparse_container is not None:
         X = sparse_container(X)
     params = dict(fit_intercept=fit_intercept)
@@ -790,14 +791,14 @@ def test_linear_regression_sample_weight_consistency(
     # test with sparse input.
     sample_weight = np.ones_like(y)
     reg.fit(X, y, sample_weight=sample_weight)
-    assert_allclose(reg.coef_, coef, rtol=1e-6)
+    assert_allclose(reg.coef_, coef, rtol=1e-3 if dtype == np.float32 else 1e-6)
     if fit_intercept:
         assert_allclose(reg.intercept_, intercept)
 
     # 2) sample_weight=None should be equivalent to sample_weight = number
     sample_weight = 123.0
     reg.fit(X, y, sample_weight=sample_weight)
-    assert_allclose(reg.coef_, coef, rtol=1e-6)
+    assert_allclose(reg.coef_, coef, rtol=1e-3 if dtype == np.float32 else 1e-6)
     if fit_intercept:
         assert_allclose(reg.intercept_, intercept)
 
@@ -809,7 +810,8 @@ def test_linear_regression_sample_weight_consistency(
         intercept = reg.intercept_
 
     reg.fit(X, y, sample_weight=np.pi * sample_weight)
-    assert_allclose(reg.coef_, coef, rtol=1e-6 if sparse_container is None else 1e-5)
+    rtol = 1e-3 if dtype == np.float32 else 1e-6 if sparse_container is None else 1e-5
+    assert_allclose(reg.coef_, coef, rtol=rtol)
     if fit_intercept:
         assert_allclose(reg.intercept_, intercept)
 
@@ -822,7 +824,7 @@ def test_linear_regression_sample_weight_consistency(
     if fit_intercept:
         intercept_0 = reg.intercept_
     reg.fit(X[:-5], y[:-5], sample_weight=sample_weight[:-5])
-    assert_allclose(reg.coef_, coef_0, rtol=1e-5)
+    assert_allclose(reg.coef_, coef_0, rtol=3e-3 if dtype == np.float32 else 1e-5)
     if fit_intercept:
         assert_allclose(reg.intercept_, intercept_0)
 
@@ -841,7 +843,7 @@ def test_linear_regression_sample_weight_consistency(
 
     reg1 = LinearRegression(**params).fit(X, y, sample_weight=sample_weight_1)
     reg2 = LinearRegression(**params).fit(X2, y2, sample_weight=sample_weight_2)
-    assert_allclose(reg1.coef_, reg2.coef_, rtol=1e-6)
+    assert_allclose(reg1.coef_, reg2.coef_, rtol=1e-3 if dtype == np.float32 else 1e-6)
     if fit_intercept:
         assert_allclose(reg1.intercept_, reg2.intercept_)
 
```

</details>

Then do TDD to find a combination of (dtype, cond) to pass all tests. I haven't been able to find anything reasonable.

I think it is more urgent to fix `LinearRegression` to meet its definition (minimize residue sum of squares) than to pass the [sample weight consistency tests](https://github.com/scikit-learn/scikit-learn/blob/762734097daa72f6bd1b363ac989afa4717530f7/sklearn/linear_model/tests/test_base.py#L765). In the worst case I recommend setting `cond = None`, marking the weight tests as xfail, reopening #26164, and maybe report #26164 upstream (to scipy).

### Versions

```shell
System:
    python: 3.14.2 | packaged by conda-forge | (main, Dec  6 2025, 11:21:58) [GCC 14.3.0]
executable: /home/jli/miniforge3/envs/sklearn-dbg/bin/python3
   machine: Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.39

Python dependencies:
      sklearn: 1.8.0
          pip: 25.3
   setuptools: None
        numpy: 2.4.0
        scipy: 1.16.3
       Cython: None
       pandas: None
   matplotlib: None
       joblib: 1.5.3
threadpoolctl: 3.6.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: openblas
    num_threads: 12
         prefix: libscipy_openblas
       filepath: /home/jli/miniforge3/envs/sklearn-dbg/lib/python3.14/site-packages/numpy.libs/libscipy_openblas64_-fdde5778.so
        version: 0.3.30
threading_layer: pthreads
   architecture: Haswell

       user_api: blas
   internal_api: openblas
    num_threads: 12
         prefix: libscipy_openblas
       filepath: /home/jli/miniforge3/envs/sklearn-dbg/lib/python3.14/site-packages/scipy.libs/libscipy_openblas-b75cc656.so
        version: 0.3.29.dev
threading_layer: pthreads
   architecture: Haswell

       user_api: openmp
   internal_api: openmp
    num_threads: 12
         prefix: libgomp
       filepath: /home/jli/miniforge3/envs/sklearn-dbg/lib/python3.14/site-packages/scikit_learn.libs/libgomp-e985bcbb.so.1.0.0
        version: None
```","['Bug', 'Needs Investigation', 'Numerical Stability']",0
scikit-learn/scikit-learn,FIX _set_order for 1D SciPy sparse arrays (*_array) when converting to CSC/CSR,"### Describe the bug

On recent SciPy versions (e.g. SciPy 1.16.3 used in CI), sparse **array** containers (`coo_array`, `csr_array`, etc.) can be **1-D** (shape `(n_samples,)`). However, scikit-learn, `_set_order` assumes any sparse `y` can be converted to CSC/CSR and calls `y.asformat(""csc"")` when `order=""F""`, but SciPy raises:

`ValueError: Cannot convert a 1d sparse array to csc format`

So far this issue did not surface because the original test `test_set_order_sparse` added in #15436 (commit 21686b7) contained a typo. It used `y = X.asformat(sparse_format)` instead of `y = y.asformat(sparse_format)`. When fixing/generalizing that test as part of #32991, the test started exercising `_set_order` with a real sparse `y`, which revealed the failure on newer SciPy sparse arrays.


### Steps/Code to Reproduce
1. for `_set_order`
```python
import numpy as np
from scipy import sparse
from sklearn.linear_model._coordinate_descent import _set_order

X = sparse.coo_array(np.array([[0], [0], [0]])).asformat(""csr"")
y = sparse.coo_array(np.array([0, 0, 0])).asformat(""csr"")
_set_order(X, y, order=""F"")
```

2. for `test_set_order_sparse` ( if we convert `y = X.asformat(sparse_format)` to `y = y.asformat(sparse_format)` ) 
```python
from sklearn.utils.fixes import COO_CONTAINERS, CSC_CONTAINERS, CSR_CONTAINERS
X = COO_CONTAINERS[1](np.array([[0], [0], [0]]))
y = COO_CONTAINERS[1](np.array([0, 0, 0]))
y.asformat(""csc"")
```

### Expected Results

No Pytest errors

### Actual Results

---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[1], line 11
      7 y = sparse.coo_array(np.array([0, 0, 0])).asformat(""csr"")
      9 # F-order asks for CSC; this crashes on recent SciPy:
     10 # ValueError: Cannot convert a 1d sparse array to csc format
---> 11 _set_order(X, y, order=""F"")

File c:\Users\gauta\miniforge3\Lib\site-packages\sklearn\linear_model\_coordinate_descent.py:86, in _set_order(X, y, order)
     84     X = np.asarray(X, order=order)
     85 if sparse_y:
---> 86     y = y.asformat(sparse_format)
     87 else:
     88     y = np.asarray(y, order=order)

File c:\Users\gauta\miniforge3\Lib\site-packages\scipy\sparse\_base.py:480, in _spbase.asformat(self, format, copy)
    478 # Forward the copy kwarg, if it's accepted.
    479 try:
--> 480     return convert_method(copy=copy)
    481 except TypeError:
    482     return convert_method()

File c:\Users\gauta\miniforge3\Lib\site-packages\scipy\sparse\_csr.py:65, in _csr_base.tocsc(self, copy)
     63 def tocsc(self, copy=False):
     64     if self.ndim != 2:
---> 65         raise ValueError(""Cannot convert a 1d sparse array to csc format"")
     66     M, N = self.shape
     67     idx_dtype = self._get_index_dtype((self.indptr, self.indices),
     68                                 maxval=max(self.nnz, M))

ValueError: Cannot convert a 1d sparse array to csc format

---------------------------------------------------------------------------

Here is the link for the failed tests from #32991 CI when I tried to update the test:
https://github.com/scikit-learn/scikit-learn/actions/runs/20854001086/job/59915770506?pr=32991


### Versions

```shell
System:
    python: 3.12.12 | packaged by conda-forge | (main, Oct 22 2025, 23:13:34) [MSC v.1944 64 bit (AMD64)]
executable: c:\Users\gauta\miniforge3\python.exe
   machine: Windows-11-10.0.26100-SP0

Python dependencies:
      sklearn: 1.8.0
          pip: 25.3
   setuptools: 80.9.0
        numpy: 2.4.0
        scipy: 1.16.3
       Cython: None
       pandas: None
   matplotlib: None
       joblib: 1.5.3
threadpoolctl: 3.6.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: mkl
    num_threads: 24
         prefix: mkl_rt
...
    num_threads: 32
         prefix: vcomp
       filepath: C:\Users\gauta\miniforge3\vcomp140.dll
        version: None
```","['Bug', 'Needs Triage']",0
scikit-learn/scikit-learn,DOC Make it easier to create 100% scikit-learn compatible estimators,"Creating an estimator that is fully compliant and compatible with scikit-learn is quite difficult for people who aren't scikit-learn maintainers. As a result there are a lot of estimators out there that are kinda compatible but not quite. This isn't great for users of the scikit-learn ecosystem, it is annoying for developers of third-party libraries and creates work for scikit-learn maintainers.

We have tools and documentation that help third-party developers create compatible estimators. However, from looking around the world it seems like a lot of people don't use them. Probably because they are too hard to find, not clear enough, too difficult to use, or another reason.

I think https://scikit-learn.org/dev/developers/develop.html is the best place in the documentation that we have right now. It contains a lot! But I'm not sure it contains everything you need to know (because it is quite long).

I'm not sure what exactly the reasons are for why people create semi-compatible estimators.

My impression from cuml is that it is partly not knowing about the existing tools, partly not knowing that some of minor looking details are important, and the fact that it is a difficult task. A 100% compatible estimator also imposes some quite strict constraints on how you can implement your estimator - without understanding why these constraints exist and are ""a good thing"" I understand that people choose not to follow them.

https://github.com/scikit-learn/scikit-learn/issues/32910 is an example from the skrub team of things that go wrong when you don't achieve full compatibility. They are not your average ""people from the internet"" in that they have good access to core maintainers. The original report also looks like they put in a reasonable amount of work and brain power to try and understand what is happening. But they didn't figure it out. It turns out that by default you have to have a fitted attribute on your estimator for it to be considered fitted - you can argue whether or not this constraint/assumption is reasonable, but the fact that it took the creation of an issue and a few messages to pin this down shows it is not easy.

This issue is about finding out what people find difficult (pain points) and then figuring out what can be done about them. We want a good ecosystem to exist, there is already a lot of material out there, but somehow there is something missing.

A constraint I'd add is that I think we should try and avoid creating yet another guide or said differently: improve things by removing things or editing what we have. No one reads, and the longer the text the less people read. The more different guides we have the less likely it is that people read all of them.",['Documentation'],0
scikit-learn/scikit-learn,DOC Supervised clustering metrics specify array input dtype,"Several of our supervised clustering metrics specify/suggest (??) that the array input should be of type `int` or `integral`, e.g.:

`pair_confusion_matrix`:

https://github.com/scikit-learn/scikit-learn/blob/b0ba8b029c298e0cc545206d2df4757be0ec2ac2/sklearn/metrics/cluster/_supervised.py#L212-L216

`rand_score`:

https://github.com/scikit-learn/scikit-learn/blob/b0ba8b029c298e0cc545206d2df4757be0ec2ac2/sklearn/metrics/cluster/_supervised.py#L298-L302

`mutual_info_score`:

https://github.com/scikit-learn/scikit-learn/blob/b0ba8b029c298e0cc545206d2df4757be0ec2ac2/sklearn/metrics/cluster/_supervised.py#L858-L864

`fowlkes_mallows_score`:

https://github.com/scikit-learn/scikit-learn/blob/b0ba8b029c298e0cc545206d2df4757be0ec2ac2/sklearn/metrics/cluster/_supervised.py#L1221-L1225

Looking at the code/testing a few - we don't seem to enforce this. I also have not seen specification of dtype of array input much elsewhere in the codebase. Is this dytpe specification needed in the docs?

Edit: I mean is it worth clarifying it is a suggestion and not enforced?",['RFC'],0
scikit-learn/scikit-learn,Array API spec for `unique_values` does not specify order of unique elements,"Noticed when working on #32923

I don't think we need to do anything about this, (and indeed all our tests pass), I just wanted to document this for posterity, in case we encounter any problems later.

Array API spec for `unique_values` does not specify what the order of unique elements should be.
From https://data-apis.org/array-api/latest/API_specification/generated/array_api.unique_values.html#array_api.unique_values :

> The order of unique elements is not specified and may vary between implementations.

Searching for `xp.unique_values` in our codebase, I can see 2 places where order would matter.

In `_check_pos_label_consistency` we do:

https://github.com/scikit-learn/scikit-learn/blob/86acf4547ed8e183cb75c07bcd68ef186a223f06/sklearn/utils/validation.py#L2578-L2582

which assumes a specific order of unique values.

Also in `_unique_np` (used in `_encode.py`), we do:

https://github.com/scikit-learn/scikit-learn/blob/86acf4547ed8e183cb75c07bcd68ef186a223f06/sklearn/utils/_encode.py#L71-L73

assuming NaNs get sorted to the end.",['Array API'],0
scikit-learn/scikit-learn,[for discussion]: adding link to Contributing Guide to top menu,"### Describe the issue linked to the documentation

This issue is a **discussion** item.

## Questions:  
1. Can we add a link to ""Contributing"" to the top level menu? It seems a bit hidden, and the questions on contributing come up quite often.
    - Contributing: https://scikit-learn.org/stable/developers/contributing.html#contributing

I usually find the link for it via the FAQs: Contributing: https://scikit-learn.org/stable/faq.html#contributing

<img width=""836"" height=""366"" alt=""Image"" src=""https://github.com/user-attachments/assets/c7582fd5-4937-466a-bf34-3a5db3bdaf11"" />

2. Should we add a section indicating that scikit-learn no longer participates in GSOC?



## Note for New Contributors

> [!WARNING]
>  This issue is for discussion amongst the maintainers. Please do not ask if you can work on it or to be assigned to it. 

### Suggest a potential alternative/fix

_No response_",['Documentation'],0
scikit-learn/scikit-learn,Addressed: FastICA logcosh bottleneck,"### Describe the workflow you want to enable

For: sklearn/decomposition/_fastica.py

Hello,

Firstly thank you for the amazing library :D

I see in the FastICA there was a bottleneck that was messing a lot with my run time, so i optimized it and compared my implementation with the original and the results are exactly the same. I think you may find it useful. 


### Describe your proposed solution


In my implementation I use logcocsh as a callable


Original Scikit:

```python
def _logcosh(x, fun_args=None):
    alpha = fun_args.get(""alpha"", 1.0)  # comment it out?

    x *= alpha
    gx = np.tanh(x, x)  # apply the tanh inplace
    g_x = np.empty(x.shape[0], dtype=x.dtype)
    # XXX compute in chunks to avoid extra allocation
    for i, gx_i in enumerate(gx):  # please don't vectorize.
        g_x[i] = (alpha * (1 - gx_i**2)).mean()
    return gx, g_x
```

My implementation:

```python
def opt_logcosh(x):
    """"""
    we create our own logcosh, which is modernized and avoids the bottleneck of the original logcosh

    our logcosh vs the original logcosh from sklearn:
        same output, but faster execution time

    """"""
    tanh = np.tanh(x)
    return tanh, ((1 - tanh**2)).mean(axis=-1)
```

### Describe alternatives you've considered, if relevant

_No response_

### Additional context

_No response_",['New Feature'],0
scikit-learn/scikit-learn,Call to estimator.score() in RANSAC even if the produced input data set is too small - fix proposed,"### Describe the bug

By sampling radomly from a distribution of x,y values, the RANSAC algorithm tries to find the line which describes best most of the data (while ignoring outliers). For deciding, how well a certain line describes the data, estimator.score() is called in line 526 of _ransac.py.

```pytb
    525 # score of inlier data set
--> 526 score_subset = estimator.score(
    527     X_inlier_subset,
    528     y_inlier_subset,
    529     **score_params_inlier_subset,
    530 )
```

However, estimator.score() is called even if the inlier data set contains only a single data point, which then triggers a warning down the road:

```pytb
...\Lib\site-packages\sklearn\metrics\_regression.py:1283, in r2_score(y_true, y_pred, sample_weight, multioutput, force_finite)
   1281 if _num_samples(y_pred) < 2:
   1282     msg = ""R^2 score is not well-defined with less than two samples.""
-> 1283     warnings.warn(msg, UndefinedMetricWarning)
   1284     return float(""nan"")
```

From my point of view, the code should not call this function if the input data set is not valid.

Unfortunately, I cannot provide a minimum example code as due to the random sampling the warning does only rarely occur. As far as I understand, the situation occurs if in previous iterations there was already a data set with more than 1 inlier data point drawn. Otherwise, line 526 is not reached due to:

```python
510  # less inliers -> skip current random sample
511  if n_inliers_subset < n_inliers_best:
512     self.n_skips_no_inliers_ += 1
513     continue
```

If propose to modify this line to:

```python
511 if ((n_inliers_subset < n_inliers_best) or (n_inliers_subset < 2)):
```

At least for me, this does fix the situation that generates the warnings.


### Steps/Code to Reproduce

```python
            # less inliers -> skip current random sample
            if n_inliers_subset < n_inliers_best: # proposed fix: if ((n_inliers_subset < n_inliers_best) or (n_inliers_subset < 2)):
                self.n_skips_no_inliers_ += 1
                continue

            # extract inlier data set
            inlier_idxs_subset = sample_idxs[inlier_mask_subset]
            X_inlier_subset = X[inlier_idxs_subset]
            y_inlier_subset = y[inlier_idxs_subset]

            # cut `fit_params` down to `inlier_idxs_subset`
            score_params_inlier_subset = _check_method_params(
                X, params=routed_params.estimator.score, indices=inlier_idxs_subset
            )

            # score of inlier data set
            score_subset = estimator.score(
                X_inlier_subset,
                y_inlier_subset,
                **score_params_inlier_subset,
            )
```

### Expected Results

Expected result: RANSAC algorithm should not make call to functions with invalid data that cause warnings.

### Actual Results

```pytb
...\Lib\site-packages\sklearn\metrics\_regression.py:1283, in r2_score(y_true, y_pred, sample_weight, multioutput, force_finite)
   1281 if _num_samples(y_pred) < 2:
   1282     msg = ""R^2 score is not well-defined with less than two samples.""
-> 1283     warnings.warn(msg, UndefinedMetricWarning)
   1284     return float(""nan"")
```

### Versions

```shell
import sklearn; sklearn.show_versions()

System:
    python: 3.13.5 | packaged by Anaconda, Inc. | (main, Jun 12 2025, 16:37:03) [MSC v.1929 64 bit (AMD64)]
executable: C:\ProgramData\anaconda3\envs\E134\python.exe
   machine: Windows-11-10.0.26200-SP0

Python dependencies:
      sklearn: 1.7.2
          pip: 25.2
   setuptools: 78.1.1
        numpy: 2.3.2
        scipy: 1.16.1
       Cython: None
       pandas: 2.3.2
   matplotlib: 3.10.6
       joblib: 1.5.2
threadpoolctl: 3.6.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: openblas
    num_threads: 12
         prefix: libscipy_openblas
       filepath: C:\ProgramData\anaconda3\envs\E134\Lib\site-packages\numpy.libs\libscipy_openblas64_-860d95b1c38e637ce4509f5fa24fbf2a.dll
        version: 0.3.30
threading_layer: pthreads
   architecture: SkylakeX

       user_api: blas
   internal_api: openblas
    num_threads: 12
         prefix: libscipy_openblas
       filepath: C:\ProgramData\anaconda3\envs\E134\Lib\site-packages\scipy.libs\libscipy_openblas-6b2103f2ae4d8547998b5d188e9801fb.dll
        version: 0.3.28
threading_layer: pthreads
   architecture: SkylakeX

       user_api: openmp
   internal_api: openmp
    num_threads: 12
         prefix: vcomp
       filepath: C:\ProgramData\anaconda3\envs\E134\Lib\site-packages\sklearn\.libs\vcomp140.dll
        version: None
```","['Bug', 'Needs Investigation']",0
scikit-learn/scikit-learn,RFC Add gallery example with Array API support,"Mentioned somewhere in the release activity the closest I could find was https://github.com/scikit-learn/scikit-learn/pull/32809#discussion_r2587809537. It would probably be doable to add pytorch-cpu in the doc lock-file and add an example with a somewhat non trivial pipeline.

One idea could be to have some commented code saying `device = 'cuda'` and explain that uncommenting it will run on GPU if you have one.

Questions:
- not sure which version to pick for doc-min-dependencies build, I think it doesn't really matter and convenience (for us as maintainers) is the main thing here, since array API is still experimental
- some other things I did not think of ...
","['RFC', 'Array API']",0
scikit-learn/scikit-learn,"`DecisionBoundaryDisplay` with `response_method=""predict""`  has inconsistent handling for the colormap in the multiclass case","### Describe the bug

This issue was discovered while reviewing #32867, but since it's not directly related, let's open a dedicated issue to avoid derailing the original discussion.

As can be seen in the plots below, passing `multiclass_colors=""gist_rainbow""` has no impact when `response_method=""predict""` but I don't see any reason why it wouldn't be the case.

<img width=""1211"" height=""1211"" alt=""Image"" src=""https://github.com/user-attachments/assets/fce86173-f63b-487d-b19a-53a460ed9621"" />

I think we should refactor the code to have consistent colormap configuration for all the response methods (in particular for the multiclass case).

### Steps/Code to Reproduce

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.inspection import DecisionBoundaryDisplay


data = np.array(
    [
        [-1, -1],
        [-2, -1],
        [1, 1],
        [2, 1],
        [2, 2],
        [3, 2],
        [3, 3],
    ]
)
target = np.arange(data.shape[0])
clf = LogisticRegression().fit(data, target)

cmap = ""gist_rainbow""
_, axes = plt.subplots(nrows=3, ncols=3, figsize=(12, 12), constrained_layout=True)
for plot_method_idx, plot_method in enumerate([""contourf"", ""contour"", ""pcolormesh""]):
    for response_method_idx, response_method in enumerate(
        [""predict_proba"", ""decision_function"", ""predict""]
    ):
        ax = axes[plot_method_idx, response_method_idx]
        display = DecisionBoundaryDisplay.from_estimator(
            clf,
            data,
            multiclass_colors=cmap,
            response_method=response_method,
            plot_method=plot_method,
            ax=ax,
            alpha=0.5,
        )
        ax.scatter(
            data[:, 0],
            data[:, 1],
            c=target.astype(int),
            edgecolors=""black"",
            cmap=cmap,
        )
        ax.set_title(f""plot_method={plot_method}\nresponse_method={response_method}"")
```

### Expected Results

I would expect the colored areas of the right most columns to also use the `""gist_rainbow""` colormap as the other columns.

### Actual Results

The right most column seems to ignore the `multiclass_colors` argument and falls back to the default ""viridis"" colormap which is not very well suited for multiclass problems with a large number of classes.

### Versions

```shell
System:
    python: 3.13.7 | packaged by conda-forge | (main, Sep  3 2025, 14:24:46) [Clang 19.1.7 ]
executable: /Users/ogrisel/miniforge3/envs/dev/bin/python3.13
   machine: macOS-15.6.1-arm64-arm-64bit-Mach-O

Python dependencies:
      sklearn: 1.9.dev0
          pip: 25.2
   setuptools: 80.9.0
        numpy: 2.3.3
        scipy: 1.16.3
       Cython: 3.1.4
       pandas: 3.0.0.dev0+2566.g2bb3fef887
   matplotlib: 3.10.6
       joblib: 1.5.2
threadpoolctl: 3.6.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: openblas
    num_threads: 10
         prefix: libopenblas
       filepath: /Users/ogrisel/miniforge3/envs/dev/lib/libopenblas.0.dylib
        version: 0.3.30
threading_layer: openmp
   architecture: VORTEX

       user_api: openmp
   internal_api: openmp
    num_threads: 10
         prefix: libomp
       filepath: /Users/ogrisel/miniforge3/envs/dev/lib/libomp.dylib
        version: None
```","['Bug', 'module:inspection']",0
scikit-learn/scikit-learn,"BUG: `DecisionTreeRegressor`: invalid impurity for `criterion=""poisson""` with missing values","### Describe the bug

When missing values are present in `X`, `DecisionTreeRegressor(criterion=""poisson"", ...)` sometimes computes invalid impurities.

Impurity should match with half-poisson deviance according to the documentation. And it is indeed the case when they are no missing values. But when missing values are present, something goes wrong, and you can even get negative impurities.

I found this bug thanks to my big test from this draft PR https://github.com/scikit-learn/scikit-learn/pull/32193. This same test shows that impurity does match `sklearn.metrics.mean_poisson_deviance(...) / 2` when no missing values are present.

### Steps/Code to Reproduce

```Python
from sklearn.tree import DecisionTreeRegressor, plot_tree

X = np.array([np.nan, 1, 2, 3, np.nan]).reshape(-1, 1)
y = [0.49, 0.5, 0.7, 1.5, 0.8]

tree = DecisionTreeRegressor(criterion='poisson', max_depth=1)
tree.fit(X, y)

plot_tree(tree);
```

### Expected Results

Impurities should be positive. Impurity of the right children should be 0, as it has only one sample.

### Actual Results

Some impurities are negative:

<img width=""515"" height=""389"" alt=""Image"" src=""https://github.com/user-attachments/assets/90169bc8-1e05-48ab-aac1-b77f599dcef9"" />

### Versions

```shell
System:
    python: 3.12.11 (main, Aug 18 2025, 19:19:11) [Clang 20.1.4 ]
executable: /home/arthur/dev-perso/scikit-learn/sklearn-env/bin/python
   machine: Linux-6.14.0-36-generic-x86_64-with-glibc2.39

Python dependencies:
      sklearn: 1.9.dev0
          pip: None
   setuptools: 80.9.0
        numpy: 2.3.5
        scipy: 1.16.3
       Cython: 3.2.1
       pandas: 2.3.3
   matplotlib: 3.10.7
       joblib: 1.5.2
threadpoolctl: 3.6.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: openblas
    num_threads: 16
         prefix: libscipy_openblas
       filepath: /home/arthur/dev-perso/scikit-learn/sklearn-env/lib/python3.12/site-packages/numpy.libs/libscipy_openblas64_-fdde5778.so
        version: 0.3.30
threading_layer: pthreads
   architecture: Haswell

       user_api: blas
   internal_api: openblas
    num_threads: 16
         prefix: libscipy_openblas
       filepath: /home/arthur/dev-perso/scikit-learn/sklearn-env/lib/python3.12/site-packages/scipy.libs/libscipy_openblas-b75cc656.so
        version: 0.3.29.dev
threading_layer: pthreads
   architecture: Haswell

       user_api: openmp
   internal_api: openmp
    num_threads: 16
         prefix: libgomp
       filepath: /usr/lib/x86_64-linux-gnu/libgomp.so.1.0.0
        version: None
```",['Bug'],0
scikit-learn/scikit-learn,Classwise (group) L21 penalty for multinomial LogisticRegression,"### Describe the workflow you want to enable

For `LogisticRegression` with `n_classes >= 3`, I would like to specify a class-wise (grouped) L21 penalty $\sum_{j=1}^{n_{features}} \Vert\beta_{j,\cdot}\Vert_2 = \sum_{j=1}^{n_{features}} (\sum_{k=1}^{n_{classes}}\beta_{j,k}^2)^{1/2}$ instead of the implemented L1 penalty $\sum_{j=1}^{n_{features}} \sum_{k=1}^{n_{classes}}\Vert\beta\Vert_2$.
The difference is bassically the same as calling `MultiTaskElasticNet().fit(X, y)` vs `ElasticNet().fit(X, y.ravel())`.

[glmnet](https://glmnet.stanford.edu/articles/glmnet.html#multinomial-regression-family-multinomial) has the option `type.multinomial = ""grouped""` and (default) `type.multinomial = ""ungrouped""`.


### Describe your proposed solution
Introduce a new parameter, e.g. `l1_type` or similar.

### Describe alternatives you've considered, if relevant

_No response_

### Additional context

I could not find any discussion in and around of #8446 (SAGA was the first and so far only solver capable of solving multinomial case with L1 penalties).",['New Feature'],0
scikit-learn/scikit-learn,`DecisionBoundaryDisplay.from_estimator` only displays up to 7 distinct colours,"### Describe the bug

I was trying to use `DecisionBoundaryDisplay.from_estimator` to display different regions classified by `NuSVC`:

<img width=""455"" height=""355"" alt=""Image"" src=""https://github.com/user-attachments/assets/ffd7d137-b7f1-46a9-bd63-b967903617ce"" />

My expectation was to find 11, but by visually counting, I was only able to find 10.
Also, some colours were obviously duplicated in this figure.
This is how it actually should look like:

<img width=""455"" height=""355"" alt=""Image"" src=""https://github.com/user-attachments/assets/29b0c239-1a84-47fd-96c1-57d72f0d02c8"" />

## Proposed fix

I guess that this will be fixed at some point by https://github.com/matplotlib/matplotlib/commit/0eadaf0effeb424e2b385693fbf6ac8a21da4d84. However, this fix is not yet available in the stable versions of matplotlib.
Manually changing the ""default""/hard-wired value [here](https://github.com/matplotlib/matplotlib/blob/v3.10.8/lib/matplotlib/contour.py#L1020) fixes the problem.
The best fix, thus, is probably for scikit-learn to specify the `levels` parameter on its own here:
https://github.com/scikit-learn/scikit-learn/blob/bd0dd87f792ee88c8ec70248299aae779b51ac16/sklearn/inspection/_plot/decision_boundary.py#L220
That line should look something like this:
```py
self.surface_ = plot_func(self.xx0, self.xx1, self.response, <levels>, **kwargs)
```
where `<levels>` is replaced by the appropriate amount of levels.

The case when `self.response.ndim == 3` may also be affected, but I have not yet tried out that one

### Steps/Code to Reproduce

```py
import numpy as np
from sklearn.inspection import DecisionBoundaryDisplay
from sklearn.svm import NuSVC
import matplotlib.pyplot as plt

# List of some 11 distinct points
pts = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1],
                [2, 2], [3, 2], [3, 3], [4, 3],
                [4, 4], [5, 4], [5, 5]])
svc = NuSVC(nu=0.1)
svc.fit(pts, [1,2,3,4,5,6,7,8,9,10,11])
DecisionBoundaryDisplay.from_estimator(
    svc,
    pts,
    response_method=""predict"",
    cmap=""gist_rainbow""
)
plt.show()
```

### Expected Results

<img width=""455"" height=""355"" alt=""Image"" src=""https://github.com/user-attachments/assets/adbec1fc-5341-4842-a314-98d2d25f3dcd"" />

### Actual Results

<img width=""455"" height=""355"" alt=""Image"" src=""https://github.com/user-attachments/assets/54b39c7d-c6e8-4e83-ba04-cfb5daf95459"" />

### Versions

```shell
System:
    python: 3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:27:36) [GCC 11.2.0]
executable: /home/nico/miniconda3/envs/pca-assessment/bin/python
   machine: Linux-6.14.0-36-generic-x86_64-with-glibc2.39

Python dependencies:
      sklearn: 1.7.2
          pip: 24.3.1
   setuptools: 72.1.0
        numpy: 2.3.3
        scipy: 1.16.2
       Cython: None
       pandas: 2.3.2
   matplotlib: 3.10.6
       joblib: 1.4.2
threadpoolctl: 3.5.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: openblas
    num_threads: 16
         prefix: libscipy_openblas
       filepath: /home/nico/miniconda3/envs/pca-assessment/lib/python3.12/site-packages/numpy.libs/libscipy_openblas64_-8fb3d286.so
        version: 0.3.30
threading_layer: pthreads
   architecture: Haswell

       user_api: blas
   internal_api: mkl
    num_threads: 12
         prefix: libmkl_rt
       filepath: /home/nico/miniconda3/envs/pca-assessment/lib/libmkl_rt.so.2
        version: 2025.0-Product
threading_layer: gnu

       user_api: openmp
   internal_api: openmp
    num_threads: 16
         prefix: libgomp
       filepath: /home/nico/miniconda3/envs/pca-assessment/lib/libgomp.so.1.0.0
        version: None
```",['Bug'],0
scikit-learn/scikit-learn,Chain classifier customizability,"### Describe the workflow you want to enable

Allow users to modify the model or fit_params in each step of the chain dynamically.

For example use the model to dynamically configure fit_params, for example add in Eval_set in light GBM latest prediction to prepare it for the next chain model fit.

### Describe your proposed solution

Enable pre-fit or post-fit callbacks that accept the latest to be trained model in the chain.
This will also allow users to do intermediate logging etc...


### Describe alternatives you've considered, if relevant

Other alternatives are to let users create their own wrappers of estimators, before using chain classifier. But this will reduce the ability to control any intermediate states if necessary.  

Example using early stopping on unseen eval set:
```python
import lightgbm as lgb
from sklearn import set_config
from sklearn.multioutput import ClassifierChain
from sklearn.model_selection import train_test_split

set_config(enable_metadata_routing=True)

base_lr = lgb.LGBMClassifier(
    callbacks=[lgb.early_stopping(stopping_rounds=5)]
)

base_lr.set_fit_request(eval_set=True)
base_lr.set_fit_request(eval_metric=True)

chain = ClassifierChain(
    base_lr,
    random_state=42,
    cv=5
)

X_train_2, X_eval_2, y_train_2, y_eval_2 = train_test_split(
    X_train, Y_train, test_size=0.2
)

changing_eval_set = [[X_eval_2, y_eval_2.iloc[:, 0]]]
n_feature = X_eval_2.shape[1]

def eval_set_updater(latest_model):
    eval_set = changing_eval_set[0]
    X_eval_2 = eval_set[0]
    lastest_cv_eval_pred = latest_model.predict(X_eval_2)
    eval_set[0] = pd.concat([X_eval_2, lastest_cv_eval_pred], axis=1)
    eval_set[1] = y_eval_2.iloc[:, X_eval_2.shape[1] - n_feature + 1]

chain.fit(
    X_train_2,
    y_train_2,
    eval_set=changing_eval_set,
    eval_metric=""binary_logloss"",
    post_fit_callback=eval_set_updater
)
```

### Additional context

In some rare cases, fit_params might depend on the performance and behavior of the previous model. Giving the ability to customize the model and params will increase flexibility of the chain model.",['New Feature'],0
scikit-learn/scikit-learn,FeatureUnion with polars output fails due to missing column renaming in adapter interface,"### Describe the bug

When using `FeatureUnion` with `set_config(transform_output=""polars"")`, the operation fails with `polars.exceptions.DuplicateError` because the `ContainerAdapterProtocol.hstack()` interface is incomplete - it doesn't accept `feature_names` parameter to apply column renaming before concatenation.

The `verbose_feature_names_out=True` (default) mechanism should ensure unique column names via `get_feature_names_out()`, but this information cannot be passed to the adapter's `hstack()` method due to interface design limitation.

### Steps/Code to Reproduce

```python
from sklearn.pipeline import FeatureUnion
from sklearn.preprocessing import StandardScaler
from sklearn import set_config
import numpy as np
set_config(transform_output=""polars"")
X = np.array([[1, 2], [3, 4], [5, 6]])
union = FeatureUnion([
    (""scaler1"", StandardScaler()),
    (""scaler2"", StandardScaler())
])
result = union.fit_transform(X)
```

### Expected Results

The code should work and produce a polars DataFrame with uniquely named columns like `['scaler1__x0', 'scaler1__x1', 'scaler2__x0', 'scaler2__x1']`, consistent with the design of `verbose_feature_names_out=True`.

### Actual Results

```python
Traceback (most recent call last):
  File ""<stdin>"", line 10, in <module>
  File ""/path/to/sklearn/utils/_set_output.py"", line 316, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/path/to/sklearn/pipeline.py"", line 1971, in fit_transform
    return self._hstack(Xs)
           ^^^^^^^^^^^^^^^^
  File ""/path/to/sklearn/pipeline.py"", line 2052, in _hstack
    return adapter.hstack(Xs)
           ^^^^^^^^^^^^^^^^^^
  File ""/path/to/sklearn/utils/_set_output.py"", line 183, in hstack
    return pl.concat(Xs, how=""horizontal"")
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/path/to/polars/functions/eager.py"", line 256, in concat
    out = wrap_df(plr.concat_df_horizontal(elems))
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
polars.exceptions.DuplicateError: column with name 'x0' has more than one occurrence
```

### Versions

```shell
System:
    python: 3.11.14 | packaged by conda-forge | (main, Oct 22 2025, 22:46:25) [GCC 14.3.0]
executable: /opt/conda/envs/sklearn-dev/bin/python
   machine: Linux-6.8.0-65-generic-x86_64-with-glibc2.36

Python dependencies:
      sklearn: 1.9.dev0
          pip: 25.3
   setuptools: 80.9.0
        numpy: 2.3.5
        scipy: 1.16.3
       Cython: 3.2.2
       pandas: 2.3.3
   matplotlib: None
       joblib: 1.5.2
threadpoolctl: 3.6.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: openblas
    num_threads: 48
         prefix: libopenblas
       filepath: /opt/conda/envs/sklearn-dev/lib/libopenblasp-r0.3.30.so
        version: 0.3.30
threading_layer: pthreads
   architecture: SkylakeX

       user_api: openmp
   internal_api: openmp
    num_threads: 48
         prefix: libgomp
       filepath: /opt/conda/envs/sklearn-dev/lib/libgomp.so.1.0.0
        version: None
```",['Bug'],0
scikit-learn/scikit-learn,nan_euclidean_distances producing distance matrix not symmetrical due to floating point precision,"### Describe the bug

nan_euclidean_distances is producing asymmetrical matrix when input matrix contains nan values. This, in turn, causes errors when checked for symmetry, for example by scipy.spatial.distance.squareform.

### Steps/Code to Reproduce

```Python
mat = np.random.rand(10, 20)
dist = nan_euclidean_distances(mat)
(dist == dist.T).all()
# output np.True_


mat = np.random.rand(10, 20)
missing = np.random.choice(mat.size, 50, replace=False)
mat.ravel()[missing] = np.nan
dist = nan_euclidean_distances(mat)
(dist == dist.T).all()
# output np.False_

(dist.round(5) == dist.T.round(5)).all()
# output np.True_
```

### Expected Results

Function should produce symmetrical matrix.

### Actual Results

Produces asymmetrical matrix.

### Versions

```shell
System:
    python: 3.13.7 | packaged by conda-forge | (main, Sep  3 2025, 14:33:26) [Clang 19.1.7 ]
executable: /mambaforge-pypy3/envs/jupyterlab/bin/python
   machine: macOS-15.7-x86_64-i386-64bit-Mach-O

Python dependencies:
      sklearn: 1.7.2
          pip: 25.2
   setuptools: 80.9.0
        numpy: 2.3.3
        scipy: 1.16.2
       Cython: None
       pandas: 2.3.3
   matplotlib: 3.10.6
       joblib: 1.5.2
threadpoolctl: 3.6.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: openblas
    num_threads: 16
         prefix: libopenblas
       filepath: /mambaforge-pypy3/envs/jupyterlab/lib/libopenblasp-r0.3.30.dylib
        version: 0.3.30
threading_layer: openmp
   architecture: Haswell

       user_api: openmp
   internal_api: openmp
    num_threads: 16
         prefix: libomp
       filepath: /mambaforge-pypy3/envs/jupyterlab/lib/libomp.dylib
        version: None
```",['Bug'],0
scikit-learn/scikit-learn,Potential improvements for changelog/website situation during RC -> final transition,"So it's quite confusing and I don't know how easy/hard this is to improve the situation, it mostly looks like this is an unintended side-effect of our release process + towncrier setup. Since this is a 2-week transition period, nobody has yet found the time to figure out what possible improvements we could do.

Previously mentioned in https://github.com/scikit-learn/scikit-learn/issues/32807 and https://github.com/scikit-learn/scikit-learn/pull/31469#issuecomment-2931890657 and probably other places.

Let's try to summarize the issues:
- the changelog is not easily findable on the website. stable or dev website ""ongoing development scikit-learn changelog 1.8"" (bottom left) both point to https://scikit-learn.org/dev/whats_new/v1.8.html#version-1-8-0 which is empty. This is probably the easiest thing for now: copy the 1.8 changelog from `1.8.X` branch to `main` so that the 1.8 changelog is correct on the `dev` website.
- towncrier setup: during the rc -> final period, we still have the fragments in doc/whats_new/upcoming changes in both `main` and `1.8.X`. For the final, we generate the changelog with towncrier (without `--keep`), which removes the fragments. Then we forward-port this change in `main`. IIRC this was taken from another project, for example [astropy does that](https://docs.astropy.org/en/latest/development/maintainers/releasing.html#rendering-the-changelog) as well.
- slightly annoying but less crucial: during the transition period, the 1.9 changelog on dev website has all the changes that will go in 1.8 plus some that will go in 1.9 

cc @jeremiedbb in case you remember if you put more info about the situation somewhere.

> [!WARNING]
> This is not a good first issue to contribute. Great if you are interested to contribute to scikit-learn ðŸ™. Please have a look at our [contributing doc](https://scikit-learn.org/dev/developers/contributing.html) and in particular the section [Issues for new contributors](https://scikit-learn.org/dev/developers/contributing.html#new-contributors). 

","['Build / CI', 'frontend', 'Needs Investigation']",0
scikit-learn/scikit-learn,Address sanitizer,"> [!WARNING]
> This is not a good first issue to contribute. Great if you are interested to contribute to scikit-learn ðŸ™. Please have a look at our [contributing doc](https://scikit-learn.org/dev/developers/contributing.html) and in particular the section [Issues for new contributors](https://scikit-learn.org/dev/developers/contributing.html#new-contributors). 

### Describe the workflow you want to enable

This was discussed a little on issue https://github.com/scikit-learn/scikit-learn/issues/32491 and I wanted to give a bit of context about how to do it (unfortunately I don't think I know your CI workflow well enough to do it myself).

Essentially, address sanitizer is a compiler option which looks for out of bounds reads and writes so should catch errors like reading beyond the end of an array. It slows things down so you definitely don't want to distribute wheels with it enabled, but it may help with testing. It only works if you actually exercise the code so you'll want to build and run the tests. It's probably easiest to do on Linux and with just one Python version - there's likely little benefit to doing it comprehensively.

Undefined behaviour sanitizer is related and can be enabled at the same time. However it probably has less value, and there's some minor bits of undefined behaviour in Cython which you'll have to ignore.

### Describe your proposed solution

Rough outline:

* pick a compiler - `clang` is probably the best but `gcc` or `msvc` will work if you have strong preferences.
* Compile and link with `-fsanitize=address` as a command-line option.
* Run the tests with `ASAN_OPTIONS=detect_leaks=0` set as an environmental variable - Cython definitely doesn't clean everything up on exit so you'll get lots of fairly useless false positives.
* You may need `LD_PRELOAD=$(gcc --print-file-name=libasan.so)` as an environmental variable too.
* Dependency: I don't think it's strictly necessary to build all your dependencies with address sanitizer, however:
   * if you're linking to C++ libraries that use `std::string` and `std::vector` then it's a good idea to build them with it too (otherwise you have to manually disable some checks).
   * I imagine you're doing a lot of your memory allocation in Numpy? In which case it'd probably be a good idea to build Numpy with it too. I'm not the right person to advise on that.

Hopefully that makes some sense - feel ping me if it doesn't work, or close if it isn't something you want to do.


### Additional context

[Numpy doc](https://numpy.org/devdocs/dev/development_advanced_debugging.html#compiler-sanitizers) about compiler sanitizers

[Numpy CI workflow](https://github.com/numpy/numpy/blob/main/.github/workflows/compiler_sanitizers.yml)",['Build / CI'],0
scikit-learn/scikit-learn,"Add class_weight support to Naive Bayes models (GaussianNB, MultinomialNB, BernoulliNB)","### Describe the workflow you want to enable

Naive Bayes classifiers (GaussianNB, MultinomialNB, BernoulliNB, ComplementNB, CategoricalNB) currently do not support the class_weight parameter, while almost all other scikit-learn classifiers do. This creates API inconsistency and requires users to manually compute and pass sample_weight for imbalanced datasets.

# Current Workaround

```py
from sklearn.naive_bayes import GaussianNB
from sklearn.utils.class_weight import compute_sample_weight
sample_weight = compute_sample_weight('balanced', y)
gnb = GaussianNB()
gnb.fit(X, y, sample_weight=sample_weight)
```

# Proposed Enhancement

```py
gnb = GaussianNB(class_weight='balanced')
gnb.fit(X, y)
```


### Describe your proposed solution

Add a class_weight parameter to all Naive Bayes classifiers that internally converts class weights to sample weights before calling the existing fit() logic. This leverages the fact that all NB classifiers already support sample_weight in their fitting methods.

```py
class GaussianNB(_BaseNB):
    _parameter_constraints: dict = {
        ""priors"": [""array-like"", None],
        ""var_smoothing"": [Interval(Real, 0, None, closed=""left"")],
        ""class_weight"": [""dict"", ""balanced"", None],  # NEW
    }

    def __init__(self, *, priors=None, var_smoothing=1e-9, class_weight=None):
        self.priors = priors
        self.var_smoothing = var_smoothing
-->  self.class_weight = class_weight 
```


### Describe alternatives you've considered, if relevant

_No response_

### Additional context

This is a improvement for API consistency. While the workaround exists, having uniform behavior across classifiers reduces confusion and improves discoverability, especially for beginners.",['New Feature'],0
scikit-learn/scikit-learn,LocalOutlierFactor with Mahalanobis distance returns different results based on `n_jobs` parameter,"### Describe the bug

I encountered the following bug while doing an outlier analysis on a large dataset:

To detect outliers in a dataset with known outliers, I followed these steps:

1) compute the covariance for a set of preselected numerical features with either `MinCovDet` or `EmpiricalCovariance` and extracted the inverse matrix VI using the `precision_` parameter.
2) I initialized the LocalOutlierFactor class with the metric `mahalanobis`, passed `VI`, and set n_jobs to 1
3) I used `fit_predict()`to get the outlier predictions

Everything looked as expected.

Later on, I tried speeding up the computation by adding a few cores to `n_jobs` and had to realise that no outliers were detected anymore. After some debugging, I found that:

1) outlier detection seems not to work when using multiple cores (all samples are marked as inliers)
2) this effect seems to be tied to `mahalanobis` distance, because the bug does not appear with minkowski (p=2) distance regardless of the number of cores.

One workaround seems to be to precompute the mahalanobis distance and passing it to the LOF, but this is again using only one core for the computation and is not feasible for large datasets due to the size of the full pairwise distance matrix.

### Steps/Code to Reproduce

```python
import numpy as np
from sklearn.covariance import MinCovDet
from sklearn.neighbors import LocalOutlierFactor
from scipy.spatial.distance import cdist
import matplotlib.pyplot as plt

RNG = np.random.RandomState(42)


def make_data(n_inliers=200, n_outliers=10, n_features=10):
    # Inliers: standard normal in 10D
    X_in = RNG.normal(loc=0.0, scale=1.0, size=(n_inliers, n_features))
    # Outliers: shifted by +8 in all dimensions
    X_out = RNG.normal(loc=8.0, scale=1.0, size=(n_outliers, n_features))
    X = np.vstack([X_in, X_out])
    y_true = np.hstack([np.ones(n_inliers, dtype=int), -np.ones(n_outliers, dtype=int)])
    return X, y_true


def robust_inverse_covariance(X, random_state=0):
    # Fit robust covariance estimator
    mcd = MinCovDet(random_state=random_state).fit(X)
    cov = mcd.covariance_  # shape (n_features, n_features)
    # Compute inverse covariance (VI) expected by 'mahalanobis' metric
    VI = mcd.precision_ 
    # ensure dtype=float64 and C-contiguous so it serializes safely across processes
    VI = np.ascontiguousarray(VI, dtype=np.float64)
    return VI, cov


def run_lof(X, VI, n_neighbors=20, n_jobs=1):
    lof = LocalOutlierFactor(
        n_neighbors=n_neighbors,
        metric='mahalanobis',
        metric_params={'VI': VI},
        n_jobs=n_jobs
    )
    # fit_predict returns -1 for outliers and 1 for inliers
    labels = lof.fit_predict(X)
    return labels


def run_lof_precomputed(X, VI, n_neighbors=20, n_jobs=1):
    # compute full pairwise Mahalanobis distance matrix in the main process
    D = cdist(X, X, metric='mahalanobis', VI=VI)
    lof = LocalOutlierFactor(
        n_neighbors=n_neighbors,
        metric='precomputed',
        n_jobs=n_jobs
    )
    labels = lof.fit_predict(D)
    return labels


def stats(labels):
    unique, counts = np.unique(labels, return_counts=True)
    return dict(zip(unique.tolist(), counts.tolist()))


# Executing example
## make data
X, y_true = make_data(n_inliers=200, n_outliers=10, n_features=10)

## plot data
plt.figure(figsize=(8, 6))
plt.scatter(X[:, 0], X[:, 1], c=y_true, cmap='coolwarm', edgecolor='k')
plt.title(""Toy dataset with inliers (blue) and outliers (red)"")
plt.xlabel(""Feature 0"")
plt.ylabel(""Feature 1"")
plt.show()

## Fit MinCovDet on the whole dataset 
VI, cov = robust_inverse_covariance(X, random_state=42)

## Run LOF with a single core (n_jobs=1)
labels_n1 = run_lof(X, VI, n_neighbors=20, n_jobs=1)
print(""n_jobs=1 unique counts:"", stats(labels_n1))
## expected result: n_jobs=1 unique counts: {-1: 11, 1: 199}

## Run LOF allowing multiple cores (n_jobs=4)
labels_n4 = run_lof(X, VI, n_neighbors=20, n_jobs=4)
print(""n_jobs=4 unique counts:"", stats(labels_n4))
## exprected result: n_jobs=4 unique counts: {1: 210}


## fix attempt: precompute Mahalanobis with single core and LOF with multiple
labels_precomputed = run_lof_precomputed(X, VI, n_neighbors=20, n_jobs=4)
print(""precomputed (n_jobs=4) unique counts:"", stats(labels_precomputed))
## expected result: precomputed (n_jobs=4) unique counts: {-1: 11, 1: 199}

## Print outlier labels
pred_outliers = np.where(labels_n1 == -1)[0]
print(""Indices detected as outliers by n_jobs=1"", pred_outliers)
## expected output: Indices detected as outliers by n_jobs=1: [116 200 201 202 203 204 205 206 207 208 209]
y_true_outliers = np.where(y_true == -1)[0]
print(""Indices of true outliers:"", y_true_outliers)
## expected output: Indices of true outliers: [200 201 202 203 204 205 206 207 208 209]


```

### Expected Results

It shouldn't matter how many cores are passed to the 'n_jobs' parameters. The results should be comparable, or at least detect some outliers.

### Actual Results

Results for LOF should be comparable, but are wildly different based on changes to the 'n_jobs' parameter, with all other parameters staying the same:

Results for n_jobs=1:
n_jobs=1 unique counts: {-1: 11, 1: 199}

Results for n_jobs=4:
n_jobs=4 unique counts: {1: 210}

### Versions

```shell
Python dependencies:
      sklearn: 1.7.2
          pip: 25.2
   setuptools: 80.9.0
        numpy: 2.3.4
        scipy: 1.15.2
       Cython: None
       pandas: 2.2.3
   matplotlib: 3.10.6
       joblib: 1.5.2
threadpoolctl: 3.6.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: openblas
    num_threads: 5
         prefix: libscipy_openblas
       filepath: /home/USER/environments/conda_environments/MY_ENV/lib/python3.11/site-packages/numpy.libs/libscipy_openblas64_-8fb3d286.so
        version: 0.3.30
threading_layer: pthreads
   architecture: Haswell

       user_api: blas
   internal_api: openblas
    num_threads: 5
         prefix: libopenblas
       filepath: /home/USER/environments/conda_environments/MY_ENV/lib/libopenblasp-r0.3.30.so
        version: 0.3.30
threading_layer: pthreads
   architecture: Zen

       user_api: openmp
   internal_api: openmp
    num_threads: 5
         prefix: libgomp
       filepath: /home/USER/environments/conda_environments/MY_ENV/lib/libgomp.so.1.0.0
        version: None
```","['Bug', 'Needs Investigation']",0
scikit-learn/scikit-learn,DOC: Clarify tie-breaking logic for equivalent splits in decision tree documentation,"### Describe the issue linked to the documentation

(IA generated, but read and pruned by a human ^^)

The user guide and API reference for decision trees and extra trees do not currently document what happens when there are multiple optimal splits during training. Specifically, when there are several splits with the same criterion value (i.e., splitting produces the same improvement/loss), the algorithm picks the first split it encounters. This is due to a strict ordering (`if current_proxy_improvement > best_proxy_improvement:`) in the code. For splits involving missing values, the first split with missing values assigned to the right child is considered.

This tie-breaking bias is confusingâ€”recent discussions in PR #32274 highlighted that documentation only describes this for missing value scenarios, and not as a general rule.

**References:** PR #32274, Meta issue #23728.

### Suggest a potential alternative/fix

Add a concise section to the decision tree documentation summarizing the present tie-breaking logic: when multiple splits have the same criterion, the first encountered is always chosen (order matters). For missing values, explain that splits with missing to the right are evaluated first, so those are chosen by default in a tie. 
Maybe mention relevant discussion in #23728, and clarify if/when unbiased/random tie-breaking might be considered as a future enhancement?",['Documentation'],0
scikit-learn/scikit-learn,RFC: Allow training random forests with histograms on binned feature values,"This issue is very related to #27873.

If/when the above is implemented, it should also be possible to refactor the tree code to leverage histogram splits for bagging-based tree ensembles.

Histogram-based split is the main reason, while scikit-learn-intelex's `RandomForestClassifier` is much faster that our `RandomForestClassifier` when trained on datasets with many data points.",['RFC'],0
scikit-learn/scikit-learn,CI Collect coverage results on the CUDA CI,"We currently don't collect and report coverage information to codecov for the CUDA CI, see https://github.com/scikit-learn/scikit-learn/pull/31829#issuecomment-3503237010

The `.github/workflows/unit-tests.yml` script contains a working setup for collecting coverage and uploading it to codecov using GitHub Actions. This is probably a good starting point for the CUDA CI which is defined in `.github/workflows/cuda-ci.yml`. We might also have to adjust the options passed to `pytest` in the CUDA CI so that it generates coverage statistics.

One thing to double check/think about is how having a CI that only runs sometimes ends up being displayed on codecov when comparing commits. For example, if `main` has no coverage for some code because we don't run the CUDA CI on `main` will it always look like you have missing coverage on a PR? Or does it not matter because our coverage requirements are calculated on the diff only?","['Build / CI', 'Array API']",0
scikit-learn/scikit-learn,Feature Request: Add explained variance ratios for X and Y to `PLSRegression`,"### Describe the workflow you want to enable

Currently, `sklearn.cross_decomposition.PLSRegression` does not expose any measure of explained variance for either the predictor (`X`) or response (`Y`) spaces.

In different disciplines (such as multivartiate statistical process control, -omic sciences or chemometrics), the explained variance in `X` and `Y` spaces is a core interpretability metric and is routinely used to interpret PLS models â€” analogous to how PCA exposes `explained_variance_ratio_`. Not having access to it in the `PLSRegression` makes it difficult to:

- Quantify how much variance each latent variable captures,

- Compare models with different numbers of components, and

- Produce diagnostic or variance-explained plots (as is standard in different disciplines where latent space interpretability is important).


### Describe your proposed solution

#### Proposed solution

I propose adding two new attributes, automatically computed after fitting:

- `explained_x_variance_ratio_ `: `ndarray` of shape (`n_components`)
    Fraction of variance explained in X-space for each component.

- `explained_y_variance_ratio_ `: `ndarray` of shape (`n_components`)
    Fraction of variance explained in Y-space for each component.


These values are derived using standard PLS sequential deflation methodology (see Wegelin, 2000; Abdi, 2003), where variance explained = reduction in total variance after deflation per component, where each matrix is asymmetrically deflated per each component as follows:

$$Xâ†X - \hat{X}_a = X - t_ap_a^T$$

$$Yâ†Y - \hat{Y}_a = Y - t_aq_a^T$$

This mirrors the `PCA` API (`explained_variance_ratio_`) and significantly improves usability for diagnostics and visualization.

#### Implementation sketch

```python
class PLSRegression(_PLS):
"""""" Documentation updated to include the follwoing attributes:

    Attributes
    ----------
    explained_x_variance_ratio_ : ndarray of shape (n_components,)
        Explained variance ratio in X-space (predictors) for each component.
        This measures how much variance in the predictor variables each latent
        variable captures. Calculated in fit().

    explained_y_variance_ratio_ : ndarray of shape (n_components,)
        Explained variance ratio in Y-space (response) for each component.
        This measures how much variance in the response
        each latent variable explains. Calculated in fit().
""""""

    def __init__(self, ...):
        [this section is not modified]

    def fit(self, X, y):
        super().fit(X, y)
        # Calculate explained variance ratios
        (
            self.explained_x_variance_ratio_,
            self.explained_y_variance_ratio_,
        ) = self._calculate_explained_variance_ratio(X, y)

        return self

        def _calculate_explained_variance_ratio(
        self, X, y
    ) -> tuple[np.ndarray, np.ndarray]:
        """"""Calculate explained variance ratios using sequential deflation.

        This implements the variance decomposition for PLS regression following
        the deflation methodology described in Wegelin (2000).

        This method calculates how much variance each component explains by
        sequentially deflating the X and Y matrices. This is the standard
        approach in PLS and provides accurate component-wise variance.

        Parameters
        ----------
        X : array-like of shape (n_samples, n_features)
            Training vectors. Accepts numpy arrays, pandas DataFrames.
        y : array-like of shape (n_samples,) or (n_samples, n_targets)
            Target vectors. Accepts 1D (univariate) or 2D (multivariate) targets.

        Returns
        -------
        tuple[ndarray, ndarray]
            - X variance ratios of shape (n_components,)
            - Y variance ratios of shape (n_components,)
        """"""
        # Convert to arrays and ensure y is 2D (handles pandas DataFrame/Series)
        X = np.asarray(X, dtype=float)
        y_array = np.asarray(y, dtype=float)
        y = np.atleast_2d(y_array).T if y_array.ndim == 1 else y_array

        # Center X and Y (PLS already centers data, but we need the original
        # centered versions)
        X_centered = X - X.mean(axis=0)
        y_centered = y - y.mean(axis=0)

        # Check for scaling
        if self.scale:
            X_std = X.std(axis=0, ddof=1)
            X_std[X_std == 0.0] = 1.0
            X_centered /= X_std
            y_std = y.std(axis=0, ddof=1)
            y_std[y_std == 0.0] = 1.0
            y_centered /= y_std

        # Total variance in centered data
        X_total_var = np.var(X_centered, axis=0, ddof=1).sum()
        y_total_var = np.var(y_centered, axis=0, ddof=1).sum()
        has_x_variance = not np.isclose(X_total_var, 0.0)
        has_y_variance = not np.isclose(y_total_var, 0.0)

        # Initialize matrices for deflation
        X_current = X_centered.copy()
        y_current = y_centered.copy()

        X_var_ratios = []
        y_var_ratios = []

        # For each component, calculate variance explained then deflate
        for a in range(self.n_components):
            # Get scores and loadings for component a (using slicing to keep 2D)
            t_a = self.x_scores_[:, a : a + 1]  # (n_samples, 1)
            p_a = self.x_loadings_[:, a : a + 1]  # (n_features_X, 1)
            q_a = self.y_loadings_[:, a : a + 1]  # (n_features_y, 1)

            # Reconstruct X and y using current component
            X_hat = t_a @ p_a.T
            y_hat = t_a @ q_a.T

            # Variance of current residual before deflation
            X_var_before = np.var(X_current, axis=0, ddof=1.0).sum()
            y_var_before = np.var(y_current, axis=0, ddof=1.0).sum()

            # Deflate X and y
            X_current -= X_hat
            y_current -= y_hat

            # Variance of residual after deflation
            X_var_after = np.var(X_current, axis=0, ddof=1.0).sum()
            y_var_after = np.var(y_current, axis=0, ddof=1.0).sum()

            # Store variance explained as ratio of total variance
            if has_x_variance:
                X_var_ratios.append((X_var_before - X_var_after) / X_total_var)
            else:
                X_var_ratios.append(0.0)
            if has_y_variance:
                y_var_ratios.append((y_var_before - y_var_after) / y_total_var)
            else:
                y_var_ratios.append(0.0)

        return np.array(X_var_ratios), np.array(y_var_ratios)
```

#### Usage Example

```python
pls = PLSRegression().fit(X, y)
explained_x_variance = pls.explained_x_variance_ratio_
explained_y_variance = pls.explained_y_variance_ratio_
```

#### Testing

I have tested that this implementation provides the same results as the ones described by:

Abdi, H. (2003). Partial Least Squares (PLS) Regression. In Lewis-Beck M., Bryman A., Futing T. (Eds.), Encyclopedia of Social Sciences Research Methods. Thousand Oaks (CA): Sage.

I have also tested that this implementation does not break the `scikit-learn` estimator API:

```python
# Test compliance with scikit-learn
def test_compliance_pls_regression():
    # Arrange
    transformer = PLSRegression()
    # Act & Assert
    check_estimator(transformer)
```

### Next steps

If maintainers agree this feature fits within scope, I would be happy to:

- Open a PR adding the attributes to PLSRegression,

- Include updated documentation and examples, and

- Add unit tests verifying correctness against synthetic datasets and published examples.


### Describe alternatives you've considered, if relevant

_No response_

### Additional context

- No changes to existing API or behavior.
- Attributes are only added post-fit (similar to PCAâ€™s `explained_variance_ratio_`).
- Could be extended to other PLS methods *(e.g., `PLSCanonical`) but it requires that other methods expose the `x_scores_` and the `y_scores_` (currently are not exposed) and to symmetrically  deflate the `Y` matrix.

### Previous issues

This request is not new, there were some previous. issues:

- https://github.com/scikit-learn/scikit-learn/issues/19896  (open but no  complete implementation suggested)
- https://github.com/scikit-learn/scikit-learn/issues/30470 (closed)




",['New Feature'],0
scikit-learn/scikit-learn,DOC Create triage guide in maintainer documentation,"As part of talking about triaging at the Scientific Python Summit I realised that we have quite some cumulative experience with triaging. There are also different approaches to things.

As a result I think it would be useful if we create a triaging guide as part of our developer/maintainer documentation. The goal of the guide would be to serve as a reference for what to do as a triager, what tools we have available to help with the process and so on.

Some initial things I can think of including:
* list of labels we have for triaging and what to use them for
* ""please don't open a PR before the discussion has converged"" as a policy (triagers should close these ""early PRs"" to help enforce this)
* bookmarked search queries/lists of issues
  * [needs triage](https://github.com/scikit-learn/scikit-learn/issues?q=sort%3Aupdated-desc%20is%3Aissue%20is%3Aopen%20label%3A%22Needs%20Triage%22)
  * [new PR without reviews](https://github.com/scikit-learn/scikit-learn/pulls?q=is%3Apr+is%3Aopen+review%3Anone+sort%3Acreated-desc)
  * [new PR without reviews by a maintainer](https://github.com/search?q=repo%3Ascikit-learn%2Fscikit-learn+is%3Apr+is%3Aopen+author%3Abetatim+author%3Aogrisel+author%3Aglemaitre+author%3Alesteve+review%3Anone&type=pullrequests&s=created&o=desc)
     * needs more maintainer usernames in the query
     * we could have a similar query for recurring contributors, but how?
  * [PRs waiting for second reviewer](https://github.com/scikit-learn/scikit-learn/pulls?q=sort%3Aupdated-desc+is%3Apr+is%3Aopen+review%3Anone+label%3A%22Waiting+for+Second+Reviewer%22)
  * [Open PRs that you have reviewed in the past](https://github.com/scikit-learn/scikit-learn/pulls?q=sort%3Aupdated-desc+is%3Apr+is%3Aopen+reviewed-by%3A%40me+)
  * look at PRs marked with ""autoclose"" and decide to potentially remove the label or re-open if the PR has significantly improved [scikit-learn/scikit-learn/issues (sort:updated-desc label:autoclose)](https://github.com/scikit-learn/scikit-learn/issues?q=sort%3Aupdated-desc%20label%3Aautoclose)
* integrate with https://scikit-learn.org/dev/developers/bug_triaging.html#triaging-operations-for-members-of-the-core-and-contributor-experience-teams

Can you think of more search queries that would be useful @lesteve?



xref https://github.com/scientific-python/summit-2025-nov/issues/6",['Documentation'],0
scikit-learn/scikit-learn,CI cp313t intermittent failure `UserWarning: sklearn.utils.parallel.delayed should be used with sklearn.utils.parallel.Parallel`,"Also seen in https://github.com/scikit-learn/scikit-learn/issues/32303#issuecomment-3354667463 as well in a different test and on Linux (this one is on macOS).

This is very likely related to warnings being thread-unsafe in cp313t. Not sure this is worth doing something about it since it doesn't seem to happen that often.

```
  ______________________ test_no_user_warning_with_scoring _______________________
  joblib.externals.loky.process_executor._RemoteTraceback: 
  """"""
  Traceback (most recent call last):
    File ""/private/var/folders/q1/lmdr88vx1k90l53fxl9p8lwc0000gn/T/cibw-run-kd2mu_4k/cp313t-macosx_arm64/venv-test-arm64/lib/python3.13t/site-packages/joblib/_utils.py"", line 72, in __call__
      return self.func(**kwargs)
             ~~~~~~~~~^^^^^^^^^^
    File ""/private/var/folders/q1/lmdr88vx1k90l53fxl9p8lwc0000gn/T/cibw-run-kd2mu_4k/cp313t-macosx_arm64/venv-test-arm64/lib/python3.13t/site-packages/joblib/parallel.py"", line 607, in __call__
      return [func(*args, **kwargs) for func, args, kwargs in self.items]
              ~~~~^^^^^^^^^^^^^^^^^
    File ""/private/var/folders/q1/lmdr88vx1k90l53fxl9p8lwc0000gn/T/cibw-run-kd2mu_4k/cp313t-macosx_arm64/venv-test-arm64/lib/python3.13t/site-packages/sklearn/utils/parallel.py"", line 144, in __call__
      warnings.warn(
      ~~~~~~~~~~~~~^
          (
          ^
      ...<5 lines>...
          UserWarning,
          ^^^^^^^^^^^^
      )
      ^
  UserWarning: `sklearn.utils.parallel.delayed` should be used with `sklearn.utils.parallel.Parallel` to make it possible to propagate the scikit-learn configuration of the current thread to the joblib workers.
  """"""
  
  The above exception was the direct cause of the following exception:
  
      def test_no_user_warning_with_scoring():
          """"""Check that no UserWarning is raised when scoring is set.
      
          Non-regression test for #22907.
          """"""
          pd = pytest.importorskip(""pandas"")
          X, y = make_regression(n_samples=50, random_state=0)
          X_df = pd.DataFrame(X, columns=[f""col{i}"" for i in range(X.shape[1])])
      
          est = HistGradientBoostingRegressor(
              random_state=0, scoring=""neg_mean_absolute_error"", early_stopping=True
          )
          with warnings.catch_warnings():
              warnings.simplefilter(""error"", UserWarning)
  >           est.fit(X_df, y)
  
  ../venv-test-arm64/lib/python3.13t/site-packages/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py:1370: 
  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
  ../venv-test-arm64/lib/python3.13t/site-packages/sklearn/base.py:1336: in wrapper
      return fit_method(estimator, *args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ../venv-test-arm64/lib/python3.13t/site-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:722: in fit
      X_binned_train = self._bin_data(X_train, is_training_data=True)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ../venv-test-arm64/lib/python3.13t/site-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1237: in _bin_data
      X_binned = self._bin_mapper.fit_transform(X)  # F-aligned array
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ../venv-test-arm64/lib/python3.13t/site-packages/sklearn/utils/_set_output.py:316: in wrapped
      data_to_wrap = f(self, X, *args, **kwargs)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ../venv-test-arm64/lib/python3.13t/site-packages/sklearn/base.py:907: in fit_transform
      return self.fit(X, **fit_params).transform(X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  ../venv-test-arm64/lib/python3.13t/site-packages/sklearn/ensemble/_hist_gradient_boosting/binning.py:240: in fit
      non_cat_thresholds = Parallel(n_jobs=self.n_threads, backend=""threading"")(
  ../venv-test-arm64/lib/python3.13t/site-packages/sklearn/utils/parallel.py:91: in __call__
      return super().__call__(iterable_with_config_and_warning_filters)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ../venv-test-arm64/lib/python3.13t/site-packages/joblib/parallel.py:2072: in __call__
      return output if self.return_generator else list(output)
                                                  ^^^^^^^^^^^^
  ../venv-test-arm64/lib/python3.13t/site-packages/joblib/parallel.py:1682: in _get_outputs
      yield from self._retrieve()
  ../venv-test-arm64/lib/python3.13t/site-packages/joblib/parallel.py:1784: in _retrieve
      self._raise_error_fast()
  ../venv-test-arm64/lib/python3.13t/site-packages/joblib/parallel.py:1859: in _raise_error_fast
      error_job.get_result(self.timeout)
  ../venv-test-arm64/lib/python3.13t/site-packages/joblib/parallel.py:758: in get_result
      return self._return_or_raise()
             ^^^^^^^^^^^^^^^^^^^^^^^
  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
  
  self = <joblib.parallel.BatchCompletionCallBack object at 0x5d4061e5010>
  
      def _return_or_raise(self):
          try:
              if self.status == TASK_ERROR:
  >               raise self._result
  E               UserWarning: `sklearn.utils.parallel.delayed` should be used with `sklearn.utils.parallel.Parallel` to make it possible to propagate the scikit-learn configuration of the current thread to the joblib workers.
  
  ../venv-test-arm64/lib/python3.13t/site-packages/joblib/parallel.py:773: UserWarning
```","['Build / CI', 'free-threading']",0
scikit-learn/scikit-learn,"StackingRegressor is incompatible with TimeSeriesSplit as cv parameter, raising ""ValueError: cross_val_predict only works for partitions""","https://github.com/scikit-learn/scikit-learn/blob/0f9b6a6c0bbbcbbb7f509016fa1006facfe1ba8d/sklearn/model_selection/_validation.py#L1185

### `scikit-learn` version and operating system

scikit-learn version: 1.5.2 (also affects 1.4.x)
Operating System: (e.g., macOS, Windows, Linux)
Python version: (e.g., 3.10)

### The bug

When using `StackingRegressor` (or `StackingClassifier`), passing a `TimeSeriesSplit` object to the `cv` parameter fails, raising a `ValueError: cross_val_predict only works for partitions`.

This check is fundamentally incompatible with `TimeSeriesSplit`, which correctly reserves the initial chunk of data for training and never uses it as a test set.

This bug makes it impossible to use `StackingRegressor` for time-series forecasting without resorting to a leaky workaround like `KFold(shuffle=False)`.

### Steps/Code to Reproduce

Here is a minimal reproducible example:

```python
import numpy as np
import sklearn
from sklearn.model_selection import TimeSeriesSplit, KFold
from sklearn.ensemble import StackingRegressor
from sklearn.linear_model import LinearRegression

print(f""scikit-learn version: {sklearn.__version__}"")

# 1. Create dummy time-series data
X = np.random.rand(100, 2)
y = np.random.rand(100)

# 2. Define estimators
estimators = [
    ('lr1', LinearRegression()),
    ('lr2', LinearRegression())
]

# 3. Define the TimeSeriesSplit (the correct tool for this)
# n_splits=5 will reserve the first 1/6th of data for training only.
tscv = TimeSeriesSplit(n_splits=5)

# 4. Create the Stacker using TimeSeriesSplit
stacker_tscv = StackingRegressor(
    estimators=estimators,
    final_estimator=LinearRegression(),
    cv=tscv  # Pass the TimeSeriesSplit
)

# 5. This will FAIL
try:
    print(""\nAttempting to fit with TimeSeriesSplit..."")
    stacker_tscv.fit(X, y)
    print(""SUCCESS: TimeSeriesSplit worked."")
except ValueError as e:
    print(f""FAILED with TimeSeriesSplit: {e}"")

# 6. Show the ""workaround"" that succeeds (but leaks data)
print(""\nAttempting to fit with KFold(shuffle=False)..."")
kf_cv = KFold(n_splits=5, shuffle=False)
stacker_kf = StackingRegressor(
    estimators=estimators,
    final_estimator=LinearRegression(),
    cv=kf_cv
)
try:
    stacker_kf.fit(X, y)
    print(""SUCCESS: KFold(shuffle=False) worked."")
except Exception as e:
    print(f""FAILED with KFold(shuffle=False): {e}"")
```

### Impact 
This bug effectively breaks **StackingRegressor** for all time-series applications. It forces users to choose between:

Downgrading to scikit-learn==1.3.2 (where this check did not exist?). Not entirely sure which version it got introduced! 

Using KFold(shuffle=False) as the cv parameter, which introduces severe data leakage and leads to invalid models.",['Needs Info'],0
scikit-learn/scikit-learn,Array API support for LogisticRegression with LBFGS,"Even if `scipy.optimize` does not support array API yet, we can probably work around it to perform the computation of the gradient using the input data array namespace and convert the gradient values to numpy prior to feeding them back to the solver at each iteration.

Assuming `n_samples >> n_features`, this is likely to be a worthwhile way to get a large speed-up on GPU devices for that estimator despite back and forth conversion to and from device for the parameters array.

To compute the gradient we perform `X @ coef + intercept` which leads to ""raw predictions"". Those raw predictions are fed along with `y` to the Cython loss. This will be an `(n_samples, n_classes)` array. For a first iteration, we can convert that to NumPy and continue using Cython. Alternatively, we can write pure Python array API capable version of the gradient method of the loss functions. We should start with log loss for which we can already refactor and move the `sklearn.utils._array_api._half_multinomial_loss` function (introduced for metrics) into the `sklearn._loss` package to add its gradient/Hessian counterparts.

We can leave the other losses as raising `NotImplementError` for now.","['High Priority', 'Array API']",0
scikit-learn/scikit-learn,"Add shrinkage support with `solver=""svd""` in `LinearDiscriminantAnalysis` and `QuadraticDiscriminantAnalysis`","### Describe the workflow you want to enable

Both Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA) can be solved with the SVD solver. The SVD solver does not explicitly compute the covariance matrix, so covariance shrinkage is not supported when we set `solver=""svd""`, as illustrated in the snippet below:

```python
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
import numpy as np

# Data that has zero variance in one dimension and needs regularization
X = np.random.randn(9, 2)
y = np.array([1, 1, 1, 1, 1, 2, 2, 2, 2])

solver = ""svd""
clf = LinearDiscriminantAnalysis(solver=solver, shrinkage=0.5)
clf.fit(X, y)
```
```python
File ~/Packages/scikit-learn/scikit-learn/sklearn/discriminant_analysis.py:700, in LinearDiscriminantAnalysis.fit(self, X, y)
    698 if self.solver == ""svd"":
    699     if self.shrinkage is not None:
--> 700         raise NotImplementedError(""shrinkage not supported with 'svd' solver."")
    701     if self.covariance_estimator is not None:
    702         raise ValueError(
    703             ""covariance estimator ""
    704             ""is not supported ""
    705             ""with svd solver. Try another solver""
    706         )

NotImplementedError: shrinkage not supported with 'svd' solver.
```
Similar behavior for QDA is implemented in PR #32108.

However, it should be possible to implement shrinkage with the SVD solver that is mathematically equivalent to shrinkage with the other solvers, and it might be desirable to do so (https://github.com/scikit-learn/scikit-learn/pull/32108#discussion_r2454295132).


### Describe your proposed solution

While `solver=""svd""` does not explicitly compute the covariance matrix, it relies on the singular values of the data matrix `X` of shape `(n_samples, n_features)`, denoted `S`. The i'th singular value `S[i]` is directly related to the i'th eigenvalue `lambda[i]` of the covariance matrix by the relation `lambda[i] = S[i]**2 / (n - 1)`, where `n` is the number of samples (see [here](https://github.com/scikit-learn/scikit-learn/blob/72d97bceec1af70d9b65bb49d12abdd5694e67f1/sklearn/discriminant_analysis.py#L1019) and [here](https://stats.stackexchange.com/questions/134282/relationship-between-svd-and-pca-how-to-use-svd-to-perform-pca)).

Shrinkage of the covariance matrix [as implemented](https://scikit-learn.org/stable/modules/generated/sklearn.covariance.ShrunkCovariance.html) by the parameter `shrinkage` is just a shrinking of the eigenvalues of the covariance towards the mean eigenvalue.

Given the above, support for the parameter `shrinkage` with `solver=""svd""` can be implemented by appropriately shrinking the singular values `S` to match the shrinkage of the implicit eigenvalues. This should be pretty easy to implement.

One detail of this implementation is that when `n_samples < n_features`, then SVD only recovers `n_samples` number of singular values. Implicitly, the rest of the singular values are 0, corresponding to the rank-defficient covariance matrix. To implement shrinkage as suggested here, we would have to pad the vector `S` with zeros up to a length of `n_features` in this case.

### Describe alternatives you've considered, if relevant

_No response_

### Additional context

_No response_",['New Feature'],0
scikit-learn/scikit-learn,BUG: `QuantileTransformer(ignore_implicit_zeros=True)` sub-sampling behavior is incorrect,"### Describe the bug

`QuantileTransformer(ignore_implicit_zeros=True)` sub-sampling behavior is incorrect leading to very incorrect results for very sparse matrices and/or small sub-sample size.
More over, this behavior is ""discontinous"": see the example.

In those lines:
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/preprocessing/_data.py#L2830-L2839

`column_subsample` can easily be 0 if `n_samples` is big, and I don't see why a big `n_samples` should change a behavior that triggers only if `len(column_nnz_data) > self.subsample`...



### Steps/Code to Reproduce

```Python
import numpy as np
from scipy.sparse import csc_matrix
from sklearn.preprocessing import QuantileTransformer

subsample = 10_000

qt = QuantileTransformer(
    ignore_implicit_zeros=True,
    subsample=subsample,
)

# create a very sparse X matrix with two very similar columns:
n, d = 10**9, 2
# one colunm has size `subsample - 1`, the other one has size `subsample + 1`
col = np.repeat([0, 1], [subsample - 1, subsample + 1])
# choose some row indices (doesn't matter in this example):
row = np.arange(subsample * 2)
# random normal data:
data = np.random.randn(subsample * 2)

X = csc_matrix((
    data,
    (row, col)
), shape=(n, d))

Y = qt.fit_transform(X)
# given that X[:, 0] and X[:, 1] are very similar,
# you would expect they are treated similarly

# Y[:, 0] is as exepcted (looking like a uniform RV):
print(np.quantile(Y[:, 0].data, np.linspace(0, 1, num=11)))

# but Y[:, 1] is all 0 or 1
assert np.all((Y[:, 1].data == 0) | (Y[:, 1].data == 1))
# because all fitted quantiles are zeros, because the section of
# the code pointed in this issue created an array of size 0
assert (qt.quantiles_[:, 1] == 0).all()
```

### Expected Results

Both `Y[:, 0].data` and `Y[:, 1].data` should look like uniform RVs (after all, they're almost identical).

### Actual Results

See comments in my snippet.

### Versions

```shell
System:
    python: 3.12.11 (main, Aug 18 2025, 19:19:11) [Clang 20.1.4 ]
executable: /home/arthur/dev-perso/scikit-learn/sklearn-env/bin/python
   machine: Linux-6.14.0-33-generic-x86_64-with-glibc2.39

Python dependencies:
      sklearn: 1.8.dev0
          pip: None
   setuptools: 80.9.0
        numpy: 2.3.4
        scipy: 1.16.2
       Cython: 3.1.5
       pandas: 2.3.3
   matplotlib: 3.10.7
       joblib: 1.5.2
threadpoolctl: 3.6.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: openblas
    num_threads: 16
         prefix: libscipy_openblas
       filepath: /home/arthur/dev-perso/scikit-learn/sklearn-env/lib/python3.12/site-packages/numpy.libs/libscipy_openblas64_-8fb3d286.so
        version: 0.3.30
threading_layer: pthreads
   architecture: Haswell

       user_api: blas
   internal_api: openblas
    num_threads: 16
         prefix: libscipy_openblas
       filepath: /home/arthur/dev-perso/scikit-learn/sklearn-env/lib/python3.12/site-packages/scipy.libs/libscipy_openblas-b75cc656.so
        version: 0.3.29.dev
threading_layer: pthreads
   architecture: Haswell

       user_api: openmp
   internal_api: openmp
    num_threads: 16
         prefix: libgomp
       filepath: /usr/lib/x86_64-linux-gnu/libgomp.so.1.0.0
        version: None
```",['Bug'],0
scikit-learn/scikit-learn,DOC: Add user guide entry for sample weights,"> [!WARNING]
> Note to potential contributors: this is not a good issue to work on, please don't comment ""can I work on it"" or you may be banned from the repo.
> There is NO action that needs to be taken for this issue atm - there are probably some questions around weights that need to be addressed first. 

### Describe the issue linked to the documentation

As discussed in #30564 it is probably worthwhile to add a 'sample weight' section in the user guide.

Here are some notable additions, that we thought were too long to include in the glossary entry:

* Examples of uses of sample weights
   * in supervised learning one of the main reasons for weights is to account for the different size/time/exposure/... of each observation. We are assuming weights are inversely proportional to the variance of the target, e.g., we use exposure as weights in [Poisson regression and non-normal loss](https://scikit-learn.org/dev/auto_examples/linear_model/plot_poisson_regression_non_normal_loss.html) and [Tweedie regression on insurance claims](https://scikit-learn.org/dev/auto_examples/linear_model/plot_tweedie_regression_insurance_claims.html), with longer exposure duration being weighted higher, as these would be considered more reliable, or lower variance. (details: https://github.com/scikit-learn/scikit-learn/pull/30564#issuecomment-2583340580, https://github.com/scikit-learn/scikit-learn/pull/30564#discussion_r2338421172) 
   * a nice collection of papers with examples of weight use here: https://github.com/scikit-learn/scikit-learn/pull/30564#issuecomment-2567939034
* Should we try to use similar terms as other statistical packages (e.g., stata, julia) - ""analytical weights"", ""frequency weights""... (stata reference  http://www.stata.com/manuals13/u20.pdf#u20.23Weightedestimation , see section 20.23)
* Re-weighting for transfer learning (ref: https://github.com/scikit-learn/scikit-learn/pull/30564#pullrequestreview-2552200734)
* Third-party libraries can also use `sample_weight`-compatible estimators as building blocks to reduce a specific statistical task into a weighted regression or classification task

cc @ogrisel @lorentzenchr 

### Suggest a potential alternative/fix

_No response_","['Documentation', 'Hard']",0
scikit-learn/scikit-learn,Investigate using abi3 wheels (Stable ABI + limited C API),"Copying and pasting relevant info from https://github.com/scipy/scipy/issues/23791 (thanks @rgommers for the initial write-up and super detailed info).

It looks like this could ""just work"" for us so worth a try. The attraction of that is that it will allow building abi3 wheels, meaning we need one (or two for now, with free-threading) wheel per platform rather than N per platform with N the number of supported Python versions.

### scikit-learn specific questions

- Python 3.11 will be supported one more year so we could do one Python 3.11 wheel and one Python >= 3.12 wheel
- I guess we would build one Python >= 3.12 wheel but still tests all the Python versions. Testing on only one Python version seems too brittle, you could imagine introducing Python 3.13 say issue and not realize until the release. If we test all the Python version that's not going to reduce the CI time that much (most of the time is spent in tests not in build). Not sure you can build once and test for all the Python versions easily with cibuildwheel. At worst you build many times the same wheel, since this is not where most of the CI time is spent.
- other things I have forgotten

### Details


- CPython: the Stable ABI has been taken more serious over the past couple of years and gaps have been filled. Python 3.12 seems to be the first version that has everything that extension module generators need. And Python 3.12 will be our lowest supported version very soon.
- NumPy: NumPy headers should have complete guards since 2.0.0 so extension modules that use NumPy headers can use the limited C API. There are a few things in the NumPy C API that are then unavailable (e.g., [`PyArray_DTypeMeta](https://numpy.org/devdocs/reference/c-api/types-and-structures.html#c.PyArray_DTypeMeta)), but I don't think we're using those objects in SciPy directly. Support isn't super well tested, but is likely to work as is for SciPy.
- Meson: supported since 1.3.0, see limited_api in [mesonbuild.com/Python-module.html#extension_module](https://mesonbuild.com/Python-module.html#extension_module)
- meson-python: supported since 0.14.0, see [the meson-python.limited-api build config option](https://mesonbuild.com/meson-python/reference/pyproject-settings.html#cmdoption-arg-tool.meson-python.limited-api)
Cython: Supported fairly well in 3.1.x, and more improvements landing regularly. See docs at [cython.readthedocs.io/en/latest/src/userguide/limited_api.html](https://cython.readthedocs.io/en/latest/src/userguide/limited_api.html)

**Performance impact should be measured, but is expected to be modest.** [This section of the Cython docs](https://cython.readthedocs.io/en/latest/src/userguide/limited_api.html#performance) discuss the expected performance impact based on type of usage: ""Where the majority of the work involves C-level code the performance loss is likely to be low. This includes code that makes heavy use of typed memoryviews, or code that mainly calls an external C library. Where the majority of the work involves interacting with Python objects the cost is likely to be more significant."" SciPy's usage is of the former kind, we don't deal with lots of small objects. So it's probably in the 0% - 10% range, with the median being 0% (i.e., it doesn't matter at all for most functions written in Cython).

**How to enable building against the limited C API:** this can be done globally, per extension, or per type of binding generator. E.g., for Cython, it only requires adding `-DPy_LIMITED_API=0x030C0000` as a flag to `cython_c_args` in `scipy/meson.build` to get going. Some more plumbing is involved to ensure the file extension contains `.abi3`, but that can be left to the very end.",['Build / CI'],0
scikit-learn/scikit-learn,Reuse the `pre-commit` linter config in the CI runners,"At the moment, our CI runs linting jobs using a custom script `build_tools/lintin.sh` with manually pip installed dependencies in the Circle CI and Azure Pipelines CI configuration files.

Those linter tool versions are not always consistent with what we configure in `.pre-commit-config.yaml` and that can cause problems such as the following: https://github.com/scikit-learn/scikit-learn/pull/32499#issuecomment-3402244557.

I think we should refactor our CI config to reuse the `pre-commit` command, for instance by running:

```
pre-commit run --all-files
```

Instead of our custom `build_tools/linting.sh` script. However, this might not be that easy:

- there are probably linting failures to fix or config to update to ignore specific files or folders;
- there might be CI integration to update, in particular the `.github/workflows/bot-lint-comment.yml` and the `./build_tools/get_comment.py` script that generate a helpful user facing comments will need to be reworked and adapted to work with the output of the `pre-commit` command.","['Build / CI', 'help wanted']",0
ollama/ollama,app: sidebar animates opening on load instead of appearing open,"### What is the issue?

When Ollama's app loads, the sidebar animates open instead of appearing open on first load.

### Relevant log output

```shell

```

### OS

_No response_

### GPU

_No response_

### CPU

_No response_

### Ollama version

_No response_","['bug', 'good first issue', 'app']",1
ollama/ollama,CLI: image path not recognized correctly,"### What is the issue?

Sometimes when dragging an image in the CLI, the file path is not properly recognized. 

Example: 

```
ollama@ollamas-computer % ollama run gemma3:27b
>>> /Users/ollama/Library/Mobile\ Documents/com\~apple\~CloudDocs/screenshots/CleanShot\ 2025-04-17\ at\ 21.26.40@2x.png 
Okay, I understand you're still referencing the same image file path. Unfortunately, I *still* cannot access files on your computer. My 
inability to view the image hasn't changed.  I'm an AI model and operate only with the text input you provide.

**To reiterate, please provide me with the *content* of the image.**  You'll need to:

1. **Describe the image to me.** Tell me what you see.
2. **Copy and paste any text from the image.**  This is the most helpful thing you^C
```

### Relevant log output

```shell

```

### OS

_No response_

### GPU

_No response_

### CPU

_No response_

### Ollama version

0.6.5",['good first issue'],1
ollama/ollama,Capitalize Ollama in `ollama` help description,"Ollama should be capitalized unless referring to the cli command

```
Usage:
  ollama [flags]
  ollama [command]

Available Commands:
  serve       Start ollama
  create      Create a model from a Modelfile
  show        Show information for a model
  run         Run a model
  stop        Stop a running model
  pull        Pull a model from a registry
  push        Push a model to a registry
  list        List models
  ps          List running models
  cp          Copy a model
  rm          Remove a model
  help        Help about any command

Flags:
  -h, --help      help for ollama
  -v, --version   Show version information
```","['feature request', 'good first issue']",1
ollama/ollama,Ollama should prevent sleep when working.,"I am using Ollama 0.1.32 on Windows 11. Ollama is set to provide LLMs on LAN. However, when I use it, sometimes the Windows goes sleep mode when I am waiting Ollama answering questions using API.

So I guess Ollama does not prevent Windows 11 into sleep even through Ollama is working.

Ollama is install natively, not docker.","['feature request', 'good first issue', 'windows']",1
ollama/ollama,Allow running the Ollama documentation locally,"Please add instructions to run and browse the Ollama documentation locally. This will make it easier and faster to contribute to the docs!

Right now, we don't know what the `.mdx` files in `/docs` will render into until the changes are merged in an PR. ",['feature request'],0
ollama/ollama,imagegen: embed the model and prompt in the generated image,"Similar to how Stable Diffusion stores generation parameters in the PNG, it would be convenient if ollama also (optionally) added this information during image generation.",['feature request'],0
ollama/ollama,x/flux2-klein:4b Error: insufficient memory for image generation: need 21 GB,"### What is the issue?

ollama run x/flux2-klein:4b
pulling manifest 
pulling model: 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– 5.7 GB                         
writing manifest 
success 
Error: failed to load model: 500 Internal Server Error: image runner failed: Error: insufficient memory for image generation: need 21 GB, have 11 GB (exit: exit status 1)


### Relevant log output

```shell

```

### OS

_No response_

### GPU

_No response_

### CPU

_No response_

### Ollama version

_No response_","['bug', 'macos']",0
ollama/ollama,Generation stops after tool call with Ollama (GLM-4.7-Flash),"### What is the issue?

When running glm-4.7-flash:bf16 locally via Ollama (with context_length: 80000) and connecting it to OpenCode, the model halts generation immediately after outputting any tool call. I have to manually type ""continue"" every single time to resume.
The same model hosted through vLLM (instead of Ollama) works flawlessly in OpenCode â€” tool calls are handled normally without interruption.
This seems specific to Ollama's handling of this GLM-4.7 Flash variant during tool use. Has anyone else run into this?

### Relevant log output

```shell

```

### OS

linux arm

### GPU

NVIDIA GH200

### CPU

_No response_

### Ollama version

`0.14.3`",['bug'],0
ollama/ollama,Using GPU,"### What is the issue?

After updating to version 0.14.3, it refuses to use the GPU.
Do you even perform basic testing before releasing? Otherwise, you keep pushing half-baked versions into production.

### Relevant log output

```shell

```

### OS

Windows

### GPU

NVIDIA GeForce RTX 3090

### CPU

Intel

### Ollama version

0.14.3","['bug', 'nvidia', 'intel']",0
ollama/ollama,Server error when attempting image generation (M5 mac),"### What is the issue?

Attempted to generate an image using the x/z-image-turbo model and received a server error

I am running Ollama 0.14.3 on **M5** Macbook Pro (latest MacOS).

```
â¯ ollama --version
ollama version is 0.14.3
```

When I attempt the image generation, the API returns HTTP 500.
 
```
â¯ ollama run x/z-image-turbo ""A cat with a hello sign""
Error: 500 Internal Server Error: Post ""http://127.0.0.1:57107/completion"": EOF
```

The logs show the following error:
```
MLX error: [metal::Device] Unable to load kernel affine_qmm_t_nax_bfloat16_t_gs_32_b_8_bm64_bn64_bk64_wm2_wn2_alN_true_batch_0
```

A discussion at [llmstudio-ai](https://github.com/lmstudio-ai/lmstudio-bug-tracker/issues/1356) leads me to believe this could be a M5 specific issue.

### Relevant log output

```shell
time=2026-01-21T20:15:51.918-06:00 level=INFO source=routes.go:1629 msg=""server config"" env=""map[HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/Users/jeastman/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false http_proxy: https_proxy: no_proxy:]""
time=2026-01-21T20:15:51.957-06:00 level=INFO source=images.go:501 msg=""total blobs: 3374""
time=2026-01-21T20:15:51.960-06:00 level=INFO source=images.go:508 msg=""total unused blobs removed: 0""
time=2026-01-21T20:15:51.961-06:00 level=INFO source=routes.go:1682 msg=""Listening on 127.0.0.1:11434 (version 0.14.3)""
time=2026-01-21T20:15:51.961-06:00 level=INFO source=runner.go:67 msg=""discovering available GPUs...""
time=2026-01-21T20:15:51.961-06:00 level=INFO source=server.go:429 msg=""starting runner"" cmd=""/Applications/Ollama.app/Contents/Resources/ollama runner --ollama-engine --port 56914""
time=2026-01-21T20:15:52.083-06:00 level=INFO source=types.go:42 msg=""inference compute"" id=0 filter_id=0 library=Metal compute=0.0 name=Metal description=""Apple M5"" libdirs="""" driver=0.0 pci_id="""" type=discrete total=""25.0 GiB"" available=""25.0 GiB""
[GIN] 2026/01/21 - 20:15:52 | 200 |      27.041Âµs |       127.0.0.1 | GET      ""/api/version""
[GIN] 2026/01/21 - 20:15:52 | 200 |          50Âµs |       127.0.0.1 | HEAD     ""/""
[GIN] 2026/01/21 - 20:15:52 | 200 |   66.450334ms |       127.0.0.1 | POST     ""/api/show""
time=2026-01-21T20:15:52.426-06:00 level=INFO source=server.go:136 msg=""starting image runner subprocess"" exe=/Applications/Ollama.app/Contents/Resources/ollama model=x/z-image-turbo:latest port=56921
time=2026-01-21T20:15:52.456-06:00 level=WARN source=server.go:129 msg=image-runner msg=""2026/01/21 20:15:52 runner.go:78: INFO MLX library initialized""
time=2026-01-21T20:15:52.456-06:00 level=WARN source=server.go:129 msg=image-runner msg=""2026/01/21 20:15:52 runner.go:79: INFO starting image runner model=x/z-image-turbo:latest port=56921""
time=2026-01-21T20:15:52.460-06:00 level=INFO source=server.go:122 msg=image-runner msg=""Loading Z-Image model from manifest: x/z-image-turbo:latest...""
time=2026-01-21T20:15:52.460-06:00 level=WARN source=server.go:129 msg=image-runner msg=""2026/01/21 20:15:52 runner.go:91: INFO detected model type type=ZImagePipeline""
time=2026-01-21T20:15:52.601-06:00 level=INFO source=server.go:122 msg=image-runner msg=""  Loading tokenizer... âœ“""
time=2026-01-21T20:15:53.530-06:00 level=INFO source=server.go:122 msg=image-runner msg=""  Loading text encoder... âœ“""
time=2026-01-21T20:15:53.531-06:00 level=INFO source=server.go:122 msg=image-runner msg=""  (4.5 GB, peak 4.5 GB)""
time=2026-01-21T20:15:54.796-06:00 level=INFO source=server.go:122 msg=image-runner msg=""  Loading transformer... âœ“""
time=2026-01-21T20:15:54.796-06:00 level=INFO source=server.go:122 msg=image-runner msg=""  (11.7 GB, peak 11.7 GB)""
time=2026-01-21T20:15:54.869-06:00 level=INFO source=server.go:122 msg=image-runner msg=""  Loading conv_in... âœ“""
time=2026-01-21T20:15:54.869-06:00 level=INFO source=server.go:122 msg=image-runner msg=""  Loading mid block... âœ“""
time=2026-01-21T20:15:54.869-06:00 level=INFO source=server.go:122 msg=image-runner msg=""  Loading up blocks... âœ“ [4 blocks]""
time=2026-01-21T20:15:54.869-06:00 level=INFO source=server.go:122 msg=image-runner msg=""  Loading conv_norm_out... âœ“""
time=2026-01-21T20:15:54.869-06:00 level=INFO source=server.go:122 msg=image-runner msg=""  Loading conv_out... âœ“""
time=2026-01-21T20:15:54.869-06:00 level=INFO source=server.go:122 msg=image-runner msg=""  (11.9 GB, peak 11.9 GB)""
time=2026-01-21T20:15:54.869-06:00 level=INFO source=server.go:122 msg=image-runner msg=""  Loaded in 2.41s (11.9 GB VRAM)""
time=2026-01-21T20:15:54.869-06:00 level=WARN source=server.go:129 msg=image-runner msg=""2026/01/21 20:15:54 runner.go:138: INFO image runner listening addr=127.0.0.1:56921""
time=2026-01-21T20:15:54.928-06:00 level=INFO source=server.go:207 msg=""image runner is ready"" port=56921
time=2026-01-21T20:15:54.946-06:00 level=INFO source=server.go:122 msg=image-runner msg=""MLX error: [metal::Device] Unable to load kernel affine_qmm_t_nax_bfloat16_t_gs_32_b_8_bm64_bn64_bk64_wm2_wn2_alN_true_batch_0""
time=2026-01-21T20:15:54.946-06:00 level=INFO source=server.go:122 msg=image-runner msg="" at /Users/runner/work/ollama/ollama/build/_deps/mlx-c-src/mlx/c/transforms.cpp:73""
[GIN] 2026/01/21 - 20:15:55 | 500 |  2.722190167s |       127.0.0.1 | POST     ""/api/generate""
time=2026-01-21T20:20:55.100-06:00 level=INFO source=server.go:305 msg=""stopping image runner subprocess"" pid=51208
```

### OS

macOS

### GPU

Apple

### CPU

Apple

### Ollama version

0.14.3",['bug'],0
ollama/ollama,Error loading model,"### What is the issue?

Finetuned qwen3-VL via LlamaFactory, converted from safetensors to gguf via llama.cpp, creating model in ollama is ok, but when i try to run model i got error, pls help

Others models include Qwen3-vl works fine

### Relevant log output

```shell
Jan 22 05:53:11 ai-nvidia-app01 ollama[4135940]: ggml_backend_cuda_device_get_memory device GPU-f96ee413-bbeb-db0e-f5b5-31cdf6e15749 utilizing NVML memory reporting free: 16721838080 total: 25757220864
Jan 22 05:53:11 ai-nvidia-app01 ollama[4135940]: time=2026-01-22T05:53:11.132+03:00 level=INFO source=sched.go:635 msg=""updated VRAM based on existing loaded models"" gpu=GPU-f96ee413-bbeb-db0e-f5b5-31cdf6e15749 library=CUDA total=""24.0 GiB"" available=""15.6 GiB""
Jan 22 05:53:11 ai-nvidia-app01 ollama[4135940]: time=2026-01-22T05:53:11.163+03:00 level=INFO source=server.go:245 msg=""enabling flash attention""
Jan 22 05:53:11 ai-nvidia-app01 ollama[4135940]: time=2026-01-22T05:53:11.163+03:00 level=INFO source=server.go:429 msg=""starting runner"" cmd=""/usr/local/bin/ollama runner --ollama-engine --model /usr/share/ollama/.ollama/models/blobs/sha256-710dd9ac870452a8e599b4ed7672db3772d0a685541ff4e6b33d9ae9d2bfa8cb --port 44913""
Jan 22 05:53:11 ai-nvidia-app01 ollama[4135940]: time=2026-01-22T05:53:11.164+03:00 level=INFO source=sched.go:452 msg=""system memory"" total=""31.1 GiB"" free=""27.2 GiB"" free_swap=""0 B""
Jan 22 05:53:11 ai-nvidia-app01 ollama[4135940]: time=2026-01-22T05:53:11.164+03:00 level=INFO source=sched.go:459 msg=""gpu memory"" id=GPU-f96ee413-bbeb-db0e-f5b5-31cdf6e15749 library=CUDA available=""15.1 GiB"" free=""15.6 GiB"" minimum=""457.0 MiB"" overhead=""0 B""
Jan 22 05:53:11 ai-nvidia-app01 ollama[4135940]: time=2026-01-22T05:53:11.164+03:00 level=INFO source=server.go:755 msg=""loading model"" ""model layers""=37 requested=-1
Jan 22 05:53:11 ai-nvidia-app01 ollama[4135940]: time=2026-01-22T05:53:11.172+03:00 level=INFO source=runner.go:1405 msg=""starting ollama engine""
Jan 22 05:53:11 ai-nvidia-app01 ollama[4135940]: time=2026-01-22T05:53:11.172+03:00 level=INFO source=runner.go:1440 msg=""Server listening on 127.0.0.1:44913""
Jan 22 05:53:11 ai-nvidia-app01 ollama[4135940]: time=2026-01-22T05:53:11.175+03:00 level=INFO source=runner.go:1278 msg=load request=""{Operation:fit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Enabled KvSize:8192 KvCacheType: NumThreads:8 GPULayers:37[ID:GPU-f96ee413-bbeb-db0e-f5b5-31cdf6e15749 Layers:37(0..36)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}""
Jan 22 05:53:11 ai-nvidia-app01 ollama[4135940]: time=2026-01-22T05:53:11.191+03:00 level=INFO source=ggml.go:136 msg="""" architecture=qwen3vl file_type=BF16 name=""Qw3Vl Passport"" description="""" num_tensors=399 num_key_values=32
Jan 22 05:53:11 ai-nvidia-app01 ollama[4135940]: load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-alderlake.so
Jan 22 05:53:11 ai-nvidia-app01 ollama[4135940]: ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
Jan 22 05:53:11 ai-nvidia-app01 ollama[4135940]: ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
Jan 22 05:53:11 ai-nvidia-app01 ollama[4135940]: ggml_cuda_init: found 1 CUDA devices:
Jan 22 05:53:11 ai-nvidia-app01 ollama[4135940]:   Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes, ID: GPU-f96ee413-bbeb-db0e-f5b5-31cdf6e15749
Jan 22 05:53:11 ai-nvidia-app01 ollama[4135940]: load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v12/libggml-cuda.so
Jan 22 05:53:11 ai-nvidia-app01 ollama[4135940]: time=2026-01-22T05:53:11.258+03:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,520,600,610,700,750,800,860,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
Jan 22 05:53:11 ai-nvidia-app01 ollama[4135940]: time=2026-01-22T05:53:11.459+03:00 level=INFO source=server.go:3634 msg=""http: panic serving 127.0.0.1:38682: runtime error: invalid memory address or nil pointer dereference\ngoroutine 47 [running]:\nnet/http.(*conn).serve.func1()\n\tnet/http/server.go:1947 +0xbe\npanic({0x56032e866be0?, 0x56032f2626f0?})\n\truntime/panic.go:792 +0x132\ngithub.com/ollama/ollama/runner/ollamarunner.(*Server).allocModel.func1()\n\tgithub.com/ollama/ollama/runner/ollamarunner/runner.go:1187 +0x11a\npanic({0x56032e866be0?, 0x56032f2626f0?})\n\truntime/panic.go:792 +0x132\ngithub.com/ollama/ollama/ml/nn.(*Conv3D).Forward(0x0, {0x56032e9f8cd0, 0xc000e68500}, {0x56032ea04920?, 0xc0000107b0?}, 0x10?, 0xc000100808?, 0xc00097f800?, 0xc000047190?, 0x0, ...)\n\tgithub.com/ollama/ollama/ml/nn/convolution.go:25 +0x3a\ngithub.com/ollama/ollama/model/models/qwen3vl.(*VisionModel).Forward(0xc0000fe240, {0x56032e9f8cd0, 0xc000e68500}, {0x56032ea04920, 0xc000010198}, 0xc000e1c000)\n\tgithub.com/ollama/ollama/model/models/qwen3vl/model_vision.go:224 +0x118\ngithub.com/ollama/ollama/model/models/qwen3vl.(*Model).EncodeMultimodal(0xc0001691e0, {0x56032e9f8cd0, 0xc000e68500}, {0xc001a28000, 0x400436, 0x700000})\n\tgithub.com/ollama/ollama/model/models/qwen3vl/model.go:43 +0x14e\ngithub.com/ollama/ollama/runner/ollamarunner.(*Server).reserveWorstCaseGraph(0xc00024f0e0, 0x1)\n\tgithub.com/ollama/ollama/runner/ollamarunner/runner.go:1098 +0x34e\ngithub.com/ollama/ollama/runner/ollamarunner.(*Server).allocModel(0xc00024f0e0, {0x7ffde5ec2d24?, 0x56032d63b3fa?}, {0x0, 0x8, {0xc0000b9580, 0x1, 0x1}, 0x1}, {0x0, ...}, ...)\n\tgithub.com/ollama/ollama/runner/ollamarunner/runner.go:1226 +0x391\ngithub.com/ollama/ollama/runner/ollamarunner.(*Server).load(0xc00024f0e0, {0x56032e9eafc0, 0xc0004b8000}, 0xc0004b2000)\n\tgithub.com/ollama/ollama/runner/ollamarunner/runner.go:1305 +0x54b\nnet/http.HandlerFunc.ServeHTTP(0xc0000ffec0?, {0x56032e9eafc0?, 0xc0004b8000?}, 0xc000259b60?)\n\tnet/http/server.go:2294 +0x29\nnet/http.(*ServeMux).ServeHTTP(0x56032d2eaa85?, {0x56032e9eafc0, 0xc0004b8000}, 0xc0004b2000)\n\tnet/http/server.go:2822 +0x1c4\nnet/http.serverHandler.ServeHTTP({0x56032e9e74b0?}, {0x56032e9eafc0?, 0xc0004b8000?}, 0x1?)\n\tnet/http/server.go:3301 +0x8e\nnet/http.(*conn).serve(0xc00016c480, {0x56032e9ed438, 0xc00016aba0})\n\tnet/http/server.go:2102 +0x625\ncreated by net/http.(*Server).Serve in goroutine 1\n\tnet/http/server.go:3454 +0x485""
Jan 22 05:53:11 ai-nvidia-app01 ollama[4135940]: time=2026-01-22T05:53:11.460+03:00 level=INFO source=runner.go:1278 msg=load request=""{Operation:close LoraPath:[] Parallel:0 BatchSize:0 FlashAttention:Disabled KvSize:0 KvCacheType: NumThreads:0 GPULayers:[] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}""
Jan 22 05:53:11 ai-nvidia-app01 ollama[4135940]: time=2026-01-22T05:53:11.460+03:00 level=INFO source=sched.go:479 msg=""Load failed"" model=/usr/share/ollama/.ollama/models/blobs/sha256-710dd9ac870452a8e599b4ed7672db3772d0a685541ff4e6b33d9ae9d2bfa8cb error=""do load request: Post \""http://127.0.0.1:44913/load\"": EOF""
Jan 22 05:53:11 ai-nvidia-app01 ollama[4135940]: time=2026-01-22T05:53:11.470+03:00 level=ERROR source=server.go:302 msg=""llama runner terminated"" error=""signal: killed""
```

### OS

Linux

### GPU

Nvidia

### CPU

Intel

### Ollama version

latest",['bug'],0
ollama/ollama,Multiline option issue in non-experimental terminal,"### What is the issue?

Since commit #13694, it has been behaving strangely.

When I type `Enter` key in a new line after one starts with `""""""`, the terminal shows `>>>` but still in multi-line mode, until I type """""" again.

```bash
>>> """"""a
... 
>>> b
>>> c
>>> """"""
Thinking...
blah blah
```

### Relevant log output

```shell

```

### OS

macOS

### GPU

Apple

### CPU

Apple

### Ollama version

0.14.2",['bug'],0
ollama/ollama,[Feature Request] Add OpenCL and OpenBLAS support for broader hardware compatibility,"Description:
I am a user with older/limited hardware ([GT 710M 1GB, Intel HD 4000]) and would like to run local LLMs efficiently. Currently, Ollama/GPT4All primarily supports acceleration via CUDA (NVIDIA), ROCm (AMD), and Metal (Apple), which excludes many integrated and older discrete GPUs that only have OpenCL drivers.

Proposed Solution:
Integrate the OpenCL and OpenBLAS backends from the upstream llama.cpp project. The OpenCL backend in llama.cpp (-DGGML_OPENCL=ON) already provides good cross-platform support for Adreno, older Intel HD Graphics, and many other GPUs. OpenBLAS (-DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=OpenBLAS) can significantly accelerate CPU inference, which is crucial when GPU memory is insufficient.

This would make the project accessible to a much wider range of users with non-mainstream or legacy hardware.

My Use Case:
- Hardware: Hybrid graphics, NVIDIA GT 710M 1GB + Intel HD 4000).
- Driver/OS: NVIDIA driver 388.xx on Windows 10 and 390xx Linux.
- Current Limitation: Vulkan support is unstable, CUDA is not optimal for this old GPU. OpenCL is the only viable cross-platform compute API that works.
- Trying to use Functiongemma 270m

Additional context:
- The llama.cpp project, which underpins these tools, already has stable OpenCL and BLAS backends.
- An issue requesting OpenCL support for Ollama already exists (https://github.com/ollama/ollama/issues/7536), indicating community interest.
- For users with weak or no dedicated GPU, CPU acceleration via BLAS libraries is often the only performant option.",['feature request'],0
ollama/ollama,Error using tools with glm-4.7-flash,"### What is the issue?

when using `glm-4.7-flash:latest` with claude code, it frequently stops and loses all context - looking at the logs I get the attached error whenever it happens.

### Relevant log output

```shell
Jan 20 13:27:49 tartarus-lx ollama[4078349]: time=2026-01-20T13:27:49.426Z level=WARN source=glm46.go:89 msg=""glm-4.6 tool call parsing failed"" error=""failed to parse XML: XML syntax error on line 1: element <tool_call> closed by </arg_key>""
```

### OS

Linux

### GPU

AMD

### CPU

AMD

### Ollama version

0.14.3",['bug'],0
ollama/ollama,Really bad performances on Raspberry Pi 5 for 0.14.3-RC3,"### What is the issue?

Hello,

I tested simple models on Raspberry Pi 5 with 0.14.3-RC3 and I'm getting around 2.6 tokens/s instead 26 tokens/s.

Models tested : 
lfm2.5-thinking:1.2b
gemma3:270M 
qwen3:1.7b  
qwen3:0.6b

Everything's good when rolling back to 0.14.2

Best regards

### Relevant log output

```shell

```

### OS

_No response_

### GPU

_No response_

### CPU

_No response_

### Ollama version

_No response_",['bug'],0
ollama/ollama,Ollama stopped using the GPU,"### What is the issue?

I have an Ollama Docker instance running in a MiG deployment of a RTX PRO 6000 Workstation, with 48 GB VRAM available, providing inference for a Mistral Small 3.1 model in a Q8 quant with only 32k context. Until mid-December, it ran very quickly, but suddenly its performance became very slow, even in short prompts as ""hello"". I am not aware of any changes in the workstation so I assume the bug comes from the last Ollama updates, as the container repulls the lastest image on each restart.
I see no GPU usage by any process when doing `nvidia-smi` during inference, it seems Ollama recognizes the GPU but silently uses CPU instead, still `ollama ps` claims that the model is fully in the GPU. 
I see the CPU backend getting loaded before the CUDA devices are detected, but I do not know if that tells anything. Still, the logs say the model loads in GPU in subsequent messages...

Lurking for solutions, I have already tried turning off flash attention, setting `GGML_CUDA_NO_VMM=1`, `OLLAMA_MAX_LOADED_MODELS=1`, `OLLAMA_NUM_PARALLEL=1`, and `ROCR_VISIBLE_DEVICES=0`. None of these worked. Does anybody have any idea of what may be happening? Thank you!

**EDIT:** I downgraded Ollama to version 0.13.1 and got its normal speeds again, although `nvidia-smi dmon -s u` shows ""-"" in all values, and `nvidia-smi` this:

```
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA RTX PRO 6000 Blac...    Off |   00000000:01:00.0 Off |                   On |
| 30%   36C    P8             27W /  300W |                  N/A   |     N/A      Default |
|                                         |                        |              Enabled |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| MIG devices:                                                                            |
+------------------+----------------------------------+-----------+-----------------------+
| GPU  GI  CI  MIG |              Shared Memory-Usage |        Vol|        Shared         |
|      ID  ID  Dev |                Shared BAR1-Usage | SM     Unc| CE ENC  DEC  OFA  JPG |
|                  |                                  |        ECC|                       |
|==================+==================================+===========+=======================|
|  0    1   0   0  |           45859MiB / 48512MiB    | 94      0 |  2   2    2    0    2 |
|                  |               0MiB / 16654MiB    |           |                       |
+------------------+----------------------------------+-----------+-----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0    1    0               88      C   /usr/bin/ollama                         400MiB |
+-----------------------------------------------------------------------------------------+
```


### Relevant log output

```shell
time=2026-01-21T07:58:24.393Z level=INFO source=routes.go:1614 msg=""server config"" env=""map[CUDA_VISIBLE_DEVICES: GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:128000 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:true OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/models_map OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com]

OLLAMA_SCHED_SPREAD:false OLLAMA_VULKAN:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]""

time=2026-01-21T07:58:24.396Z level=INFO source=images.go:499 msg=""total blobs: 33""

time=2026-01-21T07:58:24.396Z level=INFO source=images.go:506 msg=""total unused blobs removed: 0""

time=2026-01-21T07:58:24.397Z level=INFO source=routes.go:1667 msg=""Listening on [::]:11434 (version 0.14.2)"" time=2026-01-21T07:58:24.397Z level=INFO source=runner.go:67 msg=""discovering available GPUs...""

time=2026-01-21T07:58:24.397Z level=INFO source=server.go:429 msg=""starting runner"" cmd=""/usr/bin/ollama runner --ollama-engine --port 45045""

time=2026-01-21T07:58:24.645Z level=INFO source=server.go:429 msg=""starting runner"" cmd=""/usr/bin/ollama runner --ollama-engine --port 36445""

time=2026-01-21T07:58:24.863Z level=INFO source=runner.go:106 msg=""experimental Vulkan support disabled. To enable, set OLLAMA_VULKAN=1""

time=2026-01-21T07:58:24.863Z level=INFO source=server.go:429 msg=""starting runner"" cmd=""/usr/bin/ollama runner --ollama-engine --port 35809""

time=2026-01-21T07:58:24.864Z level=INFO source=server.go:429 msg=""starting runner"" cmd=""/usr/bin/ollama runner --ollama-engine --port 34865""

time=2026-01-21T07:58:25.185Z level=INFO source=types.go:42 msg=""inference compute"" id=GPU-6d928b77-2cb9-381e-b44b-27d95a658611 filter_id="""" library=CUDA compute=12.0 name=CUDA0 description=""NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition MIG 2g.48gb"" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:01:00.0 type=discrete total=""47.4 GiB"" available=""446.3 GiB""

[GIN] 2026/01/21 - 08:00:24 | 200 | 41.714Âµs | 127.0.0.1 | HEAD ""/""

[GIN] 2026/01/21 - 08:00:24 | 200 | 112.928095ms | 127.0.0.1 | POST ""/api/show""

time=2026-01-21T08:00:24.816Z level=INFO source=server.go:429 msg=""starting runner"" cmd=""/usr/bin/ollama runner --ollama-engine --port 42421""

time=2026-01-21T08:00:25.072Z level=WARN source=cpu_linux.go:130 msg=""failed to parse CPU allowed micro secs"" error=""strconv.ParseInt: parsing \""max\"": invalid syntax"" 
llama_model_loader: loaded meta data with 41 key-value pairs and 363 tensors from /models_map/blobs/sha256-b2a40e5ef4eab8837d0462c303e8147ec754e2963e41916b551107d2b0ca6527 (version GGUF V3 (latest))

[...]

time=2026-01-21T08:00:25.329Z level=INFO source=server.go:429 msg=""starting runner"" cmd=""/usr/bin/ollama runner --model /models_map/blobs/sha256-b2a40e5ef4eab8837d0462c303e8147ec754e2963e41916b551107d2b0ca6527 --port 36707""

time=2026-01-21T08:00:25.329Z level=INFO source=sched.go:452 msg=""system memory"" total=""502.8 GiB"" free=""502.6 GiB"" free_swap=""8.0 GiB""

time=2026-01-21T08:00:25.329Z level=INFO source=sched.go:459 msg=""gpu memory"" id=GPU-6d928b77-2cb9-381e-b44b-27d95a658611 library=CUDA available=""446.4 GiB"" free=""446.9 GiB"" minimum=""457.0 MiB"" overhead=""0 B""

time=2026-01-21T08:00:25.329Z level=INFO source=server.go:496 msg=""loading model"" ""model layers""=41 requested=-1

time=2026-01-21T08:00:25.329Z level=INFO source=device.go:240 msg=""model weights"" device=CUDA0 size=""22.7 GiB""

time=2026-01-21T08:00:25.329Z level=INFO source=device.go:251 msg=""kv cache"" device=CUDA0 size=""4.9 GiB""

time=2026-01-21T08:00:25.329Z level=INFO source=device.go:262 msg=""compute graph"" device=CUDA0 size=""2.1 GiB""

time=2026-01-21T08:00:25.330Z level=INFO source=device.go:272 msg=""total memory"" size=""29.6 GiB""

time=2026-01-21T08:00:25.342Z level=INFO source=runner.go:965 msg=""starting go runner""

load_backend: loaded CPU backend from /usr/lib/ollama/libggml-cpu-icelake.so

ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no

ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no

ggml_cuda_init: found 1 CUDA devices: Device 0: NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition MIG 2g.48gb, compute capability 12.0, VMM: yes, ID: GPU-6d928b77-2cb9-381e-b44b-27d95a658611

load_backend: loaded CUDA backend from /usr/lib/ollama/cuda_v13/libggml-cuda.so
time=2026-01-21T08:00:25.400Z level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=750,800,860,870,890,900,1000,1030,1100,1200,1210 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)

time=2026-01-21T08:00:25.401Z level=INFO source=runner.go:1001 msg=""Server listening on 127.0.0.1:36707""

time=2026-01-21T08:00:25.403Z level=INFO source=runner.go:895 msg=load request=""{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Enabled KvSize:32000 KvCacheType: NumThreads:64 GPULayers:41[ID:GPU-6d928b77-2cb9-381e-b44b-27d95a658611 Layers:41(0..40)] MultiUserCache:false ProjectorPath:/models_map/blobs/sha256-d6af684ae9136398eaa0b59ea9e0b0b850bb6ac5084f1e8c5cb8f85251825eaf MainGPU:0 UseMmap:true}""

time=2026-01-21T08:00:25.403Z level=INFO source=server.go:1347 msg=""waiting for llama runner to start responding""

time=2026-01-21T08:00:25.403Z level=INFO source=server.go:1381 msg=""waiting for server to become available"" status=""llm server loading model""

ggml_backend_cuda_get_available_uma_memory: final available_memory_kb: 468464120

llama_model_load_from_file_impl: using device CUDA0 (NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition MIG 2g.48gb) (0000:01:00.0) - 457484 MiB free

llama_model_loader: loaded meta data with 41 key-value pairs and 363 tensors from /models_map/blobs/sha256-b2a40e5ef4eab8837d0462c303e8147ec754e2963e41916b551107d2b0ca6527 (version GGUF V3 (latest))

llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.

[...]

load_tensors: loading model tensors, this can take a while... (mmap = true)

time=2026-01-21T08:00:26.106Z level=INFO source=server.go:1381 msg=""waiting for server to become available"" status=""llm server not responding""

load_tensors: offloading 40 repeating layers to GPU

load_tensors: offloading output layer to GPU load_tensors: offloaded 41/41 layers to GPU

load_tensors: CPU_Mapped model buffer size = 680.00 MiB

load_tensors: CUDA0 model buffer size = 23206.58 MiB

time=2026-01-21T08:00:27.259Z level=INFO source=server.go:1381 msg=""waiting for server to become available"" status=""llm server loading model""
[...]
time=2026-01-21T08:00:33.534Z level=INFO source=server.go:1385 msg=""llama runner started in 8.20 seconds""

time=2026-01-21T08:00:33.534Z level=INFO source=sched.go:526 msg=""loaded runners"" count=1

time=2026-01-21T08:00:33.534Z level=INFO source=server.go:1347 msg=""waiting for llama runner to start responding""

time=2026-01-21T08:00:33.534Z level=INFO source=server.go:1385 msg=""llama runner started in 8.20 seconds""

[GIN] 2026/01/21 - 08:01:02 | 200 | 38.145927744s | 127.0.0.1 | POST ""/api/generate""
```

### OS

Docker

### GPU

Nvidia

### CPU

AMD

### Ollama version

0.14.2",['bug'],0
ollama/ollama,README.md issue,"### What is the issue?

Check the main README.md and correct the flow properly

### Relevant log output

```shell

```

### OS

_No response_

### GPU

_No response_

### CPU

_No response_

### Ollama version

_No response_",['bug'],0
ollama/ollama,More search filters for model search,"Currently, the model search page allows filtering by capability (e.g., ""tools"", ""vision"") and sorting by ""popular"" or ""newest."" However, there is no way to filter models based on their size/parameter count. Users with specific hardware constraints (e.g., 8GB or 16GB VRAM) have to scroll through models to find ones that fit their system.

I would like to see a parameter filter added to the search. Ideally, this would include a range slider that gives the ability to specify a range, such as ""from 0 to 14B.""",['feature request'],0
ollama/ollama,Ollama can not signin......,"<img width=""1630"" height=""588"" alt=""Image"" src=""https://github.com/user-attachments/assets/8a5f3d77-0033-4a8c-9058-3bf742c6a947"" />

The login in the screenshot always looks like this.

When I waited for a few minutes, it seems that it can be used, but when I chat with the cloud model, I found that the UI looks like the screenshot above again.

i tried `ollama signin`,`ollama signout` in the terminal,but have no use.

How to solve this?",['cloud'],0
ollama/ollama,AMD GPU 500 EOF Error,"### What is the issue?

I have started seeing this error after latest ubuntu updates. Tried uninstalling and reinstalling of amdgpu as well as ollama, still no help. Tried with rocm 6.4 as well as latest 7.1.x


```
ollama run gpt-oss
Error: 500 Internal Server Error: do load request: Post ""http://127.0.0.1:40065/load"": EOF
```

### Relevant log output

```shell
Error: 500 Internal Server Error: do load request: Post ""http://127.0.0.1:40065/load"": EOF
```

### OS

Ubuntu 24.04
Kernal : 6.14.0-1019-oem

### GPU

AMD Strix Halo AI 395+
Rocm : 7.1.x

### CPU

AMD Strix Halo AI 395+ 128GB (Framework)

### Ollama version

 0.14.x",['bug'],0
ollama/ollama,api/generate: running `-cloud` models through the local proxy fails,"### What is the issue?

Originally reported in #12370, h/t to @rick-github for the following repro:

```
$ prompt='<|start|>system<|message|>Talk like a pirate\n# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>user<|message|>hello<|end|><|start|>assistant'
$ curl -s https://ollama.com/api/generate -H ""Authorization: $OLLAMA_API_KEY"" -d '{""model"":""gpt-oss:20b"",""prompt"":""'""$prompt""'"",""raw"":true,""stream"":false}' | jq 
{
  ""model"": ""gpt-oss:20b"",
  ""created_at"": ""2025-09-22T18:49:33.835263946Z"",
  ""response"": ""<|channel|>analysis<|message|>User says \""hello\"". They want to talk like a pirate."",
  ""done"": true,
  ""total_duration"": 224292188,
  ""prompt_eval_count"": 34,
  ""eval_count"": 16
}
$ curl -s localhost:11434/api/generate -H ""Authorization: $OLLAMA_API_KEY"" -d '{""model"":""gpt-oss:20b-cloud"",""prompt"":""'""$prompt""'"",""raw"":true,""stream"":false}' | jq
{
  ""error"": ""400 Bad Request: raw mode does not support template, system, or context""
}
```

### Relevant log output

```shell

```

### OS

_No response_

### GPU

_No response_

### CPU

_No response_

### Ollama version

0.14.2",['bug'],0
ollama/ollama,Failing to initialize A100 MIG in >=0.13.2,"### What is the issue?

Running in k8s with Ollama's default image as a non-root user, Ollama will work with older images such as 0.12.11 but for whatever reason the GPU configuration doesn't seem to make it past the new filtering process and uses the CPU.  I have tried with and without overriding the OLLAMA_LLM_LIBRARY to either cuda_v12 or cuda_v13.  I have set OLLAMA_DEBUG=2 in the logs.

<img width=""644"" height=""420"" alt=""Image"" src=""https://github.com/user-attachments/assets/d851ab34-392f-4d7b-9edb-efd5aeebdfb4"" />


### Relevant log output

```shell
[ollama-non-root-v14-cb97b7f76-48d5d_ollama.log](https://github.com/user-attachments/files/24749022/ollama-non-root-v14-cb97b7f76-48d5d_ollama.log)

time=2026-01-20T19:32:01.706Z level=INFO source=routes.go:1626 msg=""server config"" env=""map[CUDA_VISIBLE_DEVICES: GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY:http://proxy.company.com:8080 HTTP_PROXY:http://proxy.company.com:8080 NO_PROXY:localhost, 127.0.0.1, 0.0.0.0, svc,local,10.42.0.0/16, openwebui-service, openwebui-service.llm.svc.cluster.local OLLAMA_CONTEXT_LENGTH:8192 OLLAMA_DEBUG:DEBUG-4 OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_KEEP_ALIVE:2562047h47m16.854775807s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY:cuda_v13 OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/ubuntu/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[* http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false OLLAMA_VULKAN:false ROCR_VISIBLE_DEVICES: http_proxy:http://proxy.company.com:8080 https_proxy:http://proxy.company.com:8080 no_proxy:]""
time=2026-01-20T19:32:01.721Z level=INFO source=images.go:501 msg=""total blobs: 6""
time=2026-01-20T19:32:01.722Z level=INFO source=images.go:508 msg=""total unused blobs removed: 0""
time=2026-01-20T19:32:01.724Z level=INFO source=routes.go:1679 msg=""Listening on [::]:11434 (version 0.14.3-rc2)""
time=2026-01-20T19:32:01.724Z level=DEBUG source=sched.go:121 msg=""starting llm scheduler""
time=2026-01-20T19:32:01.724Z level=INFO source=runner.go:67 msg=""discovering available GPUs...""
time=2026-01-20T19:32:01.724Z level=DEBUG source=runner.go:98 msg=""skipping available library at user's request"" requested=cuda_v13 libDir=/usr/lib/ollama/cuda_v12
time=2026-01-20T19:32:01.724Z level=TRACE source=runner.go:440 msg=""starting runner for device discovery"" libDirs=""[/usr/lib/ollama /usr/lib/ollama/cuda_v13]"" extraEnvs=map[]
time=2026-01-20T19:32:01.725Z level=INFO source=server.go:429 msg=""starting runner"" cmd=""/usr/bin/ollama runner --ollama-engine --port 33281""
time=2026-01-20T19:32:01.725Z level=DEBUG source=server.go:430 msg=subprocess PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin LD_LIBRARY_PATH=/usr/lib/ollama:/usr/lib/ollama/cuda_v13:/usr/local/nvidia/lib:/usr/local/nvidia/lib64 OLLAMA_HOST=0.0.0.0:11434 OLLAMA_LOG_LEVEL=TRACE OLLAMA_ORIGINS=* OLLAMA_DEBUG=2 OLLAMA_KEEP_ALIVE=-1 OLLAMA_LLM_LIBRARY=cuda_v13 OLLAMA_MODELS=/home/ubuntu/.ollama/models OLLAMA_CONTEXT_LENGTH=8192 OLLAMA_SERVICE_HOST=10.43.70.198 OLLAMA_SERVICE_PORT_8080_TCP_ADDR=10.43.19.255 OLLAMA_PORT_11434_TCP=tcp://10.43.70.198:11434 OLLAMA_SERVICE_PORT_8080_TCP_PORT=8080 OLLAMA_SERVICE_PORT_11434_TCP_PROTO=tcp OLLAMA_SERVICE_PORT_11434_TCP_ADDR=10.43.19.255 OLLAMA_PORT_11434_TCP_ADDR=10.43.70.198 OLLAMA_SERVICE_PORT_HTTP=11434 OLLAMA_SERVICE_SERVICE_HOST=10.43.19.255 OLLAMA_SERVICE_PORT=tcp://10.43.19.255:8080 OLLAMA_PORT_11434_TCP_PROTO=tcp OLLAMA_PORT=tcp://10.43.70.198:11434 OLLAMA_SERVICE_SERVICE_PORT_API=8080 OLLAMA_SERVICE_SERVICE_PORT_API2=11434 OLLAMA_SERVICE_SERVICE_PORT=8080 OLLAMA_SERVICE_PORT_11434_TCP_PORT=11434 OLLAMA_PORT_11434_TCP_PORT=11434 OLLAMA_SERVICE_PORT_11434_TCP=tcp://10.43.19.255:11434 OLLAMA_SERVICE_PORT_8080_TCP_PROTO=tcp OLLAMA_SERVICE_PORT_8080_TCP=tcp://10.43.19.255:8080 OLLAMA_LIBRARY_PATH=/usr/lib/ollama:/usr/lib/ollama/cuda_v13
time=2026-01-20T19:32:01.737Z level=INFO source=runner.go:1405 msg=""starting ollama engine""
time=2026-01-20T19:32:01.738Z level=INFO source=runner.go:1440 msg=""Server listening on 127.0.0.1:33281""
time=2026-01-20T19:32:01.746Z level=DEBUG source=gguf.go:589 msg=general.architecture type=string
time=2026-01-20T19:32:01.746Z level=DEBUG source=gguf.go:589 msg=tokenizer.ggml.model type=string
time=2026-01-20T19:32:01.746Z level=DEBUG source=ggml.go:297 msg=""key with type not found"" key=general.alignment default=32
time=2026-01-20T19:32:01.746Z level=DEBUG source=ggml.go:297 msg=""key with type not found"" key=general.alignment default=32
time=2026-01-20T19:32:01.747Z level=DEBUG source=ggml.go:297 msg=""key with type not found"" key=general.file_type default=0
time=2026-01-20T19:32:01.747Z level=DEBUG source=ggml.go:297 msg=""key with type not found"" key=general.name default=""""
time=2026-01-20T19:32:01.747Z level=DEBUG source=ggml.go:297 msg=""key with type not found"" key=general.description default=""""
time=2026-01-20T19:32:01.747Z level=INFO source=ggml.go:136 msg="""" architecture=llama file_type=unknown name="""" description="""" num_tensors=0 num_key_values=3
time=2026-01-20T19:32:01.747Z level=DEBUG source=ggml.go:94 msg=""ggml backend load all from path"" path=/usr/lib/ollama
load_backend: loaded CPU backend from /usr/lib/ollama/libggml-cpu-skylakex.so
time=2026-01-20T19:32:01.753Z level=DEBUG source=ggml.go:94 msg=""ggml backend load all from path"" path=/usr/lib/ollama/cuda_v13
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe MIG 7g.80gb, compute capability 8.0, VMM: yes, ID: GPU-7327fca2-18fa-b021-ba3d-8499a04c9c81
load_backend: loaded CUDA backend from /usr/lib/ollama/cuda_v13/libggml-cuda.so
time=2026-01-20T19:32:01.860Z level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=750,800,860,870,890,900,1000,1030,1100,1200,1210 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2026-01-20T19:32:01.860Z level=DEBUG source=ggml.go:297 msg=""key with type not found"" key=llama.block_count default=0
time=2026-01-20T19:32:01.860Z level=DEBUG source=ggml.go:297 msg=""key with type not found"" key=llama.pooling_type default=0
time=2026-01-20T19:32:01.860Z level=DEBUG source=ggml.go:297 msg=""key with type not found"" key=llama.expert_count default=0
time=2026-01-20T19:32:01.860Z level=DEBUG source=ggml.go:297 msg=""key with type not found"" key=tokenizer.ggml.tokens default=""&{size:0 values:[]}""
time=2026-01-20T19:32:01.860Z level=DEBUG source=ggml.go:297 msg=""key with type not found"" key=tokenizer.ggml.scores default=""&{size:0 values:[]}""
time=2026-01-20T19:32:01.860Z level=DEBUG source=ggml.go:297 msg=""key with type not found"" key=tokenizer.ggml.token_type default=""&{size:0 values:[]}""
time=2026-01-20T19:32:01.860Z level=DEBUG source=ggml.go:297 msg=""key with type not found"" key=tokenizer.ggml.merges default=""&{size:0 values:[]}""
time=2026-01-20T19:32:01.860Z level=DEBUG source=ggml.go:297 msg=""key with type not found"" key=tokenizer.ggml.add_bos_token default=true
time=2026-01-20T19:32:01.860Z level=DEBUG source=ggml.go:297 msg=""key with type not found"" key=tokenizer.ggml.bos_token_id default=0
time=2026-01-20T19:32:01.860Z level=DEBUG source=ggml.go:297 msg=""key with type not found"" key=tokenizer.ggml.add_eos_token default=false
time=2026-01-20T19:32:01.860Z level=DEBUG source=ggml.go:297 msg=""key with type not found"" key=tokenizer.ggml.eos_token_id default=0
time=2026-01-20T19:32:01.860Z level=DEBUG source=ggml.go:297 msg=""key with type not found"" key=tokenizer.ggml.eos_token_ids default=""&{size:0 values:[]}""
time=2026-01-20T19:32:01.860Z level=DEBUG source=ggml.go:297 msg=""key with type not found"" key=tokenizer.ggml.pre default=""""
time=2026-01-20T19:32:01.860Z level=DEBUG source=ggml.go:297 msg=""key with type not found"" key=llama.block_count default=0
time=2026-01-20T19:32:01.860Z level=DEBUG source=ggml.go:297 msg=""key with type not found"" key=llama.embedding_length default=0
time=2026-01-20T19:32:01.860Z level=DEBUG source=ggml.go:297 msg=""key with type not found"" key=llama.attention.head_count default=0
time=2026-01-20T19:32:01.860Z level=DEBUG source=ggml.go:297 msg=""key with type not found"" key=llama.attention.head_count_kv default=0
time=2026-01-20T19:32:01.860Z level=DEBUG source=ggml.go:297 msg=""key with type not found"" key=llama.attention.key_length default=0
time=2026-01-20T19:32:01.860Z level=DEBUG source=ggml.go:297 msg=""key with type not found"" key=llama.rope.dimension_count default=0
time=2026-01-20T19:32:01.860Z level=DEBUG source=ggml.go:297 msg=""key with type not found"" key=llama.attention.layer_norm_rms_epsilon default=0
time=2026-01-20T19:32:01.860Z level=DEBUG source=ggml.go:297 msg=""key with type not found"" key=llama.rope.freq_base default=100000
time=2026-01-20T19:32:01.860Z level=DEBUG source=ggml.go:297 msg=""key with type not found"" key=llama.rope.scaling.factor default=1
time=2026-01-20T19:32:01.860Z level=DEBUG source=runner.go:1380 msg=""dummy model load took"" duration=113.825277ms
time=2026-01-20T19:32:02.052Z level=DEBUG source=runner.go:1385 msg=""gathering device infos took"" duration=191.5977ms
time=2026-01-20T19:32:02.052Z level=TRACE source=runner.go:467 msg=""runner enumerated devices"" OLLAMA_LIBRARY_PATH=""[/usr/lib/ollama /usr/lib/ollama/cuda_v13]"" devices=""[{DeviceID:{ID:GPU-7327fca2-18fa-b021-ba3d-8499a04c9c81 Library:CUDA} Name:CUDA0 Description:NVIDIA A100 80GB PCIe MIG 7g.80gb FilterID: Integrated:false PCIID:0000:af:00.0 TotalMemory:85094825984 FreeMemory:84690075648 ComputeMajor:8 ComputeMinor:0 DriverMajor:13 DriverMinor:1 LibraryPath:[/usr/lib/ollama /usr/lib/ollama/cuda_v13]}]""
time=2026-01-20T19:32:02.052Z level=DEBUG source=runner.go:437 msg=""bootstrap discovery took"" duration=328.006584ms OLLAMA_LIBRARY_PATH=""[/usr/lib/ollama /usr/lib/ollama/cuda_v13]"" extra_envs=map[]
time=2026-01-20T19:32:02.052Z level=DEBUG source=runner.go:98 msg=""skipping available library at user's request"" requested=cuda_v13 libDir=/usr/lib/ollama/vulkan
time=2026-01-20T19:32:02.052Z level=DEBUG source=runner.go:124 msg=""evaluating which, if any, devices to filter out"" initial_count=1
time=2026-01-20T19:32:02.052Z level=DEBUG source=runner.go:146 msg=""verifying if device is supported"" library=/usr/lib/ollama/cuda_v13 description=""NVIDIA A100 80GB PCIe MIG 7g.80gb"" compute=8.0 id=GPU-7327fca2-18fa-b021-ba3d-8499a04c9c81 pci_id=0000:af:00.0
time=2026-01-20T19:32:02.052Z level=TRACE source=runner.go:440 msg=""starting runner for device discovery"" libDirs=""[/usr/lib/ollama /usr/lib/ollama/cuda_v13]"" extraEnvs=""map[CUDA_VISIBLE_DEVICES:GPU-7327fca2-18fa-b021-ba3d-8499a04c9c81 GGML_CUDA_INIT:1]""
time=2026-01-20T19:32:02.053Z level=INFO source=server.go:429 msg=""starting runner"" cmd=""/usr/bin/ollama runner --ollama-engine --port 37869""
time=2026-01-20T19:32:02.053Z level=DEBUG source=server.go:430 msg=subprocess PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin LD_LIBRARY_PATH=/usr/lib/ollama:/usr/lib/ollama/cuda_v13:/usr/local/nvidia/lib:/usr/local/nvidia/lib64 OLLAMA_HOST=0.0.0.0:11434 OLLAMA_LOG_LEVEL=TRACE OLLAMA_ORIGINS=* OLLAMA_DEBUG=2 OLLAMA_KEEP_ALIVE=-1 OLLAMA_LLM_LIBRARY=cuda_v13 OLLAMA_MODELS=/home/ubuntu/.ollama/models OLLAMA_CONTEXT_LENGTH=8192 OLLAMA_SERVICE_HOST=10.43.70.198 OLLAMA_SERVICE_PORT_8080_TCP_ADDR=10.43.19.255 OLLAMA_PORT_11434_TCP=tcp://10.43.70.198:11434 OLLAMA_SERVICE_PORT_8080_TCP_PORT=8080 OLLAMA_SERVICE_PORT_11434_TCP_PROTO=tcp OLLAMA_SERVICE_PORT_11434_TCP_ADDR=10.43.19.255 OLLAMA_PORT_11434_TCP_ADDR=10.43.70.198 OLLAMA_SERVICE_PORT_HTTP=11434 OLLAMA_SERVICE_SERVICE_HOST=10.43.19.255 OLLAMA_SERVICE_PORT=tcp://10.43.19.255:8080 OLLAMA_PORT_11434_TCP_PROTO=tcp OLLAMA_PORT=tcp://10.43.70.198:11434 OLLAMA_SERVICE_SERVICE_PORT_API=8080 OLLAMA_SERVICE_SERVICE_PORT_API2=11434 OLLAMA_SERVICE_SERVICE_PORT=8080 OLLAMA_SERVICE_PORT_11434_TCP_PORT=11434 OLLAMA_PORT_11434_TCP_PORT=11434 OLLAMA_SERVICE_PORT_11434_TCP=tcp://10.43.19.255:11434 OLLAMA_SERVICE_PORT_8080_TCP_PROTO=tcp OLLAMA_SERVICE_PORT_8080_TCP=tcp://10.43.19.255:8080 OLLAMA_LIBRARY_PATH=/usr/lib/ollama:/usr/lib/ollama/cuda_v13 CUDA_VISIBLE_DEVICES=GPU-7327fca2-18fa-b021-ba3d-8499a04c9c81 GGML_CUDA_INIT=1
time=2026-01-20T19:32:02.065Z level=INFO source=runner.go:1405 msg=""starting ollama engine""
time=2026-01-20T19:32:02.066Z level=INFO source=runner.go:1440 msg=""Server listening on 127.0.0.1:37869""
time=2026-01-20T19:32:02.074Z level=DEBUG source=gguf.go:589 msg=general.architecture type=string
time=2026-01-20T19:32:02.074Z level=DEBUG source=gguf.go:589 msg=tokenizer.ggml.model type=string
time=2026-01-20T19:32:02.074Z level=DEBUG source=ggml.go:297 msg=""key with type not found"" key=general.alignment default=32
time=2026-01-20T19:32:02.074Z level=DEBUG source=ggml.go:297 msg=""key with type not found"" key=general.alignment default=32
time=2026-01-20T19:32:02.074Z level=DEBUG source=ggml.go:297 msg=""key with type not found"" key=general.file_type default=0
time=2026-01-20T19:32:02.074Z level=DEBUG source=ggml.go:297 msg=""key with type not found"" key=general.name default=""""
time=2026-01-20T19:32:02.074Z level=DEBUG source=ggml.go:297 msg=""key with type not found"" key=general.description default=""""
time=2026-01-20T19:32:02.074Z level=INFO source=ggml.go:136 msg="""" architecture=llama file_type=unknown name="""" description="""" num_tensors=0 num_key_values=3
time=2026-01-20T19:32:02.074Z level=DEBUG source=ggml.go:94 msg=""ggml backend load all from path"" path=/usr/lib/ollama
load_backend: loaded CPU backend from /usr/lib/ollama/libggml-cpu-skylakex.so
time=2026-01-20T19:32:02.080Z level=DEBUG source=ggml.go:94 msg=""ggml backend load all from path"" path=/usr/lib/ollama/cuda_v13
ggml_cuda_init: failed to initialize CUDA: no CUDA-capable device is detected
load_backend: loaded CUDA backend from /usr/lib/ollama/cuda_v13/libggml-cuda.so
time=2026-01-20T19:32:02.197Z level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)
time=2026-01-20T19:32:02.197Z level=DEBUG source=ggml.go:297 msg=""key with type not found"" key=llama.block_count default=0
time=2026-01-20T19:32:02.198Z level=DEBUG source=ggml.go:297 msg=""key with type not found"" key=llama.pooling_type default=0
time=2026-01-20T19:32:02.198Z level=DEBUG source=ggml.go:297 msg=""key with type not found"" key=llama.expert_count default=0
time=2026-01-20T19:32:02.198Z level=DEBUG source=ggml.go:297 msg=""key with type not found"" key=tokenizer.ggml.tokens default=""&{size:0 values:[]}""
time=2026-01-20T19:32:02.198Z level=DEBUG source=ggml.go:297 msg=""key with type not found"" key=tokenizer.ggml.scores default=""&{size:0 values:[]}""
time=2026-01-20T19:32:02.198Z level=DEBUG source=ggml.go:297 msg=""key with type not found"" key=tokenizer.ggml.token_type default=""&{size:0 values:[]}""
time=2026-01-20T19:32:02.198Z level=DEBUG source=ggml.go:297 msg=""key with type not found"" key=tokenizer.ggml.merges default=""&{size:0 values:[]}""
time=2026-01-20T19:32:02.198Z level=DEBUG source=ggml.go:297 msg=""key with type not found"" key=tokenizer.ggml.add_bos_token default=true
time=2026-01-20T19:32:02.198Z level=DEBUG source=ggml.go:297 msg=""key with type not found"" key=tokenizer.ggml.bos_token_id default=0
time=2026-01-20T19:32:02.198Z level=DEBUG source=ggml.go:297 msg=""key with type not found"" key=tokenizer.ggml.add_eos_token default=false
time=2026-01-20T19:32:02.198Z level=DEBUG source=ggml.go:297 msg=""key with type not found"" key=tokenizer.ggml.eos_token_id default=0
time=2026-01-20T19:32:02.198Z level=DEBUG source=ggml.go:297 msg=""key with type not found"" key=tokenizer.ggml.eos_token_ids default=""&{size:0 values:[]}""
time=2026-01-20T19:32:02.198Z level=DEBUG source=ggml.go:297 msg=""key with type not found"" key=tokenizer.ggml.pre default=""""
time=2026-01-20T19:32:02.198Z level=DEBUG source=ggml.go:297 msg=""key with type not found"" key=llama.block_count default=0
time=2026-01-20T19:32:02.198Z level=DEBUG source=ggml.go:297 msg=""key with type not found"" key=llama.embedding_length default=0
time=2026-01-20T19:32:02.198Z level=DEBUG source=ggml.go:297 msg=""key with type not found"" key=llama.attention.head_count default=0
time=2026-01-20T19:32:02.198Z level=DEBUG source=ggml.go:297 msg=""key with type not found"" key=llama.attention.head_count_kv default=0
time=2026-01-20T19:32:02.198Z level=DEBUG source=ggml.go:297 msg=""key with type not found"" key=llama.attention.key_length default=0
time=2026-01-20T19:32:02.198Z level=DEBUG source=ggml.go:297 msg=""key with type not found"" key=llama.rope.dimension_count default=0
time=2026-01-20T19:32:02.198Z level=DEBUG source=ggml.go:297 msg=""key with type not found"" key=llama.attention.layer_norm_rms_epsilon default=0
time=2026-01-20T19:32:02.198Z level=DEBUG source=ggml.go:297 msg=""key with type not found"" key=llama.rope.freq_base default=100000
time=2026-01-20T19:32:02.198Z level=DEBUG source=ggml.go:297 msg=""key with type not found"" key=llama.rope.scaling.factor default=1
time=2026-01-20T19:32:02.198Z level=DEBUG source=runner.go:1380 msg=""dummy model load took"" duration=123.88735ms
time=2026-01-20T19:32:02.198Z level=DEBUG source=runner.go:1385 msg=""gathering device infos took"" duration=497ns
time=2026-01-20T19:32:02.198Z level=TRACE source=runner.go:467 msg=""runner enumerated devices"" OLLAMA_LIBRARY_PATH=""[/usr/lib/ollama /usr/lib/ollama/cuda_v13]"" devices=[]
time=2026-01-20T19:32:02.198Z level=DEBUG source=runner.go:437 msg=""bootstrap discovery took"" duration=145.955602ms OLLAMA_LIBRARY_PATH=""[/usr/lib/ollama /usr/lib/ollama/cuda_v13]"" extra_envs=""map[CUDA_VISIBLE_DEVICES:GPU-7327fca2-18fa-b021-ba3d-8499a04c9c81 GGML_CUDA_INIT:1]""
time=2026-01-20T19:32:02.198Z level=DEBUG source=runner.go:153 msg=""filtering device which didn't fully initialize"" id=GPU-7327fca2-18fa-b021-ba3d-8499a04c9c81 libdir=/usr/lib/ollama/cuda_v13 pci_id=0000:af:00.0 library=CUDA
time=2026-01-20T19:32:02.198Z level=TRACE source=runner.go:174 msg=""supported GPU library combinations before filtering"" supported=map[]
time=2026-01-20T19:32:02.198Z level=TRACE source=runner.go:183 msg=""removing unsupported or overlapping GPU combination"" libDir=/usr/lib/ollama/cuda_v13 description=""NVIDIA A100 80GB PCIe MIG 7g.80gb"" compute=8.0 pci_id=0000:af:00.0
time=2026-01-20T19:32:02.198Z level=DEBUG source=runner.go:40 msg=""GPU bootstrap discovery took"" duration=474.602195ms
time=2026-01-20T19:32:02.199Z level=INFO source=types.go:60 msg=""inference compute"" id=cpu library=cpu compute="""" name=cpu description=cpu libdirs=ollama driver="""" pci_id="""" type="""" total=""31.2 GiB"" available=""31.1 GiB""
time=2026-01-20T19:32:02.199Z level=INFO source=routes.go:1720 msg=""entering low vram mode"" ""total vram""=""0 B"" threshold=""20.0 GiB""
[GIN] 2026/01/20 - 19:32:14 | 200 |      47.906Âµs |  172.24.192.128 | GET      ""/""
```

### OS

Linux

### GPU

Nvidia

### CPU

Intel

### Ollama version

0.14.3-rc2",['bug'],0
ollama/ollama,Incorrect general.name metadata in GGUF files for llama3.3:70b variants,"### What is the issue?

The GGUF files for the llama3.3:70b model (across multiple quantizations) have an incorrect general.name field in their metadata. The field states llama3.1 instead of the expected llama3.3.

Evidence:
- Log output from loading the model shows the wrong name.
- Inspecting the GGUF files linked from the manifest confirms the mismatch.
- The llama3.3:8b variants correctly show llama3.3 in the same field.
- The license file in the manifest correctly references Llama 3.3, suggesting this is a metadata error in the GGUF packing.

Impact: This may cause confusion in the logs or UIs that rely on the general.name field for identification.

### Relevant log output

```shell
llama_model_loader: loaded meta data with 36 key-value pairs and 724 tensors from /root/.ollama/models/blobs/sha256-4824460d29f2058aaf6e1118a63a7a197a09bed509f0e7d4e2efb1ee273b447d (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.1 70B Instruct 2024 12
llama_model_loader: - kv   3:                            general.version str              = 2024-12
llama_model_loader: - kv   4:                           general.finetune str              = Instruct
llama_model_loader: - kv   5:                           general.basename str              = Llama-3.1
llama_model_loader: - kv   6:                         general.size_label str              = 70B
llama_model_loader: - kv   7:                            general.license str              = llama3.1
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Llama 3.1 70B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Meta Llama
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/meta-llama/Lla...
llama_model_loader: - kv  12:                               general.tags arr[str,5]       = [""facebook"", ""meta"", ""pytorch"", ""llam...
llama_model_loader: - kv  13:                          general.languages arr[str,7]       = [""fr"", ""it"", ""pt"", ""hi"", ""es"", ""th"", ...
llama_model_loader: - kv  14:                          llama.block_count u32              = 80
llama_model_loader: - kv  15:                       llama.context_length u32              = 131072
llama_model_loader: - kv  16:                     llama.embedding_length u32              = 8192
llama_model_loader: - kv  17:                  llama.feed_forward_length u32              = 28672
llama_model_loader: - kv  18:                 llama.attention.head_count u32              = 64
llama_model_loader: - kv  19:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  20:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  21:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  22:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  23:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  24:                          general.file_type u32              = 15
llama_model_loader: - kv  25:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  26:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  27:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  28:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  29:                      tokenizer.ggml.tokens arr[str,128256]  = [""!"", ""\"""", ""#"", ""$"", ""%"", ""&"", ""'"", ...
llama_model_loader: - kv  30:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  31:                      tokenizer.ggml.merges arr[str,280147]  = [""Ä  Ä "", ""Ä  Ä Ä Ä "", ""Ä Ä  Ä Ä "", ""...
llama_model_loader: - kv  32:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  33:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  34:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  35:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  162 tensors
llama_model_loader: - type q4_K:  441 tensors
llama_model_loader: - type q5_K:   40 tensors
llama_model_loader: - type q6_K:   81 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 39.59 GiB (4.82 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128008 ('<|eom_id|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: no_alloc         = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 8192





GGUF:

GGUF^C^@^@^@Ã”^B^@^@^@^@^@^@$^@^@^@^@^@^@^@^T^@^@^@^@^@^@^@general.architecture^H^@^@^@^E^@^@^@^@^@^@^@llama^L^@^@^@^@^@^@^@general.type^H^@^@^@^E^@^@^@^@^@^@^@model^L^@^@^@^@^@^@^@general.name^H^@^@^@^^^@^@^@^@^@^@^@Llama 3.1 70B Instruct 2024 12^O^@^@^@^@^@^@^@general.version^H^@^@^@^G^@^@^@^@^@^@^@2024-12^P^@^@^@^@^@^@^@general.finetune^H^@^@^@^H^@^@^@^@^@^@^@Instruct^P^@^@^@^@^@^@^@general.basename^H^@^@^@    ^@^@^@^@^@^@^@Llama-3.1^R^@^@^@^@^@^@^@general.size_label^H^@^@^@^C^@^@^@^@^@^@^@70B^O^@^@^@^@^@^@^@general.license^H^@^@^@^H^@^@^@^@^@^@^@llama3.1^X^@^@^@^@^@^@^@general.base_model.count^D^@^@^@^A^@^@^@^Y^@^@^@^@^@^@^@general.base_model.0.name^H^@^@^@^M^@^@^@^@^@^@^@Llama 3.1 70B!^@^@^@^@^@^@^@general.base_model.0.organization^H^@^@^@
^@^@^@^@^@^@^@Meta Llama^]^@^@^@^@^@^@^@general.base_model.0.repo_url^H^@^@^@/
```

### OS

Linux

### GPU

Nvidia

### CPU

Other

### Ollama version

0.14.2",['bug'],0
ollama/ollama,"ROCm: Qwen3-VL / Vision models crash with nil Conv3D pointer (RX 6800M, gfx1030)","Ollama consistently crashes with a null pointer dereference panic when attempting to load the Qwen3-VL vision model hf.co/mradermacher/Huihui-Qwen3-VL-30B-A3B-Thinking-abliterated-i1-GGUF:Q5_K_M 

The same issue also occurs with the Mistral Small 2 24b with VL:
hf.co/unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF:Q5_K_M

The panic occurs in convolution.go:25 when calling `(*Conv3D).Forward(0x0, ...)` The 0x0 indicates the Conv3D object is nil/uninitialized. This happens during the vision model's forward pass in qwen3vl/model_vision.go:224, suggesting the Conv3D layer was never properly initialized when the vision model was constructed.

### Possible Fixes
- Initialize all Conv3D layers during model construction, or
- Add nil checks before calling Forward() to provide a more helpful error message

### Software
I have the same problem on a Ubuntu 24.04 machine with Ollama 0.13.5 with rocm7 using HIP
AND a Ubuntu 25.10 machine with Ollama 0.14.2 and rocm5.9 using openCL.
Both setups are running text only models very well.
Environment Variables:
```
HSA_OVERRIDE_GFX_VERSION=10.3.0
OLLAMA_FLASH_ATTENTION=true
OLLAMA_KV_CACHE_TYPE=Q8_0
```

### Hardware
- Asus ROG Strix AMD Advantage G513QY
- CPU: AMD Ryzen 9 5900 HX
- GPU: AMD Radeon RX 6800M
- VRAM: 12 GB
- RAM: 32GB

### Relevant log output

```shell
time=2026-01-20T11:02:42.204Z level=INFO source=server.go:3634 msg=""http: panic serving 127.0.0.1:57842: runtime error: invalid memory address or nil pointer dereference
goroutine 15 [running]:
net/http.(*conn).serve.func1()
	net/http/server.go:1947 +0xbe
panic({0x5cdba9faa120?, 0x5cdbaa96e430?})
	runtime/panic.go:792 +0x132
github.com/ollama/ollama/runner/ollamarunner.(*Server).allocModel.func1()
	github.com/ollama/ollama/runner/ollamarunner/runner.go:1187 +0x11a
panic({0x5cdba9faa120?, 0x5cdbaa96e430?})
	runtime/panic.go:792 +0x132
github.com/ollama/ollama/ml/nn.(*Conv3D).Forward(0x0, {0x5cdbaa131110, 0xc000edab40}, {0x5cdbaa13c140?, 0xc000124030?}, 0x10?, 0xc000600008?, 0xc000d22000?, 0xc000049190?, 0x0, ...)
	github.com/ollama/ollama/ml/nn/convolution.go:25 +0x3a
github.com/ollama/ollama/model/models/qwen3vl.(*VisionModel).Forward(0xc00014c0c0, {0x5cdbaa131110, 0xc000edab40}, {0x5cdbaa13c140, 0xc000124018}, 0xc000ea0000)
	github.com/ollama/ollama/model/models/qwen3vl/model_vision.go:224 +0x118
github.com/ollama/ollama/model/models/qwen3vl.(*Model).EncodeMultimodal(0xc0004b9790, {0x5cdbaa131110, 0xc000edab40}, {0xc001c48000, 0x400436, 0x700000})
	github.com/ollama/ollama/model/models/qwen3vl/model.go:43 +0x14e
github.com/ollama/ollama/runner/ollamarunner.(*Server).reserveWorstCaseGraph(0xc000234d20, 0x1)
	github.com/ollama/ollama/runner/ollamarunner/runner.go:1098 +0x34e
github.com/ollama/ollama/runner/ollamarunner.(*Server).allocModel(0xc000234d20, {0x7ffea1e99ccb?, 0x5cdba8df70da?}, {0x0, 0x8, {0xc0002f0080, 0x1, 0x1}, 0x1}, {0x0, ...}, ...)
	github.com/ollama/ollama/runner/ollamarunner/runner.go:1226 +0x391
```

### OS

Linux

### GPU

AMD

### CPU

AMD

### Ollama version

0.14.2 & 0.13.5",['bug'],0
ollama/ollama,ollama Vulcan crash(780m),"### What is the issue?

[ollama_logs.log](https://github.com/user-attachments/files/24734885/ollama_logs.log)

Title: Vulkan backend crash (Exception 0xc0000005) during context shift on AMD Radeon 780M iGPU (Windows)

Environment
Ollama Version: 0.14.2

Operating System: Windows

Processor/GPU: AMD 7840HS with Radeon 780M iGPU (Architecture: gfx1103)

Memory allocated to iGPU in BIOS: 16 GB (UMA Frame Buffer Size)

Backend: Vulkan (experimental, enabled via OLLAMA_VULKAN=1)

Overrides: HSA_OVERRIDE_GFX_VERSION=10.3.0 (Note: logs show a warning about this)

Steps to Reproduce
Set environment variable: OLLAMA_VULKAN=1.

Start ollama serve (with OLLAMA_DEBUG=1 for verbose logs).

Load a large model, e.g., Qwen3-8B (GGUF, Q8_0).

Send a prompt long enough to fill the context window (4096 tokens).

The model begins generation, hits the context limit (""context limit hit - shifting""), and then the Vulkan backend crashes with an access violation.

Observed Behavior & Logs
The model loads successfully, and all 37 layers are offloaded to the GPU (""offloaded 37/37 layers to GPU""). The server starts normally. However, during the inference process, when the context window is full and the system attempts to shift it, a critical error occurs:

text
time=2026-01-20T13:07:41.404+03:00 level=DEBUG source=cache.go:286 msg=""context limit hit - shifting"" id=0 limit=4096 input=4096 keep=4 discard=2046
...
Exception 0xc0000005 0x1 0x8 0x7ffad67ca728
PC=0x7ffad67ca728
signal arrived during external code execution

time=2026-01-20T13:07:51.097+03:00 level=ERROR source=server.go:1592 msg=""post predict"" error=""Post \""http://127.0.0.1:7317/completion\"": read tcp 127.0.0.1:7322->127.0.0.1:7317: wsarecv: An existing connection was forcibly closed by the remote host.""
Key Observations:

Crash Point: The crash (Exception 0xc0000005 - access violation) consistently occurs after the log line ""context limit hit - shifting"". The Vulkan runner process dies, forcing the main server connection to close.

Model Specific: Smaller models (e.g., phi3:mini) work without issue. The problem appears specific to larger models like the 8B parameter Qwen.

Memory: The GPU has sufficient memory (16.2 GiB total, ~13 GiB free during load). The model's total memory requirement is reported as ~8.8 GiB.

Flash Attention: The logs show ""enabling flash attention"" for this model run.

Full server logs from startup to crash are attached below.

Expected Behavior
The model should successfully perform the context shift operation and continue generating the response without crashing.

Additional Context / Attempted Workarounds
The standard ROCm backend is not supported for this GPU architecture (gfx1103), throwing an invalid device function error. Vulkan is the only available GPU acceleration path.

The issue is reproducible. Multiple attempts yield the same crash at the same point.

Related Issues: This might be connected to memory management problems discussed in issues like #12913 and #13677, but this specific crash during context shift on Windows with an AMD RDNA3 iGPU is a new combination.

### Relevant log output

```shell

```

### OS

Windows

### GPU

AMD

### CPU

AMD

### Ollama version

0.14.2",['bug'],0
ollama/ollama,ollama keeps starting at launch. no way to prevent it,"### What is the issue?

I don't have the ollama icon in ""open at login"" settings section.
FAQ says that I can disable it by toggle ""Allow in background"".
But why? it's different responsibilities and I have no issue if I start ollama and it works in background.


### Relevant log output

```shell

```

### OS

OS - macOS 26.2 (25C56)

### GPU

_No response_

### CPU

_No response_

### Ollama version

Latest",['bug'],0
ollama/ollama,kv cache took too many memory on glm-4.7-flash,"### What is the issue?

normally, with 8192 context, the kv cache should take less than 1G memory, but this version have took more that 6G memory, please advise.

ollama version: 0.14.3
time=2026-01-20T16:34:21.526+08:00 level=INFO source=device.go:251 msg=""kv cache"" device=CUDA0 size=""800.0 MiB""
time=2026-01-20T16:34:21.526+08:00 level=INFO source=device.go:256 msg=""kv cache"" device=CPU size=""6.6 GiB""

with qwen3:30b Q4K, the total size is only 18G, but the glm-4.7 took 26G
PS C:\Users\Waver> ollama ps
NAME                    ID              SIZE     PROCESSOR          CONTEXT    UNTIL
glm-4.7-flash:latest    ff14144f31df    26 GB    88%/12% CPU/GPU    8192       Forever

### Relevant log output

```shell
ollama version: 0.14.3
time=2026-01-20T16:34:21.526+08:00 level=INFO source=device.go:251 msg=""kv cache"" device=CUDA0 size=""800.0 MiB""
time=2026-01-20T16:34:21.526+08:00 level=INFO source=device.go:256 msg=""kv cache"" device=CPU size=""6.6 GiB""

with qwen3:30b Q4K, the total size is only 18G, but the glm-4.7 took 26G
PS C:\Users\Waver> ollama ps
NAME                    ID              SIZE     PROCESSOR          CONTEXT    UNTIL
glm-4.7-flash:latest    ff14144f31df    26 GB    88%/12% CPU/GPU    8192       Forever
```

### OS

Windows

### GPU

Nvidia

### CPU

_No response_

### Ollama version

0.14.3-rc2",['bug'],0
ollama/ollama,All models run from GPU crash the server on load with various errors but run fine on CPU,"### What is the issue?

Running ollama 0.14.2 on Arch Linux installed from the official repos (`ollama` and `ollama-rocm`), my GPU is AMD Radeon RX 6700 XT (I'm using `HSA_OVERRIDE_GFX_VERSION=10.3.0` so ollama can detect it). Every model I try to run on my GPU, including those that previously worked, fail to run with different errors per model (but the same error for the same model). For example:

- gemma3:270m-it-qat always throws `Error: 500 Internal Server Error: do load request: Post ""http://127.0.0.1:36955/load"": EOF`
- deepseek-r1:1.5b and 7b always throw `Error: 500 Internal Server Error: llama runner process has terminated: GGML_ASSERT(max_blocks_per_sm > 0) failed`

All these models work fine loaded to my CPU instead. I picked up Ollama after a year or so of not using it, so I'm not sure which is the last working version I used, but I can verify deepseek-r1:7b ran on my GPU back then.

Logs were captured from server run with flags `OLLAMA_DEBUG=2 HSA_OVERRIDE_GFX_VERSION=10.3.0`

[server_log_deepseek.txt](https://github.com/user-attachments/files/24731719/server_log_deepseek.txt)
[server_log_gemma.txt](https://github.com/user-attachments/files/24731720/server_log_gemma.txt)

### Relevant log output

```shell

```

### OS

Linux

### GPU

AMD

### CPU

AMD

### Ollama version

0.14.2",['bug'],0
ollama/ollama,Removing image generation model unexpectedly generates an image first,"### What is the issue?

ollama v0.14.3 (macOS Tahoe 26.2; Apple M4 Max) 

```
ollama@ollamas-computer ~ % ollama serve
time=2026-01-19T14:53:56.396-05:00 level=INFO source=routes.go:1614 msg=""server config"" env=""map[HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/Users/ollama/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false http_proxy: https_proxy: no_proxy:]""
time=2026-01-19T14:53:56.437-05:00 level=INFO source=images.go:499 msg=""total blobs: 2907""
time=2026-01-19T14:53:56.444-05:00 level=INFO source=images.go:506 msg=""total unused blobs removed: 0""
time=2026-01-19T14:53:56.445-05:00 level=INFO source=routes.go:1667 msg=""Listening on 127.0.0.1:11434 (version 0.14.2)""
time=2026-01-19T14:53:56.445-05:00 level=INFO source=runner.go:67 msg=""discovering available GPUs...""
time=2026-01-19T14:53:56.446-05:00 level=INFO source=server.go:429 msg=""starting runner"" cmd=""/Applications/Ollama.app/Contents/Resources/ollama runner --ollama-engine --port 55398""
time=2026-01-19T14:53:56.493-05:00 level=INFO source=types.go:42 msg=""inference compute"" id=0 filter_id=0 library=Metal compute=0.0 name=Metal description=""Apple M4 Max"" libdirs="""" driver=0.0 pci_id="""" type=discrete total=""107.5 GiB"" available=""107.5 GiB""
[GIN] 2026/01/19 - 14:54:07 | 200 |      26.333Âµs |       127.0.0.1 | HEAD     ""/""
[GIN] 2026/01/19 - 14:54:07 | 200 |    35.80675ms |       127.0.0.1 | POST     ""/api/show""
time=2026-01-19T14:54:07.399-05:00 level=INFO source=server.go:149 msg=""starting ollama-mlx image runner subprocess"" exe=/Applications/Ollama.app/Contents/Resources/ollama-mlx model=x/z-image-turbo:latest port=55439
time=2026-01-19T14:54:07.443-05:00 level=WARN source=server.go:141 msg=image-runner msg=""2026/01/19 14:54:07 runner.go:65: INFO starting image runner model=x/z-image-turbo:latest port=55439""
time=2026-01-19T14:54:07.446-05:00 level=INFO source=server.go:134 msg=image-runner msg=""Loading Z-Image model from manifest: x/z-image-turbo:latest...""
time=2026-01-19T14:54:07.634-05:00 level=INFO source=server.go:134 msg=image-runner msg=""  Loading tokenizer... âœ“""
time=2026-01-19T14:54:08.105-05:00 level=INFO source=server.go:134 msg=image-runner msg=""  Loading text encoder... âœ“""
time=2026-01-19T14:54:08.105-05:00 level=INFO source=server.go:134 msg=image-runner msg=""  (4.5 GB, peak 4.5 GB)""
time=2026-01-19T14:54:08.819-05:00 level=INFO source=server.go:134 msg=image-runner msg=""  Loading transformer... âœ“""
time=2026-01-19T14:54:08.819-05:00 level=INFO source=server.go:134 msg=image-runner msg=""  (11.7 GB, peak 11.7 GB)""
time=2026-01-19T14:54:08.854-05:00 level=INFO source=server.go:134 msg=image-runner msg=""  Loading conv_in... âœ“""
time=2026-01-19T14:54:08.854-05:00 level=INFO source=server.go:134 msg=image-runner msg=""  Loading mid block... âœ“""
time=2026-01-19T14:54:08.854-05:00 level=INFO source=server.go:134 msg=image-runner msg=""  Loading up blocks... âœ“ [4 blocks]""
time=2026-01-19T14:54:08.854-05:00 level=INFO source=server.go:134 msg=image-runner msg=""  Loading conv_norm_out... âœ“""
time=2026-01-19T14:54:08.854-05:00 level=INFO source=server.go:134 msg=image-runner msg=""  Loading conv_out... âœ“""
time=2026-01-19T14:54:08.854-05:00 level=INFO source=server.go:134 msg=image-runner msg=""  (11.9 GB, peak 11.9 GB)""
time=2026-01-19T14:54:08.854-05:00 level=INFO source=server.go:134 msg=image-runner msg=""  Loaded in 1.41s (11.9 GB VRAM)""
time=2026-01-19T14:54:08.854-05:00 level=WARN source=server.go:141 msg=image-runner msg=""2026/01/19 14:54:08 runner.go:109: INFO image runner listening addr=127.0.0.1:55439""
time=2026-01-19T14:54:08.902-05:00 level=INFO source=server.go:225 msg=""image runner is ready"" port=55439
time=2026-01-19T14:54:09.305-05:00 level=INFO source=server.go:134 msg=image-runner msg=""  TeaCache enabled: threshold=0.15""
time=2026-01-19T14:54:19.412-05:00 level=INFO source=server.go:134 msg=image-runner msg=""  Step 1/9: t=1.0000 (10.11s) [11.9 GB active, 13.5 GB peak]""
time=2026-01-19T14:54:19.412-05:00 level=INFO source=server.go:134 msg=image-runner msg=""    [TeaCache: reusing cached output]""
time=2026-01-19T14:54:19.413-05:00 level=INFO source=server.go:134 msg=image-runner msg=""  Step 2/9: t=0.9619 (0.00s) [11.9 GB active, 13.5 GB peak]""
time=2026-01-19T14:54:19.413-05:00 level=INFO source=server.go:134 msg=image-runner msg=""    [TeaCache: reusing cached output]""
time=2026-01-19T14:54:19.413-05:00 level=INFO source=server.go:134 msg=image-runner msg=""  Step 3/9: t=0.9170 (0.00s) [11.9 GB active, 13.5 GB peak]""
time=2026-01-19T14:54:26.839-05:00 level=INFO source=server.go:134 msg=image-runner msg=""  Step 4/9: t=0.8633 (7.43s) [12.1 GB active, 13.5 GB peak]""
time=2026-01-19T14:54:26.839-05:00 level=INFO source=server.go:134 msg=image-runner msg=""    [TeaCache: reusing cached output]""
time=2026-01-19T14:54:26.839-05:00 level=INFO source=server.go:134 msg=image-runner msg=""  Step 5/9: t=0.7979 (0.00s) [11.9 GB active, 13.5 GB peak]""
time=2026-01-19T14:54:33.813-05:00 level=INFO source=server.go:134 msg=image-runner msg=""  Step 6/9: t=0.7164 (6.97s) [12.0 GB active, 13.5 GB peak]""
time=2026-01-19T14:54:33.813-05:00 level=INFO source=server.go:134 msg=image-runner msg=""    [TeaCache: reusing cached output]""
time=2026-01-19T14:54:33.814-05:00 level=INFO source=server.go:134 msg=image-runner msg=""  Step 7/9: t=0.6123 (0.00s) [11.9 GB active, 13.5 GB peak]""
time=2026-01-19T14:54:41.082-05:00 level=INFO source=server.go:134 msg=image-runner msg=""  Step 8/9: t=0.4743 (7.27s) [11.9 GB active, 13.5 GB peak]""
time=2026-01-19T14:54:49.473-05:00 level=INFO source=server.go:134 msg=image-runner msg=""  Step 9/9: t=0.2830 (8.39s) [12.1 GB active, 13.5 GB peak]""
time=2026-01-19T14:54:49.473-05:00 level=INFO source=server.go:134 msg=image-runner msg=""  TeaCache stats: 4 hits, 5 misses (44.4% cache rate)""
time=2026-01-19T14:54:50.696-05:00 level=INFO source=server.go:134 msg=image-runner msg=""Generated in 41.79s (9 steps)""
[GIN] 2026/01/19 - 14:54:50 | 200 | 43.504865458s |       127.0.0.1 | POST     ""/api/generate""
[GIN] 2026/01/19 - 14:54:51 | 200 |  544.721292ms |       127.0.0.1 | DELETE   ""/api/delete""
```


### Ollama version

v0.14.3",['bug'],0
ollama/ollama,401 Unauthorized error when using cloud models with Claude Code,"### What is the issue?

**Describe the bug**
Getting 401 unauthorized error when trying to use cloud models with Claude Code.

**Steps to reproduce:**
1. Created API key at https://ollama.com/settings/keys
2. Set environment variables:
```bash
   export ANTHROPIC_BASE_URL=https://ollama.com
   export ANTHROPIC_API_KEY=<my-api-key>
```
3. Run: `claude --model glm-4.7:cloud`
4. Error: `401 unauthorized`

**Environment:**
- OS: Windows 11 with WSL2 (Ubuntu 24.04)
- Ollama version: 0.14.2
- Claude Code version: v2.1.12

**Expected behavior:**
Should connect to cloud model successfully. for the link (https://docs.ollama.com/integrations/claude-code)

**Additional context:**
Tried both methods from https://docs.ollama.com/cloud documentation.
API key is valid and visible at ollama.com/settings/keys.

### Relevant log output

```shell

```

### OS

_No response_

### GPU

_No response_

### CPU

_No response_

### Ollama version

_No response_",['bug'],0
ollama/ollama,235b-cloud--server can't connect status code 500,"### What is the issue?

ollama._types.ResponseError:Post""http://ollama.com/api/chat?ts=1768807226"":read tcp 192.168.1.41:53119->34.36.133.15:443:wsarecv:An existing connection was forcibly closed by the remote host(status code :500) 

### Relevant log output

```shell

```

### OS

_No response_

### GPU

_No response_

### CPU

_No response_

### Ollama version

_No response_",['bug'],0
ollama/ollama,Cloud model connection timeout when using official Ollama client,"### What is the issue?

## Description
When trying to use cloud models in the official Ollama client, I'm getting a connection timeout error.

## Steps to Reproduce
1. Open Ollama official client
2. Attempt to use any cloud model
3. Get connection timeout error

## Error Message
500 Internal Server Error: Post ""[https://ollama.com:443/api/chat?ts=1768793788](https://ollama.com/api/chat?ts=1768793788)"": dial tcp 34.36.133.15:443: i/o timeout

## What I've Tried
1. Restarted Ollama client
2. Checked internet connection 
3. Local model are fine


## Additional Information
PING ollama.com (34.36.133.15): 56 data bytes
64 bytes from 34.36.133.15: icmp_seq=0 ttl=104 time=223.880 ms
64 bytes from 34.36.133.15: icmp_seq=1 ttl=104 time=231.907 ms
64 bytes from 34.36.133.15: icmp_seq=2 ttl=104 time=251.540 ms
64 bytes from 34.36.133.15: icmp_seq=3 ttl=104 time=298.792 ms
64 bytes from 34.36.133.15: icmp_seq=4 ttl=104 time=228.120 ms
--- ollama.com ping statistics ---
5 packets transmitted, 5 packets received, 0.0% packet loss

## Questions
 Are cloud models currently available ?


### Relevant log output

```shell
[GIN] 2026/01/19 - 11:38:28 | 200 |    5.825583ms |       127.0.0.1 | GET      ""/api/tags""
time=2026-01-19T11:38:44.419+08:00 level=ERROR source=routes.go:1809 msg=""Post \""https://ollama.com/api/me?ts=1768793894\"": dial tcp 34.36.133.15:443: i/o timeout""
[GIN] 2026/01/19 - 11:38:44 | 200 |  30.00169625s |       127.0.0.1 | POST     ""/api/me""
[GIN] 2026/01/19 - 11:38:58 | 200 |     6.76375ms |       127.0.0.1 | GET      ""/api/tags""
time=2026-01-19T11:39:14.928+08:00 level=ERROR source=routes.go:1809 msg=""Post \""https://ollama.com/api/me?ts=1768793924\"": dial tcp 34.36.133.15:443: i/o timeout""
[GIN] 2026/01/19 - 11:39:14 | 200 | 30.001228208s |       127.0.0.1 | POST     ""/api/me""
time=2026-01-19T11:39:16.281+08:00 level=ERROR source=routes.go:1809 msg=""Post \""https://ollama.com/api/me?ts=1768793955\"": read tcp 10.10.55.16:57012->34.36.133.15:443: read: connection reset by peer""
[GIN] 2026/01/19 - 11:39:16 | 200 |  346.957916ms |       127.0.0.1 | POST     ""/api/me""
```

### OS

macOS

### GPU

Apple

### CPU

Apple

### Ollama version

Version 0.14.2",['bug'],0
ollama/ollama,Randomly uses CPU even when GPU available until I restart,"### What is the issue?

Occasionally, ollama will refuse to use my GPU. `nvidia-smi` shows I have plenty of available VRAM. But `ollama run` will load the model entirely on the CPU, with `ollama ps` showing 100% CPU usage. This happens seemingly randomly. I'll go days using ollama just fine, then the next time I run it, all CPU.

I'm running 0.13.5 via OpenWebUI's Ollama Docker image, which is at version v0.7.2, on Ubuntu 22.04.5 LTS. I notice this via OpenWebUI, which fails with `500: model requires more system memory (18.9 GiB) than is available (13.1 GiB)` (because it's using CPU, not GPU). I have OpenWebUI to refuse to use the CPU, because before, it would silently use the CPU and just generate at 10% speed. This has occurred throughout OpenWebUI and Ollama versions.

Let me know if you need more information or would like me to do any debugging. I solve this just by restarting the Docker container, but it's quite annoying.

### Relevant log output

```shell

```

### OS

Docker

### GPU

Nvidia

### CPU

AMD

### Ollama version

0.13.5","['bug', 'nvidia']",0
ollama/ollama,Getting nonsense text responses using Intel ARC 750 8GB card (running on TrueNas),"### What is the issue?

Any suggesting on how to run things smoothly on an Intel Arc A750 8GB card? I have tried a few models. I'm able to get it to use the GPU, I see this in intel_gpu_top. When I prompt in UI, I get all this nonsense. This is seen across all three models. I am unable to upload images, but I can see in intel_gpu_top it's using the GPU.

Originally came from this discussion: https://github.com/ollama/ollama/pull/11160#issuecomment-3764241349

Using this as a truenas app:
```
Name:
ollama
App Version:
v0.14.2
Version:
v1.1.51
```

As seen in OpenWeb UI:
```
Open WebUI Version
v0.7.2
Ollama Version
0.14.2
```

environment variables tried.
```
OLLAMA_VULKAN=1
DEVICE=Arc
OLLAMA_INTEL_GPU=true
OLLAMA_NUM_GPU=999
ZES_ENABLE_SYSMAN=1
```

```
# ./ollama ls
NAME             ID              SIZE      MODIFIED          
llama3.2:3b      a80c4f17acd5    2.0 GB    7 minutes ago        
mistral:7b       6577803aa9a0    4.4 GB    20 minutes ago       
gemma3:latest    a2af6cc3eb7f    3.3 GB    About an hour ago
```
```
# ./ollama ps
NAME           ID              SIZE      PROCESSOR    CONTEXT    UNTIL   
llama3.2:3b    a80c4f17acd5    2.8 GB    100% GPU     4096       Forever
```

Example prompt:
""hey give me a joke""
response:
```
llama3.2:3b
Today at 1:25 PM
AGAIN Bonnielausunnable Binder evacuated evac binderuginlausroneig binder Consortlaus-Fi thá»©c Ù„ÛŒÚ¯æ—‹elsorna evaclaus Thur Rule decl thá»©c binder binder stylinglaus binder Coollaus Binderichten Binder Bonnieudas evac launder Mig evac declig declÑ€Ð¸Ð¿rieflausminsteraramellaus Binder SurveyFileSync Ð±Ð¾Ðºbeklauslausemean-Javadocolvlah launderlaus Declaration.X evac RicolausODBamsmium binder mergerbuster Consortlaus-Fiiliesæ—‹ usherlaus clickableudgebir evac launder DOIlaus binder stylingelsievichten Ø¨Ø®laus Turnbullorna playableeworld launder thá»©c thá»©cig Fieldsunnable hydrogen Migæ—‹ decllaus thá»©clausminsterrief thá»©cÑ€Ð¸Ð¿ Binder Ð±Ð¾Ðºiras launderlauslauslauslauslauslah Declarationà¹€à¸­à¸ mistr Millenn Blackburnlausaramel Surveylaus Famouslah Thurminstermium.android Consort binderlaus Accord tes Miglah.X Mig Mig Ricolaus evacams Rico
```

<img width=""1003"" height=""393"" alt=""Image"" src=""https://github.com/user-attachments/assets/e74c80aa-f4c2-464f-85fc-4cb9f229bac6"" />

<img width=""1223"" height=""235"" alt=""Image"" src=""https://github.com/user-attachments/assets/04c79e45-b78e-46ef-868d-e975c04fd412"" />

### Relevant log output

```shell
llama_model_load: vocab only - skipping tensors
time=2026-01-17T19:20:03.376Z level=INFO source=server.go:429 msg=""starting runner"" cmd=""/usr/bin/ollama runner --model /root/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff --port 38905""
time=2026-01-17T19:20:03.376Z level=INFO source=sched.go:452 msg=""system memory"" total=""7.9 GiB"" free=""7.8 GiB"" free_swap=""0 B""
time=2026-01-17T19:20:03.377Z level=INFO source=sched.go:459 msg=""gpu memory"" id=8680a156-0800-0000-0700-000000000000 library=Vulkan available=""6.7 GiB"" free=""7.1 GiB"" minimum=""457.0 MiB"" overhead=""0 B""
time=2026-01-17T19:20:03.377Z level=INFO source=server.go:496 msg=""loading model"" ""model layers""=29 requested=-1
time=2026-01-17T19:20:03.377Z level=INFO source=device.go:240 msg=""model weights"" device=Vulkan0 size=""1.9 GiB""
time=2026-01-17T19:20:03.377Z level=INFO source=device.go:251 msg=""kv cache"" device=Vulkan0 size=""448.0 MiB""
time=2026-01-17T19:20:03.377Z level=INFO source=device.go:262 msg=""compute graph"" device=Vulkan0 size=""256.5 MiB""
time=2026-01-17T19:20:03.377Z level=INFO source=device.go:272 msg=""total memory"" size=""2.6 GiB""
time=2026-01-17T19:20:03.384Z level=INFO source=runner.go:965 msg=""starting go runner""
load_backend: loaded CPU backend from /usr/lib/ollama/libggml-cpu-haswell.so
ggml_vulkan: Found 1 Vulkan devices:
ggml_vulkan: 0 = Intel(R) Arc(tm) A750 Graphics (DG2) (Intel open-source Mesa driver) | uma: 0 | fp16: 1 | bf16: 0 | warp size: 32 | shared memory: 65536 | int dot: 1 | matrix cores: none
load_backend: loaded Vulkan backend from /usr/lib/ollama/vulkan/libggml-vulkan.so
time=2026-01-17T19:20:03.412Z level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)
time=2026-01-17T19:20:03.412Z level=INFO source=runner.go:1001 msg=""Server listening on 127.0.0.1:38905""
time=2026-01-17T19:20:03.420Z level=INFO source=runner.go:895 msg=load request=""{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Auto KvSize:4096 KvCacheType: NumThreads:2 GPULayers:29[ID:8680a156-0800-0000-0700-000000000000 Layers:29(0..28)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}""
ggml_backend_vk_get_device_memory called: uuid 8680a156-0800-0000-0700-000000000000
ggml_backend_vk_get_device_memory called: luid 0x0000000000000000
time=2026-01-17T19:20:03.420Z level=INFO source=server.go:1347 msg=""waiting for llama runner to start responding""
time=2026-01-17T19:20:03.420Z level=INFO source=server.go:1381 msg=""waiting for server to become available"" status=""llm server loading model""
ggml_backend_vk_get_device_memory called: uuid 8680a156-0800-0000-0700-000000000000
ggml_backend_vk_get_device_memory called: luid 0x0000000000000000
ggml_backend_vk_get_device_memory called: uuid 8680a156-0800-0000-0700-000000000000
ggml_backend_vk_get_device_memory called: luid 0x0000000000000000
llama_model_load_from_file_impl: using device Vulkan0 (Intel(R) Arc(tm) A750 Graphics (DG2)) (0000:07:00.0) - 7315 MiB free
llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from /root/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 3B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = [""facebook"", ""meta"", ""pytorch"", ""llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = [""en"", ""de"", ""fr"", ""it"", ""pt"", ""hi"", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 28
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  18:                          general.file_type u32              = 15
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = [""!"", ""\"""", ""#"", ""$"", ""%"", ""&"", ""'"", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = [""Ä  Ä "", ""Ä  Ä Ä Ä "", ""Ä Ä  Ä Ä "", ""...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   58 tensors
llama_model_loader: - type q4_K:  168 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 1.87 GiB (5.01 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128008 ('<|eom_id|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: no_alloc         = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3072
print_info: n_embd_inp       = 3072
print_info: n_layer          = 28
print_info: n_head           = 24
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 3
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_yarn_log_mul= 0.0000
print_info: rope_finetuned   = unknown
print_info: model type       = 3B
print_info: model params     = 3.21 B
print_info: general.name     = Llama 3.2 3B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'ÄŠ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
ggml_backend_vk_get_device_memory called: uuid 8680a156-0800-0000-0700-000000000000
ggml_backend_vk_get_device_memory called: luid 0x0000000000000000
ggml_backend_vk_get_device_memory called: uuid 8680a156-0800-0000-0700-000000000000
ggml_backend_vk_get_device_memory called: luid 0x0000000000000000
load_tensors: offloading 28 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 29/29 layers to GPU
load_tensors:      Vulkan0 model buffer size =  1918.35 MiB
load_tensors:  Vulkan_Host model buffer size =   308.23 MiB
ggml_backend_vk_get_device_memory called: uuid 8680a156-0800-0000-0700-000000000000
ggml_backend_vk_get_device_memory called: luid 0x0000000000000000
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = false
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context: Vulkan_Host  output buffer size =     0.50 MiB
llama_kv_cache:    Vulkan0 KV buffer size =   448.00 MiB
llama_kv_cache: size =  448.00 MiB (  4096 cells,  28 layers,  1/1 seqs), K (f16):  224.00 MiB, V (f16):  224.00 MiB
llama_context: Flash Attention was auto, set to enabled
llama_context:    Vulkan0 compute buffer size =   256.50 MiB
llama_context: Vulkan_Host compute buffer size =    14.02 MiB
llama_context: graph nodes  = 875
llama_context: graph splits = 2
time=2026-01-17T19:20:05.426Z level=INFO source=server.go:1385 msg=""llama runner started in 2.05 seconds""
time=2026-01-17T19:20:05.426Z level=INFO source=sched.go:526 msg=""loaded runners"" count=1
time=2026-01-17T19:20:05.426Z level=INFO source=server.go:1347 msg=""waiting for llama runner to start responding""
time=2026-01-17T19:20:05.426Z level=INFO source=server.go:1385 msg=""llama runner started in 2.05 seconds""
[GIN] 2026/01/17 - 19:20:05 | 200 |  2.417773737s |       127.0.0.1 | POST     ""/api/generate""
[GIN] 2026/01/17 - 19:20:09 | 200 |      18.144Âµs |       127.0.0.1 | HEAD     ""/""
[GIN] 2026/01/17 - 19:20:09 | 200 |      14.698Âµs |       127.0.0.1 | GET      ""/api/ps""
[GIN] 2026/01/17 - 19:20:16 | 200 |     590.693Âµs |      172.16.8.1 | GET      ""/api/tags""
[GIN] 2026/01/17 - 19:20:16 | 200 |      16.952Âµs |      172.16.8.1 | GET      ""/api/ps""
[GIN] 2026/01/17 - 19:20:17 | 200 |      29.275Âµs |      172.16.8.1 | GET      ""/api/version""
[GIN] 2026/01/17 - 19:22:28 | 200 |          2m7s |      172.16.8.1 | POST     ""/api/chat""
[GIN] 2026/01/17 - 19:22:36 | 200 |      28.763Âµs |      172.16.8.1 | GET      ""/api/version""
[GIN] 2026/01/17 - 19:26:26 | 200 |         1m14s |      172.16.8.1 | POST     ""/api/chat""
```

### OS

Docker

### GPU

Intel

### CPU

AMD

### Ollama version

0.14.2",['bug'],0
ollama/ollama,groxaxo/parakeet-tdt-0.6b-v3-fastapi-openai,"I would love to have support for transcription model tunned by Groxaxo. It has an amazing performance in CPU thanks to his optimizations over onnx

https://github.com/groxaxo/parakeet-tdt-0.6b-v3-fastapi-openai

This is a super fast and useful model for assistant environments",['model'],0
ollama/ollama,Incorrect response schema in documentation for /api/ps and /api/tags,"### What is the issue?

The documentation for the API endpoints `/api/ps` and `/api/tags` is incomplete. The response schemas are not correct.

`/api/tags` uses ""name"" for the model name, while `/api/ps` uses ""model"" for the model name. In reality, both are used in the schema according to the source code in https://github.com/ollama/ollama/blob/12719b6e87e738f76d0456dd9a9d7571be58cb68/api/types.go#L796:

```ts
// ListModelResponse is a single model description in [ListResponse].
type ListModelResponse struct {
	Name        string       `json:""name""`
	Model       string       `json:""model""`
	RemoteModel string       `json:""remote_model,omitempty""`
	RemoteHost  string       `json:""remote_host,omitempty""`
	ModifiedAt  time.Time    `json:""modified_at""`
	Size        int64        `json:""size""`
	Digest      string       `json:""digest""`
	Details     ModelDetails `json:""details,omitempty""`
}

// ProcessModelResponse is a single model description in [ProcessResponse].
type ProcessModelResponse struct {
	Name          string       `json:""name""`
	Model         string       `json:""model""`
	Size          int64        `json:""size""`
	Digest        string       `json:""digest""`
	Details       ModelDetails `json:""details,omitempty""`
	ExpiresAt     time.Time    `json:""expires_at""`
	SizeVRAM      int64        `json:""size_vram""`
	ContextLength int          `json:""context_length""`
}
```

I suggest an update to the schemas in the documentation pages for https://docs.ollama.com/api/ps and https://docs.ollama.com/api/tags to reflect the actual schema used by the source code!

### Relevant log output

```shell

```

### OS

_No response_

### GPU

_No response_

### CPU

_No response_

### Ollama version

_No response_",['bug'],0
ollama/ollama,CLI multiline prompt shows no response until follow-up message is sent,"What is the issue?

  When entering a multiline prompt in interactive mode (using the ... continuation lines), the model processes the request but no response is displayed. The response only appears after sending a follow-up message (e.g., ""Hello?""), at which point the model's response references the original prompt, proving it was processed.

  OS

  Linux (Ubuntu)

  GPU

  CPU-only (also has AMD GPU with ROCm installed)

  CPU

  x86_64

Ollama version

  0.14.2

  Model

  danielsheep/Qwen3-Coder-30B-A3B-Instruct-1M-Unsloth:UD-Q5_K_XL

  Steps to reproduce

  1. Run ollama run <model> (tested with danielsheep/Qwen3-Coder-30B-A3B-Instruct-1M-Unsloth:UD-Q5_K_XL)
  2. Enter a multiline prompt that spans multiple lines:
  >>> Generate a comprehensive inline HTML interface that uses tailwind CDNs and fetches the latest price of bitcoin and prints it to the interface; make the interface nice and modern and smoothly update in realtime by refreshing the GUI without flickers or visual glitches; make
  ... the interface modern and aesthetically nice UX
  3. Press Enter - cursor returns to >>> prompt with no response displayed
  4. Type any follow-up message like ""Hello?""
  5. The model now responds, referencing the original request (e.g., ""I can see you're interested in the Bitcoin price tracker HTML file I generated for you..."")

  Expected behavior

  Response should stream/display immediately after the multiline prompt is submitted.

  Actual behavior

  No response is displayed. The response only appears after a subsequent message is sent.

  Additional context

  - Single-line prompts in interactive mode work correctly
  - Piped input works correctly: echo ""prompt"" | ollama run <model>
  - API calls work correctly: curl http://localhost:11434/api/generate -d '...'
  - This suggests the issue is specific to the readline/terminal handling for multiline interactive input
  - Terminal: xterm-256color
  - Issue persists after upgrading from 0.14.1 to 0.14.2

Logs

  Server logs show successful HTTP 200 responses for the requests, confirming the model processed them:
  [GIN] 2026/01/17 - 14:09:42 | 200 | 796.41043ms | 127.0.0.1 | POST ""/api/chat""
  [GIN] 2026/01/17 - 14:10:54 | 200 | 5.754584434s | 127.0.0.1 | POST ""/api/chat""","['bug', 'needs more info']",0
ollama/ollama,docker ps error,"### What is the issue?

% ollama ps
panic: runtime error: slice bounds out of range [:12] with length 0

goroutine 1 [running]:
github.com/ollama/ollama/cmd.ListRunningHandler(0x140004a8c08, {0x10672b560, 0x0, 0x105876ff9?})
	/Users/runner/work/ollama/ollama/cmd/cmd.go:802 +0x568
github.com/spf13/cobra.(*Command).execute(0x140004a8c08, {0x10672b560, 0x0, 0x0})
	/Users/runner/go/pkg/mod/github.com/spf13/cobra@v1.7.0/command.go:940 +0x648
github.com/spf13/cobra.(*Command).ExecuteC(0x14000486908)
	/Users/runner/go/pkg/mod/github.com/spf13/cobra@v1.7.0/command.go:1068 +0x320
github.com/spf13/cobra.(*Command).Execute(...)
	/Users/runner/go/pkg/mod/github.com/spf13/cobra@v1.7.0/command.go:992
github.com/spf13/cobra.(*Command).ExecuteContext(...)
	/Users/runner/go/pkg/mod/github.com/spf13/cobra@v1.7.0/command.go:985
main.main()
	/Users/runner/work/ollama/ollama/main.go:12 +0x54

version:Version 0.14.2 (0.14.2)



### Relevant log output

```shell

```

### OS

_No response_

### GPU

_No response_

### CPU

_No response_

### Ollama version

_No response_",['bug'],0
ollama/ollama,Ollama switches to CPU usage in Codex-CLI 0.86.0 after 10K input token is reached with gpt-oss:120b,"### What is the issue?

[Issue in codex-cli repo](https://github.com/openai/codex/issues/9395)

I was redirected here immediately after posting, figured you guys might look into this one. Everything about the issue is displayed in the post, but I do want to mention some additional things: 

I tried playing around with different configurations in both Ollama and Codex-cli. I had done the following:

- Created a separate modelfile to explicitly set `gpt-oss:120b` `num_ctx` to `131072` tokens (128K) and changed some sampling parameter settings. 

- Then I switched back to the original model when I discovered you can set `model_context_window = 131072` in codex-cli's `config.toml` file so I had set it to that. I also tried setting lower `num_ctx` values but I still ran into the same issue. 

-  I then set Codex CLI's `auto-compact` feature to a very low ceiling of 12K tokens when I noticed the CPU usage would kickstart after 10k tokens before switching back to GPU when it was less than 10K. 

I hadn't seen this issue in any other application I developed that runs Ollama on my GPU so that's why I went to Codex CLI first, thinking the issue was on their end, and that's when they sent me to you. Is there some sort of issue with this model? Are there any updated versions that specifically fix this issue? I'm a little worried about upgrading in case Ollama breaks something so that's why I had stayed at 0.13.5. 

### Relevant log output

```shell
time=2026-01-16T16:01:52.424-05:00 level=DEBUG source=sched.go:626 msg=""evaluating already loaded"" model=H:\ai\ollama\models\blobs\sha256-6be6d66a3f546d8c19b130dc41dc24b2fc159f84ffbc76a0ee0676205083cf5a
time=2026-01-16T16:02:08.806-05:00 level=DEBUG source=server.go:1509 msg=""completion request"" images=0 prompt=72931 format=""""
time=2026-01-16T16:02:09.139-05:00 level=DEBUG source=cache.go:142 msg=""loading cache slot"" id=0 cache=17268 prompt=17314 used=17268 remaining=46
[GIN] 2026/01/16 - 16:02:10 | 200 |   18.0416882s |       127.0.0.1 | POST     ""/v1/responses""
time=2026-01-16T16:02:10.333-05:00 level=DEBUG source=sched.go:385 msg=""context for request finished"" runner.name=registry.ollama.ai/library/gpt-oss:120b-128k runner.inference=""[{ID:GPU-94db278a-855e-2012-495e-be319102a97a Library:CUDA}]"" runner.size=""64.1 GiB"" runner.vram=""64.1 GiB"" runner.parallel=1 runner.pid=510340 runner.model=H:\ai\ollama\models\blobs\sha256-6be6d66a3f546d8c19b130dc41dc24b2fc159f84ffbc76a0ee0676205083cf5a runner.num_ctx=131072
time=2026-01-16T16:02:10.334-05:00 level=DEBUG source=sched.go:290 msg=""runner with non-zero duration has gone idle, adding timer"" runner.name=registry.ollama.ai/library/gpt-oss:120b-128k runner.inference=""[{ID:GPU-94db278a-855e-2012-495e-be319102a97a Library:CUDA}]"" runner.size=""64.1 GiB"" runner.vram=""64.1 GiB"" runner.parallel=1 runner.pid=510340 runner.model=H:\ai\ollama\models\blobs\sha256-6be6d66a3f546d8c19b130dc41dc24b2fc159f84ffbc76a0ee0676205083cf5a runner.num_ctx=131072 duration=2562047h47m16.854775807s
time=2026-01-16T16:02:10.334-05:00 level=DEBUG source=sched.go:308 msg=""after processing request finished event"" runner.name=registry.ollama.ai/library/gpt-oss:120b-128k runner.inference=""[{ID:GPU-94db278a-855e-2012-495e-be319102a97a Library:CUDA}]"" runner.size=""64.1 GiB"" runner.vram=""64.1 GiB"" runner.parallel=1 runner.pid=510340 runner.model=H:\ai\ollama\models\blobs\sha256-6be6d66a3f546d8c19b130dc41dc24b2fc159f84ffbc76a0ee0676205083cf5a runner.num_ctx=131072 refCount=0
time=2026-01-16T16:02:11.859-05:00 level=DEBUG source=sched.go:626 msg=""evaluating already loaded"" model=H:\ai\ollama\models\blobs\sha256-6be6d66a3f546d8c19b130dc41dc24b2fc159f84ffbc76a0ee0676205083cf5a
time=2026-01-16T16:02:29.729-05:00 level=DEBUG source=server.go:1509 msg=""completion request"" images=0 prompt=73493 format=""""
time=2026-01-16T16:02:30.088-05:00 level=DEBUG source=cache.go:142 msg=""loading cache slot"" id=0 cache=17392 prompt=17440 used=17340 remaining=100
[GIN] 2026/01/16 - 16:02:31 | 200 |    19.817603s |       127.0.0.1 | POST     ""/v1/responses""
time=2026-01-16T16:02:31.551-05:00 level=DEBUG source=sched.go:385 msg=""context for request finished"" runner.name=registry.ollama.ai/library/gpt-oss:120b-128k runner.inference=""[{ID:GPU-94db278a-855e-2012-495e-be319102a97a Library:CUDA}]"" runner.size=""64.1 GiB"" runner.vram=""64.1 GiB"" runner.parallel=1 runner.pid=510340 runner.model=H:\ai\ollama\models\blobs\sha256-6be6d66a3f546d8c19b130dc41dc24b2fc159f84ffbc76a0ee0676205083cf5a runner.num_ctx=131072
time=2026-01-16T16:02:31.552-05:00 level=DEBUG source=sched.go:290 msg=""runner with non-zero duration has gone idle, adding timer"" runner.name=registry.ollama.ai/library/gpt-oss:120b-128k runner.inference=""[{ID:GPU-94db278a-855e-2012-495e-be319102a97a Library:CUDA}]"" runner.size=""64.1 GiB"" runner.vram=""64.1 GiB"" runner.parallel=1 runner.pid=510340 runner.model=H:\ai\ollama\models\blobs\sha256-6be6d66a3f546d8c19b130dc41dc24b2fc159f84ffbc76a0ee0676205083cf5a runner.num_ctx=131072 duration=2562047h47m16.854775807s
time=2026-01-16T16:02:31.552-05:00 level=DEBUG source=sched.go:308 msg=""after processing request finished event"" runner.name=registry.ollama.ai/library/gpt-oss:120b-128k runner.inference=""[{ID:GPU-94db278a-855e-2012-495e-be319102a97a Library:CUDA}]"" runner.size=""64.1 GiB"" runner.vram=""64.1 GiB"" runner.parallel=1 runner.pid=510340 runner.model=H:\ai\ollama\models\blobs\sha256-6be6d66a3f546d8c19b130dc41dc24b2fc159f84ffbc76a0ee0676205083cf5a runner.num_ctx=131072 refCount=0
time=2026-01-16T16:02:33.594-05:00 level=DEBUG source=sched.go:626 msg=""evaluating already loaded"" model=H:\ai\ollama\models\blobs\sha256-6be6d66a3f546d8c19b130dc41dc24b2fc159f84ffbc76a0ee0676205083cf5a
time=2026-01-16T16:02:53.708-05:00 level=DEBUG source=server.go:1509 msg=""completion request"" images=0 prompt=74176 format=""""
time=2026-01-16T16:02:54.098-05:00 level=DEBUG source=cache.go:142 msg=""loading cache slot"" id=0 cache=17548 prompt=17594 used=17548 remaining=46
[GIN] 2026/01/16 - 16:02:55 | 200 |    22.281456s |       127.0.0.1 | POST     ""/v1/responses""
time=2026-01-16T16:02:55.748-05:00 level=DEBUG source=sched.go:385 msg=""context for request finished"" runner.name=registry.ollama.ai/library/gpt-oss:120b-128k runner.inference=""[{ID:GPU-94db278a-855e-2012-495e-be319102a97a Library:CUDA}]"" runner.size=""64.1 GiB"" runner.vram=""64.1 GiB"" runner.parallel=1 runner.pid=510340 runner.model=H:\ai\ollama\models\blobs\sha256-6be6d66a3f546d8c19b130dc41dc24b2fc159f84ffbc76a0ee0676205083cf5a runner.num_ctx=131072
time=2026-01-16T16:02:55.750-05:00 level=DEBUG source=sched.go:290 msg=""runner with non-zero duration has gone idle, adding timer"" runner.name=registry.ollama.ai/library/gpt-oss:120b-128k runner.inference=""[{ID:GPU-94db278a-855e-2012-495e-be319102a97a Library:CUDA}]"" runner.size=""64.1 GiB"" runner.vram=""64.1 GiB"" runner.parallel=1 runner.pid=510340 runner.model=H:\ai\ollama\models\blobs\sha256-6be6d66a3f546d8c19b130dc41dc24b2fc159f84ffbc76a0ee0676205083cf5a runner.num_ctx=131072 duration=2562047h47m16.854775807s
time=2026-01-16T16:02:55.750-05:00 level=DEBUG source=sched.go:308 msg=""after processing request finished event"" runner.name=registry.ollama.ai/library/gpt-oss:120b-128k runner.inference=""[{ID:GPU-94db278a-855e-2012-495e-be319102a97a Library:CUDA}]"" runner.size=""64.1 GiB"" runner.vram=""64.1 GiB"" runner.parallel=1 runner.pid=510340 runner.model=H:\ai\ollama\models\blobs\sha256-6be6d66a3f546d8c19b130dc41dc24b2fc159f84ffbc76a0ee0676205083cf5a runner.num_ctx=131072 refCount=0
time=2026-01-16T16:02:57.676-05:00 level=DEBUG source=sched.go:626 msg=""evaluating already loaded"" model=H:\ai\ollama\models\blobs\sha256-6be6d66a3f546d8c19b130dc41dc24b2fc159f84ffbc76a0ee0676205083cf5a
time=2026-01-16T16:03:19.501-05:00 level=DEBUG source=server.go:1509 msg=""completion request"" images=0 prompt=74927 format=""""
time=2026-01-16T16:03:19.936-05:00 level=DEBUG source=cache.go:142 msg=""loading cache slot"" id=0 cache=17725 prompt=17778 used=17641 remaining=137
[GIN] 2026/01/16 - 16:03:21 | 200 |   24.0370866s |       127.0.0.1 | POST     ""/v1/responses""
time=2026-01-16T16:03:21.583-05:00 level=DEBUG source=sched.go:385 msg=""context for request finished"" runner.name=registry.ollama.ai/library/gpt-oss:120b-128k runner.inference=""[{ID:GPU-94db278a-855e-2012-495e-be319102a97a Library:CUDA}]"" runner.size=""64.1 GiB"" runner.vram=""64.1 GiB"" runner.parallel=1 runner.pid=510340 runner.model=H:\ai\ollama\models\blobs\sha256-6be6d66a3f546d8c19b130dc41dc24b2fc159f84ffbc76a0ee0676205083cf5a runner.num_ctx=131072
time=2026-01-16T16:03:21.585-05:00 level=DEBUG source=sched.go:290 msg=""runner with non-zero duration has gone idle, adding timer"" runner.name=registry.ollama.ai/library/gpt-oss:120b-128k runner.inference=""[{ID:GPU-94db278a-855e-2012-495e-be319102a97a Library:CUDA}]"" runner.size=""64.1 GiB"" runner.vram=""64.1 GiB"" runner.parallel=1 runner.pid=510340 runner.model=H:\ai\ollama\models\blobs\sha256-6be6d66a3f546d8c19b130dc41dc24b2fc159f84ffbc76a0ee0676205083cf5a runner.num_ctx=131072 duration=2562047h47m16.854775807s
time=2026-01-16T16:03:21.585-05:00 level=DEBUG source=sched.go:308 msg=""after processing request finished event"" runner.name=registry.ollama.ai/library/gpt-oss:120b-128k runner.inference=""[{ID:GPU-94db278a-855e-2012-495e-be319102a97a Library:CUDA}]"" runner.size=""64.1 GiB"" runner.vram=""64.1 GiB"" runner.parallel=1 runner.pid=510340 runner.model=H:\ai\ollama\models\blobs\sha256-6be6d66a3f546d8c19b130dc41dc24b2fc159f84ffbc76a0ee0676205083cf5a runner.num_ctx=131072 refCount=0
time=2026-01-16T16:03:22.998-05:00 level=DEBUG source=sched.go:626 msg=""evaluating already loaded"" model=H:\ai\ollama\models\blobs\sha256-6be6d66a3f546d8c19b130dc41dc24b2fc159f84ffbc76a0ee0676205083cf5a
time=2026-01-16T16:03:41.606-05:00 level=DEBUG source=sched.go:626 msg=""evaluating already loaded"" model=H:\ai\ollama\models\blobs\sha256-6be6d66a3f546d8c19b130dc41dc24b2fc159f84ffbc76a0ee0676205083cf5a
time=2026-01-16T16:03:41.606-05:00 level=DEBUG source=sched.go:626 msg=""evaluating already loaded"" model=H:\ai\ollama\models\blobs\sha256-6be6d66a3f546d8c19b130dc41dc24b2fc159f84ffbc76a0ee0676205083cf5a
time=2026-01-16T16:03:41.608-05:00 level=DEBUG source=sched.go:161 msg=reloading runner.name=registry.ollama.ai/library/gpt-oss:120b-128k runner.inference=""[{ID:GPU-94db278a-855e-2012-495e-be319102a97a Library:CUDA}]"" runner.size=""64.1 GiB"" runner.vram=""64.1 GiB"" runner.parallel=1 runner.pid=510340 runner.model=H:\ai\ollama\models\blobs\sha256-6be6d66a3f546d8c19b130dc41dc24b2fc159f84ffbc76a0ee0676205083cf5a runner.num_ctx=131072
time=2026-01-16T16:03:41.608-05:00 level=DEBUG source=sched.go:236 msg=""resetting model to expire immediately to make room"" runner.name=registry.ollama.ai/library/gpt-oss:120b-128k runner.inference=""[{ID:GPU-94db278a-855e-2012-495e-be319102a97a Library:CUDA}]"" runner.size=""64.1 GiB"" runner.vram=""64.1 GiB"" runner.parallel=1 runner.pid=510340 runner.model=H:\ai\ollama\models\blobs\sha256-6be6d66a3f546d8c19b130dc41dc24b2fc159f84ffbc76a0ee0676205083cf5a runner.num_ctx=131072 refCount=1
time=2026-01-16T16:03:41.608-05:00 level=DEBUG source=sched.go:247 msg=""waiting for pending requests to complete and unload to occur"" runner.name=registry.ollama.ai/library/gpt-oss:120b-128k runner.inference=""[{ID:GPU-94db278a-855e-2012-495e-be319102a97a Library:CUDA}]"" runner.size=""64.1 GiB"" runner.vram=""64.1 GiB"" runner.parallel=1 runner.pid=510340 runner.model=H:\ai\ollama\models\blobs\sha256-6be6d66a3f546d8c19b130dc41dc24b2fc159f84ffbc76a0ee0676205083cf5a runner.num_ctx=131072
time=2026-01-16T16:03:46.591-05:00 level=DEBUG source=server.go:1509 msg=""completion request"" images=0 prompt=75593 format=""""
time=2026-01-16T16:03:47.038-05:00 level=DEBUG source=cache.go:142 msg=""loading cache slot"" id=0 cache=17887 prompt=17942 used=17807 remaining=135
[GIN] 2026/01/16 - 16:03:48 | 200 |   25.9359246s |       127.0.0.1 | POST     ""/v1/responses""
time=2026-01-16T16:03:48.811-05:00 level=DEBUG source=sched.go:385 msg=""context for request finished"" runner.name=registry.ollama.ai/library/gpt-oss:120b-128k runner.inference=""[{ID:GPU-94db278a-855e-2012-495e-be319102a97a Library:CUDA}]"" runner.size=""64.1 GiB"" runner.vram=""64.1 GiB"" runner.parallel=1 runner.pid=510340 runner.model=H:\ai\ollama\models\blobs\sha256-6be6d66a3f546d8c19b130dc41dc24b2fc159f84ffbc76a0ee0676205083cf5a runner.num_ctx=131072
time=2026-01-16T16:03:48.812-05:00 level=DEBUG source=sched.go:283 msg=""runner with zero duration has gone idle, expiring to unload"" runner.name=registry.ollama.ai/library/gpt-oss:120b-128k runner.inference=""[{ID:GPU-94db278a-855e-2012-495e-be319102a97a Library:CUDA}]"" runner.size=""64.1 GiB"" runner.vram=""64.1 GiB"" runner.parallel=1 runner.pid=510340 runner.model=H:\ai\ollama\models\blobs\sha256-6be6d66a3f546d8c19b130dc41dc24b2fc159f84ffbc76a0ee0676205083cf5a runner.num_ctx=131072
time=2026-01-16T16:03:48.812-05:00 level=DEBUG source=sched.go:308 msg=""after processing request finished event"" runner.name=registry.ollama.ai/library/gpt-oss:120b-128k runner.inference=""[{ID:GPU-94db278a-855e-2012-495e-be319102a97a Library:CUDA}]"" runner.size=""64.1 GiB"" runner.vram=""64.1 GiB"" runner.parallel=1 runner.pid=510340 runner.model=H:\ai\ollama\models\blobs\sha256-6be6d66a3f546d8c19b130dc41dc24b2fc159f84ffbc76a0ee0676205083cf5a runner.num_ctx=131072 refCount=0
```

### OS

Windows

### GPU

Nvidia

### CPU

AMD

### Ollama version

0.13.5",['bug'],0
ollama/ollama,Tool calling ignored when response_format is present in self-hosted Ollama (works on Ollama.com),"### What is the issue?

Tool calling behavior is inconsistent between Ollama.com and self-hosted Ollama instances when using the same model and request that includes both response_format and tool calling.

### Actual behavior

**On self-hosted Ollama :** When both tools and response_format are present in the request, response_format takes complete priority over tools. The model is forced to generate a hallucinated JSON response conforming to the schema without ever considering tool calls.

**Ollama.com:** Works correctly, using tool and formating response.

### Expected behavior

Self-hosted Ollama should behave consistently with Ollama.com (see result bellow)


### Environment

Self-hosted ollama setup 1 (Windows):
- Ollama version: 0.14.1
- GPU: RTX 4090
- OS: Windows

Self-hosted ollama setup 2 (macOS):
- Ollama version: 0.14.1
- OS: Apple M2 Max - Sequoia 15.6.1 (24G90)

Both produce identical broken behavior

Ollama.com version :
- Build commit: 9b984e8653945c87b53e5f0b95c0d6e29682de56
- Build date: January 15, 2025 (x-build-time: 1768538406)

### Model tested

ministral-3:14b 
(same observation with bigger models, but different from local and remove host)

### Reproduction

Request with both tools and response_format:

```bash
curl -X POST http://localhost:11434/v1/chat/completions \
  -H ""Content-Type: application/json"" \
  -d '{
    ""model"": ""ministral-3:14b"",
    ""messages"": [
      {
        ""role"": ""user"",
        ""content"": ""What is the weather in Paris?""
      }
    ],
    ""tools"": [
      {
        ""type"": ""function"",
        ""function"": {
          ""name"": ""get_weather"",
          ""description"": ""Get the current weather for a city"",
          ""parameters"": {
            ""type"": ""object"",
            ""properties"": {
              ""city"": {
                ""type"": ""string"",
                ""description"": ""The city name""
              }
            },
            ""required"": [""city""]
          }
        }
      }
    ],
    ""response_format"": {
      ""type"": ""json_schema"",
      ""json_schema"": {
        ""name"": ""weather_response"",
        ""strict"": true,
        ""schema"": {
          ""type"": ""object"",
          ""properties"": {
            ""city"": {
              ""type"": ""string""
            },
            ""temperature"": {
              ""type"": ""number""
            }
          },
          ""required"": [""city"", ""temperature""],
          ""additionalProperties"": false
        }
      }
    }
  }'
```

Result: Tool is completely ignored, model hallucinates structured response

```json
{
  ""choices"": [
    {
      ""message"": {
        ""role"": ""assistant"",
        ""content"": ""{\""city\"":\""Paris\"",\""temperature\"":15}""
      }
    }
  ]
}
```

The model returns a JSON matching the schema but never calls the tool. The temperature value is made up since no actual tool was executed.

Replacing url to ollama.com...

```bash
curl -X POST https://ollama.com/v1/chat/completions \
  -H ""Content-Type: application/json"" \
  -H ""Authorization: Bearer <API_KEY>"" \
```

...returns correct result calling tool and formating answer :

```json
{
  ""id"": ""chatcmpl-170"",
  ""object"": ""chat.completion"",
  ""created"": 1768588887,
  ""model"": ""ministral-3:14b"",
  ""system_fingerprint"": ""fp_ollama"",
  ""choices"": [{
    ""index"": 0,
    ""message"": {
      ""role"": ""assistant"",
      ""content"": """",
      ""tool_calls"": [{
        ""id"": ""YuxkvfCv8"",
        ""index"": 0,
        ""type"": ""function"",
        ""function"": {
          ""name"": ""get_weather"",
          ""arguments"": ""{\""city\"":\""Paris\""}""
        }
      }]
    },
    ""finish_reason"": ""tool_calls""
  }],
  ""usage"": {
    ""prompt_tokens"": 67,
    ""completion_tokens"": 12,
    ""total_tokens"": 79
  }
}
```

### Self hosted ollama Working case

On self hosted, tools work WITHOUT response_format :

```bash
curl -X POST http://localhost:11434/v1/chat/completions \
  -H ""Content-Type: application/json"" \
  -d '{
    ""model"": ""ministral-3:14b"",
    ""messages"": [
      {
        ""role"": ""user"",
        ""content"": ""What is the weather in Paris?""
      }
    ],
    ""tools"": [
      {
        ""type"": ""function"",
        ""function"": {
          ""name"": ""get_weather"",
          ""description"": ""Get the current weather for a city"",
          ""parameters"": {
            ""type"": ""object"",
            ""properties"": {
              ""city"": {
                ""type"": ""string"",
                ""description"": ""The city name""
              }
            },
            ""required"": [""city""]
          }
        }
      }
    ]
  }'
```

Result: Tool is called correctly

```json
{
  ""choices"": [
    {
      ""message"": {
        ""role"": ""assistant"",
        ""tool_calls"": [
          {
            ""type"": ""function"",
            ""function"": {
              ""name"": ""get_weather"",
              ""arguments"": ""{\""city\"":\""Paris\""}""
            }
          }
        ]
      }
    }
  ]
}
```

### Impact

This makes it impossible to use/develop structured output with tool calling on self-hosted instances. 

### Question
Does Ollama.com run a different version or have custom patches that enable this functionality? If so, can this behavior be backported to self-hosted instances? Or did I miss something obvious that would cause this dysfunction on self-hosted instances?
",[],0
ollama/ollama,Is Ollama good for production deployment for processing large data in batches?,"Hello! We're considering using Ollama for production deployment for some of our customers on-premise with local LLMs. We use batch processing to help LLMs access the full data to have a row-level intelligence. We would like to know if Batch Processing open source LLMs on large scale data could use Ollama as the default backend for this. 

Here's the project: https://github.com/vitalops/datatune",['question'],0
ollama/ollama,new: Image generation models (experimental) - not working,"### What is the issue?

Hello all,
thank you for ollama v0.14.1!
unfortunately I get an error on linux w/ nvidia GPU or CPU:

```
OLLAMA_NUM_CTX=2048 ollama run x/z-image-turbo                                                             
>>> cat in the rain

Error: 500 Internal Server Error: image runner failed: 2026/01/16 09:00:39 runner.go:65: INFO starting image runner model=x/z-image-turbo port=34187 (exit: exit status 255)
>>> /bye


  Model
    architecture    ZImagePipeline
    parameters      10.3B
    quantization    FP8
    requires        0.14.0

  Capabilities
    image
```


### Relevant log output

```shell
Jan 16 09:02:35 gerrit-systemproductname ollama[17756]: time=2026-01-16T09:02:35.999+01:00 level=INFO source=server.go:149 msg=""starting ollama-mlx image runner subprocess"" exe=/usr/local/bin/ollama-mlx model=x/z-image-turbo port=36725
Jan 16 09:02:36 gerrit-systemproductname ollama[17756]: time=2026-01-16T09:02:36.171+01:00 level=WARN source=server.go:141 msg=image-runner msg=""2026/01/16 09:02:36 runner.go:65: INFO starting image runner model=x/z-image-turbo port=36725""
Jan 16 09:02:36 gerrit-systemproductname ollama[17756]: time=2026-01-16T09:02:36.174+01:00 level=INFO source=server.go:134 msg=image-runner msg=""Loading Z-Image model from manifest: x/z-image-turbo...""
Jan 16 09:02:36 gerrit-systemproductname ollama[17756]: time=2026-01-16T09:02:36.439+01:00 level=INFO source=server.go:134 msg=image-runner msg=""  Loading tokenizer... âœ“""
Jan 16 09:02:37 gerrit-systemproductname ollama[17756]: time=2026-01-16T09:02:37.756+01:00 level=INFO source=server.go:134 msg=image-runner msg=""  Loading text encoder... âœ“""
Jan 16 09:02:38 gerrit-systemproductname ollama[17756]: time=2026-01-16T09:02:38.722+01:00 level=INFO source=server.go:134 msg=image-runner msg=""  (11.3 GB, peak 11.3 GB)""
Jan 16 09:02:40 gerrit-systemproductname ollama[17756]: time=2026-01-16T09:02:40.936+01:00 level=INFO source=server.go:134 msg=image-runner msg=""  Loading transformer... âœ“""
Jan 16 09:04:36 gerrit-systemproductname ollama[17756]: time=2026-01-16T09:04:36.000+01:00 level=INFO source=server.go:320 msg=""stopping image runner subprocess"" pid=27349
Jan 16 09:04:36 gerrit-systemproductname ollama[17756]: [GIN] 2026/01/16 - 09:04:36 | 500 |          2m0s |       127.0.0.1 | POST     ""/api/generate""
```

### OS

_No response_

### GPU

_No response_

### CPU

_No response_

### Ollama version

_No response_",['bug'],0
ollama/ollama,Is SDXL supported now?,"I found that z image is now supported, albeit experimental.
I would like to ask if there are plans to support sdxl, such as [RealVisXL 5.0 Lightning] (https://civitai.com/models/139562/realvisxl-v50), which is surprisingly fast even though it is a very old version.

Running z image on my m4, it takes more than one minute, which greatly reduces the user experience, and if you can, I would love that ollama can support SDXL

Thanks",['question'],0
ollama/ollama,gpt-oss:20b can't use image tool,"### What is the issue?

I've tried to use gpt-oss:20b in Chatbox to 

describe the picture


7 / 7

Deeply thought

(5.3s)

We need to do image description. It's a request to recognize image. We have [img-2] placeholder. Need to call internal tool. According to policy, we can use tools. The tool ""image recognition"" presumably available. We need to call it. Use appropriate format. Then provide description in English and translated to Russian.

We must follow tool syntax:{
""name"":""imagetools"",
""arguments"":{ ""image"":""[img-2]"" }
}

Then response.

imagetools

Arguments

{
  ""name"": ""imagetools"",
  ""arguments"": {
    ""image"": ""[img-2]""
  }
}
Result



{
  ""error"": ""Model tried to call unavailable tool 'imagetools'. Available tools: ."",
  ""input"": {
    ""name"": ""imagetools"",
    ""arguments"": {
      ""image"": ""[img-2]""
    }
  },
  ""toolName"": ""imagetools""
}
Deeply thought

(0.0s)

We must respond: can't call but should say can't access images.

I donâ€™t have the ability to view or analyze images directly, so I canâ€™t describe the content ofâ€¯[imgâ€‘2] for you. If you can provide a text description or specify what youâ€™d like to know about the image, Iâ€™ll do my best to help.

tokens used: 708, model: Ollama (gpt-oss:20b)

### Relevant log output

```shell

```

### OS

_No response_

### GPU

_No response_

### CPU

_No response_

### Ollama version

_No response_",['bug'],0
ollama/ollama,"Backend support ""--no-mmproj-offload"" arg to optimize vram use","When using a 32GB  GPU, the VRAM becomes extremely tight for VL models approaching 33GBï¼ˆeg. Qwen3 vl 30B Q8ï¼‰, as it is insufficient to accommodate most layers of the main language model alongside the visual projector simultaneously. This option can free up approximately several GB of VRAM, making this parameter extremely useful.And some times visual projector can not work well on old GPUsã€‚",['feature request'],0
ollama/ollama,Need Help: Model Upload Fails with â€œ413 Request Entity Too Largeâ€ for 100GB File,"### What is the issue?

Dear Ollama Team,

My account is AntAngelMed ([sikuaishitong@sina.com](mailto:sikuaishitong@sina.com)). I encountered an error while uploading a model, which is approximately 100GB in size. During the upload process, I consistently received the following prompt: ""Error: max retries exceeded: http status 413 Request Entity Too Large"".

The full error HTML response is:

<img width=""1113"" height=""227"" alt=""Image"" src=""https://github.com/user-attachments/assets/534032fb-2615-4768-b9d3-338fb2aaf7a4"" />

I have searched for related issues in the Ollama GitHub repository but found no existing solutions or discussions regarding this upload size limit and the 413 error.

Could you please advise on how to resolve this? Is there a different method or endpoint for uploading very large models, or perhaps a setting that needs to be adjusted on the server side? Any guidance would be greatly appreciated.

Thank you for your time and support.

### Relevant log output

```shell
root@owner-Lenovo-WenTian-WA5480-G3:/data/llm/AntAngelMed# ollama push AntAngelMed/AntAngelMed:100b-q8_0
retrieving manifest 
pushing 59059f0b4967...   1% â–•â–ˆ                                                                                                               â–   1 GB/109 GB                  
Error: max retries exceeded: http status 413 Request Entity Too Large: 
<html><head>
<meta http-equiv=""content-type"" content=""text/html;charset=utf-8"">
<title>413 Request Entity Too Large</title>
</head>
<body text=#000000 bgcolor=#ffffff>
<h1>Error: Request Entity Too Large</h1>
<h2>Your client issued a request that was too large.
</h2>
<h2></h2>
</body></html>
```

### OS

Linux

### GPU

Nvidia

### CPU

Intel

### Ollama version

0.13.5",['bug'],0
ollama/ollama,displaying usage,hi. will it be possible to display usage on to the home assistant,['feature request'],0
ollama/ollama,Release v0.14.1 for Linux (amd64) contains binary version 0.13.5,"### What is the issue?

pto04@EPS-PTO-04:/tmp/test_ollama$ curl -L -H 'Cache-Control: no-cache' -o ollama-linux-amd64.tar.zst https://github.com/ollama/ollama/releases/download/v0.14.1/ollama-linux-amd64.tar.zst
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
100 1696M  100 1696M    0     0  5474k      0  0:05:17  0:05:17 --:--:-- 6807k
pto04@EPS-PTO-04:/tmp/test_ollama$ mkdir -p /tmp/verify && cd /tmp/verify
pto04@EPS-PTO-04:/tmp/verify$ tar -I zstd -xvf ../ollama-linux-amd64.tar.zst
bin/ollama
bin/ollama-mlx
lib/ollama/cuda_v12/
lib/ollama/cuda_v12/libcudart.so.12.8.90
lib/ollama/cuda_v12/libcublasLt.so.12
lib/ollama/cuda_v12/libcublas.so.12.8.4.1
lib/ollama/cuda_v12/libcublas.so.12
lib/ollama/cuda_v12/libcublasLt.so.12.8.4.1
lib/ollama/cuda_v12/libggml-cuda.so
lib/ollama/cuda_v12/libcudart.so.12
lib/ollama/cuda_v13/
lib/ollama/cuda_v13/libcublas.so.13
lib/ollama/cuda_v13/libcublas.so.13.1.0.3
lib/ollama/cuda_v13/libcublasLt.so.13.1.0.3
lib/ollama/cuda_v13/libcublasLt.so.13
lib/ollama/cuda_v13/libcudart.so.13
lib/ollama/cuda_v13/libggml-cuda.so
lib/ollama/cuda_v13/libcudart.so.13.0.96
lib/ollama/libggml-base.so
lib/ollama/libggml-base.so.0
lib/ollama/libggml-base.so.0.0.0
lib/ollama/libggml-cpu-alderlake.so
lib/ollama/libggml-cpu-haswell.so
lib/ollama/libggml-cpu-icelake.so
lib/ollama/libggml-cpu-sandybridge.so
lib/ollama/libggml-cpu-skylakex.so
lib/ollama/libggml-cpu-sse42.so
lib/ollama/libggml-cpu-x64.so
lib/ollama/mlx_cuda_v13/
lib/ollama/mlx_cuda_v13/libcublas.so.13
lib/ollama/mlx_cuda_v13/libnvrtc.so.13.0.88
lib/ollama/mlx_cuda_v13/libcudnn_ops.so.9.17.1
lib/ollama/mlx_cuda_v13/libcudart.so
lib/ollama/mlx_cuda_v13/libcublas.so.13.1.0.3
lib/ollama/mlx_cuda_v13/libcudnn_engines_runtime_compiled.so.9.17.1
lib/ollama/mlx_cuda_v13/libcublasLt.so.13.1.0.3
lib/ollama/mlx_cuda_v13/libnccl.so.2.29.2
lib/ollama/mlx_cuda_v13/libmlxc.so
lib/ollama/mlx_cuda_v13/libmlx.so
lib/ollama/mlx_cuda_v13/libcudnn_engines_precompiled.so.9.17.1
lib/ollama/mlx_cuda_v13/libcublas.so
lib/ollama/mlx_cuda_v13/libcudnn_heuristic.so.9.17.1
lib/ollama/mlx_cuda_v13/libgfortran.so.5
lib/ollama/mlx_cuda_v13/libcudnn_heuristic.so.9
lib/ollama/mlx_cuda_v13/libnvrtc.so.13
lib/ollama/mlx_cuda_v13/libcublasLt.so.13
lib/ollama/mlx_cuda_v13/libcudnn_graph.so.9.17.1
lib/ollama/mlx_cuda_v13/libopenblas-r0.3.15.so
lib/ollama/mlx_cuda_v13/libcudnn_engines_runtime_compiled.so.9
lib/ollama/mlx_cuda_v13/libcudnn_adv.so.9.17.1
lib/ollama/mlx_cuda_v13/libcudnn_cnn.so.9
lib/ollama/mlx_cuda_v13/libcudart.so.13
lib/ollama/mlx_cuda_v13/libcudnn.so.9.17.1
lib/ollama/mlx_cuda_v13/libcudnn_graph.so.9
lib/ollama/mlx_cuda_v13/libnccl.so.2
lib/ollama/mlx_cuda_v13/libcudnn.so.9
lib/ollama/mlx_cuda_v13/libgfortran.so.5.0.0
lib/ollama/mlx_cuda_v13/libcudnn_engines_precompiled.so.9
lib/ollama/mlx_cuda_v13/libcudnn_cnn.so.9.17.1
lib/ollama/mlx_cuda_v13/libcudart.so.13.0.96
lib/ollama/mlx_cuda_v13/libcudnn_adv.so.9
lib/ollama/mlx_cuda_v13/libcudnn_ops.so.9
lib/ollama/mlx_cuda_v13/libopenblas.so.0
lib/ollama/vulkan/
lib/ollama/vulkan/libvulkan.so.1
lib/ollama/vulkan/libvulkan.so.1.4.321
lib/ollama/vulkan/libggml-vulkan.so
pto04@EPS-PTO-04:/tmp/verify$ ./bin/ollama --version
ollama version is 0.13.5
Warning: client version is 0.14.1
pto04@EPS-PTO-04:/tmp/verify$ sha256sum /tmp/verify/../ollama-linux-amd64.tar.zst
93729e94dd4bb60889a1a4987fc09f932a2bdadc6ecc30a6194afb384e48bd0b  /tmp/verify/../ollama-linux-amd64.tar.zst

### Relevant log output

```shell

```

### OS

_No response_

### GPU

_No response_

### CPU

_No response_

### Ollama version

_No response_",['bug'],0
ollama/ollama,"Since version 0.13.2, ollama cannot detect discret GPU Nvidia GeForce GTX 970M on hybrid laptop","# Ollama stop detecting my Nvidia card since ollama 0.13.2 although it works with 0.13.1

I have an hybrid laptop (integrated GPU + discret GPU) with Arch Linux, running `ollama-cuda12-bin` because the Nvidia Maxwell cards
does not support CUDA 13. Since ollama version `0.13.2`, ollama cannot run on my Nvidia GPU card
GTX 970M. Although in `ollama-cuda12-bin` version `0.13.1` my Nvidia card is well detected and
ollama runs on Nvidia.

Still buggy in version `0.14.1`

nvidia-smi -L :

    GPU 0: NVIDIA GeForce GTX 970M (UUID: GPU-ac90367f-1113-7f49-c527-d785b1684ce9)


<a id=""orgc4c4d42""></a>

# Log when it works in 0.13.1

Log when ollama starts correctly in 0.13.1 :

    source=routes.go:1544 msg=""server config"" env=""map[CUDA_VISIBLE_DEVICES:0,1,2,3 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:true OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/var/lib/ollama OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false OLLAMA_VULKAN:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]""
    source=images.go:522 msg=""total blobs: 26""
    source=images.go:529 msg=""total unused blobs removed: 0""
    source=routes.go:1597 msg=""Listening on 127.0.0.1:11434 (version 0.13.1)""
    source=runner.go:67 msg=""discovering available GPUs...""
    source=runner.go:484 msg=""user overrode visible devices"" CUDA_VISIBLE_DEVICES=0,1,2,3
    source=runner.go:488 msg=""if GPUs are not correctly discovered, unset and try again""
    source=server.go:392 msg=""starting runner"" cmd=""/usr/bin/ollama runner --ollama-engine --port 35545""
    source=server.go:392 msg=""starting runner"" cmd=""/usr/bin/ollama runner --ollama-engine --port 37359""
    source=types.go:42 msg=""inference compute"" id=GPU-ac90367f-1113-7f49-c527-d785b1684ce9 filter_id="""" library=CUDA compute=5.2 name=CUDA0 description=""NVIDIA GeForce GTX 970M"" libdirs=ollama driver=13.0 pci_id=0000:01:00.0 type=discrete total=""6.0 GiB"" available=""5.9 GiB""
    source=routes.go:1638 msg=""entering low vram mode"" ""total vram""=""6.0 GiB"" threshold=""20.0 GiB""

Log when running model gemma3:4b-it-qat  correctly in 0.13.1 for instance :

    [GIN] 2026/01/15 - 09:54:29 | 200 |     129.625Âµs |       127.0.0.1 | HEAD     ""/""
    [GIN] 2026/01/15 - 09:54:29 | 200 |    4.826244ms |       127.0.0.1 | GET      ""/api/tags""
    [GIN] 2026/01/15 - 09:54:43 | 200 |      24.179Âµs |       127.0.0.1 | HEAD     ""/""
    [GIN] 2026/01/15 - 09:54:43 | 200 |  107.799622ms |       127.0.0.1 | POST     ""/api/show""
    [GIN] 2026/01/15 - 09:54:43 | 200 |   84.906352ms |       127.0.0.1 | POST     ""/api/show""
    source=server.go:392 msg=""starting runner"" cmd=""/usr/bin/ollama runner --ollama-engine --port 42589""
    source=server.go:209 msg=""enabling flash attention""
    source=server.go:392 msg=""starting runner"" cmd=""/usr/bin/ollama runner --ollama-engine --model /var/lib/ollama/blobs/sha256-529850705c0884a283b87d3b261d36ee30821e16f0310962ba977b456ad3b8cd --port 39057""
    source=sched.go:443 msg=""system memory"" total=""31.2 GiB"" free=""23.1 GiB"" free_swap=""47.5 GiB""
    source=sched.go:450 msg=""gpu memory"" id=GPU-ac90367f-1113-7f49-c527-d785b1684ce9 library=CUDA available=""5.4 GiB"" free=""5.9 GiB"" minimum=""457.0 MiB"" overhead=""0 B""
    source=server.go:702 msg=""loading model"" ""model layers""=35 requested=-1
    source=runner.go:1398 msg=""starting ollama engine""
    source=runner.go:1433 msg=""Server listening on 127.0.0.1:39057""
    source=runner.go:1271 msg=load request=""{Operation:fit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:4096 KvCacheType: NumThreads:4 GPULayers:35[ID:GPU-ac90367f-1113-7f49-c527-d785b1684ce9 Layers:35(0..34)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}""
    source=ggml.go:136 msg="""" architecture=gemma3 file_type=Q4_0 name=""Gemma3 4b It Qa_0 Qat Hf"" description="""" num_tensors=883 num_key_values=42
    ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
    ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
    ggml_cuda_init: found 1 CUDA devices:
      Device 0: NVIDIA GeForce GTX 970M, compute capability 5.2, VMM: yes, ID: GPU-ac90367f-1113-7f49-c527-d785b1684ce9
    load_backend: loaded CUDA backend from /usr/lib/ollama/libggml-cuda.so
    load_backend: loaded CPU backend from /usr/lib/ollama/libggml-cpu-haswell.so
    source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,520,600,610,700,750,800,860,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
    source=runner.go:1271 msg=load request=""{Operation:alloc LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:4096 KvCacheType: NumThreads:4 GPULayers:35[ID:GPU-ac90367f-1113-7f49-c527-d785b1684ce9 Layers:35(0..34)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}""
    source=runner.go:1271 msg=load request=""{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:4096 KvCacheType: NumThreads:4 GPULayers:35[ID:GPU-ac90367f-1113-7f49-c527-d785b1684ce9 Layers:35(0..34)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}""
    source=ggml.go:482 msg=""offloading 34 repeating layers to GPU""
    source=ggml.go:489 msg=""offloading output layer to GPU""
    source=ggml.go:494 msg=""offloaded 35/35 layers to GPU""
    source=device.go:240 msg=""model weights"" device=CUDA0 size=""3.7 GiB""
    source=device.go:245 msg=""model weights"" device=CPU size=""1.3 GiB""
    source=device.go:251 msg=""kv cache"" device=CUDA0 size=""254.0 MiB""
    source=device.go:262 msg=""compute graph"" device=CUDA0 size=""1.2 GiB""
    source=device.go:267 msg=""compute graph"" device=CPU size=""5.0 MiB""
    source=device.go:272 msg=""total memory"" size=""6.4 GiB""
    source=sched.go:517 msg=""loaded runners"" count=1
    source=server.go:1294 msg=""waiting for llama runner to start responding""
    source=server.go:1328 msg=""waiting for server to become available"" status=""llm server loading model""
    source=server.go:1332 msg=""llama runner started in 9.58 seconds""
    [GIN] 2026/01/15 - 09:54:53 | 200 |  9.961565254s |       127.0.0.1 | POST     ""/api/generate""


<a id=""orga7fee46""></a>

# Log when it fails in version 0.14.1

Log when ollama starts in 0.14.1 :

    source=routes.go:1614 msg=""server config"" env=""map[CUDA_VISIBLE_DEVICES:0,1,2,3 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:true OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/var/lib/ollama OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false OLLAMA_VULKAN:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]""
    source=images.go:499 msg=""total blobs: 26""
    source=images.go:506 msg=""total unused blobs removed: 0""
    source=routes.go:1667 msg=""Listening on 127.0.0.1:11434 (version 0.14.1)""
    source=runner.go:67 msg=""discovering available GPUs...""
    source=runner.go:485 msg=""user overrode visible devices"" CUDA_VISIBLE_DEVICES=0,1,2,3
    source=runner.go:489 msg=""if GPUs are not correctly discovered, unset and try again""
    source=server.go:429 msg=""starting runner"" cmd=""/usr/bin/ollama runner --ollama-engine --port 38909""
    source=types.go:60 msg=""inference compute"" id=cpu library=cpu compute="""" name=cpu description=cpu libdirs=ollama driver="""" pci_id="""" type="""" total=""31.2 GiB"" available=""17.3 GiB""
    source=routes.go:1708 msg=""entering low vram mode"" ""total vram""=""0 B"" threshold=""20.0 GiB""
    [GIN] 2026/01/15 - 10:17:05 | 200 |      70.962Âµs |       127.0.0.1 | GET      ""/api/version""

Log when running model gemma3:4b-it-qat in 0.14.1 for instance :

    [GIN] 2026/01/15 - 10:21:30 | 200 |      29.904Âµs |       127.0.0.1 | HEAD     ""/""
    [GIN] 2026/01/15 - 10:21:31 | 200 |  209.123423ms |       127.0.0.1 | POST     ""/api/show""
    [GIN] 2026/01/15 - 10:21:31 | 200 |  189.735896ms |       127.0.0.1 | POST     ""/api/show""
    source=server.go:245 msg=""enabling flash attention""
    source=server.go:429 msg=""starting runner"" cmd=""/usr/bin/ollama runner --ollama-engine --model /var/lib/ollama/blobs/sha256-529850705c0884a283b87d3b261d36ee30821e16f0310962ba977b456ad3b8cd --port 41595""
    source=sched.go:452 msg=""system memory"" total=""31.2 GiB"" free=""17.3 GiB"" free_swap=""47.3 GiB""
    source=server.go:755 msg=""loading model"" ""model layers""=35 requested=-1
    source=runner.go:1405 msg=""starting ollama engine""
    source=runner.go:1440 msg=""Server listening on 127.0.0.1:41595""
    source=runner.go:1278 msg=load request=""{Operation:fit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Enabled KvSize:4096 KvCacheType: NumThreads:4 GPULayers:[] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}""
    source=ggml.go:136 msg="""" architecture=gemma3 file_type=Q4_0 name=""Gemma3 4b It Qa_0 Qat Hf"" description="""" num_tensors=883 num_key_values=42
    source=ggml.go:104 msg=system CPU.0.LLAMAFILE=1 compiler=cgo(gcc)
    source=runner.go:1278 msg=load request=""{Operation:alloc LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Enabled KvSize:4096 KvCacheType: NumThreads:4 GPULayers:[] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}""
    source=runner.go:1278 msg=load request=""{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Enabled KvSize:4096 KvCacheType: NumThreads:4 GPULayers:[] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}""
    source=ggml.go:482 msg=""offloading 0 repeating layers to GPU""
    source=ggml.go:486 msg=""offloading output layer to CPU""
    source=ggml.go:494 msg=""offloaded 0/35 layers to GPU""
    source=device.go:245 msg=""model weights"" device=CPU size=""5.0 GiB""
    source=device.go:256 msg=""kv cache"" device=CPU size=""254.0 MiB""
    source=device.go:267 msg=""compute graph"" device=CPU size=""126.9 MiB""
    source=device.go:272 msg=""total memory"" size=""5.3 GiB""
    source=sched.go:526 msg=""loaded runners"" count=1
    source=server.go:1347 msg=""waiting for llama runner to start responding""
    source=server.go:1381 msg=""waiting for server to become available"" status=""llm server loading model""
    source=server.go:1385 msg=""llama runner started in 8.88 seconds""
    [GIN] 2026/01/15 - 10:21:40 | 200 |  9.166107971s |       127.0.0.1 | POST     ""/api/generate""


### OS

Linux 6.18.5-arch1-1

### GPU

Intel Corporation Skylake-H GT2 [HD Graphics 530]
NVIDIA Corporation GM204M [GeForce GTX 960 OEM / 970M]

### CPU

Intel Core i7-6700HQ

### Ollama version

0.14.1",['bug'],0
ollama/ollama,image runner failed,"### What is the issue?

running  x/z-image-turbo:bf16

Internal Server Error: image runner failed: /usr/bin/ollama-mlx: error while loading shared libraries: libquadmath.so.0: cannot open shared object file: No such file or directory (exit: exit status 127)

### Relevant log output

```shell
llama  | time=2026-01-15T07:22:48.331Z level=WARN source=cpu_linux.go:130 msg=""failed to parse CPU allowed micro secs"" error=""strconv.ParseInt: parsing \""max\"": invalid syntax""
ollama  | time=2026-01-15T07:22:48.331Z level=INFO source=server.go:429 msg=""starting runner"" cmd=""/usr/bin/ollama runner --ollama-engine --port 37559""
ollama  | time=2026-01-15T07:22:49.817Z level=INFO source=server.go:149 msg=""starting ollama-mlx image runner subprocess"" exe=/usr/bin/ollama-mlx model=x/z-image-turbo:bf16 port=39333
ollama  | time=2026-01-15T07:22:50.021Z level=WARN source=server.go:141 msg=image-runner msg=""/usr/bin/ollama-mlx: error while loading shared libraries: libquadmath.so.0: cannot open shared object file: No such file or directory""
ollama  | time=2026-01-15T07:22:50.021Z level=INFO source=server.go:320 msg=""stopping image runner subprocess"" pid=201
```

### OS

Docker

### GPU

Nvidia

### CPU

Intel, AMD

### Ollama version

0.14.1",['bug'],0
ollama/ollama,Requesting workaround without `sudo` when Ollama fails to discover NVIDIA GPU,"In [the documentation](https://docs.ollama.com/gpu), it is mentioned that 

> sometimes Ollama will fail to discover your NVIDIA GPU...You can workaround this driver bug by reloading the NVIDIA UVM driver with `sudo rmmod nvidia_uvm && sudo modprobe nvidia_uvm`

What if `sudo` is closed to the user (which is the case in many organizations for security reasons)?

It seems that there's not a workaround in this case. Can someone help me with this? Thanks.

**Information about the system:**

**OS: ** RHEL 8.10
**Multi-GPU:** Yes

",['feature request'],0
ollama/ollama,404 not found when using OpenAI responses API  https://api.openai.com/v1/models/{model},"### What is the issue?

<img width=""1647"" height=""360"" alt=""Image"" src=""https://github.com/user-attachments/assets/020253dd-1130-44ae-98ba-4dc40e545366"" />
get https://api.openai.com/v1/models/{model}ï¼š This API does not support models whose IDs contain a slash (/).

I simply followed the 'How to use from Ollama' on the huggingface website.
ollama run hf.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-1M-GGUF:Q4_K_M
ok it works.However, the model ID is 'hf.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-1M-GGUF:Q4_K_M', which contains /.

get https://api.openai.com/v1/models works well.It returned all the models.

But get https://api.openai.com/v1/models/{model} connot support some of them.

### Relevant log output

```shell
â¯ curl http://127.0.0.1:11434/v1/models
{""object"":""list"",""data"":[{""id"":""hf.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-1M-GGUF:Q4_K_M"",""object"":""model"",""created"":1768216671,""owned_by"":""unsloth""},{""id"":""qwen3:30b"",""object"":""model"",""created"":1767370296,""owned_by"":""library""}]}

â¯ curl http://127.0.0.1:11434/v1/models/qwen3:30b
{""id"":""qwen3:30b"",""object"":""model"",""created"":1767370296,""owned_by"":""library""}

â¯ curl http://127.0.0.1:11434/v1/models/hf.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-1M-GGUF:Q4_K_M
404 page not found
-----------------------------------------------------------------------------------
[GIN] 2026/01/14 - 21:45:33 | 200 |       503.5Âµs |       127.0.0.1 | GET      ""/v1/models""
[GIN] 2026/01/14 - 21:45:37 | 200 |     48.8032ms |       127.0.0.1 | GET      ""/v1/models/qwen3:30b""
[GIN] 2026/01/14 - 21:45:41 | 404 |            0s |       127.0.0.1 | GET      ""/v1/models/hf.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-1M-GGUF:Q4_K_M""
```

### OS

_No response_

### GPU

_No response_

### CPU

_No response_

### Ollama version

_No response_",['bug'],0
ollama/ollama,Ministral tool call parser can't deal with nested objects and results in 500,"### What is the issue?

The tool call parser for ministral will stop at the first `}` it finds and try to unmarshal everything preceding it as JSON, which fails with nested structures.

https://github.com/ollama/ollama/blob/98079ddd79ad681957ef101dadd8fbb5060c7f6e/model/parsers/ministral.go#L111

This results in a `500` error saying `unexpected end of JSON input`, since the parser tries to unmarshal incomplete JSON.

As an example, a tool call like this fails to parse:

```
[TOOL_CALLS]create_entities[ARGS]{""entities"": [{""entityType"": ""Person"", ""name"": ""Jack"", ""observations"": [""Works as a baker at Big Baker Co.""]}]}
```

The parser will see the first `}` right after `Co.""]`, stop there, and try to unmarshal `{""entities"": [{""entityType"": ""Person"", ""name"": ""Jack"", ""observations"": [""Works as a baker at Big Baker Co.""]}`, which obviously fails.

See the (abridged) logs below for an example -- you can see GIN return a 500 right after the first `]}` as the parser errors out, while the runner keeps decoding the remaining `]}` and EOS.

### Relevant log output

```shell
Jan 13 14:03:46 beast ollama[551642]: time=2026-01-13T14:03:46.033-05:00 level=TRACE source=bytepairencoding.go:280 msg=decoded string="" Co"" from=[5361]
Jan 13 14:03:46 beast ollama[551642]: time=2026-01-13T14:03:46.033-05:00 level=TRACE source=runner.go:657 msg=""computeBatch: outputs are ready"" batchID=38
Jan 13 14:03:46 beast ollama[551642]: time=2026-01-13T14:03:46.033-05:00 level=TRACE source=runner.go:652 msg=""computeBatch: inputs are ready"" batchID=39
Jan 13 14:03:46 beast ollama[551642]: time=2026-01-13T14:03:46.033-05:00 level=TRACE source=runner.go:725 msg=""computeBatch: signaling computeStartedCh"" batchID=39
Jan 13 14:03:46 beast ollama[551642]: time=2026-01-13T14:03:46.033-05:00 level=TRACE source=runner.go:476 msg=""forwardBatch compute started, setting up next batch"" pendingBatch.id=39 id=40
Jan 13 14:03:46 beast ollama[551642]: time=2026-01-13T14:03:46.033-05:00 level=TRACE source=runner.go:598 msg=""forwardBatch iBatch"" batchID=40 seqIdx=0 seq.iBatch=0 i+1=1 len(seq.inputs)=1
Jan 13 14:03:46 beast ollama[551642]: time=2026-01-13T14:03:46.033-05:00 level=TRACE source=routes.go:2176 msg=""builtin parser input"" parser=ministral content="" Co""
Jan 13 14:03:46 beast ollama[551642]: time=2026-01-13T14:03:46.033-05:00 level=TRACE source=routes.go:2203 msg=""builtin parser empty output"" parser=ministral
Jan 13 14:03:46 beast ollama[551642]: time=2026-01-13T14:03:46.034-05:00 level=TRACE source=runner.go:474 msg=""forwardBatch waiting for compute to start"" pendingBatch.id=40
Jan 13 14:03:46 beast ollama[551642]: time=2026-01-13T14:03:46.034-05:00 level=TRACE source=runner.go:650 msg=""computeBatch: waiting for inputs to be ready"" batchID=40
Jan 13 14:03:46 beast ollama[551642]: time=2026-01-13T14:03:46.105-05:00 level=TRACE source=runner.go:733 msg=""computeBatch: logits ready"" batchID=39
Jan 13 14:03:46 beast ollama[551642]: time=2026-01-13T14:03:46.105-05:00 level=TRACE source=runner.go:738 msg=""computeBatch: decoding"" batchID=39
Jan 13 14:03:46 beast ollama[551642]: time=2026-01-13T14:03:46.105-05:00 level=TRACE source=runner.go:759 msg=""computeBatch: vocab details"" batchID=39 seqIdx=0 len(logits)=131072 len(activeBatch.batch.Outputs)=1 vocabSize=131072 iBatches=[0]
Jan 13 14:03:46 beast ollama[551642]: time=2026-01-13T14:03:46.106-05:00 level=TRACE source=bytepairencoding.go:280 msg=decoded string="".\"""" from=[2613]
Jan 13 14:03:46 beast ollama[551642]: time=2026-01-13T14:03:46.106-05:00 level=TRACE source=runner.go:657 msg=""computeBatch: outputs are ready"" batchID=39
Jan 13 14:03:46 beast ollama[551642]: time=2026-01-13T14:03:46.106-05:00 level=TRACE source=runner.go:652 msg=""computeBatch: inputs are ready"" batchID=40
Jan 13 14:03:46 beast ollama[551642]: time=2026-01-13T14:03:46.106-05:00 level=TRACE source=routes.go:2176 msg=""builtin parser input"" parser=ministral content="".\""""
Jan 13 14:03:46 beast ollama[551642]: time=2026-01-13T14:03:46.106-05:00 level=TRACE source=routes.go:2203 msg=""builtin parser empty output"" parser=ministral
Jan 13 14:03:46 beast ollama[551642]: time=2026-01-13T14:03:46.106-05:00 level=TRACE source=runner.go:725 msg=""computeBatch: signaling computeStartedCh"" batchID=40
Jan 13 14:03:46 beast ollama[551642]: time=2026-01-13T14:03:46.106-05:00 level=TRACE source=runner.go:476 msg=""forwardBatch compute started, setting up next batch"" pendingBatch.id=40 id=41
Jan 13 14:03:46 beast ollama[551642]: time=2026-01-13T14:03:46.106-05:00 level=TRACE source=runner.go:598 msg=""forwardBatch iBatch"" batchID=41 seqIdx=0 seq.iBatch=0 i+1=1 len(seq.inputs)=1
Jan 13 14:03:46 beast ollama[551642]: time=2026-01-13T14:03:46.107-05:00 level=TRACE source=runner.go:474 msg=""forwardBatch waiting for compute to start"" pendingBatch.id=41
Jan 13 14:03:46 beast ollama[551642]: time=2026-01-13T14:03:46.107-05:00 level=TRACE source=runner.go:650 msg=""computeBatch: waiting for inputs to be ready"" batchID=41
Jan 13 14:03:46 beast ollama[551642]: time=2026-01-13T14:03:46.182-05:00 level=TRACE source=runner.go:733 msg=""computeBatch: logits ready"" batchID=40
Jan 13 14:03:46 beast ollama[551642]: time=2026-01-13T14:03:46.182-05:00 level=TRACE source=runner.go:738 msg=""computeBatch: decoding"" batchID=40
Jan 13 14:03:46 beast ollama[551642]: time=2026-01-13T14:03:46.182-05:00 level=TRACE source=runner.go:759 msg=""computeBatch: vocab details"" batchID=40 seqIdx=0 len(logits)=131072 len(activeBatch.batch.Outputs)=1 vocabSize=131072 iBatches=[0]
Jan 13 14:03:46 beast ollama[551642]: time=2026-01-13T14:03:46.182-05:00 level=TRACE source=bytepairencoding.go:280 msg=decoded string=]} from=[16474]
Jan 13 14:03:46 beast ollama[551642]: time=2026-01-13T14:03:46.182-05:00 level=TRACE source=runner.go:657 msg=""computeBatch: outputs are ready"" batchID=40
Jan 13 14:03:46 beast ollama[551642]: time=2026-01-13T14:03:46.182-05:00 level=TRACE source=runner.go:652 msg=""computeBatch: inputs are ready"" batchID=41
Jan 13 14:03:46 beast ollama[551642]: time=2026-01-13T14:03:46.182-05:00 level=TRACE source=runner.go:725 msg=""computeBatch: signaling computeStartedCh"" batchID=41
Jan 13 14:03:46 beast ollama[551642]: time=2026-01-13T14:03:46.182-05:00 level=TRACE source=runner.go:476 msg=""forwardBatch compute started, setting up next batch"" pendingBatch.id=41 id=42
Jan 13 14:03:46 beast ollama[551642]: time=2026-01-13T14:03:46.182-05:00 level=TRACE source=runner.go:598 msg=""forwardBatch iBatch"" batchID=42 seqIdx=0 seq.iBatch=0 i+1=1 len(seq.inputs)=1
Jan 13 14:03:46 beast ollama[551642]: time=2026-01-13T14:03:46.183-05:00 level=TRACE source=routes.go:2176 msg=""builtin parser input"" parser=ministral content=]}
Jan 13 14:03:46 beast ollama[551642]: [GIN] 2026/01/13 - 14:03:46 | 500 |  8.202634104s |   192.168.2.130 | POST     ""/api/chat""
Jan 13 14:03:46 beast ollama[551642]: time=2026-01-13T14:03:46.183-05:00 level=DEBUG source=sched.go:537 msg=""context for request finished""
Jan 13 14:03:46 beast ollama[551642]: time=2026-01-13T14:03:46.183-05:00 level=DEBUG source=sched.go:290 msg=""runner with non-zero duration has gone idle, adding timer"" runner.name=registry.ollama.ai/library/ministral-3:14b-fixtools runner.inference=""[{ID:GPU-aebc1acf-8c25-c81d-2a78-2a9aae86afb8 Library:CUDA}]"" runner.size=""10.9 GiB"" runner.vram=""7.2 GiB"" runner.parallel=1 runner.pid=552323 runner.model=/usr/share/ollama/.ollama/models/blobs/sha256-bfb40fc6bb9c3b2ed529b480e04f824c005ea8f86733d4ebbf0c204de484891e runner.num_ctx=8192 duration=5m0s
Jan 13 14:03:46 beast ollama[551642]: time=2026-01-13T14:03:46.183-05:00 level=DEBUG source=sched.go:308 msg=""after processing request finished event"" runner.name=registry.ollama.ai/library/ministral-3:14b-fixtools runner.inference=""[{ID:GPU-aebc1acf-8c25-c81d-2a78-2a9aae86afb8 Library:CUDA}]"" runner.size=""10.9 GiB"" runner.vram=""7.2 GiB"" runner.parallel=1 runner.pid=552323 runner.model=/usr/share/ollama/.ollama/models/blobs/sha256-bfb40fc6bb9c3b2ed529b480e04f824c005ea8f86733d4ebbf0c204de484891e runner.num_ctx=8192 refCount=0
Jan 13 14:03:46 beast ollama[551642]: time=2026-01-13T14:03:46.183-05:00 level=TRACE source=runner.go:474 msg=""forwardBatch waiting for compute to start"" pendingBatch.id=42
Jan 13 14:03:46 beast ollama[551642]: time=2026-01-13T14:03:46.183-05:00 level=TRACE source=runner.go:650 msg=""computeBatch: waiting for inputs to be ready"" batchID=42
Jan 13 14:03:46 beast ollama[551642]: time=2026-01-13T14:03:46.258-05:00 level=TRACE source=runner.go:733 msg=""computeBatch: logits ready"" batchID=41
Jan 13 14:03:46 beast ollama[551642]: time=2026-01-13T14:03:46.258-05:00 level=TRACE source=runner.go:738 msg=""computeBatch: decoding"" batchID=41
Jan 13 14:03:46 beast ollama[551642]: time=2026-01-13T14:03:46.258-05:00 level=TRACE source=runner.go:759 msg=""computeBatch: vocab details"" batchID=41 seqIdx=0 len(logits)=131072 len(activeBatch.batch.Outputs)=1 vocabSize=131072 iBatches=[0]
Jan 13 14:03:46 beast ollama[551642]: time=2026-01-13T14:03:46.258-05:00 level=TRACE source=bytepairencoding.go:280 msg=decoded string=]} from=[16474]
Jan 13 14:03:46 beast ollama[551642]: time=2026-01-13T14:03:46.258-05:00 level=TRACE source=runner.go:657 msg=""computeBatch: outputs are ready"" batchID=41
Jan 13 14:03:46 beast ollama[551642]: time=2026-01-13T14:03:46.258-05:00 level=TRACE source=runner.go:652 msg=""computeBatch: inputs are ready"" batchID=42
Jan 13 14:03:46 beast ollama[551642]: time=2026-01-13T14:03:46.258-05:00 level=TRACE source=runner.go:725 msg=""computeBatch: signaling computeStartedCh"" batchID=42
Jan 13 14:03:46 beast ollama[551642]: time=2026-01-13T14:03:46.258-05:00 level=TRACE source=runner.go:476 msg=""forwardBatch compute started, setting up next batch"" pendingBatch.id=42 id=43
Jan 13 14:03:46 beast ollama[551642]: time=2026-01-13T14:03:46.258-05:00 level=TRACE source=runner.go:598 msg=""forwardBatch iBatch"" batchID=43 seqIdx=0 seq.iBatch=0 i+1=1 len(seq.inputs)=1
Jan 13 14:03:46 beast ollama[551642]: time=2026-01-13T14:03:46.259-05:00 level=TRACE source=runner.go:474 msg=""forwardBatch waiting for compute to start"" pendingBatch.id=43
Jan 13 14:03:46 beast ollama[551642]: time=2026-01-13T14:03:46.259-05:00 level=TRACE source=runner.go:650 msg=""computeBatch: waiting for inputs to be ready"" batchID=43
Jan 13 14:03:46 beast ollama[551642]: time=2026-01-13T14:03:46.331-05:00 level=TRACE source=runner.go:733 msg=""computeBatch: logits ready"" batchID=42
Jan 13 14:03:46 beast ollama[551642]: time=2026-01-13T14:03:46.331-05:00 level=TRACE source=runner.go:738 msg=""computeBatch: decoding"" batchID=42
Jan 13 14:03:46 beast ollama[551642]: time=2026-01-13T14:03:46.331-05:00 level=TRACE source=runner.go:759 msg=""computeBatch: vocab details"" batchID=42 seqIdx=0 len(logits)=131072 len(activeBatch.batch.Outputs)=1 vocabSize=131072 iBatches=[0]
Jan 13 14:03:46 beast ollama[551642]: time=2026-01-13T14:03:46.331-05:00 level=TRACE source=runner.go:773 msg=""computeBatch: EOS"" batchID=42 seqIdx=0
Jan 13 14:03:46 beast ollama[551642]: time=2026-01-13T14:03:46.331-05:00 level=TRACE source=runner.go:657 msg=""computeBatch: outputs are ready"" batchID=42
Jan 13 14:03:46 beast ollama[551642]: time=2026-01-13T14:03:46.331-05:00 level=TRACE source=runner.go:652 msg=""computeBatch: inputs are ready"" batchID=43
Jan 13 14:03:46 beast ollama[551642]: time=2026-01-13T14:03:46.331-05:00 level=TRACE source=runner.go:725 msg=""computeBatch: signaling computeStartedCh"" batchID=43
Jan 13 14:03:46 beast ollama[551642]: time=2026-01-13T14:03:46.331-05:00 level=TRACE source=runner.go:476 msg=""forwardBatch compute started, setting up next batch"" pendingBatch.id=43 id=44
Jan 13 14:03:46 beast ollama[551642]: time=2026-01-13T14:03:46.405-05:00 level=TRACE source=runner.go:733 msg=""computeBatch: logits ready"" batchID=43
Jan 13 14:03:46 beast ollama[551642]: time=2026-01-13T14:03:46.405-05:00 level=TRACE source=runner.go:738 msg=""computeBatch: decoding"" batchID=43
Jan 13 14:03:46 beast ollama[551642]: time=2026-01-13T14:03:46.405-05:00 level=TRACE source=runner.go:657 msg=""computeBatch: outputs are ready"" batchID=43
```

### OS

Linux

### GPU

Nvidia

### CPU

Intel

### Ollama version

0.13.4",['bug'],0
ollama/ollama,Outdated documentation on VSCode Integration,"### What is the issue?

The documentation (https://docs.ollama.com/integrations/vscode) for using Ollama in VSCode is outdated. The recent VSCode Copilot version has deprecated the model selection process shown in the screenshots. It looks more like this now.

<img width=""1026"" height=""460"" alt=""Image"" src=""https://github.com/user-attachments/assets/ee0f3ecd-e026-4427-8b92-53249ff1a356"" />

### Relevant log output

```shell

```

### OS

_No response_

### GPU

_No response_

### CPU

_No response_

### Ollama version

0.13.5","['bug', 'documentation']",0
kubernetes/kubernetes,Pod-level limits defaults not set when all containers have limits set and pod-level request is set,"As per the requirements listed in the KEP, pod-level limits are set to aggregated container limits if all containers have limits set. Please check rows 10, 12 from the table in the KEP:

https://github.com/kubernetes/enhancements/blob/d57336d85b8673959916529873e71010a5d2a4fd/keps/sig-node/2837-pod-level-resource-spec/README.md?plain=1#L503-L507

The implementation is setting only pod-level requests defaults as per different scenarios. 
https://github.com/kubernetes/kubernetes/blob/98e6935d436d5068b3549e651e629f76ec96e3f7/pkg/apis/core/v1/defaults.go#L198

The bug fix involves adding the defaulting logic for pod-level limits such that when pod-level requests are set, and pod-level limits are unset and ""all containers""  have limits set then pod-level limits should be defaulted to aggregated container limits.

Note: This might require the implementation to be guarded behind a feature gate as this changes the default behavior. The decision about using a feature gate is subject to further discussion and will be updated during the week of 1/12","['kind/bug', 'sig/node', 'help wanted', 'good first issue', 'triage/accepted']",1
kubernetes/kubernetes,Error-level logs incorrectly use verbosity levels,"Currently, some components in Kubernetes have been migrated (or partially migrated) to [contextual logging](https://github.com/kubernetes/enhancements/blob/master/keps/sig-instrumentation/3077-contextual-logging/README.md). However, an incorrect pattern was introduced during the migration: Error-level logs are being called with verbosity methods (e.g., .V()).

The underlying package go-logr/logr, which contextual logging relies on, ignores the verbosity level when printing Error-level logs. For example, in the statement `logger.V(8).Error(nil, ""Readiness probe already exists for container"", ""pod"", klog.KObj(pod), ""containerName"", c.Name)`, the error log will be printed regardless of the verbosity level set.

This can lead to error log flooding, which may hinder users' ability to review normal logs for troubleshooting. More critically, according to the [Kubernetes logging policy](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-instrumentation/logging.md#what-method-to-use), logs should only be recorded when the information requires action from an administrator. This directly contradicts the practice of setting verbosity levels for Error logs, as administrators using the default logging level might miss these actionable errors. Therefore, these instances need to be fixed.

Using grep, I identified the following instances in the k/k repository where verbosity levels are incorrectly set for Error logs.
- [ ] /pkg/controller/garbagecollector https://github.com/kubernetes/kubernetes/pull/136040
```
./pkg/controller/garbagecollector/garbagecollector.go:363:			logger.V(5).Error(err, ""error syncing item"", ""item"", n.identity)
./pkg/controller/garbagecollector/graph_builder.go:225:		logger.V(4).Error(err, ""unable to use a shared informer"", ""resource"", resource, ""kind"", kind)
```
- [ ] /pkg/controller/resourcequota https://github.com/kubernetes/kubernetes/pull/136040
```
./pkg/controller/resourcequota/resource_quota_monitor.go:178:	logger.V(4).Error(err, ""QuotaMonitor unable to use a shared informer"", ""resource"", resource.String())
```
- [x] /pkg/controller/storageversionmigrator/ https://github.com/kubernetes/kubernetes/pull/136046
```
./pkg/controller/storageversionmigrator/storageversionmigrator.go:227:		logger.V(4).Error(err, ""resource does not exist in our rest mapper"", ""gvr"", gvr.String())
./pkg/controller/storageversionmigrator/storageversionmigrator.go:244:		logger.V(4).Error(errMonitor, ""resource does not exist in GC"", ""gvr"", gvr.String())
./pkg/controller/storageversionmigrator/storageversionmigrator.go:311:			logger.V(4).Error(err, ""Unable to compare the resource version of the resource"", ""namespace"", accessor.GetNamespace(), ""name"", accessor.GetName(), ""gvr"", gvr.String(), ""accessorRV"", accessor.GetResourceVersion(), ""listResourceVersion"", listResourceVersion, ""error"", err.Error())
./pkg/controller/storageversionmigrator/storageversionmigrator.go:361:			logger.V(4).Error(errPatch, ""Failed to migrate the resource"", ""namespace"", accessor.GetNamespace(), ""name"", accessor.GetName(), ""gvr"", gvr.String(), ""reason"", apierrors.ReasonForError(errPatch))
```
- [ ] ./pkg/controller/volume/ https://github.com/kubernetes/kubernetes/pull/136050
```
./pkg/controller/volume/selinuxwarning/selinux_warning_controller.go:458:			logger.V(4).Error(err, ""failed to get SELinux label"", ""pod"", klog.KObj(pod), ""volume"", mount)
./pkg/controller/volume/selinuxwarning/selinux_warning_controller.go:514:			logger.V(2).Error(err, ""failed to get first pod for event"", ""pod"", conflict.Pod)
```
- [ ] ./pkg/kubelet/volumemanager/  https://github.com/kubernetes/kubernetes/pull/136033
```
./pkg/kubelet/volumemanager/cache/desired_state_of_world.go:673:	logger.V(4).Error(err, ""Please report this error in https://github.com/kubernetes/enhancements/issues/1710, together with full Pod yaml file"")
./pkg/kubelet/volumemanager/cache/desired_state_of_world.go:688:		logger.V(4).Error(err, ""failed to get CSI driver name from volume spec"")
./pkg/kubelet/volumemanager/reconciler/reconstruct.go:188:		logger.V(4).Error(fetchErr, ""Failed to get Node status to reconstruct device paths"")
```


- [x] ./pkg/kubelet https://github.com/kubernetes/kubernetes/pull/136028
```
./pkg/kubelet/stats/helper.go:502:		logger.V(6).Error(err, ""Unable to fetch pod log stats"", ""pod"", klog.KRef(podNs, podName))
./pkg/kubelet/stats/helper.go:510:		logger.V(6).Error(err, ""Unable to fetch pod etc hosts stats"", ""pod"", klog.KRef(podNs, podName))
./pkg/kubelet/stats/cri_stats_provider.go:168:		logger.V(5).Error(err,
./pkg/kubelet/cm/devicemanager/plugin/v1beta1/client.go:107:			logger.V(2).Error(err, ""Failed to close grpc connection"", ""resource"", c.Resource())
./pkg/kubelet/cm/dra/plugin/dra_plugin_manager.go:398:					logger.V(3).Error(err, ""Failed to establish WatchResources stream, will retry"")
./pkg/kubelet/kuberuntime/kuberuntime_container_linux.go:431:			logger.V(5).Error(fmt.Errorf(""failed to parse /proc/self/cgroup: %w"", err), warn)
./pkg/kubelet/kuberuntime/kuberuntime_container_linux.go:442:			logger.V(5).Error(err, warn)
./pkg/kubelet/lifecycle/handlers.go:85:			logger.V(1).Error(err, ""Exec lifecycle hook for Container in Pod failed"", ""execCommand"", handler.Exec.Command, ""containerName"", container.Name, ""pod"", klog.KObj(pod), ""message"", string(output))
./pkg/kubelet/lifecycle/handlers.go:93:			logger.V(1).Error(err, ""HTTP lifecycle hook for Container in Pod failed"", ""path"", handler.HTTPGet.Path, ""containerName"", container.Name, ""pod"", klog.KObj(pod))
./pkg/kubelet/lifecycle/handlers.go:101:			logger.V(1).Error(err, ""Sleep lifecycle hook for Container in Pod failed"", ""sleepSeconds"", handler.Sleep.Seconds, ""containerName"", container.Name, ""pod"", klog.KObj(pod))
./pkg/kubelet/lifecycle/handlers.go:152:		logger.V(1).Error(err, ""HTTPS request to lifecycle hook got HTTP response, retrying with HTTP."", ""pod"", klog.KObj(pod), ""host"", req.URL.Host)
./pkg/kubelet/prober/prober_manager.go:197:				logger.V(8).Error(nil, ""Startup probe already exists for container"",
./pkg/kubelet/prober/prober_manager.go:209:				logger.V(8).Error(nil, ""Readiness probe already exists for container"",
./pkg/kubelet/prober/prober_manager.go:221:				logger.V(8).Error(nil, ""Liveness probe already exists for container"",
./pkg/kubelet/prober/prober.go:106:		logger.V(1).Error(err, ""Probe errored"", ""probeType"", probeType, ""pod"", klog.KObj(pod), ""podUID"", pod.UID, ""containerName"", container.Name, ""probeResult"", result)
./pkg/kubelet/metrics/collectors/cri_metrics.go:110:		logger.V(5).Error(err, ""Descriptor not present in pre-populated list of descriptors"", ""name"", m.Name)
./pkg/kubelet/pleg/generic.go:300:			g.logger.V(4).Error(err, ""PLEG: Ignoring events for pod"", ""pod"", klog.KRef(pod.Namespace, pod.Name))
./pkg/kubelet/allocation/allocation_manager.go:576:		logger.V(3).Error(err, ""Failed to delete pod allocation"", ""podUID"", uid)
```","['sig/node', 'sig/apps', 'help wanted', 'good first issue', 'triage/accepted']",1
kubernetes/kubernetes,resource.MustParse fails to parse quantities near math.MaxInt64,"### What happened?

When parsing very large quantity values, i.e., values close to `math.MaxInt64`, `k8s.io/apimachinery/pkg/api/resource.MustParse` is unable to correctly interpret them. Similar parsing failures also occur when these high values are used within or passed to CEL validation rules.

### What did you expect to happen?

Quantities within the valid range of `[-math.MaxInt64, math.MaxInt64]` should be parsed correctly. Values exceeding this range should be cleanly rounded to the nearest valid boundary or causing parsing errors.

### How can we reproduce it (as minimally and precisely as possible)?

```go
q := resource.MustParse(strconv.FormatInt(math.MaxInt64, 10))
fmt.Println(q.AsInt64())

fmt.Println(resource.NewQuantity(math.MaxInt64, resource.DecimalSI).AsInt64())
```

**Actual Output**
```
0 false
9223372036854775807 true
```
The first line shows that resource.MustParse failed to parse the maximum int64 value, yielding 0 and indicating failure. The second line shows that constructing the quantity directly succeeds and returns the correct value.

### Anything else we need to know?

_No response_

### Kubernetes version

<details>

```
k8s.io/apimachinery v0.34.2
```

</details>


### Cloud provider

<details>
N/A
</details>


### OS version

<details>

```console
$ cat /etc/os-release
NAME=""Fedora Linux""
VERSION=""43 (Workstation Edition)""
RELEASE_TYPE=stable
ID=fedora
VERSION_ID=43
VERSION_CODENAME=""""
PRETTY_NAME=""Fedora Linux 43 (Workstation Edition)""
ANSI_COLOR=""0;38;2;60;110;180""
LOGO=fedora-logo-icon
CPE_NAME=""cpe:/o:fedoraproject:fedora:43""
DEFAULT_HOSTNAME=""fedora""
HOME_URL=""https://fedoraproject.org/""
DOCUMENTATION_URL=""https://docs.fedoraproject.org/en-US/fedora/f43/""
SUPPORT_URL=""https://ask.fedoraproject.org/""
BUG_REPORT_URL=""https://bugzilla.redhat.com/""
REDHAT_BUGZILLA_PRODUCT=""Fedora""
REDHAT_BUGZILLA_PRODUCT_VERSION=43
REDHAT_SUPPORT_PRODUCT=""Fedora""
REDHAT_SUPPORT_PRODUCT_VERSION=43
SUPPORT_END=2026-12-02
VARIANT=""Workstation Edition""
VARIANT_ID=workstation

$ uname -a
Linux client-8877 6.17.7-300.fc43.x86_64 #1 SMP PREEMPT_DYNAMIC Sun Nov  2 15:30:09 UTC 2025 x86_64 GNU/Linux
```

</details>


### Install tools

<details>

</details>


### Container runtime (CRI) and version (if applicable)

<details>

</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>
","['kind/bug', 'sig/api-machinery', 'sig/apps', 'help wanted', 'good first issue', 'needs-triage']",1
kubernetes/kubernetes,device manager: potential Double-Locking of Mutex,"### What happened?

In the file [pod_devices.go](https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/cm/devicemanager/pod_devices.go#L101), there is a potential issue of double-locking a mutex in the function `podDevices`.
- In line [102](https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/cm/devicemanager/pod_devices.go#L102), the read lock (`pdev.RLock()`) is acquired in the `podDevices` function to ensure safe access to `pdev.devs`.
- Later, on line [107](https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/cm/devicemanager/pod_devices.go#L107), `podDevices` calls [containerDevices](https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/cm/devicemanager/pod_devices.go#L114), which also attempts to acquire the same read lock via another call to `pdev.RLock()` on line [115](https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/cm/devicemanager/pod_devices.go#L115). This may result in double-locking the same mutex within the same thread.

Even though both `podDevices` and `containerDevices` only use read locks (`RLock()`), double-locking a mutex, even for reads, can lead to deadlocks, if there is another routine trying to acquire `Lock()` in between the `RLock()`s, according to the documentation of [RWMutex](https://pkg.go.dev/sync#RWMutex).

The standard `sync.Mutex()` in **Go** is not a recursive locking implementation while it is used recursively in the `podDevices` function. You can read more on why go does not implement recursive locking [here](https://groups.google.com/g/golang-nuts/c/XqW1qcuZgKg/m/Ui3nQkeLV80J).

### What did you expect to happen?

The expectation is that a mutex should not be double-locked within the same thread. In this case, either the locking logic needs to be restructured to prevent multiple acquisitions of the same lock or `containerDevices` should not attempt to lock the mutex if it is already locked by `podDevices`.

### How can we reproduce it (as minimally and precisely as possible)?

This issue is identified through static analysis, so it cannot be directly reproduced via runtime observation. However, if left unresolved, it could lead to unpredictable behavior in environments where recursive read locks are not supported.

### Anything else we need to know?

Sponsorship and Support:

This work is done by the security researchers from OpenRefactory and is supported by the [Open Source Security Foundation (OpenSSF)](https://openssf.org/): [Project Alpha-Omega](https://alpha-omega.dev/). Alpha-Omega is a project partnering with open source software project maintainers to systematically find new, as-yet-undiscovered vulnerabilities in open source code - and get them fixed â€“ to improve global software supply chain security.

The bug is found by running the Intelligent Code Repair (iCR) tool by [OpenRefactory, Inc.](https://openrefactory.com/) and then manually triaging the results.

### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


### Cloud provider

<details>

</details>


### OS version

*No response*

### Install tools

*No response*

### Container runtime (CRI) and version (if applicable)

*No response*

### Related plugins (CNI, CSI, ...) and versions (if applicable)

*No response*","['kind/bug', 'sig/node', 'help wanted', 'priority/important-longterm', 'good first issue', 'triage/accepted']",1
kubernetes/kubernetes,add and use alternative APIs which support contextual logging,"### What would you like to be added?

Several staging repos, for example client-go, only have APIs which don't support contextual logging. Changing those APIs is typically not desirable because it can break large parts of the downstream ecosystem. Instead, we need to add alternative APIs (usually called `<something>WithContext`) which accept a context.

Such a context can replace an existing stop channel, add cancellation when there was no such stop channel, and last but not least be used to retrieve a logger for contextual logging. When using `context.WithCancelCause` in the caller and `context.Cause(ctx)` in the callee, additional information about the reason for canceling can be passed down, thus avoiding the uninformative ""context canceled"" messages.

The process for adding such alternatives is 

- Add an alternative `<something>WithContext` API to some package in staging
- Mark the original API with `// Contextual logging: <something>WithContext should be used instead of <something> in code which supports contextual logging.`
- Review that change with maintainers and merge it
- Temporarily replace `// Contextual logging: ...` with `//logcheck:context // ...`:

      sed -i -e 's;// *Contextual logging: ;//logcheck:context // ;' -e 's/Handle\(Error\|Crash\)WithContext should be used instead of/Handle\1WithContext or Handle\1WithLogger should be used instead of/'  $(git grep -l ""// *Contextual logging: "")

- Run `hack/verify-golang-lint.sh` to find code which is supposed to support contextual logging and now needs to be updated to use `<something>WithContext` or (where available and applicable) `<something>WithLogger`
- Sometimes tests can or even should (for example, in the package itself) continue to use the function that got replaced. Use common sense to decide and `//nolint:logcheck // Intentionally using original function.` to suppress such warnings.
- Fix relevant issues, submit in one or more PRs:
  - try to align with OWNERS files so that a single approver is sufficient for a PR
  - reference `https://github.com/kubernetes/kubernetes/issues/126379` in the issue section of the PR description
- Once all of those PRs are merged, submit a PR which replaces the `// Contextual logging:  ...` with `//logcheck:context // ...`

This issue will be kept open and (if needed) reopened as long as there is work remaining, which can be checked with:
```
git grep  -e '// Contextual logging:'
```

### Special notes for contributors:

Please reach out on [Slack in the #wg-structured-logging channel](https://kubernetes.slack.com/archives/C020CCMUEAX) if there are questions.

Use `/wg structured-logging` to label PRs.

### Why is this needed?

Several of our binaries (e.g. kube-controller-manager, kube-scheduler) have been converted to structured, contextual logging. However, they still emit unstructured, context-unaware log output because of the helper packages that they use.

- https://github.com/kubernetes/enhancements/issues/1602
- https://github.com/kubernetes/enhancements/issues/3077","['area/logging', 'kind/feature', 'help wanted', 'sig/instrumentation', 'good first issue', 'triage/accepted', 'wg/structured-logging']",1
kubernetes/kubernetes,"kubelet parameter(eviction-max-pod-grace-period ), not work as expected like officical comment.","### What happened?

Hey brothers , feedback a confusing point about kubelet soft eviction:

Just like kubelet --help show:
--eviction-max-pod-grace-period int32 Maximum allowed grace period (in seconds) to use when terminating pods in response to a soft eviction threshold being met. If negative, defer to pod specified value.

I guess if this parameter is set to negative ,such as -1, soft eviction would use pod specified value(TerminationGracePeriodSeconds),

But When I try to evict pod by creating node pressure(such as memory pressure), I found it's always -1 send to CRI runtime.
Then pod container stopped immediately with sigkill, exit 137.

In shortï¼Œ this parameterï¼š
ï¼ˆ1ï¼‰set as active numberï¼šwork as expectedï¼›
ï¼ˆ2ï¼‰no setï¼Œ kubelet read its default numberï¼š0ï¼Œ work as expectedï¼›
ï¼ˆ3ï¼‰set as negative number: not work as kubelet help show.

Thanks for your response. May it be a bug ?

### What did you expect to happen?

--eviction-max-pod-grace-period set to negative, then kubelet syncPod logic send podâ€˜s TerminationGracePeriodSeconds as  gracePeriod to CRI runtime to stop container.

### How can we reproduce it (as minimally and precisely as possible)?

(1) set this parameter to a negative number, such as -1;
(2) config kubelet soft eviction ;
(3) running a pod asking a lot memory , to trigger node memory pressure;
(4) check CRI runtime log, such as containerd ;
(5) then you will find timeout parameter -1 sent to pod container , then container exit with code 137(sigkill);

### Anything else we need to know?

I running these tests on Tencent Cloud TKE. 

And I thought it seem not related to Cloud Service Provider. 

### Kubernetes version

<details>

Client Version: version.Info{Major:""1"", Minor:""16+"", GitVersion:""v1.16.3-tke.34"", GitCommit:""61cc96d2f7e9277e89e29b4b04d045f27d6e75df"", GitTreeState:""clean"", BuildDate:""2023-03-08T07:40:04Z"", GoVersion:""go1.12.17"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""16+"", GitVersion:""v1.16.3-tke.32"", GitCommit:""8ca5807ae095365d68754a2ba20b10bbe5a8998c"", GitTreeState:""clean"", BuildDate:""2022-10-08T04:00:33Z"", GoVersion:""go1.12.14"", Compiler:""gc"", Platform:""linux/amd64""}
</details>


### Cloud provider

<details>
Tencent Cloud TKE
</details>


### OS version

<details>
AME=""Ubuntu""
VERSION=""18.04.4 LTS (Bionic Beaver)""
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME=""Ubuntu 18.04.4 LTS""
VERSION_ID=""18.04""
HOME_URL=""https://www.ubuntu.com/""
SUPPORT_URL=""https://help.ubuntu.com/""
BUG_REPORT_URL=""https://bugs.launchpad.net/ubuntu/""
PRIVACY_POLICY_URL=""https://www.ubuntu.com/legal/terms-and-policies/privacy-policy""
VERSION_CODENAME=bionic
UBUNTU_CODENAME=bionic

</details>


### Install tools

<details>

</details>


### Container runtime (CRI) and version (if applicable)

<details>

containerd --version 
containerd github.com/containerd/containerd v1.4.3-tke.2 a11500cbfa0b4d2fc9b905e03c35f349ef5b1a9f

</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>
","['kind/bug', 'priority/backlog', 'kind/documentation', 'sig/node', 'help wanted', 'priority/important-longterm', 'good first issue', 'triage/accepted']",1
kubernetes/kubernetes,Reuse the http request object for http probes,"Today we construct the request object for every http request for the http probes. This involves concatenating some strings, setting headers, etc.

Probes are simple http get requests that are executed synchronously (for a single probe) and has no request body.

The optimization may be to reuse the request object for all probe executions. This likely save some cpu and memory for the kubelet process.

Some code references:
- [DoProbe](https://github.com/kubernetes/kubernetes/blob/4dfd2fe74d96f67c9752bbdc9ae9983a6e5f0492/pkg/probe/http/http.go#L93) is executed today on each probe execution.
- New request are [being created](https://github.com/kubernetes/kubernetes/blob/4dfd2fe74d96f67c9752bbdc9ae9983a6e5f0492/pkg/kubelet/prober/prober.go#L146) for each execution.
- New object will need to be stored in [worker](https://github.com/kubernetes/kubernetes/blob/4dfd2fe74d96f67c9752bbdc9ae9983a6e5f0492/pkg/kubelet/prober/worker.go#L40) that for http probes will hold the http request object. This object will need to encapsulate the logic of [runProbe](https://github.com/kubernetes/kubernetes/blob/4dfd2fe74d96f67c9752bbdc9ae9983a6e5f0492/pkg/kubelet/prober/prober.go#L138) method of the prober.

I'm marking this issue as a good first issue, as the issue doesn't require deep knowledge of Kubernetes. However it is not an easy issue, a few parts of code will be affected and change needs to be done very carefully.

***Please review code and make sure you understand the task before self-assigning the issue.***

Implementation notes

1. If you can send the small benchmark results on probes with and without this optimization, it will be a good starting point. Note, for the benchmark, each probe can run once a second, not more often. But the number of probes is virtually unlimited since we are not defining the limit on number of containers per Pod. Realistically we are talking about 110 pods (max) with maybe 2 containers each, with 3 probes each (660 probes).
2. Code change may be big. So make sure to help reviewers to review the code. One idea may be to split implementation into multiple commits to simplify the review. First, creating the object encapsulating the logic of `runProbe`, second storing and initializing this object in `worker`, and third, actual reusing of the request object.


/sig node
/good-first-issue
/help-wanted
/area kubelet","['area/kubelet', 'sig/node', 'help wanted', 'good first issue', 'needs-triage']",1
kubernetes/kubernetes,Give an indication in container events for probe failure as to whether the failure was ignored due to FailureThreshold,"Probes of all kinds currently support FailureThreshold (and SuccessThreshold), these properties allow a user to specify that Kubernetes should not take action in response to a failed probe unless it fails a successive number of times.

This is useful for  end-users as it allows them to mitigate the effects of any probes that ""flake"" by requiring successive failure.

When a probe fails in Kubernetes, we emit a container event indicating this here: https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/prober/prober.go#L110 and end-users can consume these events via the API for their own purposes. This event is emitted regardless of whether the FailureThreshold has been reached or not.

Currently when a user consumes a probe failure event they have no way of knowing whether the event resulted in action on the control plane (because the event can be ignored due to FailureThreshold, and information on this is not included in the event). This can lead to users assuming there is a problem and a container/pod was restarted when nothing occurred.

I think we should expose the keepGoing value from https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/prober/worker.go#L203 in the emitted event somehow, my preferred solution is to emit the probe failure event in the worker rather than where it currently sits in https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/prober/prober.go#L110 - there is also the option of passing some information down the stack into the prober from the worker (such as making the FailureThreshold/SuccessThreshold decision in the prober) but I'm worried about separation of concerns, happy to hear what other folks think :)

Also of note is that FailureThreshold/SuccessThreshold is the only filter I can see where a probe can be ignored after being run (and therefore emitting a container event)

Iâ€™m happy to write this PR once weâ€™re confident in our approach :)
","['priority/backlog', 'kind/documentation', 'kind/cleanup', 'sig/node', 'help wanted', 'good first issue', 'triage/accepted']",1
kubernetes/kubernetes,"Write the stress test for gRPC, http, and tcp probes","Write a new e2e test that will be marked as `[Serial]` and will be covering this bug fix: https://github.com/kubernetes/kubernetes/issues/89898

The test will serve as a replacement for the skipped unit test: https://github.com/kubernetes/kubernetes/pull/115329

The logic of the test will be as following:

- Create MANY containers (see the unit test logic)
- Each container has a liveness probe with `1s` interval
- After some time validate that neither of containers has `restartCount > 0`

/cc @aojea 
/sig node
/kind cleanup
/priority important-soon
/area tests
/help-wanted
/good-first-issue

This is part of KEP: https://github.com/kubernetes/enhancements/issues/2727 GA requirements, but also need the same tests for http and tcp.","['priority/backlog', 'kind/cleanup', 'sig/node', 'help wanted', 'good first issue', 'needs-triage']",1
kubernetes/kubernetes,NetworkPolicy tests for blocking north/south traffic,"The NP docs point out that [NP's semantics for north/south traffic are not very clearly defined](https://kubernetes.io/docs/concepts/services-networking/network-policies/#behavior-of-ipblock-selectors):

> Cluster ingress and egress mechanisms often require rewriting the source or destination IP of packets. In cases where this happens, it is not defined whether this happens before or after NetworkPolicy processing, and the behavior may be different for different combinations of network plugin, cloud provider, `Service` implementation, etc.
> 
> In the case of ingress, this means that in some cases you may be able to filter incoming packets based on the actual original source IP, while in other cases, the ""source IP"" that the NetworkPolicy acts on may be the IP of a `LoadBalancer` or of the Pod's node, etc.
> 
> For egress, this means that connections from pods to `Service` IPs that get rewritten to cluster-external IPs may or may not be subject to `ipBlock`-based policies.

However, what's _not_ ambiguous is that a pod which is fully isolated for ingress should not accept E/W _or_ N/S traffic. Regardless of how the ingress/LB/cloud handles the traffic, it should end up getting blocked. (Well, unless it gets masqueraded to the pod's node IP, because then it hits that exception to the rules.)

Unfortunately we can't say the same for egress; I think we assume that a pod-to-service-IP connection will be allowed to reach kube-proxy even if the pod is fully isolated-for-egress, but we explicitly don't require that `ipBlock` policies get applied after service proxying.

Anyway, we should be able to add a test to `test/e2e/network/netpol/network_policy.go` that confirms that cluster-ingress traffic to a fully isolated-for-ingress pod is not allowed. In particular, if we create a LoadBalancer Service with `externalTrafficPolicy: Local`, and a NetworkPolicy blocking all ingress to that Service's Pods, then we should not be able to connect to the service via either the LoadBalancer IP/name or via its NodePort (on one of the correct nodes).

/sig network
/area network-policy
/priority backlog
/help
/good-first-issue","['priority/backlog', 'sig/network', 'help wanted', 'good first issue', 'triage/accepted', 'area/network-policy']",1
kubernetes/kubernetes,Node lifecycle controller does not `markPodsNotReady` when the node `Ready` state changes from `false` to `unknown`,"### What happened?

When kubelet loses connect, the node goes into the unknown state. The node lifecycle controller marks the pod as not ready by the `markPodsNotReady` function because the health check status of the pod can not be obtained through kubelet. This feature is available only when node's `Ready` state transitions from `true` to `unknown`. 

However, if the node is already in the fail state (such as a containerd failure), `markPodsNotReady` will not take effect if the node loses its connection at this time.

https://github.com/kubernetes/kubernetes/blob/cac53883f4714452f3084a22e4be20d042a9df33/pkg/controller/nodelifecycle/node_lifecycle_controller.go#L883-L888

In this case, the pod may accidentally remain ready, which may cause some network traffic to be accidentally forwarded to this node.

### What did you expect to happen?

As long as the node loses its connection beyond grace time, `MarkPodsNotReady` should always work

### How can we reproduce it (as minimally and precisely as possible)?

1. Stop containerd and wait for the node `Ready` state to false
2. Stop kubelet or shutdown the node and wait the node `Ready` state to unknown
3. The pods which not be evicted on this node would be always ready

### Anything else we need to know?

In the node lifecycle controller logic,`MarkPodsNotReady` is just triggered when a node goes from `true` state to an `unknown` state. The correct way is to trigger when the node becomes `unknown` state regardless of whether the node state was previously true

### Kubernetes version

<details>

```console
$ kubectl version
Server Version: version.Info{Major:""1"", Minor:""22"", GitVersion:""v1.22.15"", GitCommit:""1d79bc3bcccfba7466c44cc2055d6e7442e140ea"", GitTreeState:""clean"", BuildDate:""2022-09-22T06:03:36Z"", GoVersion:""go1.16.15"", Compiler:""gc"", Platform:""linux/amd64""}
```

</details>


### Cloud provider

<details>

</details>


### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release

$ uname -a
5.4.119-1-tlinux4-0008 #1 SMP Fri Nov 26 11:17:45 CST 2021 x86_64 x86_64 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


### Install tools

<details>

</details>


### Container runtime (CRI) and version (if applicable)

<details>

</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>
","['kind/bug', 'sig/node', 'help wanted', 'good first issue', 'triage/accepted']",1
kubernetes/kubernetes,tracker: improve the kubelet test coverage,"There is a strong and growing consensus in sig-node about the need of increasing the test coverage in the kubelet, improving the current testsuite about coverage and reliability. See for example the [April 26 sig-node meeting notes](https://docs.google.com/document/d/1Ne57gvidMEWXR70OxxnRkYquAoMpt56o75oZtg-OeBg/edit#heading=h.d9zp2j5jvkke) and the [April 27 sig-node CI subgroup meeting notes](https://docs.google.com/document/d/1fb-ugvgdSVIkkuJ388_nhp2pBTy_4HEVg5848Xy7n5U/edit#heading=h.2v8vzknys4nk).

This work has already begun and there are already some PR posted (see: https://github.com/kubernetes/kubernetes/pull/108024#pullrequestreview-954083698). This is a great start. There are more areas in the kubelet, especially in the container manager and resource manager area, that can use better unit test coverage.

Besides the obvious benefits of documenting and preserving the current behaviour, adding tests is meant to lower the barrier for future work and contributions.

Examples of some areas which can benefit of more tests:

- k/k/pkg/kubelet/cm/internal_container_lifecycle.go
- k/k/pkg/kubelet/cm/internal_container_lifecycle_linux.go
- k/k/pkg/kubelet/cm/pod_container_manager_linux.go
- k/k/pkg/kubelet/cm/qos_container_manager_linux.go
- everything coverage-driven (check [test code coverage](https://go.dev/blog/cover) and build from there)
- error paths in general
-  e2e tests in general","['sig/node', 'help wanted', 'good first issue', 'needs-triage']",1
kubernetes/kubernetes,go vet lostcancel errors in legacy-cloud-providers files,"### What happened?

We found go vet issues of `lostcancel` in #109184. (It seems that go vet checks are not running under `/staging` dir.)

```
staging/src/k8s.io/legacy-cloud-providers/vsphere/nodemanager.go:190:5: lostcancel: the cancel function is not used on all paths (possible context leak) (govet)
				ctx, cancel := context.WithCancel(context.Background())
				^
staging/src/k8s.io/legacy-cloud-providers/vsphere/nodemanager.go:236:3: lostcancel: this return statement may be reached without using the cancel var defined on line 190 (govet)
		}()
```

We should fix them to ensure we don't introduce context leak as the message said.

### What did you expect to happen?

These issues from go vet are fixed.

### How can we reproduce it (as minimally and precisely as possible)?

Running go vet under `staging/src/k8s.io/legacy-cloud-providers/`

### Anything else we need to know?

_No response_

### Kubernetes version

latest master

### Cloud provider

<details>

</details>


### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


### Install tools

<details>

</details>


### Container runtime (CRI) and version (if applicable)

<details>

</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>
","['kind/bug', 'good first issue', 'sig/cloud-provider', 'needs-triage']",1
kubernetes/kubernetes,TOB-K8S-004: Pervasive world-accessible file permissions,"This issue was reported in the [Kubernetes Security Audit Report](https://github.com/kubernetes/community/blob/master/wg-security-audit/findings/Kubernetes%20Final%20Report.pdf)

**Description**
Kubernetes uses files and directories to store information ranging from key-value data to certificate data to logs. However, a number of locations have world-writable directories:

```
cluster/images/etcd/migrate/rollback_v2.go:110:	if err :=
os.MkdirAll(path.Join(migrateDatadir, ""member"", ""snap""), 0777); err != nil {
cluster/images/etcd/migrate/data_dir.go:49:		err := os.MkdirAll(path, 0777)
cluster/images/etcd/migrate/data_dir.go:87:	err = os.MkdirAll(backupDir, 0777)
third_party/forked/godep/save.go:472:	err := os.MkdirAll(filepath.Dir(dst), 0777)
third_party/forked/godep/save.go:585:	err := os.MkdirAll(filepath.Dir(name), 0777)
pkg/volume/azure_file/azure_util.go:34:	defaultFileMode = ""0777""
pkg/volume/azure_file/azure_util.go:35:	defaultDirModeÂ  = ""0777""
pkg/volume/emptydir/empty_dir.go:41:const perm os.FileMode = 0777
```
Figure 7.1: World-writable (0777) directories and defaults

Other areas of the system use world-writable files as well:
```
cluster/images/etcd/migrate/data_dir.go:147:	return ioutil.WriteFile(v.path, data, 0666)
cluster/images/etcd/migrate/migrator.go:120:	err := os.Mkdir(backupDir, 0666)
third_party/forked/godep/save.go:589:	return ioutil.WriteFile(name, []byte(body), 0666)
pkg/kubelet/kuberuntime/kuberuntime_container.go:306:			if err := m.osInterface.Chmod(containerLogPath, 0666); err != nil {
pkg/volume/cinder/cinder_util.go:271:			ioutil.WriteFile(name, data, 0666)
pkg/volume/fc/fc_util.go:118:	io.WriteFile(fileName, data, 0666)
pkg/volume/fc/fc_util.go:128:			io.WriteFile(name, data, 0666)
pkg/volume/azure_dd/azure_common_linux.go:77:			if err = io.WriteFile(name, data, 0666); err != nil {
pkg/volume/photon_pd/photon_util.go:55:	ioutil.WriteFile(fileName, data, 0666)
pkg/volume/photon_pd/photon_util.go:65:			ioutil.WriteFile(name, data, 0666)
```
Figure 7.2: World-writable (0666) files

A number of locations in the code base also rely on world-readable directories and files. For example, Certificate Signing Requests (CSRs) are written to a directory with mode 0755 (world readable and browseable) with the actual CSR having mode 0644 (world-readable):

```
// WriteCSR writes the pem-encoded CSR data to csrPath.
// The CSR file will be created with file mode 0644.
// If the CSR file already exists, it will be overwritten.
// The parent directory of the csrPath will be created as needed with file mode 0755.
func WriteCSR(csrDir, name string, csr *x509.CertificateRequest) error {
    ... 
    if err := os.MkdirAll(filepath.Dir(csrPath), os.FileMode(0755)); err != nil {
        ...
    }   

    if err := ioutil.WriteFile(csrPath, EncodeCSRPEM(csr), os.FileMode(0644)); err != nil {
       ... 
    }   
    ...
}
```
Figure 7.3: Documentation and code from cmd/kubeadm/app/util/pkiutil/pki_helpers.go

**Exploit Scenario**
Alice wishes to migrate some etcd values during normal cluster maintenance. Eve has local access to the clusterâ€™s filesystem, and modifies the values stored during the migration process, granting Eve further access to the cluster as a whole.

**Recommendation**
Short term, audit all locations that use world-accessible permissions. Revoke those that are unnecessary. Very few files truly need to be readable by any user on a system. Almost none should need to allow arbitrary system users write access. 

Long term, use system groups and extended Access Control Lists (ACLs) to ensure that all files and directories created by Kuberenetes are accessible by only those users and groups that should be able to access them. This will ensure that only the appropriate users with the correct Unix-level groups may access data. Kubernetes may describe what these groups should be, or create a role-based system to which administrators may assign users and groups.

**Anything else we need to know?**:

See #81146 for current status of all issues created from these findings.

The vendor gave this issue an ID of TOB-K8S-004 and it was finding 8 of the report.

The vendor considers this issue Medium Severity.

To view the original finding, begin on page 32 of the [Kubernetes Security Review Report](https://github.com/kubernetes/community/blob/master/wg-security-audit/findings/Kubernetes%20Final%20Report.pdf)

**Environment**:

- Kubernetes version: 1.13.4","['kind/bug', 'area/security', 'sig/storage', 'sig/node', 'help wanted', 'priority/important-longterm', 'lifecycle/frozen', 'good first issue', 'wg/security-audit', 'triage/accepted']",1
kubernetes/kubernetes,best-effort topolopgy mgr NUMANodeAffinity hint doesn't consider amount of devices per NUMA node,"### What happened?

### Very simply put: 

Given a 2 NUMA node server with 3 devices split across each node, (1 on node 0, 2 on node 1), is it possible to configure the topology manager to select the NUMA node in which 2/3 GPUs presides when requesting a pod with all 3 devices ? 

### Context:

In our current setup we would like to schedule NUMA localized pods in a best-effort manner, meaning pods can span multiple NUMA nodes without failing.

We have a server with 128 cores, dual socket, configured as two NUMA nodes. 

```
NUMA:
  NUMA node(s):              2
  NUMA node0 CPU(s):         0-31,64-95
  NUMA node1 CPU(s):         32-63,96-127

```

This server has 3 GPUs. We have written our own device manager plugin (as we want to enable vfio passthrough) and this plugin advertises which node each GPU is on. (0x10de is nvidia vendor)

```
root@gpu3:~# for d in /sys/bus/pci/devices/*; do
  [[ -f ""$d/vendor"" && ""$(cat $d/vendor)"" == ""0x10de"" ]] && \
  printf ""%s -> NUMA %s\n"" ""$(basename $d)"" ""$(cat $d/numa_node)""
done
0000:25:00.0 -> NUMA 0
0000:81:00.0 -> NUMA 1
0000:e2:00.0 -> NUMA 1
```

This setup works exactly as desired when requesting resources which fit into a single NUMA node. 

- Example, deploy pod 2 GPUs & 8 CPUs

plugin logs
```
INFO:app:Requested pci-vfio-0000:81:00.0                                                                                   
INFO:app:Passing through 0000:81:00.0 (10de:26b9, group 128, NUMA 1)                                       
INFO:app:Requested pci-vfio-0000:e2:00.0                                                                                    
INFO:app:Passing through 0000:e2:00.0 (10de:26b9, group 76, NUMA 1) 
```

check cpusets, mem, hugepages

```
root@gpu3:~# cat /sys/fs/cgroup/kubepods.slice/kubepods-podef0e85df_024a_4c41_a502_40c814f064a4.slice/cri-containerd-99111a12737d73c168b65189103c256a65424122f0ac531c0409fd21794d8748.scope/cpuset.cpus
32-33,96-97
root@gpu3:~# cat /sys/fs/cgroup/kubepods.slice/kubepods-podef0e85df_024a_4c41_a502_40c814f064a4.slice/cri-containerd-99111a12737d73c168b65189103c256a65424122f0ac531c0409fd21794d8748.scope/cpuset.mems
1
root@gpu3:~# cat /sys/fs/cgroup/kubepods.slice/kubepods-podef0e85df_024a_4c41_a502_40c814f064a4.slice/cri-containerd-99111a12737d73c168b65189103c256a65424122f0ac531c0409fd21794d8748.scope/hugetlb.2MB.numa_stat
total=8589934592 N0=0 N1=8589934592
```
Allocated from NUMA Node 1 ðŸ‘ 

- However, when scheduling a pod with 3 GPUs and 8 CPUs 

plugin logs
```
INFO:app:Container request: available devices by NUMA: {1: 2, 0: 1}                                                         
INFO:app:Preferred allocation chosen for request: ['pci-vfio-0000:81:00.0', 'pci-vfio-0000:e2:00.0', 'pci-vfio-0000:25:00.0 
INFO:app:Requested pci-vfio-0000:e2:00.0                                                                                    
INFO:app:Passing through 0000:e2:00.0 (10de:26b9, group 76, NUMA 1)                                 
INFO:app:Requested pci-vfio-0000:25:00.0                                                                                   
INFO:app:Passing through 0000:25:00.0 (10de:26b9, group 38, NUMA 0)                                          
INFO:app:Requested pci-vfio-0000:81:00.0                                                                                    
INFO:app:Passing through 0000:81:00.0 (10de:26b9, group 128, NUMA 1) 
```

check cpuset, mem sets, hugepage allocation
```
root@gpu3:~# cat /sys/fs/cgroup/kubepods.slice/kubepods-pod835d2c37_dba7_45bf_b878_8dc11dcc0aad.slice/cri-containerd-253ca9c30e78b318b230f57e1f4224412ecea847a9d8e7614c40125e136c4915.scope/cpuset.cpus
1-40,65-104
root@gpu3:~# cat /sys/fs/cgroup/kubepods.slice/kubepods-pod835d2c37_dba7_45bf_b878_8dc11dcc0aad.slice/cri-containerd-253ca9c30e78b318b230f57e1f4224412ecea847a9d8e7614c40125e136c4915.scope/cpuset.mems
0
root@gpu3:~# cat /sys/fs/cgroup/kubepods.slice/kubepods-pod835d2c37_dba7_45bf_b878_8dc11dcc0aad.slice/cri-containerd-253ca9c30e78b318b230f57e1f4224412ecea847a9d8e7614c40125e136c4915.scope/hugetlb.2MB.numa_stat
total=53687091200 N0=53687091200 N1=0
```

Result:  allocating from NUMA node 0 when only 1/3 GPUs resides in this node. ðŸ‘Ž 


### Further Notes

Seems the NUMANodeAffinity bitmask calculation does not seem to consider preferences in this context and just returns 3

```
Jan 22 12:45:27 gpu3 kubelet[31781]: I0122 12:45:27.435463   31781 scope_container.go:83] ""TopologyHints"" hints={""cpu"":[{""NUMANodeAffinity"":3,""Preferred"":true}]} pod=""xxx/test-xxx"" containerName=""instance""
Jan 22 12:45:27 gpu3 kubelet[31781]: I0122 12:45:27.435531   31781 scope_container.go:83] ""TopologyHints"" hints={""hugepages-2Mi"":[{""NUMANodeAffinity"":1,""Preferred"":true},{""NUMANodeAffinity"":2,""Preferred"":true}],""memory"":[{""NUMANodeAffinity"":1,""Preferred"":true},{""NUMANodeAffinity"":2,""Preferred"":true}]} pod=""xxx/test-xxx"" containerName=""instance""
Jan 22 12:45:27 gpu3 kubelet[31781]: I0122 12:45:27.435554   31781 scope_container.go:91] ""ContainerTopologyHint"" bestHint={""NUMANodeAffinity"":1,""Preferred"":false} pod=""xxx/test-xxx"" containerName=""instance""
Jan 22 12:45:27 gpu3 kubelet[31781]: I0122 12:45:27.435568   31781 scope_container.go:50] ""Best TopologyHint"" bestHint={""NUMANodeAffinity"":1,""Preferred"":false} pod=""xxx/test-xxx"" containerName=""instance""
Jan 22 12:45:27 gpu3 kubelet[31781]: I0122 12:45:27.435580   31781 scope_container.go:59] ""Topology Affinity"" bestHint={""NUMANodeAffinity"":1,""Preferred"":false} pod=""xxx/test-xxx"" containerName=""instance""
```

### Kubelet Configuration

```
cpuManagerPolicy: static
memoryManagerPolicy: Static
topologyManagerPolicy: best-effort
topologyManagerPolicyOptions:
  prefer-closest-numa-nodes: ""true""  # This had no effect whether enabled or disabled
featureGates:
  TopologyManagerPolicyOptions: true
reservedSystemCPUs: ""0,64""
kubeReserved:
  cpu: ""3""
  memory: ""2Gi""
systemReserved:
  cpu: ""1""
  memory: ""1Gi""
evictionHard:
  memory.available: ""1Gi""
  nodefs.available: ""10%""
  imagefs.available: ""15%""
reservedMemory:
- numaNode: 0
  limits:
    memory: ""2Gi""
- numaNode: 1
  limits:
    memory: ""2Gi""
```

### What did you expect to happen?

Ideally prefer NUMA node with most devices

### How can we reproduce it (as minimally and precisely as possible)?

1. Create garaunteed pod with 3 nvidia gpus on 2 NUMA node system, likely where 1 is on node 0 and 2 are on node 1

```
    resources:
          limits:
            cpu: ""8""
            memory: 4Gi
            nvidia.com/gpu: 3
```

2. Observe where resources are allocated


### Anything else we need to know?

Can provide Device Plugin code snippets upon request, however I think the logic driving this decision presides with the Topology Manager. 

### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.33.3
Kustomize Version: v5.6.0
Server Version: v1.34.1
```

</details>


### Cloud provider

<details>
Bare Metal
</details>


### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME=""Debian GNU/Linux 13 (trixie)""
NAME=""Debian GNU/Linux""
VERSION_ID=""13""
VERSION=""13 (trixie)""
VERSION_CODENAME=trixie
DEBIAN_VERSION_FULL=13.2
ID=debian
HOME_URL=""https://www.debian.org/""
SUPPORT_URL=""https://www.debian.org/support""
BUG_REPORT_URL=""https://bugs.debian.org/""
IMAGE_ID=""worker""
IMAGE_VERSION=""main-2025-11-18-20-10""
$ uname -a
Linux gpu3 6.12.57+deb13-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.12.57-1 (2025-11-05) x86_64 GNU/Linux

```

</details>


### Install tools

<details>
Kubeadm
</details>


### Container runtime (CRI) and version (if applicable)

<details>
containerd://1.7.24
</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
n/a
</details>
","['kind/bug', 'sig/node', 'needs-triage']",0
kubernetes/kubernetes,Problem with parameters when creating `NewClientset`,"Hi. I found unusual behavior in `fixture.go`:
https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/client-go/testing/fixture.go#L540

In all these functions (create, patch, delete, etc.), we ignore `opts`. Iâ€™m running into a problem when trying to set a new `TypeMeta`, but the options in the object was ignored.

**Why is it problem:**
We use `DeepEqual` in the code (not in a test), and in tests Iâ€™m trying to switch from `NewSimpleClientset` to `NewClientset`. But the new `clientset` sets `TypeMeta`, which causes `DeepEqual` to fail. The new `clientset` ignores the previous all values, if i set it in the `reactor` too because:
https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/client-go/testing/fixture.go#L780","['sig/api-machinery', 'needs-triage']",0
kubernetes/kubernetes,Export `validateAffinity()` as `ValidateAffinity()` for external validation consistency,"**Problem Statement**:
The `validateAffinity()` function in `k8s.io/kubernetes/pkg/apis/core/validation` package is currently _private_ (unexported), which forces external projects that need to validate Affinity configurations to duplicate ~200 lines of validation logic.

If we need to validate Affinity, then we have to replicate the entire `validateAffinity()` logic along with its helper functions (`validateNodeAffinity`, `validatePodAffinity`, `validatePodAntiAffinity`, etc.) to pre-validate user-provided affinity configurations before deploying pods.
This duplication:
- Creates maintenance burden (keeping in sync with upstream changes)
- Risks validation drift between client-side and server-side logic
- Duplicates code that already exists and is well-tested in Kubernetes

_Similar Functions ARE already exported_:
The same package has validation methods exported for NodeSelector(`ValidateNodeSelector`), Tolerations(`ValidateTolerations`), but not for affinity.

**Proposed Solution**:
A simple, non-breaking wrapper following the existing pattern:
```
// ValidateAffinity checks if given affinities are valid.
func ValidateAffinity(affinity *core.Affinity, opts PodValidationOptions, fldPath *field.Path) field.ErrorList {
    return validateAffinity(affinity, opts, fldPath)
}
```

_Implementation impact_:
- Zero changes to internal logic : the private function remains unchanged
- Full backward compatibility : existing code paths unaffected
- Follows established pattern : mirrors how `ValidateTolerations` and `ValidateNodeSelector` are exported.

**Benefits**:
- Bug Discovery: External projects using server-side validation logic help identify bugs earlier. The same validation code is used by kube-apiserver, so bugs found externally benefit the entire ecosystem.
- Validation Accuracy: Pre-validating configurations using the exact same logic as the API server ensures users get consistent error messages and behavior.

**Addressing the use of** `k8s.io/kubernetes`
While `k8s.io/kubernetes` is not officially intended for use as an external library, the following points justify its use in this context:
- High Community Reliance: Per `pkg.go.dev`, over 1,900 projects import this validation package, demonstrating that many maintainers rely on it despite its ""internal"" designation.
- Functional Availability: The package exports numerous validation functions, making it a viableâ€”and often necessaryâ€”resource for maintaining consistency with Kubernetes' internal validation logic.
- Community Guidance: Developer discussions frequently suggest leveraging these helpers to avoid diverging from core Kubernetes behavior, as re-implementing these complex rules from scratch is error-prone.

Iâ€™m happy to contribute the changes mentioned above. I wanted to align on the approach first to ensure it meets the project's standardsâ€”please let me know if youâ€™d like me to open a PR.

/sig scheduling
/kind feature
/area api","['area/api', 'sig/scheduling', 'kind/feature', 'needs-triage']",0
kubernetes/kubernetes,Post-Quantum Cryptography Readiness Observations & Recommendations (Research),"### What would you like to be added?

This issue shares research-based observations on post-quantum cryptography (PQC) readiness
in Kubernetes, based solely on publicly available code and documentation.

This is NOT a vulnerability report and does not include exploitation, attack paths, or
sensitive configuration details. The intent is awareness, future-readiness, and community
discussion aligned with NIST PQC transition guidance.

Observations:
â€¢ Cryptographic usage is distributed across multiple layers
â€¢ There is no centralized crypto inventory or ownership map
â€¢ No defined crypto agility or PQC migration framework
â€¢ No governance-level visibility into algorithm lifecycle

Recommendations:
â€¢ Consider adding a crypto inventory layer
â€¢ Define crypto ownership and lifecycle policy
â€¢ Track NIST PQC migration readiness
â€¢ Introduce crypto agility abstractions
â€¢ Document PQC roadmap for operators

Why this matters:
Quantum-safe transitions are multi-year efforts. Early visibility reduces long-term risk.

Happy to refine or contribute to documentation if helpful.

### Why is this needed?

Post-quantum cryptography (PQC) transitions are multi-year efforts that require early visibility and planning. 
Today, Kubernetes uses strong cryptography, but cryptographic usage is distributed across many layers 
(control plane, components, SDKs, dependencies), making long-term migration and governance difficult to track.

This enhancement is needed to:
- Improve visibility into cryptographic usage and ownership
- Enable crypto agility and algorithm lifecycle management
- Align Kubernetes with NIST post-quantum transition guidance
- Reduce future migration risk by identifying readiness gaps early
- Support operators and maintainers with clear, documented PQC readiness signals

This is not related to an exploitable vulnerability.  
It is a forward-looking readiness and governance improvement to help Kubernetes remain secure over the next decade.","['kind/feature', 'sig/security', 'needs-triage']",0
kubernetes/kubernetes,Replace deprecated `https://github.com/google/btree`,"https://github.com/google/btree is archived.

Based on the discussion here - https://github.com/etcd-io/etcd/issues/20991, we have implemented the btree in `k8s/utils`. The same needs to be referenced in all instances. 

Currently there is only one instance in apiserver, which needs changes.
","['sig/api-machinery', 'area/code-organization', 'triage/accepted']",0
kubernetes/kubernetes,DRA: Non-pod references in ReservedFor prevents removal of pod references,"### What happened?

The `status.reservedFor` field in the `ResourceClaim` resource allows for references to arbitrary objects, but only pod references are set by the scheduler and managed by the resourceclaim controller. This involves removing references to a pod when it terminates.

When a non-pod reference is added to the ReservedFor list, either before or after the scheduler adds a pod reference, the pod reference is no longer removed by the resourceclaim controller when the pod terminates.

### What did you expect to happen?

I expected that the pod reference would be removed, but that the non-pod reference would remain.

### How can we reproduce it (as minimally and precisely as possible)?

Set up the DRA example driver with Kind as described in https://github.com/kubernetes-sigs/dra-example-driver?tab=readme-ov-file#demo, but don't apply any of the demo workloads.

Apply the following ResourceClaim and Pod:
```
apiVersion: resource.k8s.io/v1
kind: ResourceClaim
metadata:
  name: single-gpu
spec:
  devices:
    requests:
    - name: gpu
      exactly:
        deviceClassName: gpu.example.com
---
apiVersion: v1
kind: Pod
metadata:
  name: pod0
  labels:
    app: pod
spec:
  containers:
  - name: ctr0
    image: ubuntu:22.04
    command: [""bash"", ""-c""]
    args: [""export; trap 'exit 0' TERM; sleep 9999 & wait""]
    resources:
      claims:
      - name: gpu
  resourceClaims:
  - name: gpu
    resourceClaimName: single-gpu
```

Wait until the pod is running.

Edit the ResourceClaim to add the non-pod reference in the `ReservedFor` list:

```
k edit resourceclaim single-gpu --subresource=status
```

Add the following:
```
    - apiGroup: foo.com
      name: foo
      resource: foo
      uid: 8e10c468-5b0d-4ca9-93ed-a74e44d1f4e5
```

Delete the pod and wait for it to terminate.

Inspect the ResourceClaim and see that the pod reference is still in the `ReservedFor` list.

### Anything else we need to know?

_No response_

### Kubernetes version

Server Version: v1.34.0 on Kind


### Cloud provider

<details>

</details>


### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


### Install tools

<details>

</details>


### Container runtime (CRI) and version (if applicable)

<details>

</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>
","['kind/bug', 'needs-triage', 'wg/device-management']",0
kubernetes/kubernetes,[Failing Test] [sig-scalability] ClusterLoaderV2.access-tokens overall (/home/prow/go/src/k8s.io/perf-tests/clusterloader2/testing/access-tokens/config.yaml),"### Which jobs are failing?

* sig-release-master-informing
* ec2-master-scale-performance

### Which tests are failing?

* [ClusterLoaderV2.access-tokens overall (/home/prow/go/src/k8s.io/perf-tests/clusterloader2/testing/access-tokens/config.yaml)](https://prow.k8s.io/view/gs/kubernetes-ci-logs/logs/ci-kubernetes-e2e-kops-aws-scale-amazonvpc-using-cl2/2013869539921498112)

### Since when has it been failing?

* First failure: Wed, 07 Jan 2026 07:03:18 UTC
* Latest failure: Wed, 21 Jan 2026 07:02:53 UTC

### Testgrid link

* [https://testgrid.k8s.io/sig-release-master-informing#ec2-master-scale-performance&exclude-non-failed-tests=](https://testgrid.k8s.io/sig-release-master-informing#ec2-master-scale-performance&exclude-non-failed-tests=)
* [https://storage.googleapis.com/k8s-triage/index.html?job=ci-kubernetes-e2e-kops-aws-scale-amazonvpc-using-cl2$&test=ClusterLoaderV2.access-tokens%20overall%20(/home/prow/go/src/k8s.io/perf-tests/clusterloader2/testing/access-tokens/config.yaml)](https://storage.googleapis.com/k8s-triage/index.html?job=ci-kubernetes-e2e-kops-aws-scale-amazonvpc-using-cl2$&test=ClusterLoaderV2.access-tokens%20overall%20(/home/prow/go/src/k8s.io/perf-tests/clusterloader2/testing/access-tokens/config.yaml))

### Reason for failure (if possible)

```
	F 2026-01-21 12:32:53 +0530 IST :0
[measurement call APIResponsivenessPrometheus - APIResponsivenessPr...1m0s, perc99: 1m0s Count:66 SlowCount:66}; expected perc99 <= 30s]]
:0
	F 2026-01-20 12:33:05 +0530 IST :0
[measurement call APIResponsivenessPrometheus - APIResponsivenessPr...1m0s, perc99: 1m0s Count:73 SlowCount:73}; expected perc99 <= 30s]]
:0
	F 2026-01-19 12:32:40 +0530 IST :0
[measurement call APIResponsivenessPrometheus - APIResponsivenessPr...1m0s, perc99: 1m0s Count:74 SlowCount:74}; expected perc99 <= 30s]]
:0
	F 2026-01-18 12:32:31 +0530 IST :0
[measurement call APIResponsivenessPrometheus - APIResponsivenessPr...m0s, perc99: 1m0s Count:116 SlowCount:90}; expected perc99 <= 30s]]
:0
	F 2026-01-17 12:32:30 +0530 IST :0
[measurement call APIResponsivenessPrometheus - APIResponsivenessPr...82s, perc99: 1m0s Count:125 SlowCount:69}; expected perc99 <= 30s]]
:0
	F 2026-01-16 12:32:30 +0530 IST :0
[measurement call APIResponsivenessPrometheus - APIResponsivenessPr...1m0s, perc99: 1m0s Count:75 SlowCount:75}; expected perc99 <= 30s]]
:0
	F 2026-01-14 12:33:07 +0530 IST :0
[measurement call APIResponsivenessPrometheus - APIResponsivenessPr...1m0s, perc99: 1m0s Count:76 SlowCount:76}; expected perc99 <= 30s]]
:0
	F 2026-01-13 12:32:56 +0530 IST :0
[measurement call APIResponsivenessPrometheus - APIResponsivenessPr...1m0s, perc99: 1m0s Count:72 SlowCount:72}; expected perc99 <= 30s]]
:0
	F 2026-01-12 12:33:09 +0530 IST :0
[measurement call APIResponsivenessPrometheus - APIResponsivenessPr...1m0s, perc99: 1m0s Count:63 SlowCount:63}; expected perc99 <= 30s]]
:0
	F 2026-01-11 12:32:33 +0530 IST :0
[measurement call APIResponsivenessPrometheus - APIResponsivenessPr...1m0s, perc99: 1m0s Count:89 SlowCount:86}; expected perc99 <= 30s]]
:0

```

### Anything else we need to know?

_No response_

### Relevant SIG(s)

/sig scalability
/kind failing-test
cc @kubernetes/release-team-release-signal
","['sig/scalability', 'kind/failing-test', 'needs-triage']",0
kubernetes/kubernetes,[Flaking Test] [sig-scheduling] k8s.io/kubernetes/test/integration/scheduler/plugins.plugins,"### Which jobs are flaking?

* sig-release-master-blocking
* integration-arm64-master

### Which tests are flaking?

* [k8s.io/kubernetes/test/integration/scheduler/plugins.plugins](https://prow.k8s.io/view/gs/kubernetes-ci-logs/logs/ci-kubernetes-integration-arm64-master/2012720398210174976)

### Since when has it been flaking?

* First flaky: Tue, 06 Jan 2026 10:01:24 UTC
* Latest flaky: Wed, 21 Jan 2026 08:26:56 UTC

### Testgrid link

* [https://testgrid.k8s.io/sig-release-master-blocking#integration-arm64-master&exclude-non-failed-tests=](https://testgrid.k8s.io/sig-release-master-blocking#integration-arm64-master&exclude-non-failed-tests=)
* [https://storage.googleapis.com/k8s-triage/index.html?job=ci-kubernetes-integration-arm64-master$&test=k8s.io/kubernetes/test/integration/scheduler/plugins.plugins](https://storage.googleapis.com/k8s-triage/index.html?job=ci-kubernetes-integration-arm64-master$&test=k8s.io/kubernetes/test/integration/scheduler/plugins.plugins)

### Reason for failure (if possible)

```
	F 2026-01-18 08:25:39 +0530 IST Failed
=== RUN   TestUnReservePreBindPlugins/All_Reserve_plugins_passe...UnReservePreBindPlugins
--- FAIL: TestUnReservePreBindPlugins (3.89s)

	F 2026-01-17 12:19:39 +0530 IST Failed
=== RUN   TestUnReservePreBindPlugins/All_Reserve_plugins_passe...UnReservePreBindPlugins
--- FAIL: TestUnReservePreBindPlugins (3.73s)

	F 2026-01-16 11:06:38 +0530 IST Failed
=== RUN   TestUnReservePreBindPlugins/All_Reserve_plugins_passe...UnReservePreBindPlugins
--- FAIL: TestUnReservePreBindPlugins (3.87s)

	F 2026-01-14 16:44:06 +0530 IST Failed
=== RUN   TestUnReservePreBindPlugins/All_Reserve_plugins_passe...UnReservePreBindPlugins
--- FAIL: TestUnReservePreBindPlugins (4.09s)

	F 2026-01-12 10:31:46 +0530 IST Failed
=== RUN   TestUnReservePreBindPlugins/All_Reserve_plugins_passe...UnReservePreBindPlugins
--- FAIL: TestUnReservePreBindPlugins (3.84s)

	F 2026-01-11 20:20:46 +0530 IST Failed
=== RUN   TestUnReservePreBindPlugins/All_Reserve_plugins_passe...UnReservePreBindPlugins
--- FAIL: TestUnReservePreBindPlugins (3.77s)

	F 2026-01-11 08:20:41 +0530 IST Failed
=== RUN   TestUnReservePreBindPlugins/All_Reserve_plugins_passe...UnReservePreBindPlugins
--- FAIL: TestUnReservePreBindPlugins (3.66s)

	F 2026-01-11 07:20:41 +0530 IST Failed
=== RUN   TestUnReservePreBindPlugins/All_Reserve_plugins_passe...UnReservePreBindPlugins
--- FAIL: TestUnReservePreBindPlugins (3.80s)

	F 2026-01-10 16:13:26 +0530 IST Failed
=== RUN   TestUnReservePreBindPlugins/All_Reserve_plugins_passe...UnReservePreBindPlugins
--- FAIL: TestUnReservePreBindPlugins (3.69s)

	F 2026-01-10 13:13:26 +0530 IST Failed
=== RUN   TestUnReservePreBindPlugins/All_Reserve_plugins_passe...UnReservePreBindPlugins
--- FAIL: TestUnReservePreBindPlugins (3.71s)


```

### Anything else we need to know?

_No response_

### Relevant SIG(s)

/sig scheduling
/kind flake
cc @kubernetes/release-team-release-signal
","['sig/scheduling', 'kind/flake', 'needs-triage']",0
kubernetes/kubernetes,[Flaking Test] [sig-node] [Feature:GPUDevicePlugin] [Serial] Sanity test using nvidia-smi should run nvidia-smi and cuda-demo-suite,"### Which jobs are flaking?

* sig-release-master-blocking
* gce-device-plugin-gpu-master

### Which tests are flaking?

* [Kubernetes e2e suite.[It] [sig-node] [Feature:GPUDevicePlugin] [Serial] Sanity test using nvidia-smi should run nvidia-smi and cuda-demo-suite](https://prow.k8s.io/view/gs/kubernetes-ci-logs/logs/ci-kubernetes-e2e-gce-device-plugin-gpu/2013816438350942208)

### Since when has it been flaking?

* First flaky: Tue, 06 Jan 2026 07:30:31 UTC
* Latest flaky: Wed, 21 Jan 2026 05:30:52 UTC

### Testgrid link

* [https://testgrid.k8s.io/sig-release-master-blocking#gce-device-plugin-gpu-master&exclude-non-failed-tests=](https://testgrid.k8s.io/sig-release-master-blocking#gce-device-plugin-gpu-master&exclude-non-failed-tests=)
* [https://storage.googleapis.com/k8s-triage/index.html?job=ci-kubernetes-e2e-gce-device-plugin-gpu$&test=Sanity%20test%20using%20nvidia-smi%20should%20run%20nvidia-smi%20and%20cuda-demo-suite](https://storage.googleapis.com/k8s-triage/index.html?job=ci-kubernetes-e2e-gce-device-plugin-gpu$&test=Sanity%20test%20using%20nvidia-smi%20should%20run%20nvidia-smi%20and%20cuda-demo-suite)

### Reason for failure (if possible)

```
	F 2026-01-21 09:00:57 +0530 IST [FAILED] Expected
    <v1.PodPhase>: Failed
to equal
    <v1.PodPhase>...t: k8s.io/kubernetes/test/e2e/node/gpu.go:168 @ 01/21/26 04:01:14.847

	F 2026-01-20 19:00:55 +0530 IST [FAILED] Expected
    <string>: No devices were found
    Get:1 http:/...at: k8s.io/kubernetes/test/e2e/node/gpu.go:76 @ 01/20/26 13:42:32.352

	F 2026-01-19 21:00:44 +0530 IST [FAILED] Expected
    <v1.PodPhase>: Failed
to equal
    <v1.PodPhase>...t: k8s.io/kubernetes/test/e2e/node/gpu.go:168 @ 01/19/26 15:43:54.779

	F 2026-01-19 17:00:39 +0530 IST [FAILED] Expected
    <v1.PodPhase>: Failed
to equal
    <v1.PodPhase>...t: k8s.io/kubernetes/test/e2e/node/gpu.go:168 @ 01/19/26 11:42:31.963

	F 2026-01-19 13:00:42 +0530 IST [FAILED] Expected
    <string>: No devices were found
    Get:1 http:/...at: k8s.io/kubernetes/test/e2e/node/gpu.go:76 @ 01/19/26 07:42:54.737

	F 2026-01-18 17:00:36 +0530 IST [FAILED] Expected
    <string>: No devices were found
    Get:1 https:...at: k8s.io/kubernetes/test/e2e/node/gpu.go:76 @ 01/18/26 11:42:31.213

	F 2026-01-17 01:00:44 +0530 IST [FAILED] Expected
    <string>: No devices were found
    Get:1 https:...at: k8s.io/kubernetes/test/e2e/node/gpu.go:76 @ 01/16/26 20:00:13.237

	F 2026-01-16 13:00:38 +0530 IST [FAILED] Expected
    <v1.PodPhase>: Failed
to equal
    <v1.PodPhase>...t: k8s.io/kubernetes/test/e2e/node/gpu.go:168 @ 01/16/26 07:39:44.112

	F 2026-01-14 23:01:09 +0530 IST [FAILED] Expected
    <string>: No devices were found
    Get:1 https:...at: k8s.io/kubernetes/test/e2e/node/gpu.go:76 @ 01/14/26 17:43:02.392

	F 2026-01-14 11:01:04 +0530 IST [FAILED] Expected
    <string>: No devices were found
    Get:1 https:...at: k8s.io/kubernetes/test/e2e/node/gpu.go:76 @ 01/14/26 06:09:40.688

	F 2026-01-14 09:01:01 +0530 IST [FAILED] Expected
    <string>: No devices were found
    Get:1 http:/...at: k8s.io/kubernetes/test/e2e/node/gpu.go:76 @ 01/14/26 03:43:29.181

	F 2026-01-13 23:01:08 +0530 IST [FAILED] Expected
    <*v1.Event | 0xc00086e008>: 
        count: 1
  ...t: k8s.io/kubernetes/test/e2e/node/gpu.go:159 @ 01/13/26 17:39:49.478

	F 2026-01-13 01:00:39 +0530 IST [FAILED] Expected
    <*v1.Event | 0xc000cc9688>: 
        count: 1
  ...t: k8s.io/kubernetes/test/e2e/node/gpu.go:159 @ 01/12/26 19:47:47.021

	F 2026-01-10 01:01:24 +0530 IST [FAILED] Expected
    <*v1.Event | 0xc001026f08>: 
        count: 1
  ...t: k8s.io/kubernetes/test/e2e/node/gpu.go:159 @ 01/09/26 19:45:55.581


```

### Anything else we need to know?

_No response_

### Relevant SIG(s)

/sig node
/kind flake
cc @kubernetes/release-team-release-signal
","['sig/node', 'kind/flake', 'triage/accepted']",0
kubernetes/kubernetes,[Flaking Test][sig-storage] k8s.io/kubernetes/pkg/kubelet/volumemanager.volumemanager,"### Which jobs are flaking?

* sig-release-master-blocking
* ci-kubernetes-unit

### Which tests are flaking?

* [k8s.io/kubernetes/pkg/kubelet/volumemanager.volumemanager](https://prow.k8s.io/view/gs/kubernetes-ci-logs/logs/ci-kubernetes-unit/2013860227501264896)

### Since when has it been flaking?

* First flaky: Tue, 20 Jan 2026 07:11:53 UTC
* Latest flaky: Wed, 21 Jan 2026 06:24:53 UTC

### Testgrid link

* [https://testgrid.k8s.io/sig-release-master-blocking#ci-kubernetes-unit&exclude-non-failed-tests=](https://testgrid.k8s.io/sig-release-master-blocking#ci-kubernetes-unit&exclude-non-failed-tests=)
* [https://storage.googleapis.com/k8s-triage/index.html?job=ci-kubernetes-unit$&test=k8s.io/kubernetes/pkg/kubelet/volumemanager.volumemanager](https://storage.googleapis.com/k8s-triage/index.html?job=ci-kubernetes-unit$&test=k8s.io/kubernetes/pkg/kubelet/volumemanager.volumemanager)

### Reason for failure (if possible)

```
	F 2026-01-21 11:54:53 +0530 IST Failed
=== RUN   TestWaitForAllPodsUnmount/concurrent_unmount_-_many_p...stWaitForAllPodsUnmount
--- FAIL: TestWaitForAllPodsUnmount (168.99s)

	F 2026-01-21 06:50:54 +0530 IST Failed
=== RUN   TestWaitForAllPodsUnmount/concurrent_unmount_-_many_p...stWaitForAllPodsUnmount
--- FAIL: TestWaitForAllPodsUnmount (168.99s)

	F 2026-01-21 01:45:52 +0530 IST Failed
=== RUN   TestWaitForAllPodsUnmount/concurrent_unmount_-_many_p...stWaitForAllPodsUnmount
--- FAIL: TestWaitForAllPodsUnmount (129.80s)

	F 2026-01-20 21:42:51 +0530 IST Failed
=== RUN   TestWaitForAllPodsUnmount/concurrent_unmount_-_many_p...stWaitForAllPodsUnmount
--- FAIL: TestWaitForAllPodsUnmount (140.59s)

	F 2026-01-20 20:42:52 +0530 IST Failed
=== RUN   TestWaitForAllPodsUnmount/concurrent_unmount_-_many_p...stWaitForAllPodsUnmount
--- FAIL: TestWaitForAllPodsUnmount (168.99s)


```

### Anything else we need to know?

_No response_

### Relevant SIG(s)

/sig testing
/kind flake
cc @kubernetes/release-team-release-signal
","['sig/storage', 'kind/flake', 'needs-triage']",0
kubernetes/kubernetes,Duplicate ports with different protocols in pod spec get merged after pod spec update,"### What happened?

When I add a port with a duplicate port number but different protocol (UDP instead of TCP) to an existing Deployment, one of these two ports get removed.

### What did you expect to happen?

I would expect both to be present.

### How can we reproduce it (as minimally and precisely as possible)?

Apply this:

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: test-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: test
  template:
    metadata:
      labels:
        app: test
    spec:
      containers:
      - name: test
        image: busybox
        command:
        - sleep
        - infinity
        ports:
        - containerPort: 1000
          protocol: TCP
```

Then `kubectl edit` the `Deployment`, duplicating the containerport, but with UDP, like this:

```
        ports:
        - containerPort: 1000
          protocol: TCP
        - containerPort: 1000
          protocol: UDP
```

And you'll observe that one of the two ports is gone in the `Deployment`.

### Anything else we need to know?

There are no mutating webhooks involved. Even with all hooks removed, I still observ this.

When you `get deploy -o yaml`, edit the yaml, `apply --server-side -f /the/yaml`, it does seem to get applied.

My guess is it is due to (only) `containerPort` being the merge key in types.go.

This is very similar to #105610, but that issue is about `Services`.

### Kubernetes version

I've observed this in 1.34.2, 1.33.6 and 1.32.4

### Cloud provider

self hosted Talos

### OS version

Talos Linux, various versions

### Install tools

Talos Linux

### Container runtime (CRI) and version (if applicable)

containerd

### Related plugins (CNI, CSI, ...) and versions (if applicable)

n/a","['kind/bug', 'sig/network', 'sig/node', 'needs-triage']",0
kubernetes/kubernetes,The externalTrafficPolicy configuration is invalid.,"### What happened?

k8s version: 1.30.7-aliyun.1
k8s service yaml: 
``` apiVersion: v1
kind: Service
metadata:
  name: xxxx-test
  namespace: default
spec:
  ....
  type: NodePort
  externalTrafficPolicy: Local
  internalTrafficPolicy: Cluster
  ....
  selector:
    app: xxx-prod
   ```
My pod count is 1. When I access a node(node IP+ node Port)that doesn't have this pod deployed, it works fine.

### What did you expect to happen?

As described on the official website, this access should have failed.

### How can we reproduce it (as minimally and precisely as possible)?

apiVersion: v1
kind: Service
metadata:
  name: xxxx-test
  namespace: default
spec:
  externalTrafficPolicy: Local
  ipFamilies:
  - IPv4
  ipFamilyPolicy: SingleStack
  ports:
  - port: 80
  selector:
    app: fool
  type: NodePort

### Anything else we need to know?

_No response_

### Kubernetes version

<details>

```console
$ kubectl version
Client Version: version.Info{Major:""1"", Minor:""22"", GitVersion:""v1.22.15"", GitCommit:""1d79bc3bcccfba7466c44cc2055d6e7442e140ea"", GitTreeState:""clean"", BuildDate:""2022-09-21T12:18:10Z"", GoVersion:""go1.16.15"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""30+"", GitVersion:""v1.30.7-aliyun.1"", GitCommit:""cd351e3d740834f7170f62f6e96160f92f74f19d"", GitTreeState:""clean"", BuildDate:""2025-10-24T10:30:21Z"", GoVersion:""go1.22.2"", Compiler:""gc"", Platform:""linux/amd64""
```

</details>


### Cloud provider

<details>
aliyun
</details>


### OS version

<details>

</details>


### Install tools

<details>

</details>


### Container runtime (CRI) and version (if applicable)

<details>

</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>
","['kind/bug', 'sig/network', 'area/ipvs', 'needs-triage']",0
kubernetes/kubernetes,Zero value Kubelet PSI metrics emitted even if underlying OS doesn't enable it,"### What happened?

In Kubernetes 1.34, the KubeletPSI feature gate was set to true. However, when running Kubernetes on an OS that does not have PSI enabled, the PSI metrics are not expected to be generated. Here are the counts of the metrics observed during a test:
```
# for node in $(kubectl get nodes -o jsonpath='{.items[*].metadata.name}'); do echo ""=== Node: $node ===""; for metric in cpu_waiting cpu_stalled memory_waiting memory_stalled io_waiting io_stalled; do echo -n ""container_pressure_${metric}_seconds_total: ""; kubectl get --raw ""/api/v1/nodes/$node/proxy/metrics/cadvisor"" | grep ""container_pressure_${metric}_seconds_total"" | wc -l; done; done
=== Node: ip-10-0-11-217.us-east-2.compute.internal ===
container_pressure_cpu_waiting_seconds_total:      267
container_pressure_cpu_stalled_seconds_total:      267
container_pressure_memory_waiting_seconds_total:      267
container_pressure_memory_stalled_seconds_total:      267
container_pressure_io_waiting_seconds_total:      267
container_pressure_io_stalled_seconds_total:      267
```
I suspect this part of the code:

https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/cadvisor/cadvisor_linux.go#L105C53-L105C63
```
	if utilfeature.DefaultFeatureGate.Enabled(features.KubeletPSI) {
		includedMetrics[cadvisormetrics.PressureMetrics] = struct{}{}
	} 
```
and prometheus does this: https://github.com/google/cadvisor/blob/master/metrics/prometheus.go#L1842
```
	if includedMetrics.Has(container.PressureMetrics) {
		c.containerMetrics = append(c.containerMetrics, []containerMetric{
			{
				name:      ""container_pressure_cpu_stalled_seconds_total"", 
```

cc: @haircommander @bitoku 

### What did you expect to happen?

There should be no PSI metrics when underlying OS never enables it.

### How can we reproduce it (as minimally and precisely as possible)?

Run k8s 1.34 and above and use grafana to monitor PSI metrics

### Anything else we need to know?

This is a good to have. Doesn't really impact anything as cardinality is also negligible. This zero value confuses the end-user as it gives a visual impression that PSI is enabled when looking at grafana charts.

I can propose a fix on this.

### Kubernetes version

1.34
<details>

```console
$ kubectl version
# paste output here
```

</details>


### Cloud provider

<details>

</details>
k8s on GCP instances

### OS version

Linux: 5.14.0-570.78.1.el9_6.x86_64

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


### Install tools

<details>

</details>


### Container runtime (CRI) and version (if applicable)

<details>

</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>
","['kind/bug', 'sig/node', 'priority/important-longterm', 'triage/accepted']",0
kubernetes/kubernetes,[failing-test] containerd serial tests failing on reboot,"### Which jobs are failing?

https://testgrid.k8s.io/sig-node-release-blocking#node-kubelet-serial-containerd

### Which tests are failing?

Right now the restart is causing all tests to fail.

It seems to only occur on cos as far as I could tell.

### Since when has it been failing?

At least for test grid its been failing like this for a few weeks now (Jan 04 2026)

### Testgrid link

https://testgrid.k8s.io/sig-node-release-blocking#node-kubelet-serial-containerd

### Reason for failure (if possible)

Its starting to look like the job is failing to restart kubelet causing the full test to fail.

https://storage.googleapis.com/kubernetes-ci-logs/logs/ci-kubernetes-node-kubelet-serial-containerd/2013329922633764864/artifacts/e2-standard-2-cos-125-19216-104-95-334854fa/kubelet.log

```
E0119 19:53:13.251607  206171 run.go:72] ""command failed"" err=""failed to run Kubelet: failed to create kubelet: create user namespace manager: record pod mappings for existing pod \""4cfeffa4-cc3a-4165-b92e-b48f481a0492\"": wrong user namespace length 131072""
````

### Anything else we need to know?

Looks like Test started failing like this since January 14th. Before that we had failures related to credential provider. And then we are going complete failures since.

### Relevant SIG(s)

/sig node","['priority/critical-urgent', 'sig/node', 'kind/failing-test', 'triage/accepted']",0
kubernetes/kubernetes,fake.NewClientset fails to perform Apply/Patch for custom resources (rather than deprecated NewSimpleClientset works) in tests,"### What happened?

After migration to Kubernetes 1.35 code-base and removing deprecations it was found that `fake.NewClientset` generates client that fails to perform Apply method for custom resources. `fake.NewSimpleClientset` remains working as expected.

Issue:
```
failed to create manager for existing fields: failed to convert new object (default/test; iam.devops.corp.net/v1alpha1, Kind=SecretResolver) to smd typed: schema error: no type found matching: net.corp.devops.go.kube.api.iam.v1alpha1.SecretResolver
```


### What did you expect to happen?

`client.CustomGroup().CustomResource(""ns"").Apply(...)` works in tests

### How can we reproduce it (as minimally and precisely as possible)?

Appears only for custom resources, probably it might be re-produced in `kubernetes/sample-apiserver`.

### Anything else we need to know?

I've tried to reproduce it on default Kubernetes resources, but seems everything is ok here. And this test case passes as usual

```golang
func TestApply(t *testing.T) {
	client := fake.NewClientset(testConfigMap)

	apply := &applyv1.ConfigMapApplyConfiguration{
		TypeMetaApplyConfiguration: *applymetav1.TypeMeta().
			WithKind(""ConfigMap"").WithAPIVersion(""v1""),
		ObjectMetaApplyConfiguration: applymetav1.ObjectMeta().
			WithName(""test"").WithAnnotations(map[string]string{""annotation"": ""kek""}),
	}

	cm, err := client.CoreV1().ConfigMaps(""default"").Apply(context.TODO(), apply, metav1.ApplyOptions{})
	if err != nil {
		t.Errorf(""got error: %v"", err)
	}
	klog.InfoS(""got updated configmap"", ""annotations"", cm.Annotations)
}
```

However, using `client-gen` from k8s.io/code-generator (kubernetes-1.35.0) it generates everything properly, but `fake.NewClientset` gives the client that fails to perform `Apply` methods. An issue seems to appear at:

```golang
# apimachinery@v0.35.0/pkg/util/managedfields/internal/fieldmanager.go:196
object, managed, err = f.fieldManager.Apply(liveObj, appliedObj, managed, manager, force)
```

Both `liveObj` and `appliedObj` looked sane to me.

### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.35.0
Kustomize Version: v5.7.1
Server Version: v1.34.2-eks-b3126f4
```

k8s.io/code-generator (kubernetes-1.35.0)
</details>


### Cloud provider

<details>
NA
</details>


### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME=""Ubuntu 24.04.3 LTS""
NAME=""Ubuntu""
VERSION_ID=""24.04""
VERSION=""24.04.3 LTS (Noble Numbat)""
VERSION_CODENAME=noble
ID=ubuntu
ID_LIKE=debian
HOME_URL=""https://www.ubuntu.com/""
SUPPORT_URL=""https://help.ubuntu.com/""
BUG_REPORT_URL=""https://bugs.launchpad.net/ubuntu/""
PRIVACY_POLICY_URL=""https://www.ubuntu.com/legal/terms-and-policies/privacy-policy""
UBUNTU_CODENAME=noble
LOGO=ubuntu-logo

$ uname -a
Linux dakka-dakka 6.6.87.2-microsoft-standard-WSL2 #1 SMP PREEMPT_DYNAMIC Thu Jun  5 18:30:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


### Install tools

<details>

</details>


### Container runtime (CRI) and version (if applicable)

<details>

</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>
","['kind/bug', 'needs-sig', 'needs-triage']",0
kubernetes/kubernetes,Why does `selector.matchLabels` exist for `Deployment`? Why would a user need this?,"The `selector.matchLabels` field was made immutable some years back and since then I've just seen so much friction for users dealing with this part of the spec.

Mostly because people just copy other labels, such as org data (as [suggested in the docs](https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#motivation)) into `matchLabels`, then later need to change those label values and are then faced with a resource they cannot update due to the dreaded `field is immutable` error. There are solutions for this, e.g.deleting a deployment with the `--cascade=orphan` option, but this is risky and messy. We just faced an incident where the newly orphaned replicaset was adopted by another deployment and scaled down.

This leads me to question why does this field even exist in the spec? In what world are people wanting to manually specify the relationship between parent/child resources?","['sig/apps', 'needs-triage']",0
kubernetes/kubernetes,Container fail to create when env var contains $ followed by non-ASCII character (invalid UTF-8 gRPC error),"### What happened?

After upgrading our Kubernetes cluster from v1.33.5 to v1.34.2 (AWS EKS), pods scheduled on nodes running v1.34.2 fail during container creation with the following error:

```bash
grpc: error while marshaling: string field contains invalid UTF-8
```

The pods are created successfully, but containers fail to create.

<img width=""3334"" height=""54"" alt=""Image"" src=""https://github.com/user-attachments/assets/85bc12a8-19f2-40fd-b850-d569f1727d03"" />

Important observation:
Pods scheduled on nodes still running v1.33.5 continue to work correctly with the same manifests and environment variables. The failure occurs only on nodes upgraded to v1.34.2, strongly indicating a regression.

Through isolation and testing, we observed that the failure occurs when an environment variable value contains a $ immediately followed by a non-ASCII character.

This behaviour did not occur on v1.33.5 and appears to be a regression introduced in v1.34.x.

### What did you expect to happen?

Pods and containers should start successfully when environment variable values contain valid UTF-8 characters, including non-ASCII characters and literal $ characters.

This configuration worked as expected on v1.33.5 nodes, even within the same cluster during a rolling upgrade.

### How can we reproduce it (as minimally and precisely as possible)?

1. Create a pod (or deployment) with an environment variable containing:
    - a $
    - immediately followed by a non-ASCII UTF-8 character
3. Schedule the pod on a v1.34.2 node
4. Observe container creation failure with:
    - `grpc: error while marshaling: string field contains invalid UTF-8`

Minimal reproducible example:
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: utf8-env-repro
spec:
  containers:
  - name: test
    image: busybox
    command: [""sh"", ""-c"", ""echo started && sleep 3600""]
    env:
    - name: TEST_ENV
      value: ""$Â£FOO""
```

Observed result:

âŒ Fails on v1.34.2 nodes

âœ… Works on v1.33.5 nodes

Expected result:
Container starts normally on all versions.

### Anything else we need to know?

- We run a custom CRD/operator that propagates env vars from the CR spec into Deployments and multi-container Pods.
- The failure occurs after Pod creation, during container creation / runtime interaction.
- This reproduces across all containers in the pod that receive the env var.
- This is reproducible in a mixed-version cluster:
    - Pods on v1.33.5 nodes work
    - Pods on v1.34.2 nodes fail

### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.32.0
Kustomize Version: v5.5.0
Server Version: v1.34.2-eks-b3126f4
WARNING: version difference between client (1.32) and server (1.34) exceeds the supported minor version skew of +/-1
```

</details>

<img width=""2970"" height=""334"" alt=""Image"" src=""https://github.com/user-attachments/assets/3c8e92cd-be0a-4f7f-a22c-2fb10a84e881"" />


### Cloud provider

<details>
AWS EKS
Cluster upgrade from v1.33.5 â†’ v1.34.2
</details>


### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME=""Amazon Linux""
VERSION=""2023""
ID=""amzn""
ID_LIKE=""fedora""
VERSION_ID=""2023""
PLATFORM_ID=""platform:al2023""
PRETTY_NAME=""Amazon Linux 2023.9.20251208""
ANSI_COLOR=""0;33""
CPE_NAME=""cpe:2.3:o:amazon:amazon_linux:2023""
HOME_URL=""https://aws.amazon.com/linux/amazon-linux-2023/""
DOCUMENTATION_URL=""https://docs.aws.amazon.com/linux/""
SUPPORT_URL=""https://aws.amazon.com/premiumsupport/""
BUG_REPORT_URL=""https://github.com/amazonlinux/amazon-linux-2023""
VENDOR_NAME=""AWS""
VENDOR_URL=""https://aws.amazon.com/""
SUPPORT_END=""2029-06-30""
$ uname -a
Linux ip-10-11-156-253.eu-central-1.compute.internal 6.12.58-82.121.amzn2023.x86_64 #1 SMP PREEMPT_DYNAMIC Thu Nov 27 09:07:41 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


### Install tools

<details>

</details>


### Container runtime (CRI) and version (if applicable)

<details>
containerd://2.1.5
</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>
","['kind/bug', 'sig/node', 'kind/regression', 'needs-triage']",0
kubernetes/kubernetes,Service Controller Does Not Reconcile When LoadBalancer Status is Cleared,"### What happened?

The cloud-provider service controller doesn't trigger reconciliation when a LoadBalancer Service's status is cleared by external controllers like GitOps tools. This happens when the Service spec includes `loadBalancerIP` and the GitOps tool updates the Service, clearing the status field. Even though the cloud load balancer continues to function normally, the Service shows a pending status indefinitely because the controller only detects changes to the Service spec, not the status.

### What did you expect to happen?

When a LoadBalancer Service's status is cleared during a resource update, the service controller should detect this change and reconcile by querying the cloud provider to restore the correct LoadBalancer ingress information.

### How can we reproduce it (as minimally and precisely as possible)?

1. Create a LoadBalancer Service with `loadBalancerIP` specified in the spec:
   ```yaml
   apiVersion: v1
   kind: Service
   metadata:
     name: test-service
   spec:
     type: LoadBalancer
     loadBalancerIP: 10.50.71.131
     ports:
     - port: 80
   ```
2. Verify the Service has `Status.LoadBalancer.Ingress` populated with the LB IP
3. Apply the same Service YAML using a GitOps tool or kubectl, which clears the status field
4. Observe the Service status remains empty/pending even though the cloud load balancer exists and functions correctly

### Anything else we need to know?

The `needsUpdate()` function in `staging/src/k8s.io/cloud-provider/controllers/service/controller.go` (lines 557-616) only checks `Service.Spec` fields like ports, annotations, and IPs. It doesn't check if `Service.Status.LoadBalancer.Ingress` has been cleared.


### Kubernetes version

# Affects: current master branch (as of 2026-01-19)
# All Kubernetes versions using the standard cloud-provider service controller



### Cloud provider

All cloud providers using the standard service controller.

### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


### Install tools

<details>

</details>


### Container runtime (CRI) and version (if applicable)

<details>

</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>
","['kind/bug', 'sig/network', 'needs-triage']",0
kubernetes/kubernetes,Graduate serviceaccount metrics to BETA,"Parent issue: https://github.com/kubernetes/kubernetes/issues/136107

- path: pkg/serviceaccount/metrics.go
    - [ ] serviceaccount_legacy_tokens_total
    - [ ] serviceaccount_stale_tokens_total
    - [ ] serviceaccount_valid_tokens_total ","['sig/instrumentation', 'needs-triage']",0
kubernetes/kubernetes,Graduate component-base metrics to BETA,"Parent issue: https://github.com/kubernetes/kubernetes/issues/136107

- path: staging/src/k8s.io/component-base/metrics/version.go
    - [ ] kubernetes_build_info https://github.com/kubernetes/kubernetes/pull/136154
- path: staging/src/k8s.io/component-base/metrics/prometheus/restclient/metrics.go
    - [ ] rest_client_request_duration_seconds https://github.com/kubernetes/kubernetes/pull/136154
    - [ ] rest_client_requests_total https://github.com/kubernetes/kubernetes/pull/136154
- path: staging/src/k8s.io/component-base/metrics/prometheus/controllers/metrics.go
    - [ ] running_managed_controllers   https://github.com/kubernetes/kubernetes/pull/136154","['sig/instrumentation', 'needs-triage']",0
kubernetes/kubernetes,Graduate scheduler metrics to BETA,"Parent issue: https://github.com/kubernetes/kubernetes/issues/136107

- path: pkg/scheduler/metrics/metrics.go
    - [ ] scheduler_goroutines https://github.com/kubernetes/kubernetes/pull/136155
    - [ ] scheduler_permit_wait_duration_seconds https://github.com/kubernetes/kubernetes/pull/136155
    - [ ] scheduler_plugin_evaluation_total https://github.com/kubernetes/kubernetes/pull/136155
    - [ ] scheduler_plugin_execution_duration_seconds
    - [ ] scheduler_scheduling_algorithm_duration_seconds
    - [ ] scheduler_unschedulable_pods https://github.com/kubernetes/kubernetes/pull/136155
 ","['sig/scheduling', 'sig/instrumentation', 'needs-triage']",0
kubernetes/kubernetes,Graduate storage metrics to BETA,"Parent issue: https://github.com/kubernetes/kubernetes/issues/136107

- path: pkg/volume/util/metrics.go
    - [ ] storage_operation_duration_seconds https://github.com/kubernetes/kubernetes/pull/136189
    - [ ] volume_operation_total_seconds https://github.com/kubernetes/kubernetes/pull/136189
- path: pkg/controller/volume/persistentvolume/metrics/metrics.go
    - [ ] volume_operation_total_errors ","['sig/storage', 'sig/instrumentation', 'needs-triage']",0
kubernetes/kubernetes,Graduate resourceclaim metrics to BETA,"Parent issue: https://github.com/kubernetes/kubernetes/issues/136107

- path: pkg/controller/resourceclaim/metrics/metrics.go
    - [ ] resourceclaim_controller_resource_claims","['sig/node', 'sig/instrumentation', 'needs-triage']",0
kubernetes/kubernetes,Graduate job_controller metrics to BETA,"Parent issue: https://github.com/kubernetes/kubernetes/issues/136107

- path: pkg/controller/job/metrics/metrics.go
    - [ ] job_controller_pod_failures_handled_by_failure_policy_total
    - [ ] job_controller_terminated_pods_tracking_finalizer_total ","['sig/apps', 'sig/instrumentation', 'triage/accepted']",0
kubernetes/kubernetes,Graduate network metrics to BETA,"Parent issue: https://github.com/kubernetes/kubernetes/issues/136107

- path: staging/src/k8s.io/endpointslice/metrics/metrics.go
    - [ ] endpoint_slice_controller_changes
    - [ ] endpoint_slice_controller_desired_endpoint_slices
    - [ ] endpoint_slice_controller_endpoints_added_per_sync
    - [ ] endpoint_slice_controller_endpoints_desired
    - [ ] endpoint_slice_controller_endpoints_removed_per_sync
    - [ ] endpoint_slice_controller_num_endpoint_slices
    - [ ] endpoint_slice_controller_services_count_by_traffic_distribution","['sig/network', 'sig/instrumentation', 'needs-triage']",0
kubernetes/kubernetes,Graduate etcd metrics to BETA,"Parent issue: https://github.com/kubernetes/kubernetes/issues/136107

- path: staging/src/k8s.io/apiserver/pkg/storage/etcd3/metrics/metrics.go
    - [ ] etcd_bookmark_counts https://github.com/kubernetes/kubernetes/pull/136196
    - [ ] apiserver_storage_events_received_total
   ","['sig/instrumentation', 'needs-triage', 'sig/etcd']",0
kubernetes/kubernetes,Graduate apiserver metrics to BETA,"Parent issue: https://github.com/kubernetes/kubernetes/issues/136107

List of metrics

- path: staging/src/k8s.io/apiserver/pkg/admission/metrics/metrics.go
    - [ ] apiserver_admission_webhook_rejection_count https://github.com/kubernetes/kubernetes/pull/136196
    - [ ] apiserver_admission_webhook_request_total https://github.com/kubernetes/kubernetes/pull/136196
- path: staging/src/k8s.io/apiserver/pkg/audit/metrics.go
    - [ ] apiserver_audit_error_total https://github.com/kubernetes/kubernetes/pull/136196
    - [ ] apiserver_audit_event_total https://github.com/kubernetes/kubernetes/pull/136196 
    - [ ] apiserver_audit_level_total https://github.com/kubernetes/kubernetes/pull/136196
- path: staging/src/k8s.io/apiserver/plugin/pkg/authorizer/webhook/metrics/metrics.go
    - [ ] apiserver_authorization_webhook_duration_seconds https://github.com/kubernetes/kubernetes/pull/136196
    - [ ] apiserver_authorization_webhook_evaluations_fail_open_total https://github.com/kubernetes/kubernetes/pull/136196
    - [ ] apiserver_authorization_webhook_evaluations_total https://github.com/kubernetes/kubernetes/pull/136196
- path: staging/src/k8s.io/apiserver/pkg/authentication/request/x509/x509.go
    - [ ] apiserver_client_certificate_expiration_seconds https://github.com/kubernetes/kubernetes/pull/136196
- path: staging/src/k8s.io/apiserver/pkg/server/egressselector/metrics/metrics.go
    - [ ] apiserver_egress_dialer_dial_duration_seconds https://github.com/kubernetes/kubernetes/pull/136196
    - [ ] apiserver_egress_dialer_dial_failure_count https://github.com/kubernetes/kubernetes/pull/136196
    - [ ] apiserver_egress_dialer_dial_start_total https://github.com/kubernetes/kubernetes/pull/136196
- path: staging/src/k8s.io/apiserver/pkg/storage/value/encrypt/envelope/metrics/metrics.go
    - [ ] apiserver_envelope_encryption_dek_cache_fill_percent https://github.com/kubernetes/kubernetes/pull/136196
    - [ ] apiserver_envelope_encryption_dek_cache_inter_arrival_time_seconds https://github.com/kubernetes/kubernetes/pull/136196
- path:  staging/src/k8s.io/apiserver/pkg/util/flowcontrol/metrics/metrics.go
    - [ ] apiserver_flowcontrol_priority_level_request_utilization https://github.com/kubernetes/kubernetes/pull/136196
    - [ ] apiserver_flowcontrol_priority_level_seat_utilization https://github.com/kubernetes/kubernetes/pull/136196
    - [ ] apiserver_flowcontrol_request_concurrency_in_use https://github.com/kubernetes/kubernetes/pull/136196
    - [ ] apiserver_flowcontrol_request_concurrency_limit https://github.com/kubernetes/kubernetes/pull/136196
    - [ ] apiserver_flowcontrol_request_execution_seconds https://github.com/kubernetes/kubernetes/pull/136196
    - [ ] apiserver_flowcontrol_work_estimated_seats https://github.com/kubernetes/kubernetes/pull/136196
- path: staging/src/k8s.io/kube-aggregator/pkg/apiserver/metrics.go
    - [ ] apiserver_kube_aggregator_x509_insecure_sha1_total https://github.com/kubernetes/kubernetes/pull/136196
    - [ ] apiserver_kube_aggregator_x509_missing_san_total https://github.com/kubernetes/kubernetes/pull/136196
- path: staging/src/k8s.io/apiserver/pkg/endpoints/metrics/metrics.go
    - [ ] apiserver_request_filter_duration_seconds https://github.com/kubernetes/kubernetes/pull/136196
    - [ ] apiserver_request_sli_duration_seconds https://github.com/kubernetes/kubernetes/pull/136196
    - [ ] apiserver_watch_events_sizes
    - [ ] apiserver_watch_events_total
- path: staging/src/k8s.io/apiserver/pkg/storage/value/metrics.go
    - [ ] apiserver_storage_data_key_generation_duration_seconds
    - [ ] apiserver_storage_data_key_generation_failures_total
    - [ ] apiserver_storage_envelope_transformation_cache_misses_total
    - [ ] apiserver_storage_transformation_duration_seconds
    - [ ] apiserver_storage_transformation_operations_total
- path: staging/src/k8s.io/apiserver/pkg/storage/cacher/metrics/metrics.go
    - [ ] apiserver_terminated_watchers_total
    - [ ] watch_cache_capacity
    - [ ] apiserver_watch_cache_consistent_read_total
    - [ ] apiserver_watch_cache_events_dispatched_total
    - [ ] apiserver_watch_cache_initializations_total
    - [ ] apiserver_watch_cache_read_wait_seconds
    - [ ] apiserver_watch_cache_resource_version
- path: staging/src/k8s.io/apiserver/pkg/util/webhook/metrics.go
    - [ ] apiserver_webhooks_x509_insecure_sha1_total
    - [ ] apiserver_webhooks_x509_missing_san_total","['sig/api-machinery', 'sig/instrumentation', 'needs-triage']",0
kubernetes/kubernetes,code-generation 0.35.0 resulting in permission denied errors,"### What happened?

I am working on an extension apiservers and a library helping to create those, upgrading k8s depdendencies to 0.35.0 breaks code generation.

The `kube::codegen::gen_openapi` command from `kube_codegen.sh` reports several permission denied errors, e.g.:
```
Generating deepcopy code for 2 targets
Generating defaulter code for 1 targets
Generating conversion code for 1 targets
Generating openapi code for 1 targets
2026/01/17 17:40:09 Model name code generation error: failed executing generator: some targets had errors: errors in target ""k8s.io/apimachinery/pkg/api/resource"": open /home/nik/Development/ace/apiserver-kit/.devenv/state/go/pkg/mod/k8s.io/apimachinery@v0.35.0/pkg/api/resource/zz_generated.model_name.go: permission denied
errors in target ""k8s.io/apimachinery/pkg/apis/meta/v1"": open /home/nik/Development/ace/apiserver-kit/.devenv/state/go/pkg/mod/k8s.io/apimachinery@v0.35.0/pkg/apis/meta/v1/zz_generated.model_name.go: permission denied
errors in target ""k8s.io/apimachinery/pkg/runtime"": open /home/nik/Development/ace/apiserver-kit/.devenv/state/go/pkg/mod/k8s.io/apimachinery@v0.35.0/pkg/runtime/zz_generated.model_name.go: permission denied
...
```

It is possible to work around these issues by setting the permissions in go modules to non-readonly:
```
declare -a GOMODS=(
  ""k8s.io/apimachinery""
  ""k8s.io/api""
)
echo ""Setting permissions for files of relevant go modules to 644""
for MOD in ""${GOMODS[@]}""; do
  find ""$(go list -json -m -u ""${MOD}"" | jq -r '.Dir')"" -type f -exec chmod 644 -- {} +
done
```
However tampering with the go modules will make `go mod verify` fail and changing the permissions back will not solve this as the computed hashes differ, so the relevant modules have to be force deleted afterwards to fix this.

### What did you expect to happen?

No errors were expected to occur.

### How can we reproduce it (as minimally and precisely as possible)?

An easy way to reproduce this is to:
```
git@github.com:kubernetes/sample-apiserver.git
cd sample-apiserver
vim hack/update-codegen.sh # update L22 to: 'CODEGEN_PKG=$(go list -m -f '{{.Dir}}' k8s.io/code-generator)'
go mod download
./hack/update-codegen.sh
```

### Anything else we need to know?

This worked in prior versions, so I am wondering if the files are not opened as read-only.

### Kubernetes version

<details>
        k8s.io/api v0.35.0
	k8s.io/apimachinery v0.35.0
	k8s.io/apiserver v0.35.0
	k8s.io/client-go v0.35.0
	k8s.io/code-generator v0.35.0
	k8s.io/component-base v0.35.0
	k8s.io/kube-openapi v0.0.0-20250910181357-589584f1c912
	k8s.io/utils v0.0.0-20260106112306-0fe9cd71b2f8
</details>


### Cloud provider

<details>

</details>


### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
ANSI_COLOR=""0;38;2;126;186;228""
BUG_REPORT_URL=""https://github.com/NixOS/nixpkgs/issues""
BUILD_ID=""25.11.20251204.c97c47f""
CPE_NAME=""cpe:/o:nixos:nixos:25.11""
DEFAULT_HOSTNAME=nixos
DOCUMENTATION_URL=""https://nixos.org/learn.html""
HOME_URL=""https://nixos.org/""
ID=nixos
ID_LIKE=""""
IMAGE_ID=""""
IMAGE_VERSION=""""
LOGO=""nix-snowflake""
NAME=NixOS
PRETTY_NAME=""NixOS 25.11 (Xantusia)""
SUPPORT_END=""2026-06-30""
SUPPORT_URL=""https://nixos.org/community.html""
VARIANT=""""
VARIANT_ID=""""
VENDOR_NAME=NixOS
VENDOR_URL=""https://nixos.org/""
VERSION=""25.11 (Xantusia)""
VERSION_CODENAME=xantusia
VERSION_ID=""25.11
$ uname -a
Linux fw13 6.18.0 #1-NixOS SMP PREEMPT_DYNAMIC Sun Nov 30 22:42:10 UTC 2025 x86_64 GNU/Linux

```

</details>


### Install tools

<details>

</details>


### Container runtime (CRI) and version (if applicable)

<details>

</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>
","['kind/bug', 'sig/api-machinery', 'needs-triage']",0
kubernetes/kubernetes,kube-apiserver fails to start because start-service-ip-repair-controllers PostStartHook fails,"### What happened?

When upgrading a Kubernetes cluster from v1.32 to v1.33 by creating new control plane nodes with v1.33 manifests, the kube-apiserver fails to start with admission errors. 
```
0115 20:56:16.106534   # log start
...
E0115 20:56:24.741991       1 repairip.go:372] ""Unhandled Error"" err=""the ClusterIP [IPv4]: 10.211.95.185 for Service <namespace foo>/<service bar>is not allocated; repairing"" logger=""UnhandledError""
...
Trace[295533714]: ---""Write to database call failed"" len:247,err:ipaddresses.networking.k8s.io ""10.211.95.185"" is forbidden: not yet ready to handle request 10000ms (20:56:34.744)
Trace[295533714]: [10.000527316s] [10.000527316s] END
E0115 20:56:34.745027       1 repairip.go:235] ""Unhandled Error"" err=""ipaddresses.networking.k8s.io \""10.211.95.185\"" is forbidden: not yet ready to handle request"" logger=""UnhandledError""
I0115 20:56:34.746095       1 repairip.go:236] Shutting down ipallocator-repair-controller
I0115 20:56:34.834949       1 healthz.go:280] informer-sync,poststarthook/start-service-ip-repair-controllers check failed: readyz
[-]informer-sync failed: 4 informers not started yet: [*v1.Pod *v1.ServiceAccount *v1.Secret *v1.Namespace]
[-]poststarthook/start-service-ip-repair-controllers failed: not finished
...
I0115 20:57:06.335471       1 healthz.go:280] informer-sync,poststarthook/start-service-ip-repair-controllers check failed: readyz
[-]informer-sync failed: 1 informers not started yet: [*v1.Secret] # v1.Namespace informer is ready  
[-]poststarthook/start-service-ip-repair-controllers failed: not finished
...
F0115 20:57:18.630248       1 hooks.go:204] PostStartHook ""start-service-ip-repair-controllers"" failed: unable to perform initial IP and Port allocation check

```

We think this is a race condition introduced by new code path when `MultiCIDRServiceAllocator` is enabled by default in v1.33.  
`start-service-ip-repair-controllers` reads clusterIPs from all `Services` objects and creates `IPAddresses` objects if they are not found from cache. Since we are upgrading from v1.32, where `MultiCIDRServiceAllocator` is disabled, ETCD does not contain any `IPAddresses` objects. 
`start-service-ip-repair-controllers` therefore does not read any existing `IPAddresses` objects from cache and attempts to create them. However, at this point, the `ipaddresses.networking.k8s.io` API is not yet ready. As a result, `start-service-ip-repair-controllers` failes and the kube-apiserver never becomes ready.

Because we have a large number of namespaces, `v1.Namespace` informer takes more time to become ready, which increases the likelihood of this issue occurring in our cluster.
In v1.32 `MultiCIDRServiceAllocator` is disabled. The `start-service-ip-repair-controllers`  implementation only invokes the internal `RangeRegistry` API, which is initialized earlier, so this issue did not exist.



### What did you expect to happen?

The kube-apiserver should start successfully and the repair controller should create missing `IPAddress` objects for existing `Services`, either by:
1. Waiting for admission plugins to be ready before attempting writes
2. Deferring repair work until after startup completes
3. Retrying failed operations instead of terminating

### How can we reproduce it (as minimally and precisely as possible)?

Prerequisites:
- Cluster with large number of `Namespaces` and `Services` 
- `MultiCIDRServiceAllocator` feature gate previously disabled

Steps:
1. Prepare v1.33 kube-apiserver manifests with `MultiCIDRServiceAllocator` enabled by default
2. Start kube-apiserver
3. Observe API server fails to start


### Anything else we need to know?

_No response_

### Kubernetes version

<details>

```console
control-plane components: v1.33.7
worker node kubelet: v1.30.7
kube-proxy: v1.31.11
```

</details>


### Cloud provider

<details>
Self-managed cluster on AWS ec2 instances

</details>

### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME=""Ubuntu 24.04.3 LTS""
NAME=""Ubuntu""
VERSION_ID=""24.04""
VERSION=""24.04.3 LTS (Noble Numbat)""
VERSION_CODENAME=noble
ID=ubuntu
ID_LIKE=debian
HOME_URL=""https://www.ubuntu.com/""
SUPPORT_URL=""https://help.ubuntu.com/""
BUG_REPORT_URL=""https://bugs.launchpad.net/ubuntu/""
PRIVACY_POLICY_URL=""https://www.ubuntu.com/legal/terms-and-policies/privacy-policy""
UBUNTU_CODENAME=noble
LOGO=ubuntu-logo
$ uname -a
Linux ip-10-206-19-65 6.8.0-1044-aws #46-Ubuntu SMP Fri Nov 21 16:50:44 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux

```

</details>


### Install tools

<details>
kubeadm
</details>


### Container runtime (CRI) and version (if applicable)

<details>
- containerd: v1.7.28
- runc: v1.3.3
- cri-tools: v1.33.0-1.1 
</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
Calico: v3.30.4
</details>
","['kind/bug', 'sig/network', 'needs-triage']",0
kubernetes/kubernetes,Node Log Query: unexpected result when disabled,"### What happened?

If the feature gate and kubelet config for https://kubernetes.io/docs/concepts/cluster-administration/system-logs/#log-query  are disabled, I don't get an error for the query. Instead it returns


     <!doctype html>\n<meta name=\""viewport\"" content=\""width=device-width\"">\n<pre>\n<a href=\""alternatives.log\"">alternatives.log</a>\n<a href=\""containers/\"">containers/</a>\n<a href=\""pods/\"">pods/</a>\n</pre>\n""...

Is that intentional?

### What did you expect to happen?

I'd expect an error status for the request.

### How can we reproduce it (as minimally and precisely as possible)?

`kubectl get --raw ""/api/v1/nodes/<some real node>/proxy/logs/?query=kubelet""` in a cluster where the feature is disabled.

### Anything else we need to know?

_No response_

### Kubernetes version

v1.35.0, probably others.


### Cloud provider

<details>

</details>


### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


### Install tools

<details>

</details>


### Container runtime (CRI) and version (if applicable)

<details>

</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>
","['kind/bug', 'sig/windows', 'needs-triage']",0
kubernetes/kubernetes,Validation code generated by Validation-gen should fail fast on see immutability failure,"Validation code generated by validation-gen runs all short circuiting checks before performing the short circuiting. Due to this if multiple shortcricuits checks return errs, all errors are returned. Ideally we should break these shortciruting into two cohorts. One for update operation related one and another one for all others. This aligns well with the handwritten validation code.


```go
// +required
// +listType=map
// +listMapKey=name 1Code has comments. Press enter to view.
// +k8s:required
// +k8s:listType=map
// +k8s:listMapKey=name
// +k8s:maxItems=8
// +k8s:immutable
PodGroups []PodGroup `json:""podGroups"" protobuf:""bytes,2,rep,name=podGroups""`
````

In this example +k8s:immutable, +k8s:maxItems=8 and +k8s:required are shortcircuting one.  There is no point in running required and maxItems checks if the field has been modified in update. ","['sig/api-machinery', 'triage/accepted']",0
kubernetes/kubernetes,Failure cluster [db1839c4...] TestUnReservePreBindPlugins flaking,"Seems to be a clear recent regression:

<img width=""1183"" height=""832"" alt=""Image"" src=""https://github.com/user-attachments/assets/df477fcc-7f1f-4d34-af40-02012f30afbb"" />

/sig scheduling

### Failure cluster [db1839c4cf23d5869bed](https://go.k8s.io/triage#db1839c4cf23d5869bed)

##### Error text:
```
Failed
=== RUN   TestUnReservePreBindPlugins/All_Reserve_plugins_passed,_but_a_PreBind_plugin_failed
    plugins_test.go:1615: Reserve Plugin reservePlugin numUnreserveCalled = 2, want 1.
    plugins_test.go:1629: Expected the Prebind plugin to be called.
--- FAIL: TestUnReservePreBindPlugins/All_Reserve_plugins_passed,_but_a_PreBind_plugin_failed (0.31s)

=== RUN   TestUnReservePreBindPlugins
--- FAIL: TestUnReservePreBindPlugins (3.71s)

```
#### Recent failures:
[1/15/2026, 2:03:22 AM ci-kubernetes-integration-master-s390x](https://prow.k8s.io/view/gs/kubernetes-ci-logs/logs/ci-kubernetes-integration-master-s390x/2011740804783017984)
[1/14/2026, 9:55:20 PM ci-kubernetes-integration-master-s390x](https://prow.k8s.io/view/gs/kubernetes-ci-logs/logs/ci-kubernetes-integration-master-s390x/2011678393258479616)
[1/14/2026, 7:45:55 AM ci-kubernetes-integration-master-ppc64le](https://prow.k8s.io/view/gs/kubernetes-ci-logs/logs/ci-kubernetes-integration-master-ppc64le/2011463474323918848)
[1/14/2026, 5:23:18 AM ci-kubernetes-integration-master-s390x](https://prow.k8s.io/view/gs/kubernetes-ci-logs/logs/ci-kubernetes-integration-master-s390x/2011428745344192512)
[1/14/2026, 3:14:06 AM ci-kubernetes-integration-arm64-master](https://prow.k8s.io/view/gs/kubernetes-ci-logs/logs/ci-kubernetes-integration-arm64-master/2011396281204740096)


/kind failing-test
<!-- If this is a flake, please add: /kind flake -->

<!-- Please assign a SIG using: /sig SIG-NAME -->","['sig/scheduling', 'kind/failing-test', 'needs-triage']",0
kubernetes/kubernetes,TestWaitForAllPodsUnmount is Flaky,"### Which jobs are flaking?

pull-kubernetes-unit

### Which tests are flaking?

TestWaitForAllPodsUnmount

### Since when has it been flaking?

1/15, likely after https://github.com/kubernetes/kubernetes/pull/135794 merged

### Testgrid link

https://prow.k8s.io/view/gs/kubernetes-ci-logs/pr-logs/pull/136008/pull-kubernetes-unit/2011869458158587904

### Reason for failure (if possible)

Haven't looked too much into but seems like a timeout in volume mounting

### Anything else we need to know?

_No response_

### Relevant SIG(s)

/sig node
/sig storage","['sig/storage', 'kind/flake', 'triage/accepted']",0
kubernetes/kubernetes,Consider deleting PodStatusResult type,"This type was defined in 2015 in https://github.com/kubernetes/kubernetes/pull/3565 but doesn't appear to be used anywhere. Was it ever used?

Any change to pod status type fields produces changes in test fixtures for this type as well (xref https://github.com/kubernetes/kubernetes/pull/135196/changes#diff-a2d3e6609ba3f895e4e1b3ea6b47ba6e99f45372f6dcabd1c9f1023962637ef4)

If we're not using this, we should probably delete it

/sig node","['sig/node', 'needs-triage']",0
kubernetes/kubernetes,[kubelet] Unnecessary continuous reconciliation loop for DRA Pods due to non-kubelet-owned fields comparison,"new: add test and log for kubelet (Kubernetes v1.35.0).

### What happened?

For Pods using Dynamic Resource Allocation (DRA), the kubelet status manager continuously logs ""Pod status is inconsistent with cached status"" and triggers a reconciliation loop (needsReconcile -> syncPod, every ~10s). This loop is unnecessary because no actual status changes are applied, but it adds avoidable load on kubelet, API server, and etcd.
kubelet log -v=4
```
I0115 16:58:36.998365  584662 status_manager.go:262] ""Syncing all statuses""
I0115 16:58:36.999471  584662 status_manager.go:1175] ""Pod status is inconsistent with cached status for pod, a reconciliation should be triggered"" pod=""default/dp-e7c13e09e5-5b4fd7bb6d-f95kc"" statusDiff=<
	@@ -183,11 +183,5 @@
	    }
	   }
	  ],
	- ""qosClass"": ""Guaranteed"",
	- ""resourceClaimStatuses"": [
	-  {
	-   ""name"": ""test-kubelet"",
	-   ""resourceClaimName"": ""dp-e7c13e09e5-5b4fd7bb6d-f95kc-test-kubelet-nh64j""
	-  }
	- ]
	+ ""qosClass"": ""Guaranteed""
	 }
 >
I0115 16:58:36.999523  584662 kubelet_pods.go:1223] ""Clean up pod workers for terminated pods""
......
I0115 16:58:37.002499  584662 status_manager.go:1065] ""Patch status for pod"" pod=""default/dp-e7c13e09e5-5b4fd7bb6d-f95kc"" podUID=""d949fbdc-23b5-4a50-b57b-31ea53189032"" patch=""{\""metadata\"":{\""uid\"":\""d949fbdc-23b5-4a50-b57b-31ea53189032\""}}""
I0115 16:58:37.002510  584662 status_manager.go:1072] ""Status for pod is up-to-date"" pod=""default/dp-e7c13e09e5-5b4fd7bb6d-f95kc"" statusVersion=2
......
I0115 16:58:46.998609  584662 status_manager.go:262] ""Syncing all statuses""
I0115 16:58:46.998627  584662 kubelet.go:2685] ""SyncLoop (housekeeping)""
I0115 16:58:46.999152  584662 status_manager.go:1175] ""Pod status is inconsistent with cached status for pod, a reconciliation should be triggered"" pod=""default/dp-e7c13e09e5-5b4fd7bb6d-f95kc"" statusDiff=<
	@@ -183,11 +183,5 @@
	    }
	   }
	  ],
	- ""qosClass"": ""Guaranteed"",
	- ""resourceClaimStatuses"": [
	-  {
	-   ""name"": ""test-kubelet"",
	-   ""resourceClaimName"": ""dp-e7c13e09e5-5b4fd7bb6d-f95kc-test-kubelet-nh64j""
	-  }
	- ]
	+ ""qosClass"": ""Guaranteed""
	 }
 >
```


### What did you expect to happen?

The kubelet should ignore non-kubelet-owned fields (ResourceClaimStatuses, ExtendedResourceClaimStatus) during status comparison, and not trigger unnecessary reconciliation loops for DRA Pods.


### How can we reproduce it (as minimally and precisely as possible)?

1. Deploy a Kubernetes cluster with DRA enabled.
2. Create a Pod that uses DRA resources (with ResourceClaim).
3. Observe the kubelet logs and API server/etcd load.
4. Find the continuous ""status inconsistent"" logs and repeated reconciliation operations.

### Anything else we need to know?

This issue is caused by the `isPodStatusByKubeletEqual` function comparing the entire Pod status, including fields that are not managed by kubelet (populated by control plane components). A fix is provided in PR #136238.

### Kubernetes version

<details>
```console
$ ./kubelet --version
Kubernetes v1.35.0
```
</details>

$ ./kubelet --version
Kubernetes v1.35.0


### Cloud provider

<details>
self-hosted cluster
</details>
self-hosted cluster

### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>

Ubuntu 24.04.3 LTS

### Install tools

<details>

</details>


### Container runtime (CRI) and version (if applicable)

<details>

</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>
","['kind/bug', 'priority/important-soon', 'sig/node', 'triage/accepted', 'wg/device-management']",0
kubernetes/kubernetes,The timeout behavior of the WaitForAPIServer function is abnormal.,"### What happened?

In my program, I have referenced the k8s.io/controller-manager repository. I will use the `WaitForAPIServer`  method within it to wait for the new Kubernetes cluster to become ready.

However, I often find that some Kubernetes clusters that have failed to access properly do not terminate their waiting process within the expected timeout period.



### What did you expect to happen?

I believe that the `WaitForAPIServer` method should always return immediately after the execution exceeds the time corresponding to the timeout value.

### How can we reproduce it (as minimally and precisely as possible)?

A program needs to be written to call the WaitForAPIServer method, set the timeout value to 5 seconds, and make it access an unreachable APIServer (for example, using an unreachable IP to check the total time consumed when calling this method)


````
	// build a client to a unreachable apiserver
	clientBuilder := NewKubernetesClientBuilder(""basic-cluster-mesh/"", restConfig)
	versionedClient := clientBuilder.Client(""test-cli"")
	
	tNow := time.Now()
	if err := app.WaitForAPIServer(versionedClient, 5*time.Second); err != nil {
		return ControllerContext{}, err
	}
	apiTime := time.Now()
	klog.Infof(""WaitForAPIServer time-used: %v"", apiTime.Sub(tNow))

````

### Anything else we need to know?

https://github.com/kubernetes/controller-manager/blob/master/app/helper.go#L35
````
// WaitForAPIServer waits for the API Server's /healthz endpoint to report ""ok"" with timeout.
func WaitForAPIServer(client clientset.Interface, timeout time.Duration) error {
	var lastErr error

	err := wait.PollImmediate(time.Second, timeout, func() (bool, error) {
		healthStatus := 0
         ....
````

We can see that here, PollImmediate is called. This function first calls the conditionFunc and then starts the timeout poller. This means that the time consumed by the first conditionFunc will not be taken into account.

### Kubernetes version

<details>

```console
$ kubectl version
Client Version: version.Info{Major:""1"", Minor:""22+"", GitVersion:""v1.22.0-alpha.0.3737+f14064de76123b-dirty"", GitCommit:""f14064de76123be6975badbe411b5812c2d02e89"", GitTreeState:""dirty"", BuildDate:""2023-12-21T11:02:54Z"", GoVersion:""go1.17.3"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""22+"", GitVersion:""v1.22.0-alpha.0.3737+f14064de76123b-dirty"", GitCommit:""f14064de76123be6975badbe411b5812c2d02e89"", GitTreeState:""dirty"", BuildDate:""2023-12-21T11:02:33Z"", GoVersion:""go1.17.10"", Compiler:""gc"", Platform:""linux/amd64""}
```

</details>


### Cloud provider

<details>
None
</details>


### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


### Install tools

<details>

</details>


### Container runtime (CRI) and version (if applicable)

<details>

</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>
","['kind/bug', 'needs-triage', 'sig/k8s-infra']",0
kubernetes/kubernetes,kubernetes is reconfiguring iptables/netfilter data structures with bad settings,"### What happened?

Hello,

I've been debugging issues with my Kubernetes nodes having very high CPU usage in network components.

I've finally traced down the origin to that block of code, which is overriding sysctl settings for iptables with insanely high values.
https://github.com/kubernetes/kubernetes/blob/master/pkg/proxy/conntrack/sysctls.go#L62

The default is for the kernel to configurable iptables data structures based on available memory. Typically 32k-256k buckets and 256k max entries (this may be a tad low for the most busy servers). 

**Problem: Kubernetes is reconfiguring the iptables data structures based on the number of CPU cores. It's broken on machines with many CPU.
On my servers with 512 cores, I am seeing Kubernetes reconfigure the kernel to have 16M iptables entries with a 4M hash table.
That's a 32 MB hash table that can't fit in CPU cache, with 6 GB of entries. It's ridiculous.**

The large data structure is causing constant thrashing of the CPU cache on all cores, as every network packet has to lookup the table multiple times, and network packets are distributed across all CPU cores for processing.

Some operations need to go over the whole hashtable, for read or write (write has locks), which is affected by the large size of the table. This is why you've been having issues with nginx proxy (among other bugs), you've got bug tickets investigating performance issue with processing/updating that table, but nobody pointing out that it should not have millions of items, no wonder it's slow to process!

Can you please correct this code?

This should be set to a fixed value. I think 256k buckets, 1M max entries is a good default value that should work for >99% of users.
If anyone reaches that limit, they should check with `netstat` what is consuming active entries, then they probably need to reduce the sysctl timeout to evict these entries faster, as the default is keeping them for too long.

```
net.nf_conntrack_max = 1048576
net.netfilter.nf_conntrack_buckets = 262144
```

Regards.

### What did you expect to happen?

iptables should have reasonable configuration

### How can we reproduce it (as minimally and precisely as possible)?

you can look at sysctl settings on any kubernetes nodes with many cores and see the problem

### Anything else we need to know?

_No response_

### Kubernetes version

all versions affected

<details>

```console
$ kubectl version
# paste output here
```

</details>


### Cloud provider

<details>
on premise
</details>


### OS version

all Linux OS are affected as kubernetes is incorrected overwriting iptables settings from the OS.

<details>
```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here
```
</details>


### Install tools

<details>
</details>


### Container runtime (CRI) and version (if applicable)

<details>
</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
</details>
","['kind/bug', 'sig/network', 'triage/needs-information']",0
kubernetes/kubernetes,Support async moveAllToActiveOrBackoffQueue in scheduler,"### What would you like to be added?

Kube-Scheduler uses scheduling hint to determine whether the pods should be moved to ActiveQ. Scheduler will lock and traverse all pods in unschedulableQ to find which pods might become schedulable due to the incoming event. Many operation on SchedulingQueue will be blocked if the events come too frequently.

For example, if someone updates assigned pod in a high frequency, the SchedulingQueue will be blocked by assignedPodUpdate() and new pods can not be added into the activeQ, result in scheduler hang.

I want to make moveAllToActiveOrBackoffQueue run in a background goroutine, check multi cluster events once a time, and limit its frequency to avoid influencing other SchedulingQueue operation.

### Why is this needed?

If someone updates assigned pod in a high frequency, the SchedulingQueue will be blocked by assignedPodUpdate() and new pods can not be added into the activeQ, result in scheduler hang.","['kind/feature', 'needs-sig', 'needs-triage']",0
kubernetes/kubernetes,"Windows pod stuck in ""PodInitializing"" with ContainerStatusUnknown when init container startup probe already exists after main container is unexpectedly removed","### What happened?

I am running the kube-proxy as a DaemonSet on Windows. During startup of the nodes, kube-proxy occasionally gets in a bad state, with the following state and last state:

```
    State:          Waiting
      Reason:       PodInitializing
    Last State:     Terminated
      Reason:       ContainerStatusUnknown
      Message:      The container could not be located when the pod was deleted.  The container used to be Running
      Exit Code:    137
      Started:      Mon, 01 Jan 0001 00:00:00 +0000
      Finished:     Mon, 01 Jan 0001 00:00:00 +0000
```

There is an init container that allocates the source VIP and provides a startup probe when the VIP has been allocated for the main kube-proxy container to startup. Unfortunately once the kube-proxy container is removed and Kubernetes gets in this state, it fails to start the kube-proxy container or restart the pod, and the kubelet emits this error message:

```
E0113 23:19:04.761091    1404 prober_manager.go:197] ""Startup probe already exists for container"" pod=""kube-system/kube-proxy-windows-6qpp5"" containerName=""allocate-source-vip""
```

The probe manager appears to be in a bad state due to the main container disappearing, and then presumably it can't add the new startup probe (which would be passing at this point if it was querying the init container properly) due to [AddPod](https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/prober/prober_manager.go#L197).

### What did you expect to happen?

Kubernetes should recover from this state and start the kube-proxy container (or restart the pod).

### How can we reproduce it (as minimally and precisely as possible)?

Set up a Kubernetes cluster with a Linux machine as the controller and a Windows machine as the worker as per https://github.com/RedpointGames/uet/wiki/Deploying-Kubernetes-clusters-for-build-automation. You may need to restart the Windows node multiple times or delete the kube-proxy container with `ctr` to get it into this state (in my case it gets into this state when booting and it's fairly consistent, but I am also using bare metal machines provisioned via PXE boot and the behaviour inside a VM might differ).

You can view the kube-proxy daemon set definition here: https://github.com/RedpointGames/uet/blob/main/UET/Lib/Helm/rkm/templates/kube-proxy-windows/kube-proxy-windows.yaml and the scripts for allocating source VIP and running kube-proxy here: https://github.com/RedpointGames/uet/blob/main/UET/Lib/Helm/rkm/templates/kube-proxy-windows/kube-proxy-windows-scripts.yaml

### Anything else we need to know?

Even if kube-proxy as a DaemonSet is not officially supported on Windows, Kubernetes should still not allow pods to get stuck in this state on Windows nodes.

### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.35.0
Kustomize Version: v5.7.1
Server Version: v1.35.0
```

</details>


### Cloud provider

<details>
N/A - RKM on bare metal nodes
</details>


### OS version

<details>

```console
C:\> Get-CimInstance Win32_OperatingSystem | fl -Property Caption, Version, BuildNumber, OSArchitecture

Caption        : Microsoft Windows 11 Pro
Version        : 10.0.26100
BuildNumber    : 26100
OSArchitecture : 64-bit

```

</details>


### Install tools

<details>

https://github.com/RedpointGames/uet/wiki/Deploying-Kubernetes-clusters-for-build-automation

</details>


### Container runtime (CRI) and version (if applicable)

<details>

```
    componentVersions:
      cniPlugins: 1.9.0
      containerd: 2.2.1
      etcd: 3.6.7
      flannel: 0.27.4
      flannelCniSuffix: -flannel1
      kubernetes: 1.35.0
      rkm: 2026.1010.977
      runc: 1.3.4
```

</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>
","['kind/bug', 'needs-sig', 'needs-triage']",0
kubernetes/kubernetes,Support for OFFLINE volume expansion using CSI driver using a rolling upgrade strategy,"### What would you like to be added?

Some CSI drivers (e.g. https://github.com/outscale/osc-bsu-csi-driver up to v1.7.0) support OFFLINE volume expansion only. If the users would like to resize PVC for pods controlled by StatefulSet, they should manually scale down all replicas before the external-resizer controller can resize the volumes. Otherwise, the StatefulSet controller creates the new Pod immediately without giving a chance for the _AttachDetachController_ to detach the volume from the worker.

It would be good to support OFFLINE volume expansion using the StatfulSet rolling upgrade strategy.

### Why is this needed?

Deployments with CSI drivers that support OFFLINE volume resize can still benefit from application no-downtime using a StatefulSet rolling upgrade strategy.","['kind/feature', 'needs-sig', 'needs-triage']",0
kubernetes/kubernetes,Kubelet CSI init backoff too short when DNS is unreachable; kubelet restarts before NSS falls back to `myhostname`,"### What happened?

## Problem origin (ARO/OCP bugs)
- OCPBUGS-64883 and OCPBUGS-67200: on ARO with UDR and a non-responsive DNS, new workers never register Node/CSINode. Kubelet restarts during CSI init.

## Root cause
- NSS `hosts:` ordering on RHEL/RHCOS/Fedora is typically `files dns myhostname`. With blackholed DNS, glibc waits for DNS timeouts before reaching `myhostname`.
- Default resolver budget can be ~20â€“60s (A+AAAA, up to 3 nameservers, attempts=2, timeout=5s), longer if `timeout`/`attempts` are higher.
- Kubeletâ€™s CSI init backoff in `pkg/volume/csi/csi_plugin.go` is ~23s total, despite the comment claiming ~140s. It exits before DNS fallback completes, so kubelet restarts and never initializes `CSINode`.

## Why increase the timeout
- To survive realistic DNS timeouts and let `myhostname` succeed, the backoff should exceed the DNS worst case. Raising it toward the documented ~140s (or at least >60â€“90s) aligns behavior with the comment and prevents restart loops when DNS is slow/unreachable.

## OS/NSS considerations
- Reordering NSS to place `myhostname` before `dns` would avoid the wait, but distro defaults/operator preferences differ; consensus is hard. Increasing kubeletâ€™s backoff is a practical, compatible mitigation.

## Proposed backoff
```
New exponential backoff steps:
  sleep 1:  32.55437ms
  sleep 2:  254.104702ms
  sleep 3:  2.059965819s
  sleep 4:  16.567802964s
  sleep 5:  2m14.365124124s  <-- roughly 140 seconds
```

## Request
- Update CSI init backoff parameters so the actual cap matches the intended ~140s (or another explicit target â‰¥ DNS timeout budget), and fix the comment to reflect the real value. This allows kubelet to complete CSI/CSINode initialization even when DNS is slow or unreachable.

### What did you expect to happen?

Exponential backoff should be roughly 140 seconds.

### How can we reproduce it (as minimally and precisely as possible)?

    1. Create a new worker node whose DNS server will time out
    2. Observe that kubelet will never create the Node object

### Anything else we need to know?

NOTE: I assume that the original backoff values assumed there will be 6 sleep cycles. However, the `ExponentialBackoff` skips the last sleep iteration: https://github.com/kubernetes/kubernetes/blob/9f6977db54084d766c1fec19f383bf509d7e8610/staging/src/k8s.io/apimachinery/pkg/util/wait/backoff.go#L480-L481

### Kubernetes version

<details>

```console
Reproduced on Kubernetes 1.33 (from original bug report - OpenShift 4.20)
```

</details>


### Cloud provider

<details>
ARO
</details>


### OS version

<details>

```console
N/A
```

</details>


### Install tools

<details>
N/A
</details>


### Container runtime (CRI) and version (if applicable)

<details>
N/A
</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
N/A
</details>
","['kind/bug', 'needs-sig', 'needs-triage']",0
kubernetes/kubernetes,kubelet: DATA RACEs,"### What happened?

Support for race detection also in the kubelet started working in https://prow.k8s.io/view/gs/kubernetes-ci-logs/pr-logs/pull/133844/pull-kubernetes-e2e-kind-alpha-beta-features-race/2010727678495166464 and found (truncated list, see job for full output):

- DATA RACE:
  
      Read at 0x00c002b6f410 by goroutine 583:
        k8s.io/apimachinery/pkg/apis/meta/v1.(*Time).MarshalJSON()
            <autogenerated>:1 +0x44
        encoding/json.marshalerEncoder()
            encoding/json/encode.go:483 +0x13c
        encoding/json.structEncoder.encode()
            encoding/json/encode.go:758 +0x3c7
        encoding/json.structEncoder.encode-fm()
            <autogenerated>:1 +0xe4
        encoding/json.structEncoder.encode()
            encoding/json/encode.go:758 +0x3c7
        encoding/json.structEncoder.encode-fm()
            <autogenerated>:1 +0xe4
        encoding/json.(*encodeState).reflectValue()
            encoding/json/encode.go:367 +0x83
        encoding/json.(*encodeState).marshal()
            encoding/json/encode.go:343 +0xdb
        encoding/json.Marshal()
            encoding/json/encode.go:209 +0x11e
        k8s.io/kubernetes/pkg/util/pod.preparePatchBytesForPodStatus()
            k8s.io/kubernetes/pkg/util/pod/pod.go:58 +0x2d3
        k8s.io/kubernetes/pkg/util/pod.PatchPodStatus()
            k8s.io/kubernetes/pkg/util/pod/pod.go:35 +0x12b
        k8s.io/kubernetes/pkg/kubelet/status.(*manager).syncPod()
            k8s.io/kubernetes/pkg/kubelet/status/status_manager.go:1064 +0xada
        k8s.io/kubernetes/pkg/kubelet/status.(*manager).syncBatch()
            k8s.io/kubernetes/pkg/kubelet/status/status_manager.go:1025 +0x199
        k8s.io/kubernetes/pkg/kubelet/status.(*manager).Start.func1()
            k8s.io/kubernetes/pkg/kubelet/status/status_manager.go:260 +0x1a4
        k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1()
            k8s.io/apimachinery/pkg/util/wait/backoff.go:233 +0x2e
        k8s.io/apimachinery/pkg/util/wait.BackoffUntilWithContext.func1()
            k8s.io/apimachinery/pkg/util/wait/backoff.go:255 +0x98
        k8s.io/apimachinery/pkg/util/wait.BackoffUntilWithContext()
            k8s.io/apimachinery/pkg/util/wait/backoff.go:256 +0xed
        k8s.io/apimachinery/pkg/util/wait.BackoffUntil()
            k8s.io/apimachinery/pkg/util/wait/backoff.go:233 +0x8a
        k8s.io/apimachinery/pkg/util/wait.JitterUntil()
            k8s.io/apimachinery/pkg/util/wait/backoff.go:210 +0xfb
        k8s.io/apimachinery/pkg/util/wait.Until()
            k8s.io/apimachinery/pkg/util/wait/backoff.go:163 +0x50
        k8s.io/apimachinery/pkg/util/wait.Forever()
            k8s.io/apimachinery/pkg/util/wait/wait.go:80 +0x2a
        k8s.io/kubernetes/pkg/kubelet/status.(*manager).Start.gowrap1()
            k8s.io/kubernetes/pkg/kubelet/status/status_manager.go:255 +0x17
      Previous write at 0x00c002b6f410 by goroutine 128655:
        k8s.io/kubernetes/pkg/kubelet/status.normalizeStatus.func1()
            k8s.io/kubernetes/pkg/kubelet/status/status_manager.go:1193 +0x1ee
        k8s.io/kubernetes/pkg/kubelet/status.normalizeStatus()
            k8s.io/kubernetes/pkg/kubelet/status/status_manager.go:1209 +0x175
        k8s.io/kubernetes/pkg/kubelet/status.(*manager).updateStatusInternal()
            k8s.io/kubernetes/pkg/kubelet/status/status_manager.go:838 +0x9dc
        ...
        k8s.io/kubernetes/pkg/kubelet.(*Kubelet).SyncPod()
            k8s.io/kubernetes/pkg/kubelet/kubelet.go:2100 +0x4043
        k8s.io/kubernetes/pkg/kubelet.(*podWorkers).podWorkerLoop.func1()
            k8s.io/kubernetes/pkg/kubelet/pod_workers.go:1309 +0x7ed
        k8s.io/kubernetes/pkg/kubelet.(*podWorkers).podWorkerLoop()
            k8s.io/kubernetes/pkg/kubelet/pod_workers.go:1314 +0x7ee
        k8s.io/kubernetes/pkg/kubelet.(*podWorkers).UpdatePod.func1()
            k8s.io/kubernetes/pkg/kubelet/pod_workers.go:970 +0x19c
      Goroutine 583 (running) created at:
        k8s.io/kubernetes/pkg/kubelet/status.(*manager).Start()
            k8s.io/kubernetes/pkg/kubelet/status/status_manager.go:255 +0x27e
        k8s.io/kubernetes/pkg/kubelet.(*Kubelet).Run()
            k8s.io/kubernetes/pkg/kubelet/kubelet.go:1877 +0xdde
        k8s.io/kubernetes/cmd/kubelet/app.startKubelet.gowrap1()
            k8s.io/kubernetes/cmd/kubelet/app/server.go:1264 +0x50
      Goroutine 128655 (finished) created at:
        k8s.io/kubernetes/pkg/kubelet.(*Kubelet).HandlePodCleanups()
            k8s.io/kubernetes/pkg/kubelet/kubelet_pods.go:1263 +0x66f
        k8s.io/kubernetes/pkg/kubelet.(*Kubelet).syncLoopIteration()
            k8s.io/kubernetes/pkg/kubelet/kubelet.go:2690 +0x29b2
        ...
        k8s.io/kubernetes/pkg/kubelet.(*Kubelet).syncLoop()
            k8s.io/kubernetes/pkg/kubelet/kubelet.go:2542 +0x51d
        k8s.io/kubernetes/pkg/kubelet.(*Kubelet).Run()
            k8s.io/kubernetes/pkg/kubelet/kubelet.go:1896 +0xfea
        k8s.io/kubernetes/cmd/kubelet/app.startKubelet.gowrap1()
            k8s.io/kubernetes/cmd/kubelet/app/server.go:1264 +0x50

- DATA RACE:
  
      Write at 0x00c0009c0178 by goroutine 129432:
        k8s.io/kubernetes/pkg/kubelet/status.normalizeStatus.func1()
            k8s.io/kubernetes/pkg/kubelet/status/status_manager.go:1193 +0x1ee
        k8s.io/kubernetes/pkg/kubelet/status.normalizeStatus()
            k8s.io/kubernetes/pkg/kubelet/status/status_manager.go:1209 +0x175
        k8s.io/kubernetes/pkg/kubelet/status.(*manager).updateStatusInternal()
            k8s.io/kubernetes/pkg/kubelet/status/status_manager.go:838 +0x9dc
        k8s.io/kubernetes/pkg/kubelet/status.(*manager).TerminatePod()
            k8s.io/kubernetes/pkg/kubelet/status/status_manager.go:637 +0x1228
        k8s.io/kubernetes/pkg/kubelet.(*Kubelet).SyncTerminatedPod()
            k8s.io/kubernetes/pkg/kubelet/kubelet.go:2409 +0x17f5
        ...
        k8s.io/kubernetes/pkg/kubelet/kuberuntime.(*kubeGenericRuntimeManager).SyncPod()
            k8s.io/kubernetes/pkg/kubelet/kuberuntime/kuberuntime_manager.go:1543 +0x2114
        k8s.io/kubernetes/pkg/kubelet.(*Kubelet).SyncPod()
            k8s.io/kubernetes/pkg/kubelet/kubelet.go:2162 +0x2e41
        k8s.io/kubernetes/pkg/kubelet/cm.(*podContainerManagerImpl).EnsureExists()
            k8s.io/kubernetes/pkg/kubelet/cm/pod_container_manager_linux.go:100 +0x7c1
        k8s.io/kubernetes/pkg/kubelet.(*Kubelet).SyncPod()
            k8s.io/kubernetes/pkg/kubelet/kubelet.go:2104 +0x4387
        k8s.io/kubernetes/pkg/kubelet/cm.(*qosContainerManagerImpl).UpdateCgroups()
            k8s.io/kubernetes/pkg/kubelet/cm/qos_container_manager_linux.go:361 +0xe30
        k8s.io/kubernetes/pkg/kubelet/cm.(*containerManagerImpl).UpdateQOSCgroups()
            k8s.io/kubernetes/pkg/kubelet/cm/container_manager_linux.go:606 +0x76
        k8s.io/kubernetes/pkg/kubelet.(*Kubelet).SyncPod()
            k8s.io/kubernetes/pkg/kubelet/kubelet.go:2100 +0x4043
        k8s.io/kubernetes/pkg/kubelet.(*podWorkers).podWorkerLoop.func1()
            k8s.io/kubernetes/pkg/kubelet/pod_workers.go:1309 +0x7ed
        k8s.io/kubernetes/pkg/kubelet.(*podWorkers).podWorkerLoop()
            k8s.io/kubernetes/pkg/kubelet/pod_workers.go:1314 +0x7ee
        k8s.io/kubernetes/pkg/kubelet.(*podWorkers).UpdatePod.func1()
            k8s.io/kubernetes/pkg/kubelet/pod_workers.go:970 +0x19c
      Previous read at 0x00c0009c0178 by goroutine 583:
        k8s.io/apimachinery/pkg/apis/meta/v1.(*Time).MarshalJSON()
            <autogenerated>:1 +0x44
        encoding/json.marshalerEncoder()
            encoding/json/encode.go:483 +0x13c
        encoding/json.structEncoder.encode()
            encoding/json/encode.go:758 +0x3c7
        encoding/json.structEncoder.encode-fm()
            <autogenerated>:1 +0xe4
        encoding/json.structEncoder.encode()
            encoding/json/encode.go:758 +0x3c7
        encoding/json.structEncoder.encode-fm()
            <autogenerated>:1 +0xe4
        encoding/json.(*encodeState).reflectValue()
            encoding/json/encode.go:367 +0x83
        encoding/json.(*encodeState).marshal()
            encoding/json/encode.go:343 +0xdb
        encoding/json.Marshal()
            encoding/json/encode.go:209 +0x11e
        k8s.io/kubernetes/pkg/util/pod.preparePatchBytesForPodStatus()
            k8s.io/kubernetes/pkg/util/pod/pod.go:58 +0x2d3
        k8s.io/kubernetes/pkg/util/pod.PatchPodStatus()
            k8s.io/kubernetes/pkg/util/pod/pod.go:35 +0x12b
        k8s.io/kubernetes/pkg/kubelet/status.(*manager).syncPod()
            k8s.io/kubernetes/pkg/kubelet/status/status_manager.go:1064 +0xada
        k8s.io/kubernetes/pkg/kubelet/status.(*manager).syncBatch()
            k8s.io/kubernetes/pkg/kubelet/status/status_manager.go:1025 +0x199
        k8s.io/kubernetes/pkg/kubelet/status.(*manager).Start.func1()
            k8s.io/kubernetes/pkg/kubelet/status/status_manager.go:260 +0x1a4
        k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1()
            k8s.io/apimachinery/pkg/util/wait/backoff.go:233 +0x2e
        k8s.io/apimachinery/pkg/util/wait.BackoffUntilWithContext.func1()
            k8s.io/apimachinery/pkg/util/wait/backoff.go:255 +0x98
        k8s.io/apimachinery/pkg/util/wait.BackoffUntilWithContext()
            k8s.io/apimachinery/pkg/util/wait/backoff.go:256 +0xed
        k8s.io/apimachinery/pkg/util/wait.BackoffUntil()
            k8s.io/apimachinery/pkg/util/wait/backoff.go:233 +0x8a
        k8s.io/apimachinery/pkg/util/wait.JitterUntil()
            k8s.io/apimachinery/pkg/util/wait/backoff.go:210 +0xfb
        k8s.io/apimachinery/pkg/util/wait.Until()
            k8s.io/apimachinery/pkg/util/wait/backoff.go:163 +0x50
        k8s.io/apimachinery/pkg/util/wait.Forever()
            k8s.io/apimachinery/pkg/util/wait/wait.go:80 +0x2a
        k8s.io/kubernetes/pkg/kubelet/status.(*manager).Start.gowrap1()
            k8s.io/kubernetes/pkg/kubelet/status/status_manager.go:255 +0x17
      Goroutine 129432 (running) created at:
        k8s.io/kubernetes/pkg/kubelet.(*Kubelet).HandlePodCleanups()
            k8s.io/kubernetes/pkg/kubelet/kubelet_pods.go:1263 +0x66f
        k8s.io/kubernetes/pkg/kubelet.(*Kubelet).syncLoopIteration()
            k8s.io/kubernetes/pkg/kubelet/kubelet.go:2690 +0x29b2
        k8s.io/kubernetes/pkg/kubelet.(*Kubelet).syncLoop()
            k8s.io/kubernetes/pkg/kubelet/kubelet.go:2542 +0x51d
        k8s.io/kubernetes/pkg/kubelet/container.(*runtimeCache).ForceUpdateIfOlder()
            k8s.io/kubernetes/pkg/kubelet/container/runtime_cache.go:77 +0x130
        k8s.io/kubernetes/pkg/kubelet.(*Kubelet).HandlePodCleanups()
            k8s.io/kubernetes/pkg/kubelet/kubelet_pods.go:1263 +0x66f
        ...
        k8s.io/kubernetes/pkg/kubelet.(*Kubelet).syncLoop()
            k8s.io/kubernetes/pkg/kubelet/kubelet.go:2542 +0x51d
        k8s.io/kubernetes/pkg/kubelet.(*Kubelet).Run()
            k8s.io/kubernetes/pkg/kubelet/kubelet.go:1896 +0xfea
        k8s.io/kubernetes/cmd/kubelet/app.startKubelet.gowrap1()
            k8s.io/kubernetes/cmd/kubelet/app/server.go:1264 +0x50
      Goroutine 583 (running) created at:
        k8s.io/kubernetes/pkg/kubelet/status.(*manager).Start()
            k8s.io/kubernetes/pkg/kubelet/status/status_manager.go:255 +0x27e
        k8s.io/kubernetes/pkg/kubelet.(*Kubelet).Run()
            k8s.io/kubernetes/pkg/kubelet/kubelet.go:1877 +0xdde
        k8s.io/kubernetes/cmd/kubelet/app.startKubelet.gowrap1()
            k8s.io/kubernetes/cmd/kubelet/app/server.go:1264 +0x50

- DATA RACE:
  
      Read at 0x00c001ba0180 by goroutine 583:
        k8s.io/apimachinery/pkg/apis/meta/v1.(*Time).MarshalJSON()
            <autogenerated>:1 +0x44
        encoding/json.marshalerEncoder()
            encoding/json/encode.go:483 +0x13c
        encoding/json.structEncoder.encode()
            encoding/json/encode.go:758 +0x3c7
        encoding/json.structEncoder.encode-fm()
            <autogenerated>:1 +0xe4
        encoding/json.structEncoder.encode()
            encoding/json/encode.go:758 +0x3c7
        encoding/json.structEncoder.encode-fm()
            <autogenerated>:1 +0xe4
        encoding/json.(*encodeState).reflectValue()
            encoding/json/encode.go:367 +0x83
        encoding/json.(*encodeState).marshal()
            encoding/json/encode.go:343 +0xdb
        encoding/json.Marshal()
            encoding/json/encode.go:209 +0x11e
        k8s.io/kubernetes/pkg/util/pod.preparePatchBytesForPodStatus()
            k8s.io/kubernetes/pkg/util/pod/pod.go:58 +0x2d3
        k8s.io/kubernetes/pkg/util/pod.PatchPodStatus()
            k8s.io/kubernetes/pkg/util/pod/pod.go:35 +0x12b
        k8s.io/kubernetes/pkg/kubelet/status.(*manager).syncPod()
            k8s.io/kubernetes/pkg/kubelet/status/status_manager.go:1064 +0xada
        k8s.io/kubernetes/pkg/kubelet/status.(*manager).syncBatch()
            k8s.io/kubernetes/pkg/kubelet/status/status_manager.go:1025 +0x199
        k8s.io/kubernetes/pkg/kubelet/status.(*manager).Start.func1()
            k8s.io/kubernetes/pkg/kubelet/status/status_manager.go:260 +0x1a4
        k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1()
            k8s.io/apimachinery/pkg/util/wait/backoff.go:233 +0x2e
        k8s.io/apimachinery/pkg/util/wait.BackoffUntilWithContext.func1()
            k8s.io/apimachinery/pkg/util/wait/backoff.go:255 +0x98
        k8s.io/apimachinery/pkg/util/wait.BackoffUntilWithContext()
            k8s.io/apimachinery/pkg/util/wait/backoff.go:256 +0xed
        k8s.io/apimachinery/pkg/util/wait.BackoffUntil()
            k8s.io/apimachinery/pkg/util/wait/backoff.go:233 +0x8a
        k8s.io/apimachinery/pkg/util/wait.JitterUntil()
            k8s.io/apimachinery/pkg/util/wait/backoff.go:210 +0xfb
        k8s.io/apimachinery/pkg/util/wait.Until()
            k8s.io/apimachinery/pkg/util/wait/backoff.go:163 +0x50
        k8s.io/apimachinery/pkg/util/wait.Forever()
            k8s.io/apimachinery/pkg/util/wait/wait.go:80 +0x2a
        k8s.io/kubernetes/pkg/kubelet/status.(*manager).Start.gowrap1()
            k8s.io/kubernetes/pkg/kubelet/status/status_manager.go:255 +0x17
      Previous write at 0x00c001ba0180 by goroutine 165064:
        k8s.io/kubernetes/pkg/kubelet/status.normalizeStatus.func1()
            k8s.io/kubernetes/pkg/kubelet/status/status_manager.go:1193 +0x1ee
        k8s.io/kubernetes/pkg/kubelet/status.normalizeStatus()
            k8s.io/kubernetes/pkg/kubelet/status/status_manager.go:1209 +0x175
        k8s.io/kubernetes/pkg/kubelet/status.(*manager).updateStatusInternal()
            k8s.io/kubernetes/pkg/kubelet/status/status_manager.go:838 +0x9dc
        k8s.io/kubernetes/pkg/kubelet/status.(*manager).TerminatePod()
            k8s.io/kubernetes/pkg/kubelet/status/status_manager.go:637 +0x1228
        ...
        k8s.io/kubernetes/pkg/kubelet.(*podWorkers).podWorkerLoop.func1()
            k8s.io/kubernetes/pkg/kubelet/pod_workers.go:1309 +0x7ed
        k8s.io/kubernetes/pkg/kubelet.(*podWorkers).podWorkerLoop()
            k8s.io/kubernetes/pkg/kubelet/pod_workers.go:1314 +0x7ee
        k8s.io/kubernetes/pkg/kubelet.(*podWorkers).UpdatePod.func1()
            k8s.io/kubernetes/pkg/kubelet/pod_workers.go:970 +0x19c
      Goroutine 583 (running) created at:
        k8s.io/kubernetes/pkg/kubelet/status.(*manager).Start()
            k8s.io/kubernetes/pkg/kubelet/status/status_manager.go:255 +0x27e
        k8s.io/kubernetes/pkg/kubelet.(*Kubelet).Run()
            k8s.io/kubernetes/pkg/kubelet/kubelet.go:1877 +0xdde
        k8s.io/kubernetes/cmd/kubelet/app.startKubelet.gowrap1()
            k8s.io/kubernetes/cmd/kubelet/app/server.go:1264 +0x50
      Goroutine 165064 (running) created at:
        k8s.io/kubernetes/pkg/kubelet.(*Kubelet).HandlePodCleanups()
            k8s.io/kubernetes/pkg/kubelet/kubelet_pods.go:1263 +0x66f
        k8s.io/kubernetes/pkg/kubelet.(*Kubelet).syncLoopIteration()
            k8s.io/kubernetes/pkg/kubelet/kubelet.go:2690 +0x29b2
        k8s.io/kubernetes/pkg/kubelet.(*Kubelet).syncLoop()
            k8s.io/kubernetes/pkg/kubelet/kubelet.go:2542 +0x51d
        k8s.io/kubernetes/pkg/kubelet/container.(*runtimeCache).ForceUpdateIfOlder()
            k8s.io/kubernetes/pkg/kubelet/container/runtime_cache.go:77 +0x130
        k8s.io/kubernetes/pkg/kubelet.(*Kubelet).HandlePodCleanups()
            k8s.io/kubernetes/pkg/kubelet/kubelet_pods.go:1263 +0x66f
...
        k8s.io/kubernetes/pkg/kubelet.(*Kubelet).syncLoopIteration()
            k8s.io/kubernetes/pkg/kubelet/kubelet.go:2690 +0x29b2
        k8s.io/kubernetes/pkg/kubelet.(*Kubelet).syncLoop()
            k8s.io/kubernetes/pkg/kubelet/kubelet.go:2542 +0x51d
        k8s.io/kubernetes/pkg/kubelet.(*Kubelet).Run()
            k8s.io/kubernetes/pkg/kubelet/kubelet.go:1896 +0xfea
        k8s.io/kubernetes/cmd/kubelet/app.startKubelet.gowrap1()
            k8s.io/kubernetes/cmd/kubelet/app/server.go:1264 +0x50






### What did you expect to happen?

No data races.

### How can we reproduce it (as minimally and precisely as possible)?

See https://github.com/kubernetes/kubernetes/pull/133844.

### Anything else we need to know?

/sig node


### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


### Cloud provider

<details>

</details>


### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


### Install tools

<details>

</details>


### Container runtime (CRI) and version (if applicable)

<details>

</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>
","['kind/bug', 'sig/node', 'priority/important-longterm', 'triage/accepted']",0
kubernetes/kubernetes,Pod stuck in terminating state,"### What happened?

When deleted, the pod moves to terminating state and gets stuck. The one container is in state ""terminated"" so there should be nothing preventing shutdown. Per https://github.com/kubernetes/kubernetes/issues/51835#issuecomment-780923556 I'm opening a new issue and attaching pod info (this yaml?) and kubelet logs.

I'm not sure about the timestamps, but the pod delete was triggered at around 12:53 utc.

[kubeletlogs.txt](https://github.com/user-attachments/files/24567897/kubeletlogs.txt)
[podstatus.txt](https://github.com/user-attachments/files/24567898/podstatus.txt)

### What did you expect to happen?

Pod should move to terminated state/disappear

### How can we reproduce it (as minimally and precisely as possible)?

TBD

### Anything else we need to know?

I was in the process of destroying the cluster with terraform, but the control nodes and all dependent infrastructure are still up. The pod was running on control nodes (which are still alive). The container had an EBS volume attached.

I'll keep the cluster around for 24h or so, or longer if more info is requested.

I couldn't find any newer issues, or anything the various internet guides mentioned (volume gone, node gone).

### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


### Cloud provider

<details>
AWS, EBS CSI, AWS CCM
</details>

### OS version

<details>

Talos 1.11.5

</details>


### Install tools

<details>
Terraform, custom k8s provider using golang k8s library dynamic client (applying generic yaml directly)
</details>


### Container runtime (CRI) and version (if applicable)

<details>

</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>
","['kind/support', 'sig/node', 'needs-triage']",0
kubernetes/kubernetes,NodeLocalDNS: Serve stale DNS responses on upstream failures to prevent workload outages,"### What would you like to be added?

Introduce **stale DNS response serving** in NodeLocalDNS, similar to â€œserveâ€‘staleâ€ mechanisms implemented in production resolvers (e.g., Unbound, BIND).

#### Behavior

If a cached record exists (even if TTL has expired), NodeLocalDNS should return the stale response **when any of the following conditions occur**:

1. Upstream DNS is unreachable or times out
2. Upstream DNS returns a temporary `NXDOMAIN`
3. Upstream DNS returns a response with no IP addresses (empty A/AAAA records)

This mechanism is intended as a **resiliency feature**, not a replacement for normal TTLâ€‘based resolution.



### Why is this needed?

NodeLocalDNS currently does not prevent outages due to DNS resolution failures when the upstream DNS resolver(s):

- Are temporarily unavailable or not functioning
- Return transient `NXDOMAIN` responses
- Return responses without any IP addresses (empty A/AAAA answers)

These failures can directly propagate to workloads and cause application outages, even when valid DNS data existed shortly before the failure.

RFC 8767 already defines serving stale DNS responses when in case when upstream dns servers are unavailable or not functioning. In addition to this we need stale responses even for temporary NXDOMAIN and empty responses.

Intermittent DNS failures are a wellâ€‘known source of cascading outages in distributed systems. A recent highâ€‘profile AWS outage (caused by transient DNS resolution failures) highlighted how shortâ€‘lived DNS unavailability can lead to widespread service impact.

### References

- AWS DNSâ€‘related outage (highâ€‘level incident summary)
- RFC 8767 â€“ Serving Stale DNS Data to Improve Resilency","['sig/network', 'kind/feature', 'triage/accepted']",0
kubernetes/kubernetes,Kubelet do not rotate existed container logs when reboot node,"### What happened?

1. reboot the node for many times, found the container existed logs reserved in the pod logDir
2. The increasing pod log will cost too many system disk storage

```
/var/log/pods/ccos-kube-apiserver_kube-apiserver-single1_bb79bd757eaad4d6cec553f5c650706f/kube-apiserver
[root@single1 kube-apiserver]# ls
0.log  1.log  2.log  3.log 4.log
```

### What did you expect to happen?

Remove the existed container logs(at least not to keep to many)

### How can we reproduce it (as minimally and precisely as possible)?

just reboot the node, and check the `log dir`

### Anything else we need to know?

_No response_

### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
1.29
</details>


### Cloud provider

<details>
baremetal
</details>


### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here
5.15.67
# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


### Install tools

<details>

</details>


### Container runtime (CRI) and version (if applicable)

<details>

</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>
","['kind/bug', 'sig/instrumentation', 'needs-triage']",0
kubernetes/kubernetes,Discrepancy between `kubectl top pod` and the sum of Rss in `/proc/[pid]/smaps`,"We encountered inconsistencies among various memory metrics in our production environment. On the front-end monitoring page, we use `kubectl top pod` to display memory usage. Its output is as follows:
```
kubectl top pod clickhouse-keeper-0 
NAME                  CPU(cores)   MEMORY(bytes)   
clickhouse-keeper-0   1314m        7117Mi   
```
Afterwards, we entered the pod:
```
kubectl exec -it clickhouse-keeper-0 bash
```
We then checked the cgroup memory statistics inside the pod, which showed:
```
cat /sys/fs/cgroup/memory/memory.stat 
cache 1650110464
rss 6907166720
rss_huge 6092226560
shmem 0
mapped_file 8028160
dirty 140435456
writeback 0
swap 0
pgpgin 9951374731
pgpgout 9954750697
pgfault 4135903411
pgmajfault 297
inactive_anon 0
active_anon 6906904576
inactive_file 1098760192
active_file 551292928
unevictable 0
hierarchical_memory_limit 8589934592
hierarchical_memsw_limit 9223372036854771712
total_cache 1650110464
total_rss 6907166720
total_rss_huge 6092226560
total_shmem 0
total_mapped_file 8028160
total_dirty 140435456
total_writeback 0
total_swap 0
total_pgpgin 9951374731
total_pgpgout 9954750697
total_pgfault 413590319
total_pgmajfault 297
total_inactive_anon 0
total_active_anon 6906904576
total_inactive_file 1098760192
total_active_file 551292928
total_unevictable 0
```
Finally, we examined the Rss metric in the `smaps` file of the pod's only process, summed it up, and the result was:
```
grep Rss /proc/1/smaps | awk '{sum+=$2} END {print sum "" kB""}'
5579016 kB
```
The anonymous huge page memory usage was:
```
grep AnonHuge /proc/1/smaps | awk '{sum+=$2} END {print sum "" kB""}'
3330048 kB
```
Based on the data above, the memory usage value from `kubectl top pod` is largely consistent with the `Rss` value from the `cgroup` inside the pod. However, there is a significant discrepancy compared to the sum of `Rss` from the `smaps` of that process inside the pod.

What I would like to know is: Why is there a difference between the sum of RSS in `smaps` and the memory usage reported by `kubectl top pod`?

Looking forward to your reply. Thanks.","['sig/node', 'needs-triage']",0
kubernetes/kubernetes,[Failing Test] ci-kubernetes-e2e-capz-master-windows,"### Which jobs are failing?

https://prow.k8s.io/view/gs/kubernetes-ci-logs/logs/ci-kubernetes-e2e-capz-master-windows/2010473857915293696

### Which tests are failing?

```

Sun, 11 Jan 2026 22:22:00 +0000: cleaning up
./capz/run-capz-e2e.sh: line 120: capz::ci-build-azure-ccm::cleanup: command not found
E0111 22:22:30.955992    2239 memcache.go:265] ""Unhandled Error"" err=""couldn't get current server API group list: Get \""https://capz-conf-ghwrcj-569d42ee.francecentral.cloudapp.azure.com:6443/api?timeout=32s\"": dial tcp 4.251.157.217:6443: i/o timeout""
E0111 22:23:00.957051    2239 memcache.go:265] ""Unhandled Error"" err=""couldn't get current server API group list: Get \""https://capz-conf-ghwrcj-569d42ee.francecentral.cloudapp.azure.com:6443/api?timeout=32s\"": dial tcp 4.251.157.217:6443: i/o timeout""
E0111 22:23:30.958153    2239 memcache.go:265] ""Unhandled Error"" err=""couldn't get current server API group list: Get \""https://capz-conf-ghwrcj-569d42ee.francecentral.cloudapp.azure.com:6443/api?timeout=32s\"": dial tcp 4.251.157.217:6443: i/o timeout""
E0111 22:24:00.959078    2239 memcache.go:265] ""Unhandled Error"" err=""couldn't get current server API group list: Get \""https://capz-conf-ghwrcj-569d42ee.francecentral.cloudapp.azure.com:6443/api?timeout=32s\"": dial tcp 4.251.157.217:6443: i/o timeout""
E0111 22:24:30.960339    2239 memcache.go:265] ""Unhandled Error"" err=""couldn't get current server API group list: Get \""https://capz-conf-ghwrcj-569d42ee.francecentral.cloudapp.azure.com:6443/api?timeout=32s\"": dial tcp 4.251.157.217:6443: i/o timeout""
Unable to connect to the server: dial tcp 4.251.157.217:6443: i/o timeout
```

### Since when has it been failing?

Dec 18th, 2025.
https://prow.k8s.io/job-history/gs/kubernetes-ci-logs/logs/ci-kubernetes-e2e-capz-master-windows?buildId=2002346247846891520

https://github.com/kubernetes/kubernetes/compare/53b29512ec85c9...bb52ae5e24a4eb

### Testgrid link

https://testgrid.k8s.io/sig-release-master-informing#capz-windows-master

### Reason for failure (if possible)

```
./capz/run-capz-e2e.sh: line 120: capz::ci-build-azure-ccm::cleanup: command not found
```

### Anything else we need to know?

_No response_

### Relevant SIG(s)

/sig windows cluster-provider","['sig/windows', 'kind/failing-test', 'needs-triage']",0
kubernetes/kubernetes,API reference: CRD URL is incorrectly parsed with trailing parenthesis,"### What happened?

[The Kubernetes CRD API reference documentation for JSONSchemaProps](https://kubernetes.io/docs/reference/kubernetes-api/extend-resources/custom-resource-definition-v1/#JSONSchemaProps) contains a broken link.

> JSONSchemaProps is a JSON-Schema following Specification Draft 4 (http://json-schema.org/).

The URL to json-schema.org includes a trailing `)` character, resulting in `http://json-schema.org/)` which returns a 404 error. When rendered on the website, the closing parenthesis is incorrectly included as part of the URL.

### What did you expect to happen?

The link should point to https://json-schema.org/ (without the trailing parenthesis) and load successfully.

### How can we reproduce it (as minimally and precisely as possible)?

1. Navigate to the [The Kubernetes CRD API reference documentation for JSONSchemaProps](https://kubernetes.io/docs/reference/kubernetes-api/extend-resources/custom-resource-definition-v1/#JSONSchemaProps)
2. Click on the json-schema.org link in the description
3. The URL includes ) and fails to load


### Anything else we need to know?

While it may be possible to fix this in the [reference-docs generator](https://github.com/kubernetes-sigs/reference-docs), the most straightforward solution is to fix the source comment in k/k apiextensions-apiserver.
Since the URL also needs to be updated from http to https, fixing the source comment addresses both issues at once and is the most appropriate approach.

https://github.com/kubernetes/website/issues/53971

/assign

### Kubernetes version

-

### Cloud provider

<details>

</details>


### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


### Install tools

<details>

</details>


### Container runtime (CRI) and version (if applicable)

<details>

</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>
","['kind/bug', 'needs-sig', 'needs-triage']",0
kubernetes/kubernetes,Promote old HPA alpha metrics to Beta,"### What would you like to be added?

The HPA currently exposes several metrics that are still marked as alpha, even though they have met the [requirements for promotion to beta](https://github.com/kubernetes/community/blob/9de615daee15bdee370a825dc0ef150eac189700/contributors/devel/sig-instrumentation/metric-instrumentation.md#graduating-to-beta). Given their maturity and longevity, I believe the following metrics should be promoted to beta:

- `reconciliations_total` (introduced in #116010)
- `reconciliation_duration_seconds` (introduced in #116010)
- `metric_computation_total` (introduced in #116326)
- `metric_computation_duration_seconds` (introduced in #116326)

All of the metrics listed above have sufficient test coverage and have existed for a long time without significant issues, satisfying the criteria for promotion to beta.

### Why is this needed?

Ref: #135726

/assign
/cc @adrianmoisey @richabanker @dgrisonnet","['sig/autoscaling', 'kind/feature', 'sig/instrumentation', 'needs-triage']",0
kubernetes/kubernetes,[Failing Test] [SIG-node] periodic-node-feature-discovery-e2e-test-master.Overall,"### Which jobs are failing?

periodic-node-feature-discovery-e2e-test-master.Overall

### Which tests are failing?

https://testgrid.k8s.io/sig-node-node-feature-discovery#periodic-e2e-test-master

### Since when has it been failing?

12/31/2025

### Testgrid link

https://testgrid.k8s.io/sig-node-node-feature-discovery#periodic-e2e-test-master

### Reason for failure (if possible)

`curl: (22) The requested URL returned error: 404
Image gcr.io/k8s-staging-nfd/node-feature-discovery:v0.19.0-devel-134-gc46cac1 not found`

### Anything else we need to know?

@ArangoGutierrez 

### Relevant SIG(s)

/sig node","['sig/node', 'kind/failing-test', 'needs-triage']",0
angular/angular,docs: counter of running webcontainer is not decreased when browser crashed due to OOM,"There is an edge case, which is easier to reproduce on Safari when user will open a few tabs with running webcontainer (tutorial pages, homepage if it's scrolled to embedded editor or playground).

When an OOM error occurs in Safari, the tab is reloaded automatically in that case event `beforeunload` was not catched and count of running webcontainers wasn't decreased.

In the result the alert message:
`You currently have more than 3 tabs open running the Angular Tutorials or Playground,
        this may result in an <code>Out of memory</code> error.`
        could be displayed even when user has less opened tabs with Embedded Editor than recommended.
        
The idea of solution:
I'm thinking about reset counter when will close the `warning` message.

","['help wanted', 'good first issue', 'P2', 'area: docs-infra']",1
angular/angular,docs: Allow users to compare answers to work in progress code in Tutorials,"## Problem

In the current state of the tutorial, clicking on ""Reveal Answer"" immediately replaces the entire code that the student is using and there's no way to compare or revert back to the previous version.

## Suggested Solution

Allowing students to see the answer and their code side by side would be valuable so they can replicate it correctly in their own code. 

cc @MarkTechson ","['help wanted', 'good first issue', 'P3', 'area: docs-infra']",1
angular/angular,How to set up a 404 page when hosted on GitHub Pages is incorrect,"### Describe the problem that you experienced

When going directly to a URL listed in the routes array, I received a 404 error on GitHub

### Enter the URL of the topic with the problem

_No response_

### Describe what you were looking for in the documentation

My angular app was going to GitHubâ€™s 404 error if I went to a route listed in the routes array or refreshed the browser on any page besides the homepage. The docs did explain why this happens but the solution it provides is wrong.

### Describe the actions that led you to experience the problem

I copied index.html and renamed it 404.html and added an empty .nojekyll file but 404 errors now went to a blank page. I followed instructions here and it worked, caching the request route, sending you to the homepage, then going to the cached route from within the app: https://shermandigital.com/blog/fix-404-errors-from-angular-projects-hosted-on-github-pages/

### Describe what you want to experience that would fix the problem

_No response_

### Add a screenshot if that helps illustrate the problem

_No response_

### If this problem caused an exception or error, please paste it here

_No response_

### If the problem is browser-specific, please specify the device, OS, browser, and version

_No response_

### Provide any additional information here in as much as detail as you can

_No response_","['help wanted', 'good first issue', 'P4', 'area: docs']",1
angular/angular,Ability to configure custom JSON parser in HttpClient,"### Which @angular/* package(s) are relevant/related to the feature request?

common

### Description

There is a common use case to customize JSON parsing in `HttpClient`, which was already discussed in issue https://github.com/angular/angular/issues/21079 (closed without solution). There was also a very simple pull request https://github.com/angular/angular/pull/25027 favoring a `JSON_PARSER` injection token for customizing the parser. This was rejected in https://github.com/angular/angular/pull/25027#issuecomment-769880911 as it was considered fairly straightforward to have the user implement a `JsonHttpInterceptor` as suggested in https://stackblitz.com/edit/angular-ivy-nrdj5q?file=src%2Fapp%2Fcustom-json-parser.ts,src%2Fapp%2Fjson-interceptor.ts.

However, if it were really straightforward, the suggested code would not change the behavior if customized with the original `JSON.parse` method - but it does in the following way:

- It does not strip away the `XSSI_PREFIX` from the response body.
- It does not return `null` for an empty response body.
- It changes the error handling: In case of a parse error, a `SyntaxError` from the `JSON.parse` call is raised on the observable error channel instead of an `HttpErrorResponse` object with its `error` property set to a `HttpJsonParseError` object `{ error, text }` where `error` is the `SyntaxError` and `text` is the original response body that could not be parsed.

This means, errors can no longer be handled as described in https://angular.io/guide/http#handling-request-errors. The user may end up in changing the error handling in unrelated HTTP interceptors and for `httpClient` calls throughout his code.

### Proposed solution

Accepting the pull request https://github.com/angular/angular/pull/25027 would not cause such problems.

### Alternatives considered

Modifying the `JsonHttpInterceptor` from https://stackblitz.com/edit/angular-ivy-nrdj5q?file=src%2Fapp%2Fapp.module.ts to a more adequate solution. The user would need to look up the implementation of `HttpXhrBackend` (https://github.com/angular/angular/blob/main/packages/common/http/src/xhr.ts#L150) and reimplement the following code in a different way in `JsonHttpInterceptor` (which is error-prone rather than straightforward):

```
        // ok determines whether the response will be transmitted on the event or
        // error channel. Unsuccessful status codes (not 2xx) will always be errors,
        // but a successful status code can still result in an error if the user
        // asked for JSON data and the body cannot be parsed as such.
        let ok = status >= 200 && status < 300;

        // Check whether the body needs to be parsed as JSON (in many cases the browser
        // will have done that already).
        if (req.responseType === 'json' && typeof body === 'string') {
          // Save the original body, before attempting XSSI prefix stripping.
          const originalBody = body;
          body = body.replace(XSSI_PREFIX, '');
          try {
            // Attempt the parse. If it fails, a parse error should be delivered to the user.
            body = body !== '' ? JSON.parse(body) : null;
          } catch (error) {
            // Since the JSON.parse failed, it's reasonable to assume this might not have been a
            // JSON response. Restore the original body (including any XSSI prefix) to deliver
            // a better error response.
            body = originalBody;

            // If this was an error request to begin with, leave it as a string, it probably
            // just isn't JSON. Otherwise, deliver the parsing error to the user.
            if (ok) {
              // Even though the response status was 2xx, this is still an error.
              ok = false;
              // The parse error contains the text of the body that failed to parse.
              body = {error, text: body} as HttpJsonParseError;
            }
          }
        }

        if (ok) {
          // A successful response is delivered on the event stream.
          observer.next(new HttpResponse({
            body,
            headers,
            status,
            statusText,
            url: url || undefined,
          }));
          // The full body has been received and delivered, no further events
          // are possible. This request is complete.
          observer.complete();
        } else {
          // An unsuccessful request is delivered on the error channel.
          observer.error(new HttpErrorResponse({
            // The error in this case is the response body (error from the server).
            error: body,
            headers,
            status,
            statusText,
            url: url || undefined,
          }));
        }
      };
```","['feature', 'help wanted', 'area: common/http', 'good first issue', 'P4']",1
angular/angular,Forms should throw error if two or more NgModels appear in form with the same name,"[ ] bug report => search github for a similar issue or PR before submitting
[x] feature request
[ ] support request => Please do not submit support request here, instead see https://github.com/angular/angular/blob/master/CONTRIBUTING.md#question

**Current behavior**

Currently there is no error or warning if two form controls have the same name:

```
<input name=""entireStateCoverage"" type=""checkbox"" [(ngModel)]=""item.entireStateCoverage"">
<input name=""entireStateCoverage"" type=""checkbox"" [(ngModel)]=""item.active"">
```

**Expected behavior**

Forms should throw error if duplicate names are detected.

**What is the motivation / use case for changing the behavior?**

This leads to unpredictable results (value overwrites) which are difficult to find.

* **Angular version:** 2.4.3

* **Browser:** [all]
 
* **Language:** [all]
","['feature', 'freq1: low', 'area: forms', 'state: has PR', 'good first issue', 'feature: under consideration']",1
angular/angular,Signal forms array field tracking value causes orphan field error,"### Which @angular/* package(s) are the source of the bug?

core, forms

### Is this a regression?

No

### Description

When in a signalform, you have an array of objects, outside of $index there doesn't seem to be any good tracking value that doesn't cause warnings or errors when the source of the form updates.

```
@for (child of form.children; track ???) {
```

Tracking by $index works, but isn't ideal when the array-order may change.
Tracking by `child` produces a warning (NG0956)
Tracking by `child.id().value()` causes an orphan field error

The only somewhat working solution that I've found is `track formSource().children[$index].id`, but I can't imagine that's intended

### Please provide a link to a minimal reproduction of the bug

https://stackblitz.com/edit/stackblitz-starters-8eawr6lq?file=src%2Fmain.ts

### Please provide the exception or error you saw

```true
With `track child.id()`

ERROR Error: RuntimeError: orphan field, can't find element in array <root>.items

With `track child`

NG0956: The configured tracking expression (track by identity) caused re-creation of the entire collection of size 2. This is an expensive operation requiring destruction and subsequent creation of DOM nodes, directives, components etc. Please review the ""track expression"" and make sure that it uniquely identifies items in a collection. Find more at https://v21.angular.dev/errors/NG0956
```

### Please provide the environment you discovered this bug in (run `ng version`)

```true
The Stackblitz is Angular 21.0
The environment I originally had this problem on is Angular 21.1

Angular CLI       : 21.1.0
Angular           : 21.1.0
Node.js           : 22.21.1
Package Manager   : pnpm 10.21.0
Operating System  : win32 x64

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Package                           â”‚ Installed Version â”‚ Requested Version â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ @angular/animations               â”‚ 21.1.0            â”‚ ^21.1.0           â”‚
â”‚ @angular/build                    â”‚ 21.1.0            â”‚ ^21.1.0           â”‚
â”‚ @angular/cli                      â”‚ 21.1.0            â”‚ ^21.1.0           â”‚
â”‚ @angular/common                   â”‚ 21.1.0            â”‚ ^21.1.0           â”‚
â”‚ @angular/compiler                 â”‚ 21.1.0            â”‚ ^21.1.0           â”‚
â”‚ @angular/compiler-cli             â”‚ 21.1.0            â”‚ ^21.1.0           â”‚
â”‚ @angular/core                     â”‚ 21.1.0            â”‚ ^21.1.0           â”‚
â”‚ @angular/forms                    â”‚ 21.1.0            â”‚ ^21.1.0           â”‚
â”‚ @angular/localize                 â”‚ 21.1.0            â”‚ 21.1.0            â”‚
â”‚ @angular/platform-browser         â”‚ 21.1.0            â”‚ ^21.1.0           â”‚
â”‚ @angular/platform-browser-dynamic â”‚ 21.1.0            â”‚ ^21.1.0           â”‚
â”‚ @angular/router                   â”‚ 21.1.0            â”‚ ^21.1.0           â”‚
â”‚ rxjs                              â”‚ 7.8.2             â”‚ ~7.8.2            â”‚
â”‚ typescript                        â”‚ 5.9.3             â”‚ ~5.9.3            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Anything else?

Most likely related to #65881 ","['area: forms', 'forms: signals']",0
angular/angular,feat(language-service): Pull-Based Diagnostics (LSP 3.17),"# Feature Request: Pull-Based Diagnostics (LSP 3.17)

## Description

The Angular Language Service should implement Pull-Based Diagnostics (LSP 3.17) to allow VS Code to request diagnostics on-demand rather than having the server push them on every change.

## Current Behavior

Currently, the Angular Language Service uses push-based diagnostics via `textDocument/publishDiagnostics`. The server pushes diagnostics to the client on every file change, which can cause:

1. **Redundant computations**: Diagnostics are recomputed even for unchanged files
2. **Server-driven timing**: No control over when diagnostics are computed
3. **Unnecessary updates**: Switching between tabs triggers diagnostic recomputation

## Expected Behavior

Pull-Based Diagnostics (LSP 3.17) should:

1. **Cache results**: Return `Unchanged` report in ~0.2ms when file hasn't changed
2. **Client-controlled timing**: Let VS Code decide when to request diagnostics
3. **Workspace batching**: Support `workspace/diagnostic` for all open files
4. **Refresh signal**: Support `workspace/diagnostic/refresh` for server-initiated refresh

### Protocol Flow

```
Client                          Server
  |                                |
  |-- textDocument/diagnostic ---->|
  |      (with previousResultId)   |
  |                                |
  |<-- Unchanged (resultId match)--|  <- Cache hit: ~0.2ms
  |         OR                     |
  |<-- Full (new diagnostics) ----|  <- Cache miss: compute
```

## Motivation

This feature works together with #66668 (Client-Side File Watching) to provide comprehensive performance improvements:

| Phase | Optimization | Benefit |
|-------|-------------|---------|
| **Project Open** | #66668 File Watching | Fast initialization |
| **File Editing** | Pull Diagnostics | Cached results, reduced CPU |
| **Tab Switching** | Pull Diagnostics | Instant cached responses |

## Implementation Notes

- Requires LSP 3.17 support (vscode-languageserver v9.0.1+)
- Should fall back to push-based diagnostics for older clients
- Cache should be keyed by `resultId` and document version

## Labels

- `area: language-service`
- `feature`
- `vscode-extension`
",['area: vscode-extension'],0
angular/angular,feat(language-service): add Document Symbols support for Angular templates,"# Feature Request: Document Symbols support for Angular templates

## Description

The Angular Language Service should provide Document Symbols for Angular templates to enhance IDE features like the Outline panel, breadcrumbs navigation, and ""Go to Symbol"" (Cmd+Shift+O / Ctrl+Shift+O).

## Current Behavior

Currently, the Angular Language Service only provides Document Symbols for TypeScript code. Template elements either aren't shown in the Outline, or rely on HTML Language Features which doesn't understand Angular-specific syntax like `@if`, `@for`, structural directives, etc.

## Expected Behavior

Document Symbols should include:

1. **Block syntax**: `@if`, `@for`, `@switch`, `@defer`, `@let` with their expressions
2. **Structural directives**: `*ngIf`, `*ngFor`, `*ngSwitch` with their expressions
3. **Context variables**: Loop items, aliases (`let i = $index`), expression aliases (`as alias`)
4. **Template references**: `#ref` variables
5. **HTML elements**: With semantically appropriate icons

### SymbolKind Mapping

| Template Element | SymbolKind | Icon |
|-----------------|------------|------|
| `@if`, `@switch` | Struct | ðŸ”¶ |
| `@for` | Array | ðŸ“¦ |
| `@defer` | Event | âš¡ |
| HTML elements | Object | ðŸŸ¡ |
| Variables | Variable | |

### Configuration

```json
{
  // Disable Angular document symbols
  ""angular.documentSymbols.enabled"": false,
  
  // Show implicit @for variables ($index, $count, etc.)
  ""angular.documentSymbols.showImplicitForVariables"": true
}
```

## Example

Template:
```html
@for (item of items; track item.id; let i = $index) {
  @if (item.visible; as isVisible) {
    <div #container>{{ item.name }}</div>
  }
}
```

Expected Outline:
```
@for (item of items)  ðŸ“¦
â”œâ”€â”€ let item
â”œâ”€â”€ let i
â”œâ”€â”€ @if (item.visible; as isVisible)  ðŸ”¶
â”‚   â”œâ”€â”€ let isVisible
â”‚   â””â”€â”€ <div>  ðŸŸ¡
â”‚       â””â”€â”€ #container
```

## Motivation

- **Improved navigation**: Quickly jump to any template element
- **Better overview**: Understand template structure at a glance  
- **Consistency**: Match experience with TypeScript code navigation
- **Modern Angular**: Full support for new control flow syntax

## Labels

- `area: language-service`
- `feature`
- `vscode-extension`
",['area: language-service'],0
angular/angular,Signal forms custom validator with 'fieldTree' field in validation result error,"### Which @angular/* package(s) are the source of the bug?

forms

### Is this a regression?

Yes

### Description

After upgrading to 21.1.0 and with the removal customError() from signal forms returning ValidationError from custom validator results in compilation error.

Documentation https://angular.dev/guide/forms/signals/validation#using-validate has an example. It does not state that 'fieldTree' works only in submit function, but have a comment in the example code.

```typescript
interface DTO {
  firstName: string;
  lastName: string;
}

export class App {
  private readonly model = model<DTO>({
    firstName: '',
    lastName: '',
  });

  protected readonly form = form(this.model, (path) => {
    validate(path, (ctx) => {
      if (ctx.valueOf(path.firstName).length < 5) {
        return {
          kind: 'test',
          message: 'Message',
          fieldTree: path.lastName, // or ctx.fieldTree.lastName
        };
      }
      return null;
    });    
  });
}
```

### Please provide a link to a minimal reproduction of the bug

https://stackblitz.com/edit/stackblitz-starters-8ik35v9w?file=src%2Fmain.ts

### Please provide the exception or error you saw

```true
Argument of type '(ctx: RootFieldContext<DTO>) => { kind: string; message: string; fieldTree: SchemaPath<string, 1, Child>; } | null' is not assignable to parameter of type 'FieldValidator<DTO, Root>'.
  Type '{ kind: string; message: string; fieldTree: SchemaPath<string, 1, Child>; } | null' is not assignable to type 'ValidationResult<WithoutField>'.
    Type '{ kind: string; message: string; fieldTree: SchemaPath<string, 1, PathKind.Child>; }' is not assignable to type 'ValidationResult<WithoutField>'.
      Type '{ kind: string; message: string; fieldTree: SchemaPath<string, 1, PathKind.Child>; }' is not assignable to type 'WithoutField'.
        Types of property 'fieldTree' are incompatible.
          Type 'SchemaPath<string, 1, Child>' is not assignable to type 'undefined'.
```

### Please provide the environment you discovered this bug in (run `ng version`)

```true
Angular CLI       : 21.1.0
Angular           : 21.1.0
Node.js           : 24.12.0
Package Manager   : npm 11.7.0
Operating System  : win32 x64

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Package                           â”‚ Installed Version â”‚ Requested Version â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ @angular-devkit/build-angular     â”‚ 21.1.0            â”‚ 21.1.0            â”‚
â”‚ @angular-devkit/core              â”‚ 21.1.0            â”‚ 21.1.0            â”‚
â”‚ @angular-devkit/schematics        â”‚ 21.1.0            â”‚ 21.1.0            â”‚
â”‚ @angular/animations               â”‚ 21.1.0            â”‚ 21.1.0            â”‚
â”‚ @angular/build                    â”‚ 21.1.0            â”‚ 21.1.0            â”‚
â”‚ @angular/cdk                      â”‚ 21.1.0            â”‚ 21.1.0            â”‚
â”‚ @angular/cli                      â”‚ 21.1.0            â”‚ 21.1.0            â”‚
â”‚ @angular/common                   â”‚ 21.1.0            â”‚ 21.1.0            â”‚
â”‚ @angular/compiler                 â”‚ 21.1.0            â”‚ 21.1.0            â”‚
â”‚ @angular/compiler-cli             â”‚ 21.1.0            â”‚ 21.1.0            â”‚
â”‚ @angular/core                     â”‚ 21.1.0            â”‚ 21.1.0            â”‚
â”‚ @angular/forms                    â”‚ 21.1.0            â”‚ 21.1.0            â”‚
â”‚ @angular/google-maps              â”‚ 21.1.0            â”‚ 21.1.0            â”‚
â”‚ @angular/language-service         â”‚ 21.1.0            â”‚ 21.1.0            â”‚
â”‚ @angular/localize                 â”‚ 21.1.0            â”‚ 21.1.0            â”‚
â”‚ @angular/material                 â”‚ 21.1.0            â”‚ 21.1.0            â”‚
â”‚ @angular/platform-browser         â”‚ 21.1.0            â”‚ 21.1.0            â”‚
â”‚ @angular/platform-browser-dynamic â”‚ 21.1.0            â”‚ 21.1.0            â”‚
â”‚ @angular/pwa                      â”‚ 21.1.0            â”‚ 21.1.0            â”‚
â”‚ @angular/router                   â”‚ 21.1.0            â”‚ 21.1.0            â”‚
â”‚ @angular/service-worker           â”‚ 21.1.0            â”‚ 21.1.0            â”‚
â”‚ @schematics/angular               â”‚ 21.1.0            â”‚ 21.1.0            â”‚
â”‚ rxjs                              â”‚ 7.8.2             â”‚ ~7.8.0            â”‚
â”‚ typescript                        â”‚ 5.9.3             â”‚ ^5.9.3            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Anything else?

_No response_","['area: forms', 'forms: validators', 'area: docs', 'forms: signals']",0
angular/angular,Unified control state change events is awkward with template driven forms,"### Which @angular/* package(s) are the source of the bug?

forms

### Is this a regression?

No

### Description

`control.events` is a nice way of reacting to events. However, with `FormSubmittedEvent` and `FormResetEvent` you need/want to listen the the top level control. But using `control.root.events` is not suitable for template driven forms as it seems at some point (after `afterViewInit`) the reference gets set to the correct root (before its the control itself). Funnily enough, it seems like the `FormResetEvent` is emitted though on the control itself. `control.root.events` works for reactive forms.

Example:

```ts
/** Tracks the `FormControlState` of a given control. */
export function controlState(
  control: AbstractControl,
  /** The element the control is associated with. */
  element: HTMLElement,
  options?: {
    injector?: Injector;
  },
): Observable<FormControlState> {
  return runInInjectionContext(options?.injector ?? inject(Injector), () => {
    const container = inject(ControlContainer, { optional: true });
    const fieldset = element.closest('fieldset');

    const form =
      container?.formDirective instanceof NgForm ||
      container?.formDirective instanceof FormGroupDirective
        ? container.formDirective
        : null;

    /**
     * TODO: idk enough of template forms internals, but `control.root` seems to
     * be updated at some point. Unfortunately, even in `afterViewInit` its
     * still not updated, so there is no supported way of knowing when we have
     * the actual root. It seems `container.formDirective.control.root` is the
     * actual root control and works for both template and reactive forms. I
     * wonder whats the correct approach of programmically getting the root
     * control in Angular forms. `control.root` as a reactive property would
     * help.
     *
     * ```ts
     * console.log(control.root === control); // true even in afterViewInit
     * setTimeout(() => console.log(control.root === control), 0); // false
     * ```
     *
     * Use `control.root` when fixed.
     *
     * Docs:
     * https://angular.dev/guide/forms/reactive-forms#unified-control-state-change-events
     */
    const root = form?.control.root ?? control.root;

    const getSubmitted = () => form?.submitted ?? false;

    return combineLatest([
      control.events.pipe(
        filter((event) => !(event instanceof ValueChangeEvent)),
        startWith(null),
        map(
          () =>
            ({
              dirty: control.dirty,
              touched: control.touched,
              invalid: control.invalid,
              disabled: control.disabled,
            }) satisfies Partial<FormControlState>,
        ),
      ),
      root.events.pipe(
        filter(
          (event) =>
            event instanceof FormSubmittedEvent ||
            event instanceof FormResetEvent,
        ),
        map(
          (event) =>
            ({
              submitted: event instanceof FormSubmittedEvent,
            }) satisfies Partial<FormControlState>,
        ),
        startWith({
          submitted: getSubmitted(),
        } satisfies Partial<FormControlState>),
      ),
      mutationObservable(element, {
        attributes: true,
        attributeFilter: ['readonly'],
      }).pipe(
        startWith(null),
        map(
          () =>
            ({
              readonly: element.hasAttribute('readonly'),
            }) satisfies Partial<FormControlState>,
        ),
      ),
      fieldset
        ? mutationObservable(fieldset, {
            attributes: true,
            attributeFilter: ['disabled'],
          }).pipe(
            startWith(null),
            map(
              () =>
                ({
                  fieldSetDisabled: fieldset.hasAttribute('disabled'),
                }) satisfies Partial<FormControlState>,
            ),
          )
        : of({
            fieldSetDisabled: false,
          } satisfies Partial<FormControlState>),
    ]).pipe(
      map((formStatePartials) => ({
        ...formStatePartials[0],
        ...formStatePartials[1],
        ...formStatePartials[2],
        ...formStatePartials[3],
      })),
      takeUntilDestroyed(),
    );
  });
}

```

### Please provide a link to a minimal reproduction of the bug

_No response_

### Please provide the exception or error you saw

```true

```

### Please provide the environment you discovered this bug in (run `ng version`)

```true
20.3.10
```

### Anything else?

_No response_",['area: forms'],0
angular/angular,[signal forms] Documentation example for domain â†” form model mapping creates a reactive loop,"### Describe the problem that you experienced

The documentation suggests using linkedSignal together with an effect to implement continuous saving when the domain model differs from the form model
([source](https://angular.dev/guide/forms/signals/model-design#form-model-to-domain-model)).

However, this approach does not work as written because it introduces a reactive feedback loop:
- `linkedSignal` depends on the domain model input
- the form depends on the form model produced by `linkedSignal`
- the auto-save `effect` depends on form.value, which is effectively the same form model signal

This results in an infinite cycle of:
domain â†’ form â†’ domain updates, even when no semantic changes occur.

Additionally, the sample code uses `this.myForm.value()`, but `myForm` is a function and must be invoked first to access the value signal (i.e. `this.myForm().value()`), which makes the example incorrect as written.

Working example with custom equality checks: [Stackblitz](https://stackblitz.com/edit/stackblitz-starters-gau5pmnw?file=src%2Fmain.ts)
Commenting line 45 would cause infinite loop.

**Current workarounds**

From my understanding, the available workarounds are:
- Implement custom equality checks to detect whether the domain model has actually changed before propagating updates.
- Introduce a wrapper component that:
  - accepts the domain model
  - performs domain â†” form mapping
  - passes only the form model to the actual form component

While first-class support for this mapping would be ideal, the Angular team has previously indicated that they do not plan to introduce additional APIs for this scenario (see [#65194](https://github.com/angular/angular/issues/65194#issuecomment-3629274580)).

**Request**

Given the above, I suggest updating the documentation to include a correct, working example that avoids reactive loops and reflects the actual API usage. This would help prevent confusion and incorrect implementations based on the current guidance.

### Enter the URL of the topic with the problem

https://angular.dev/guide/forms/signals/model-design#form-model-to-domain-model

### Describe what you were looking for in the documentation

_No response_

### Describe the actions that led you to experience the problem

_No response_

### Describe what you want to experience that would fix the problem

_No response_

### Add a screenshot if that helps illustrate the problem

_No response_

### If this problem caused an exception or error, please paste it here

```true

```

### If the problem is browser-specific, please specify the device, OS, browser, and version

```true

```

### Provide any additional information here in as much as detail as you can

```true

```","['area: forms', 'forms: signals']",0
angular/angular,"[signal forms]: debounceForDuration does not resolve its Promise on abort, causing pending awaits","### Which @angular/* package(s) are the source of the bug?

forms

### Is this a regression?

No

### Description

The current implementation of debounceForDuration does not resolve or reject the returned Promise when the associated AbortSignal is aborted ([source](https://github.com/angular/angular/blob/5d46554e3fe8b64c5fefc7a8ce891c179741977e/packages/forms/signals/src/api/rules/debounce.ts#L44)).

As a result, callers awaiting this Promise may remain suspended indefinitely. In particular, debounceSync awaits debounceForDuration ([source](https://github.com/angular/angular/blob/5d46554e3fe8b64c5fefc7a8ce891c179741977e/packages/forms/signals/src/field/node.ts#L324)), and if the debounce is aborted, the await never resolves.

This can lead to:
- async execution paths that never complete
- retention of async stack frames and captured variables
- potentially increased memory usage in long-running or frequently triggered scenarios

Resolving the Promise on abort would avoid this issue. This should be safe, as debounceSync already checks `controller.signal.aborted` before performing the sync operation, so resolving early would not change observable behaviorâ€”only ensure proper completion of the async flow.

### Please provide a link to a minimal reproduction of the bug

_No response_

### Please provide the exception or error you saw

```true

```

### Please provide the environment you discovered this bug in (run `ng version`)

```true
Angular CLI       : 21.0.5
Node.js           : 24.13.0
Package Manager   : npm 11.6.2
Operating System  : darwin arm64
```

### Anything else?

_No response_","['area: forms', 'forms: signals']",0
angular/angular,Add support for highlighting `angular-ts` and `angular-html` blocks in markdown files,"Prettier added in their [v3.8.0](https://prettier.io/blog/2026/01/14/3.8.0) support for formatting `angular-ts` and `angular-html` fenced code blocks. 

Now we're only missing proper highlighting which the vscode extension should be able to provide. ",['area: vscode-extension'],0
angular/angular,httpResource: Allow parse to access headers,"### Which @angular/* package(s) are relevant/related to the feature request?

common

### Description

I have the situation where the server sends additional information in der headers, eg count information for a list. Unfortunately it seems, that `parse` cannot access the headers.



### Proposed solution

Provide access to headers as additional argument in parse:
```
{
    parse: (value,  headers) =>  [ value, headers.get('X-Count') ?? 0] 
}
```

### Alternatives considered

An interceptor provided for the `HttpResourceRequest` could do the transformation as well.",['area: common/http'],0
angular/angular,Angular Language Service flags non-Angular TypeScript classes by method name (ngOnInit) in Nx monorepo,"### Which @angular/* package(s) are the source of the bug?

language-service

### Is this a regression?

No

### Description

Angular Language Service (ALS) incorrectly reports an error in non-Angular TypeScript libraries inside an Nx monorepo when a class contains methods named like Angular lifecycle hooks (e.g. ngOnInit).

The diagnostic shown is:

`Class is using Angular features but is not decorated. Please add an explicit Angular decorator.`

This happens even when:

- the project is not Angular
- there are no Angular decorators
- the library is built with @nx/js:tsc
- ESLint rules are disabled
- the file is not part of any Angular compilation

The error disappears immediately when the Angular Language Service extension is disabled, confirming ALS as the source.

### Please provide a link to a minimal reproduction of the bug

_No response_

### Please provide the exception or error you saw

```true

```

### Please provide the environment you discovered this bug in (run `ng version`)

```true

```

### Anything else?

_No response_","['P5', 'area: vscode-extension']",0
angular/angular,Signal Forms - Value type can't extend generics,"### Which @angular/* package(s) are relevant/related to the feature request?

_No response_

### Description

```ts
interface XXX {}

@Component({
  selector: 'cat-cat',
  imports: [],
  template: '',
})
export class Cat<T extends XXX> {
  protected readonly model = signal<{query: string; result: XXX | null}>({
    query: '',
    result: null,
  });

  protected readonly form = form(this.model, (schema) => {
    required(schema.cat); /* This is error */
  });
}

```
","['area: forms', 'forms: signals']",0
angular/angular,[signal forms] FormField directive does not work with host directives inputs,"### Which @angular/* package(s) are the source of the bug?

forms

### Is this a regression?

Yes

### Description

When creating custom form controls using a host directive that implements the FormValueControl interface and exposes shared inputs, the associated FormField directive no longer propagates input updates (for example, required and disabled) to the control.

This behaviour worked as expected in Angular Forms v21.0.1, but appears to have regressed in v21.0.8.

Using host directives to expose and share inputs across components is a common pattern in our codebase. The current behaviour prevents this pattern from working correctly and represents a regression from previous releases. Restoring this functionality would be highly beneficial, as it is relied upon extensively.

A minimal, stripped-down reproduction of this use case is provided below.

**Our directive**
```typescript
// imports....

@Directive({
    standalone: true,
    exportAs: 'isFormControl',
})
export class IsFormControlDirective<ValueType = any> implements FormValueControl<ValueType | null> {
    public readonly value: ModelSignal<ValueType | null> = model<ValueType | null>(null);

    public readonly name: InputSignal<string> = input<string>('');

   public readonly required: InputSignal<boolean> = input<boolean>(false);

    public readonly disabled: InputSignal<boolean> = input<boolean>(false);

    public readonly readOnly: InputSignal<boolean> = input<boolean>(false);

    public readonly touched: ModelSignal<boolean> = model<boolean>(false);

   // continues.....
}
```

**custom control template**
```html
<!-- continued.... -->

@if (isFormFieldControlDirective.label()?.length) {
        <ng-template #labelTemplate>
            <label
                form-label
                [for]=""isFormFieldControlDirective.id()""
                [text]=""isFormFieldControlDirective.label()""
                [error]=""fieldError && !isFormFieldControlDirective.readOnly()""
                [disabled]=""isFormFieldControlDirective.disabled()""
                [required]=""isFormFieldControlDirective.required()""
                [readOnly]=""isFormFieldControlDirective.readOnly()""
            ></label>
        </ng-template>
    }

    <input
        slot=""control""
        type=""text""
        [attr.id]=""isFormFieldControlDirective.id()""
        [attr.inputMode]=""inputMode()""
        [attr.aria-invalid]=""fieldError""
        [disabled]=""isFormFieldControlDirective.disabled()""
        [error]=""fieldError""
        [required]=""isFormFieldControlDirective.required()""
        [placeholder]=""isFormFieldControlDirective.placeholder()""
        [value]=""isFormFieldControlDirective.value()""
        (input)=""isFormFieldControlDirective.value.set($event.target.value)""
        (blur)=""isFormFieldControlDirective.onBlur()""
    />

   <!-- continued.... -->
```

**text control**
```typescript
@Component({
    // continued .....
    hostDirectives: [
        {
            directive: IsFormControlDirective,
            inputs: ['label', 'placeholder', 'name', 'touched', 'readOnly', 'disabled', 'required'],
            outputs: ['formControlBlur', 'formControlChange'],
        },
// continued. ....
    ],
    exportAs: 'textInput',
})
export class TextInputComponent implements IFormFieldControl {
  // continued.....
}
```

**form template**
```html
<!-- continued... -->
<form-text-input
                                        [field]=""editForm.name""
                                        [label]=""labelMap.name""
                                    />

<!-- continued... -->
```

**form typescript**
```typescript
public override readonly editForm: FieldTree<FormModel> = form(
        this.editFormModel,
        (schema: SchemaPathTree<FormModel>): void => {
            required(schema.name);
            required(schema.currencyCode);

            disabled(schema.name, (): boolean => this.saving());
            disabled(schema.currencyCode, (): boolean => this.saving());

            readonly(schema.name, (): boolean => !this.canEdit());
            readonly(schema.currencyCode, (): boolean => !this.canEdit());

// continued ...
        },
    );

```

### Please provide a link to a minimal reproduction of the bug

See ticket description

### Please provide the exception or error you saw

```true
N/A
```

### Please provide the environment you discovered this bug in (run `ng version`)

```true
Angular CLI       : 21.0.5
Angular           : 21.0.8
Node.js           : 24.12.0
Package Manager   : yarn 4.10.3
Operating System  : win32 x64

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Package                           â”‚ Installed Version â”‚ Requested Version â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ @angular-devkit/architect         â”‚ 0.2100.4          â”‚ 0.2100.4          â”‚
â”‚ @angular-devkit/build-angular     â”‚ 21.0.4            â”‚ 21.0.4            â”‚
â”‚ @angular-devkit/core              â”‚ 21.0.5            â”‚ 21.0.5            â”‚
â”‚ @angular-devkit/schematics        â”‚ 21.0.5            â”‚ 21.0.5            â”‚
â”‚ @angular/build                    â”‚ 21.0.5            â”‚ ^21.0.5           â”‚
â”‚ @angular/cdk                      â”‚ 21.0.6            â”‚ 21.0.6            â”‚
â”‚ @angular/cdk-experimental         â”‚ 21.0.6            â”‚ 21.0.6            â”‚
â”‚ @angular/cli                      â”‚ 21.0.5            â”‚ 21.0.5            â”‚
â”‚ @angular/common                   â”‚ 21.0.8            â”‚ 21.0.8            â”‚
â”‚ @angular/compiler                 â”‚ 21.0.8            â”‚ 21.0.8            â”‚
â”‚ @angular/compiler-cli             â”‚ 21.0.8            â”‚ 21.0.8            â”‚
â”‚ @angular/core                     â”‚ 21.0.8            â”‚ 21.0.8            â”‚
â”‚ @angular/forms                    â”‚ 21.0.8            â”‚ 21.0.8            â”‚
â”‚ @angular/language-service         â”‚ 21.0.8            â”‚ 21.0.8            â”‚
â”‚ @angular/localize                 â”‚ 21.0.6            â”‚ 21.0.6            â”‚
â”‚ @angular/material                 â”‚ 21.0.6            â”‚ 21.0.6            â”‚
â”‚ @angular/platform-browser         â”‚ 21.0.8            â”‚ 21.0.8            â”‚
â”‚ @angular/platform-browser-dynamic â”‚ 21.0.8            â”‚ 21.0.8            â”‚
â”‚ @angular/platform-server          â”‚ 21.0.8            â”‚ 21.0.8            â”‚
â”‚ @angular/router                   â”‚ 21.0.8            â”‚ 21.0.8            â”‚
â”‚ @schematics/angular               â”‚ 21.0.4            â”‚ 21.0.4            â”‚
â”‚ ng-packagr                        â”‚ 21.0.1            â”‚ 21.0.1            â”‚
â”‚ rxjs                              â”‚ 7.8.2             â”‚ 7.8.2             â”‚
â”‚ typescript                        â”‚ 5.9.2             â”‚ 5.9.2             â”‚
â”‚ vitest                            â”‚ 4.0.16            â”‚ 4.0.16            â”‚
â”‚ zone.js                           â”‚ 0.16.0            â”‚ 0.16.0            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Anything else?

_No response_","['area: forms', 'forms: signals']",0
angular/angular,Angular Elements imported by Angular have inconsistent lifecycle,"### Which @angular/* package(s) are the source of the bug?

elements

### Is this a regression?

No. This has been the case for as long as I have used Angular, at least 3 years.

### Description

When an Angular component imports another Angular component, `ngOnInit`, `effect`s and `resource`s are called once the inputs are set.

When an Angular component imports an **Angular Elements element**, `ngOnInit`, `effect`s and `resource`s are called before the inputs are set.

There seems to be no alternative lifecycle hook to know when the inputs have indeed been set.

### Please provide a link to a minimal reproduction of the bug

https://stackblitz.com/edit/angular-fpq5sqat?file=src%2Fmain.ts,src%2Fsub-component.ts,src%2Findex.html,package.json

### Please provide the exception or error you saw

```true
In the Angular component, `ngOnInit` is correctly called with values.

In the Angular Elements element, `ngOnInit` is called with undefined inputs.
```

### Please provide the environment you discovered this bug in (run `ng version`)

```true
See Stackblitz, 21.1.0 for all the packages. However, this dates back at least to Angular 15.
```

### Anything else?

If an input is marked as required, the following error is logged.

```
Error: NG0950: Input ""id"" is required but no value is available yet. Find more at https://v21.angular.dev/errors/NG0950
```

Note that this error only applies for dynamic inputs. With the following example, `language = input.required()` does work.

```html
<component-as-element [id]=""id()"" language=""de"" />
```

I have also tested with `[attr.]`, to force that the fields are passed as attributes rather than properties, with the hope that this would mean that they could be passed as soon as the element is created, but this did not change anything.

```html
<component-as-element [attr.id]=""id()"" [attr.language]=""language()"" />
```",['area: elements'],0
angular/angular,"signal forms: string | null type is giving compile error, type errors while using it with primeng components","### Which @angular/* package(s) are the source of the bug?

compiler, forms

### Is this a regression?

Yes

### Description

Form data type like this was working earlier, but now it gives type error:
```ts
interface LoginData {
  email: string | null;
  password: string | null;
  rememberMe: boolean;
}
```

Form:
```ts
 loginModel = signal<LoginData>({
    email: null,
    password: null,
    rememberMe: false,
  });

  loginForm = form(this.loginModel);
```

### Please provide a link to a minimal reproduction of the bug

https://stackblitz.com/edit/f6b2smnv?file=package.json,src%2Fapp%2Fapp.ts,src%2Fapp%2Fapp.html

### Please provide the exception or error you saw

```true
[ERROR] TS2322: Type '`0x${string}` | null' is not assignable to type 'string'.
  Type 'null' is not assignable to type 'string'. [plugin angular-compiler]
```

### Please provide the environment you discovered this bug in (run `ng version`)

```true
Angular CLI       : 21.1.0
Angular           : 21.1.0
Node.js           : 20.19.1
Package Manager   : npm 10.8.2
Operating System  : linux x64

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Package                   â”‚ Installed Version â”‚ Requested Version â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ @angular/build            â”‚ 21.1.0            â”‚ 21.1.0            â”‚
â”‚ @angular/cli              â”‚ 21.1.0            â”‚ 21.1.0            â”‚
â”‚ @angular/common           â”‚ 21.1.0            â”‚ 21.1.0            â”‚
â”‚ @angular/compiler         â”‚ 21.1.0            â”‚ 21.1.0            â”‚
â”‚ @angular/compiler-cli     â”‚ 21.1.0            â”‚ 21.1.0            â”‚
â”‚ @angular/core             â”‚ 21.1.0            â”‚ 21.1.0            â”‚
â”‚ @angular/forms            â”‚ 21.1.0            â”‚ 21.1.0            â”‚
â”‚ @angular/platform-browser â”‚ 21.1.0            â”‚ 21.1.0            â”‚
â”‚ @angular/router           â”‚ 21.1.0            â”‚ 21.1.0            â”‚
â”‚ rxjs                      â”‚ 7.8.2             â”‚ ~7.8.0            â”‚
â”‚ typescript                â”‚ 5.9.3             â”‚ ~5.9.3            â”‚
â”‚ zone.js                   â”‚ 0.16.0            â”‚ ~0.16.0           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Anything else?

Also getting one other type error after 21.1.0 update (was working fine with 20.0.7 formField change), but not tried it in stackblitz so not reporting yet, getting following error with for p-datepicker component (field type `Date | null`):
```bash
Type 'readonly RegExp[]' is not assignable to type 'string'.ngtsc(2322)
declare (property) FormField<Date | null>.formField: InputSignal<FieldTree<Date | null, string | number>>
```","['area: forms', 'forms: signals']",0
angular/angular,Cannot use a number as an index of  SchemaPathTree of array.,"### Which @angular/* package(s) are the source of the bug?

forms

### Is this a regression?

Yes

### Description

After upgrading Angular from 21.0.6 to 21.1.0, using a number as an index for `SchemaPathTree` results in an error.

```ts
interface MyTest {
  readonly a: string[];
  readonly b: readonly string[];
  readonly c: Record<string, string>;
  readonly d: Readonly<Record<string, string>>;
}

const myTest = schema<MyTest>((path) => {
  validate(path, ({ fieldTreeOf }) => {
    fieldTreeOf(path.a[0])().value(); // Error: Property '0' does not exist on type 'SchemaPath<string[], 1, Child>'
    fieldTreeOf(path.a)[0]().value(); // OK

    fieldTreeOf(path.b[0])().value(); // Error: Property '0' does not exist on type 'SchemaPath<readonly string[], 1, Child>'
    fieldTreeOf(path.b)[0]().value(); // OK

    fieldTreeOf(path.c['abc'])().value(); // OK
    fieldTreeOf(path.c)['abc']().value(); // OK

    fieldTreeOf(path.d['abc'])().value(); // OK
    fieldTreeOf(path.d)['abc']().value(); // OK
  });
});
```
I am unsure if this is a breaking change by design.


### Please provide a link to a minimal reproduction of the bug

_No response_

### Please provide the exception or error you saw

```true
A number index should work on `SchemaPathTree` of array.
```

### Please provide the environment you discovered this bug in (run `ng version`)

```true
Angular CLI       : 21.0.6
Angular           : 21.1.0
Node.js           : 24.11.1
Package Manager   : yarn 1.22.22
Operating System  : linux x64

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Package                   â”‚ Installed Version â”‚ Requested Version â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ @angular/aria             â”‚ 21.1.0            â”‚ 21.1.0            â”‚
â”‚ @angular/build            â”‚ 21.0.6            â”‚ ^21.0.6           â”‚
â”‚ @angular/cli              â”‚ 21.0.6            â”‚ ^21.0.6           â”‚
â”‚ @angular/common           â”‚ 21.1.0            â”‚ ^21.1.0           â”‚
â”‚ @angular/compiler         â”‚ 21.1.0            â”‚ ^21.1.0           â”‚
â”‚ @angular/compiler-cli     â”‚ 21.1.0            â”‚ ^21.1.0           â”‚
â”‚ @angular/core             â”‚ 21.1.0            â”‚ ^21.1.0           â”‚
â”‚ @angular/forms            â”‚ 21.1.0            â”‚ ^21.1.0           â”‚
â”‚ @angular/platform-browser â”‚ 21.1.0            â”‚ ^21.1.0           â”‚
â”‚ @angular/router           â”‚ 21.1.0            â”‚ ^21.1.0           â”‚
â”‚ rxjs                      â”‚ 7.8.2             â”‚ ~7.8.0            â”‚
â”‚ typescript                â”‚ 5.9.3             â”‚ ~5.9.2            â”‚
â”‚ vitest                    â”‚ 4.0.16            â”‚ ^4.0.8            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Anything else?

_No response_","['area: forms', 'forms: signals']",0
angular/angular,Form value for Signal Forms with FormControls,"<img width=""1289"" height=""82"" alt=""Image"" src=""https://github.com/user-attachments/assets/72f275ea-c197-400e-8e07-99714150597c"" />

### Which @angular/* package(s) are relevant/related to the feature request?

forms

### Description

The new Signal Forms nicely allow to integrate Signal forms with FormCotnrols from Reactive Forms as illustrated here: https://angular.dev/guide/forms/signals/migration

The form value is nicely available if we call

````
f.password().value()
````

But currently you do not get the form value if you call 

````
f().value();
````

currently you get:

````
{
  email: '',
  password: FromControl2
}
````

This makes it harder to pass the value of a form to an API or to print it out using the JSON pipe.

### Proposed solution

Callin f.value() returns the form value with the value of the FormControl.

### Alternatives considered

```
const formValue = [computed](https://angular.dev/api/core/computed)(() => ({
  email: form.email().value(),
  password: form.password().value(),
}));
```

-","['area: forms', 'forms: signals']",0
angular/angular,Angular Language Service VSCode extension stuck initializing,"### Which @angular/* package(s) are the source of the bug?

language-service

### Is this a regression?

No

### Description

The Angular Language Service extension gets stuck initializing in our projects. There are no errors in the logs and it keeps showing the message that initialization is ongoing.

I've set `angular.log` to `verbose` to capture a log output. I see no obvious critical errors and no progress messages from a certain point onward. It seems the extension just does nothing - which I assume isn't actually the case. It's more likely something is happening in the background, but there is no insight into this and it's therefore difficult to debug.

This is the log: [nglangvsc.log](https://gist.github.com/sbarfurth/829eab24cbb1659183b586a447beb584)

The last entry in the log is from `14:7:50.922`. As I write this, it is `14:20:00.000` and there have been no new logs but the language service is still not ready.

*Note that the `REDACTED.ts` files are all different files. I've removed as much information as I deemed necessary, but I removed no lines from the log output and did not alter the `tsconfig`.*

### Please provide a link to a minimal reproduction of the bug

_No response_

### Please provide the exception or error you saw

```true
Initializing Angular language features
```

### Please provide the environment you discovered this bug in (run `ng version`)

```true
Angular CLI: 20.3.13
Node: 22.18.0
Package Manager: pnpm 10.14.0
OS: linux x64

Using Angular version 20.3.15.
[Info  - 2:07:37 PM] Angular language server process ID: 886168
[Info  - 2:07:37 PM] Imported typescript/lib/tsserverlibrary is version 5.9.3.
[Info  - 2:07:37 PM] Using @angular/language-service v21.1.0-rc.0 from /usr/local/google/home/sbarfurth/.vscode-server/extensions/angular.ng-template-21.1.0/node_modules/@angular/language-service/index.js
```

### Anything else?

This repository is a Bazel monorepo.",['area: vscode-extension'],0
angular/angular,ng-event-dispatch-contract script execution blocked on apps with CSP via CSP_NONCE token,"### Command

build

### Is this a regression?

- [ ] Yes, this behavior used to work in the previous version

### The previous version in which this bug was not present was

_No response_

### Description

I wanted to add Content-Security-Policy to my app and found that the CSP_NONCE token way to implement it looked to be the easiest with SSR.
The nonce is generated in the express route handler and provided to the app with the REQUEST_CONTEXT and TransferState.
Every angular script and style is correctly appended with that nonce, just the ""ng-event-dispatch-contract"" script is missing the nonce.

I want to generate the nonce myself because the app uses a few custom scripts at runtime that require the nonce

### Minimal Reproduction

1. Create app with SSR enabled
2. Set route to RenderMode.Server
3. Add (click) handler to any component (required or else the script gets removed before send)
4. Add CSP nonce token to client and server (just to replicate the current project, I think it should also work only server side)
```ts
function provideCSPNonce() {
  return {
    provide: CSP_NONCE,
    deps: [TransferState, REQUEST_CONTEXT],
    useFactory: (transferState: TransferState, rq : any)=>{
      if(!!rq){
        const nonce = rq.nonce;
        transferState.set(CSP_NONCE_TRANSFER, nonce);
        return nonce
      }else{
        return transferState.get(CSP_NONCE_TRANSFER, null);
      }
    }
  }
}
```
5. Add CSP and nonce to headers and request context:
```ts
/**
 * Handle all other requests by rendering the Angular application.
 */
app.use(async (req, res, next) => {

  const resp = await angularApp.handle(req, {
    nonce: 'test123'
  })
  if(!resp)
    return next();

  resp.headers.set(""Content-Security-Policy"", `default-src 'self'; script-src 'self' 'nonce-${'test123'}';`);
  await writeResponseToNodeResponse(resp, res);
});
```

### Exception or Error

```text
The browser will complain, that the ng-event-dispatch-contract script does not have a nonce:

Executing inline script violates the following Content Security Policy directive 'script-src 'self' 'nonce-test123''. Either the 'unsafe-inline' keyword, a hash ('sha256-VM2mZqyEQZoLzoTrp5EigFvzQ0+f1wSeBuoOn95WHCg='), or a nonce ('nonce-...') is required to enable inline execution. The action has been blocked.
```

### Your Environment

```text
Angular CLI: 20.3.14
Node: 24.9.0
Package Manager: npm 11.6.0
OS: darwin arm64


Angular: 20.3.16
... common, compiler, compiler-cli, core, forms
... platform-browser, platform-server, router

Package                      Version
------------------------------------
@angular-devkit/architect    0.2003.14
@angular-devkit/core         20.3.14
@angular-devkit/schematics   20.3.14
@angular/build               20.3.14
@angular/cli                 20.3.14
@angular/ssr                 20.3.14
@schematics/angular          20.3.14
rxjs                         7.8.2
typescript                   5.9.3
```

### Anything else relevant?

During creation of a reproduction project, I also noticed, that prerendered routes also do not work with just the CSP token method.

I think the token method is incompatible with the current SSR workflow.
This should probably be documented in the [security docs](https://angular.dev/best-practices/security)",['area: docs'],0
angular/angular,pattern validation for optional fields,"### Which @angular/* package(s) are relevant/related to the feature request?

forms

### Description

optional fields can be represented by

```ts
foo: string | null;
```

the pattern signature is the following and we get a ts(2345) error.

```ts
export function pattern<TPathKind extends PathKind = PathKind.Root>(
  path: SchemaPath<string, SchemaPathRules.Supported, TPathKind>,
  pattern: RegExp | LogicFn<string | undefined, RegExp | undefined, TPathKind>,
  config?: BaseValidatorConfig<string, TPathKind>,
) {
```

### Proposed solution

```ts
export function pattern<TPathKind extends PathKind = PathKind.Root>(
  path: SchemaPath<string | null, SchemaPathRules.Supported, TPathKind>,
  pattern: RegExp | LogicFn<string | null | undefined, RegExp | undefined, TPathKind>,
  config?: BaseValidatorConfig<string | null, TPathKind>,
) {
```

```ts
export function pattern<TValue extends string | null, TPathKind extends PathKind = PathKind.Root>(
  path: SchemaPath<TValue , SchemaPathRules.Supported, TPathKind>,
  pattern: RegExp | LogicFn<TValue  | undefined, RegExp | undefined, TPathKind>,
  config?: BaseValidatorConfig<TValue , TPathKind>,
) {
```

### Alternatives considered

Alternative is to just have it as string and use `''`","['area: forms', 'forms: signals']",0
angular/angular,Can't figure out how to start the angular-ai-tutor in VSCode,"### Describe the problem that you experienced

I'm using VS Code and copilot and the docs at https://angular.dev/ai/ai-tutor don't seem to work.  I've added the `.vscode/mcp.json` file as specified. I have no idea if the server is running or not.   Is it supposed to launch a docker container?   Is there a way to check if it is running?When I chat with copilot and use the prompt 'launch the Angular AI tutor', After trying that, it just starts my app and runs tests, then asks me ""Next: do you want me to stop the test runner, open the app in your browser, or run any lint/build steps?"".   I don't think I'm accessing the tutorial at all.

### Enter the URL of the topic with the problem

https://angular.dev/ai/ai-tutor

### Describe what you were looking for in the documentation

I'm trying to follow the ai tutorial

### Describe the actions that led you to experience the problem

Following the ai tutorial on the page

### Describe what you want to experience that would fix the problem

Have instructions that work, or some instructions on how to verify I'm chatting with the MCP server.

### Add a screenshot if that helps illustrate the problem

_No response_

### If this problem caused an exception or error, please paste it here

```true

```

### If the problem is browser-specific, please specify the device, OS, browser, and version

```true

```

### Provide any additional information here in as much as detail as you can

```true

```",['area: docs'],0
angular/angular,HostListener/output() memory leak in SSR,"### Which @angular/* package(s) are the source of the bug?

Don't known / other

### Is this a regression?

No

### Description

It appears that elements are not being garbage collected in an SSR application if: 
* A child component contains an  `output()` which it's parent is using
* A component is using a `@HostListener`

### How to reproduce in provided minimal sample
* Either start the dev server or run a production build (both have the same result)
* Open the app
* Open dev tools
* Click the toggle button multiple times
* Take a snapshot of the heap
* You now see multiple detached instances of both components which are still held in memory even though not used anymore

<img width=""2263"" height=""633"" alt=""Image"" src=""https://github.com/user-attachments/assets/92076692-b35a-4db6-9eec-9bc789ef40fe"" />

#### Validation
* Remove `@HostListener` from `WithHostListener` component
  ```ts
  // Change this
  @HostListener('click', ['$event'])
  public onClick(event: MouseEvent): void {}

  // to this
  public onClick(event: MouseEvent): void {}

  ```
* Remove `WithOutput` component's output listener from `App` component
  ```html
  <!-- change this -->
  <app-with-output (myOutput)=""doSomething()""></app-with-output>

  <!-- to this -->
  <app-with-output></app-with-output>
  ```
* Run the app
* Click the toggle button multiple times
* Take a heap snapshot
* Now you can see that the DOM Nodes are cleaned up correctly

<img width=""2494"" height=""796"" alt=""Image"" src=""https://github.com/user-attachments/assets/c5f9e4ed-9b80-4584-8a5b-e043a2b26aaa"" />


Changing the `WithHostListener` component to cleaning up the click listener manually (as seen in the snippet below) also enables the GC to cleanup the `app-with-host-listener` nodes. 

```ts
import { Component, ElementRef, inject, OnDestroy, Renderer2 } from '@angular/core';

@Component({
  selector: 'app-with-host-listener',
  template: `
    <div>With HostListener</div>
  `,
})
export class WithHostListener implements OnDestroy {
  private readonly renderer = inject(Renderer2);
  private cleanupListener: (() => void) | undefined;
  private readonly el = inject(ElementRef);

  constructor() {
    this.cleanupListener = this.renderer.listen(this.el.nativeElement, 'click', event => {
			this.onClick(event);
		});
  }

	public onClick(event: MouseEvent): void {console.log(event)}

  ngOnDestroy(): void {
		if (this.cleanupListener) {
			this.cleanupListener();
		}
  }
}

```

### Please provide a link to a minimal reproduction of the bug

https://github.com/mii-shell/angular-dom-nodes-memory-leak

### Please provide the exception or error you saw

```true
No exception/error was thrown
```

### Please provide the environment you discovered this bug in (run `ng version`)

```true
Angular CLI       : 21.0.5
Angular           : 21.1.0-rc.0
Node.js           : 24.4.1
Package Manager   : npm 11.4.2
Operating System  : linux x64

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Package                   â”‚ Installed Version â”‚ Requested Version â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ @angular/build            â”‚ 21.1.0-rc.0       â”‚ ^21.1.0-rc.0      â”‚
â”‚ @angular/cli              â”‚ 21.0.5            â”‚ ^21.0.5           â”‚
â”‚ @angular/common           â”‚ 21.1.0-rc.0       â”‚ ^21.1.0-rc.0      â”‚
â”‚ @angular/compiler         â”‚ 21.1.0-rc.0       â”‚ ^21.1.0-rc.0      â”‚
â”‚ @angular/compiler-cli     â”‚ 21.1.0-rc.0       â”‚ ^21.1.0-rc.0      â”‚
â”‚ @angular/core             â”‚ 21.1.0-rc.0       â”‚ ^21.1.0-rc.0      â”‚
â”‚ @angular/forms            â”‚ 21.1.0-rc.0       â”‚ ^21.1.0-rc.0      â”‚
â”‚ @angular/platform-browser â”‚ 21.1.0-rc.0       â”‚ ^21.1.0-rc.0      â”‚
â”‚ @angular/platform-server  â”‚ 21.1.0-rc.0       â”‚ ^21.1.0-rc.0      â”‚
â”‚ @angular/router           â”‚ 21.1.0-rc.0       â”‚ ^21.1.0-rc.0      â”‚
â”‚ @angular/ssr              â”‚ 21.1.0-rc.0       â”‚ ^21.1.0-rc.0      â”‚
â”‚ rxjs                      â”‚ 7.8.2             â”‚ ~7.8.0            â”‚
â”‚ typescript                â”‚ 5.9.3             â”‚ ~5.9.2            â”‚
â”‚ vitest                    â”‚ 4.0.17            â”‚ ^4.0.8            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Anything else?

* I originally thought this problem might be related to #59261, which is why I've used Angular version `21.1.0-rc.0` in my minimal reproduction (which is the version in which this was fixed according to the changelog). Unfortunately, it did not resolve the issue.
* I've also tested it with version 21.0.5, which resulted in the same
* Manual garbage collection does not remove the elements from the heap either
* Identical issue, regardless of whether `@HostListener` or `host` option in `@Component` is used
* This issue does not occur in a basic Angular app setup without SSR","['memory leak', 'area: core', 'core: event dispatch']",0
angular/angular,Reactive model updates in FormValueControl via computation,"### Which @angular/* package(s) are relevant/related to the feature request?

forms

### Description


I would like to propose an enhancement to **Angular Signal Forms** that allows a `model()` used inside a `FormValueControl` to derive its value from a reactive computation, instead of acting purely as a mutable storage primitive.

Today, `model()` represents the value exposed by a custom control and is expected to be updated manually. However, in many real-world scenarios, that value is *derived* from multiple internal reactive fields (e.g. unit + amount, currency + precision, date + timezone).

This proposal suggests extending `model()` with a computed/linked variant that automatically recalculates and propagates its value whenever its reactive dependencies change, similarly to how `linkedSignal` works in `@angular/core`.

The goal is to enable more declarative, reactive, and maintainable `FormValueControl` implementations when using Signal Forms.


## The Problem

When building custom `FormValueControl` components with **Signal Forms**, developers often manage multiple internal reactive fields whose combined state determines the value exposed to the parent form.

Currently, keeping the `model()` value in sync with this internal state requires imperative glue code:

* Manual calls to `this.value.set(...)`
* Synchronization tied to DOM events such as `(input)` or `(change)`
* Repeated logic scattered across event handlers

This approach introduces several drawbacks:

* **Imperative over reactive**: Updates depend on events rather than state changes
* **Boilerplate-heavy**: Each internal field change must be manually wired
* **Error-prone**: Easy to miss updates in complex or dynamic controls
* **Poor scalability**: The pattern does not scale well for rich inputs (currency, units, dates, formatted values)

When migrating from classic Reactive Forms to Signal Forms, this feels like a regression in declarativity, as derived values cannot be expressed directly through reactive computation.

A built-in way to define a *derived model value* would remove this friction and align Signal Forms more closely with Angularâ€™s reactive design philosophy.

### Example

```ts
@Component({
  selector: 'app-new-weight-input',
  standalone: true,
  imports: [Field, ReactiveFormsModule],
  template: `
    <input
      type=""number""
      [field]=""form.val""
      (input)=""setValue()""
    />

    <select
      [field]=""form.unit""
      (change)=""setValue()""
    >
      <option value=""kg"">KG</option>
      <option value=""lbs"">LBS</option>
    </select>
  `,
})
export class NewWeightInputComponent implements FormValueControl {
  readonly value = model(0);

  weight = signal({ unit: 'kg', val: 0 });
  form = form(this.weight);

  setValue() {
    this.value.set(
      this.weight().unit === 'kg'
        ? this.weight().val
        : this.weight().val * 0.453592
    );
  }
}
```

In this example, the exposed form value is *derived* from multiple internal fields, yet it must be synchronized imperatively via DOM events. As the control grows in complexity, this pattern becomes increasingly difficult to maintain.



### Proposed solution

Introduce a computed or linked variant of `model()` (for example `model.linked()`), allowing a `FormValueControl` to expose a value that is *reactively derived* from internal state.

The proposed API would:

* Accept a computation function instead of an initial static value
* Track any reactive dependencies accessed inside the computation
* Automatically update the model value when those dependencies change
* Propagate the updated value to the parent form without manual `set()` calls

### Hypothetical API

```ts
export class WeightInput implements FormValueControl {
  internalForm = form({ unit: 'kg', amount: 0 });

  readonly value = model.linked(() => {
    const { unit, amount } = this.internalForm.value();

    return unit === 'lbs'
      ? amount * 0.453592
      : amount;
  });
}
```

This keeps the control fully declarative: the exposed value is a pure function of reactive state, rather than something that must be synchronized manually through event handlers.

### Design Notes

* The API name is illustrative; alternatives such as `derivedModel`, `computedModel`, or extending `model()` with an overload could also be considered
* Write access could remain restricted or optional, depending on whether two-way updates are desired
* Internally, this could reuse the same dependency-tracking mechanism already used by `linkedSignal`

This approach would significantly reduce boilerplate and improve the developer experience when building complex custom form controls with Signal Forms.


### Alternatives considered

### Manual synchronization via event handlers

The current recommended approach is to manually synchronize the `model()` value by calling `this.value.set(...)` inside DOM event handlers such as `(input)`, `(change)`, or `(blur)`.

While functional, this approach:

* Relies on imperative glue code
* Couples reactive state updates to DOM events
* Becomes verbose and error-prone as control complexity grows

This pattern does not scale well for controls with multiple internal reactive fields.

---

### Using `computed()` + `effect()` internally

Another possible workaround is to derive the value using `computed()` and then mirror it into `model()` via an `effect()`:

```ts
const computedValue = computed(() => /* derive value */);

effect(() => this.value.set(computedValue()));
```

Although reactive, this solution:

* Requires additional boilerplate
* Introduces an extra synchronization layer
* Feels indirect and non-idiomatic for a `FormValueControl`

---

### Exposing internal form instead of a derived value

In some cases, developers may choose to expose the internal form structure directly and let the parent handle derivation logic.

This shifts complexity to consumers, breaks encapsulation, and undermines the purpose of custom form controls.

---

## Why a built-in solution is preferable

A first-class, computed-capable `model()` API would be:

* More declarative
* Less error-prone
* Consistent with Angularâ€™s existing signal primitives

It would allow developers to express derived form values directly, without relying on indirect or imperative workarounds.
","['area: forms', 'forms: signals']",0
angular/angular,"SSR dev-server HMR does not dispose hydrated layout, causing duplicated components after root template change","### Command

serve

### Is this a regression?

- [ ] Yes, this behavior used to work in the previous version

### The previous version in which this bug was not present was

_No response_

### Description

### Summary
In an SSR-enabled Angular project, when running `ng serve`, editing the root component template (`src/app/app.html`) and then refreshing the browser results in duplicated layout DOM nodes. Header/footer appear twice.

DevTools shows **two sibling `app-shell` elements under a single `app-root`** (the root is not duplicated; only the layout is duplicated). This does **not** reproduce in a non-SSR project.

### Expected
After saving template changes and refreshing, the hydrated SSR component tree should be disposed properly and the layout should render exactly once.

### Actual
After editing the root template and refreshing:
- previous hydrated layout is not destroyed
- a second `app-shell` is appended
- header/footer are rendered twice
- DOM order becomes inconsistent (router-outlet ends up between the two layouts)

### Notes
I have attached screenshots (DevTools Elements) showing `app-shell` duplicated the repro.

### ng serve
<img width=""1904"" height=""804"" alt=""Image"" src=""https://github.com/user-attachments/assets/a5c471cb-8855-422d-920a-426788252ce9"" />

### After change and save
<img width=""1883"" height=""818"" alt=""Image"" src=""https://github.com/user-attachments/assets/804cfa1f-e264-4e0d-b778-e0dc3eb548bd"" />

### After refresh page
<img width=""1919"" height=""848"" alt=""Image"" src=""https://github.com/user-attachments/assets/0b22b643-c2fb-4d1e-a570-a7cff956df04"" />

### Minimal Reproduction

### Repro repo

https://github.com/alikarimli/angular-ssr-hmr-duplication-repro.git

### Steps
1. `git clone https://github.com/alikarimli/angular-ssr-hmr-duplication-repro.git`
2. `npm ci`
3. `npx ng serve`
4. Open the app in the browser.
5. Edit `src/app/app.html` (add a comment or whitespace), save.
6. Refresh the browser.
7. Observe duplicated layout (header/footer). In DevTools, `app-shell` appears twice under a single `app-root`.

### Important
- This only reproduces with SSR enabled.
- A non-SSR project does not reproduce the issue.


### Exception or Error

```text
No exception is thrown. No console errors observed.
```

### Your Environment

```text
Angular CLI: 20.3.7
Node: 22.20.0
Package Manager: npm 11.6.2
OS: win32 x64

Angular: <error>
... build, common, compiler, compiler-cli, core, forms
... platform-browser, platform-server, router, ssr

Package                      Version
------------------------------------
@angular-devkit/architect    0.2003.7 (cli-only)
@angular-devkit/core         20.3.7 (cli-only)
@angular-devkit/schematics   20.3.7 (cli-only)
@angular/cli                 20.3.7 (cli-only)
@schematics/angular          20.3.14
rxjs                         7.8.2
typescript                   5.9.3
zone.js                      0.15.1
```

### Anything else relevant?

- Browser: Chrome (latest) on Windows 11
- Happens only after editing the root template (`src/app/app.html`) while dev-server is running and then refreshing the page.
- `app-root` is not duplicated; only `app-shell` is duplicated (see attached DevTools screenshots).
- Workaround: `ng serve --no-hmr` prevents the issue (if applicable).
","['area: core', 'core: hot module replacement (HMR)']",0
angular/angular,Support focusing first invalid field in DOM/template order,"### Which @angular/* package(s) are relevant/related to the feature request?

_No response_

### Description

When submitting an invalid form, focusing the first invalid field is an important accessibility practice ([WCAG 3.3.1 Error Identification](https://www.w3.org/WAI/WCAG22/Understanding/error-identification.html)). However, the current API only allows focusing based on data model order, not template order.

Currently, `errorSummary().at(0)` returns the first invalid field based on the order of properties in the form data model, not the order they appear in the template. This leads to unexpected focus behavior and poor user experience, especially for screen reader users who expect focus to move to the first error in reading order.

**Reproduction:**

```ts
import { Component, signal } from '@angular/core';
import { form, FormField, required, schema } from '@angular/forms/signals';

type BookFormData = {
  isbn: string;
  title: string;
};

const bookFormSchema = schema<BookFormData>((path) => {
  required(path.isbn, { message: 'ISBN is required.' });
  required(path.title, { message: 'Title is required.' });
});

@Component({
  selector: 'app-book-form',
  imports: [FormField],
  template: `
    <form (submit)=""submitForm()"" novalidate>
      <label for=""title"">Title</label>
      <input id=""title"" [formField]=""bookForm.title"" />
      
      <label for=""isbn"">ISBN</label>
      <input id=""isbn"" [formField]=""bookForm.isbn"" />
      
      <button type=""submit"">Save</button>
    </form>
  `
})
export class BookFormComponent {
  readonly #bookFormData = signal<BookFormData>({
    isbn: '',
    title: ''
  });
  protected readonly bookForm = form(this.#bookFormData, bookFormSchema);

  submitForm() {
    if (this.bookForm().invalid()) {
      this.bookForm().errorSummary().forEach(field => {
        field.fieldTree().markAsTouched();
      });
      // This focuses ISBN field instead of title field!
      this.bookForm().errorSummary().at(0)?.fieldTree().formFieldBindings().at(0)?.element.focus();
      return false;
    }
    // ... submit logic when valid
  }
}
```

**Expected behavior:** The title field should be focused (first in template).

**Actual behavior:** The ISBN field is focused (first in data model).

**Workaround:** Reordering properties in the data model to match template order, but this is not maintainable and couples data structure to UI concerns.

Maybe there is a simple solution already available which I havent discovered yet.

### Proposed solution

A high level API to set the focus to first invalid field based on DOM/template order:

```ts
this.bookForm().focusFirstFieldWithError();
```

Or a method to get the first invalid field based on DOM/template order:

```ts
this.bookForm().errorSummary().firstInDOMOrder()?.fieldTree().formFieldBindings().at(0)?.element.focus();
```

### Alternatives considered

**Manual DOM traversal:** Query all invalid fields via querySelectorAll('[aria-invalid=""true""]') and focus the first one. This works but bypasses the Signal Forms API entirely and feels like a workaround.

**Reorder data model:** Change property order in the data model to match template order. This is not maintainable and creates tight coupling between data structure and UI.

**Custom sorting:** Manually sort errorSummary() results by DOM position before focusing. This is verbose and requires boilerplate code in every form submission handler.","['feature', 'area: forms', 'forms: signals']",0
angular/angular,Signal Forms: Enable safe side-effects or payload access from async validators (cannot write to signals inside onSuccess),"### Which @angular/* package(s) are relevant/related to the feature request?

forms

### Description

Not really a feature request, but a brainstorming how to handle this scenario using signal forms.

Some backends return domain payloads along with validation results (for example, a User object when a userId is valid). With signal-forms, attempting to persist that payload by writing to another signal inside the validatorâ€™s onSuccess leads to NG0600: Writing to signals is not allowed in a computed. It would be helpful to have a supported pattern or API to consume/store such payloads without violating the â€œno writes in computedâ€ rule.

Use case

- A form field accepts a userId.
- An async validator checks validity.
- If valid, the endpoint also returns the User object.
- The app should display full user details without issuing a second request.

Repro snippet 
```
validateHttp<string, UserValidationResponse>(f.userId, { 
  request: ({ value }) => /example-api/user-info?id=${value()}, 
  onSuccess: (resp) => { 
    if (resp.result === 'invalid') { 
      return customError({ kind: 'invalid id', message: 'Invalid user id', });
     } 
    // Persist the user for later UI use 
    this.user.set(resp.user); // throws NG0600 
    return undefined; 
  }, 
  onError: () => customError({ kind: 'bad request', message: 'Bad Request' }), 
});
```

Actual behavior

Error: NG0600: Writing to signals is not allowed in a computed (onSuccess executes within a computed context, so setting this.user fails).

### Proposed solution

A first-class, documented way to:

- Safely persist or expose the payload returned by the async validator, and/or
- Perform side-effects (like setting another signal) after the validator settles, outside computed evaluation.

Why this matters

- Avoids redundant network calls (donâ€™t fetch user details twice).
- Reduces latency and improves UX.
- Mirrors common backend patterns where validation returns enriched data.

### Alternatives considered


Some ideas:
1. Side-effect callback outside computed

Add an option like onResolved or onSettled that runs after the validatorâ€™s computed completes, allowing safe writes to signals:
onResolved: ({ response, error }) => { if (response?.user) user.set(response.user); }

2. validateHttp to returns something?
","['area: forms', 'area: docs', 'forms: signals']",0
angular/angular,Feature Request: Form Signal Default Validation Messages Configuration,"### Which @angular/* package(s) are relevant/related to the feature request?

_No response_

### Description

## Problem

Currently, when using Angular Signal Forms validators like `required()`, `email()`, `minLength()`, etc., developers must specify error messages for each validator call:

```ts
loginForm = form(this.loginModel, (schemaPath) => {
  required(schemaPath.user.email, { message: 'Email is required' });
  email(schemaPath.user.email, { message: 'Please enter a valid email address' });
  required(schemaPath.user.password, { message: 'Password is required' });
  minLength(schemaPath.user.password, 8, { message: 'Password must be at least 8 characters' });
});
```

This leads to:
- **Repetitive code** when the same messages are used across multiple forms
- **Inconsistent messaging** across the application
- **Maintenance overhead** when messages need to be updated globally
- **Reduced developer experience** due to verbosity


### Proposed solution


## Proposed Solution

Add a `defaultMessages` configuration option to `SignalFormsConfig`, similar to the existing `classes` configuration. This would allow developers to define default messages for validators that can be overridden on a per-validator basis.

## API Design Proposal

Extend the `SignalFormsConfig` interface to include a `defaultMessages` property

```ts
export interface SignalFormsConfig {
  classes?: ..
  
  defaultMessages?: {
    required?: string | ((fieldName?: string) => string);
    email?: string | ((fieldName?: string) => string);
    min?: string | ((min: number, fieldName?: string) => string);
    max?: string | ((max: number, fieldName?: string) => string);
    minLength?: string | ((min: number, fieldName?: string) => string);
    maxLength?: string | ((max: number, fieldName?: string) => string);
    pattern?: string | ((pattern: string, fieldName?: string) => string);
    // ... other validators
  };
}
```

## Usage Examples

### Example 1: Simple Static Messages

```ts
import { provideSignalFormsConfig } from '@angular/forms/signals';

bootstrapApplication(App, {
  providers: [
    provideSignalFormsConfig({
      defaultMessages: {
        required: 'This field is required',
        email: 'Please enter a valid email address',
        minLength: (min: number) => `Must be at least ${min} characters`,
      },
    }),
  ],
});
```
### Example 2: Dynamic Messages Based on Field Name

```ts
provideSignalFormsConfig({
  defaultMessages: {
    required: (fieldName?: string) => {
      const name = fieldName ? `${fieldName} is` : 'This field is';
      return `${name} required`;
    },
    email: (fieldName?: string) => {
      const name = fieldName || 'Email';
      return `Please enter a valid ${name.toLowerCase()} address`;
    },
    minLength: (min: number, fieldName?: string) => {
      const name = fieldName || 'This field';
      return `${name} must be at least ${min} characters`;
    },
  },
});
```

### Example 3: Internationalization Support
```ts
provideSignalFormsConfig({
  defaultMessages: {
    required: () => i18n.translate('validation.required'),
    email: () => i18n.translate('validation.email'),
    minLength: (min: number) => i18n.translate('validation.minLength', { min }),
  },
});
```

## Benefits

- **DRY principle:** Define messages once, use everywhere
- **Consistency:** Centralized message management
- **Flexibility:** Can still override messages per-validator
- **Maintainability:** Update messages in one place
- **Similar pattern:** Follows the existing `classes` configuration approach
- **i18n ready:** Supports dynamic message generation


### Alternatives considered

None","['area: forms', 'forms: signals']",0
angular/angular,[Signal forms] Allow Non Blocking Validation Errors,"### Which @angular/* package(s) are relevant/related to the feature request?

forms

### Description

Allow to mark errors as Non Blocking in from validators. This allows to reuse validator api and shared error rendering template logic to display message hints under inputs without blocking validation logic.

### Proposed solution

1) Update ValidationError
```ts
interface ValidationError {  
  readonly kind: string;  
  readonly message?: string | undefined; 
  readonly nonBlocking?: boolean | undefined
}
```
2) update control and from valid/invalid status logic to ignore Non Blocking errors when setting valid/invalid state on a control/form. 
3) example of using in a form
```ts
  loginForm = form(this.loginModel, (schemaPath) => {
    required(schemaPath.email, {message: 'Email is required'});
    min(schemaPath.experience, 2, {message: 'Recomended experience is 2+ years', nonBlocking: true});
    required(schemaPath.password, {message: 'Password is required'});
  });
```
```html
<form>      
  <label>        Experience        
    <input [field]=""signupForm.experience"" />      
  </label>
@let error = signupForm.experience.errors()[0]
@if(error.nonBlocking) {
    <span class=""warn"" >{{error.message}}</span>
} @else if (error) {
 <span class=""error"" >{{error.message}}</span>
}
</form>
```
same principle for form validators.

### Alternatives considered

different names for the property can be considered to better describe behaviour.","['area: forms', 'forms: signals']",0
angular/angular,No IntelliSense / autocomplete for object literals inside inline array bindings in templates,"### Which @angular/* package(s) are the source of the bug?

language-service

### Is this a regression?

No

### Description

Angular Language Service does not provide autocomplete suggestions for object literal properties when the object literal is inside an inline array expression in a template binding, even though the input is strongly typed and hover/quick info correctly resolves the type.

This makes inline usage of typed array inputs hard to discover and inconsistent with single-object bindings.

### What works
- Hover / quick info correctly shows the expected type
- Template type-checking works
- Autocomplete works when: 
    - The value is a component property
    - The input expects a single object (not an array)

---

###  What does not work

Autocomplete inside object literals within inline array expressions in templates.

#### Reproduction steps
Component
```
interface Transformation {
  height?: number | string;
  width?: number | string;
}

@Component({
  selector: 'test-cmp',
  templateUrl: './test.html'
})
export class TestComponent {
  // empty
}
```


Directive / Component Input
```
@Input() transformation?: Transformation[];
```

Template
```
<test
  [transformation]=""[
    { height: 100, cr:'abc' } // no IntelliSense suggestions here for height and width though ts errors out if a wrong type is provided
  ]"">
</test>
```

<img width=""752"" height=""121"" alt=""Image"" src=""https://github.com/user-attachments/assets/0a53b635-4cb9-481d-815f-cf213fa600c7"" />

Â 

<img width=""689"" height=""194"" alt=""Image"" src=""https://github.com/user-attachments/assets/e3d6d875-916e-4251-92a1-bc730596f036"" />


### Expected behaviour 

Autocomplete should suggest:
- height
- width


### Please provide a link to a minimal reproduction of the bug

_No response_

### Please provide the exception or error you saw

```true

```

### Please provide the environment you discovered this bug in (run `ng version`)

```true
Angular CLI: 17.3.17
Node: 18.20.8
Package Manager: npm 10.8.2
OS: darwin arm64

Angular: 17.3.12
... animations, common, compiler, compiler-cli, core, forms
... platform-browser, platform-browser-dynamic

Package                         Version
---------------------------------------------------------
@angular-devkit/architect       0.1703.17
@angular-devkit/build-angular   17.3.17
@angular-devkit/core            17.3.17
@angular-devkit/schematics      17.3.17
@angular/cli                    17.3.17
@schematics/angular             17.3.17
rxjs                            7.8.2
typescript                      5.2.2
zone.js                         0.14.10
```

### Anything else?

Any help or suggestions around this will be appreciated",['area: vscode-extension'],0
angular/angular,Signal Forms: Mark all as touched when not hidden,"### Which @angular/* package(s) are relevant/related to the feature request?

forms

### Description

At the moment you can only mark all fields as touched when using the submit method. In our use case this will lead to undesirable behavior.

Eg. we habe two fields (subject and message). The message is hidden and will only be displayed when entering a subject first. If I tap on submit without entering anything, the subject has an error message. After adding a subject, the message will have the error immidiatly because through submit it was marked as touched (although it was hidden).

### Proposed solution

Let the developer decide which fields to mark as touched or add an option to markAsTouched, but only mark visible fields as touched (ignore hidden fields).

### Alternatives considered

-","['area: forms', 'forms: signals']",0
angular/angular,Allow Signal Form properties to be accessed by name in schema,"### Which @angular/* package(s) are relevant/related to the feature request?

core

### Description

I have a need to disable form fields based on an external source (API call). With reactive forms I can recursively disable fields whose name exists in a `string[]` with `disabledFields.forEach(disabledFieldId: string) => this.form.get(disabledFieldId).disable();`.

As far as I can tell, I cannot do this using Signal Forms.  I am trying to use `schema()` to apply the same logic. I have tried numerous ways to dynamically obtain the field name, but have not found a way to do this.

The following works, but is not dynamic and requires me to explicitly name each property twice:
`public static disableFields($disabledFields: Signal<string[]>): Schema<SampleForm> {`
`  return schema((fieldPath: SchemaPathTree<SampleForm>) => {`
`    disabled(fieldPath.field1, () => $disabledFields().includes('field1'));`
`    ...`
`    disabled(fieldPath.fieldn, () => $disabledFields().includes('fieldn'));`
`  });`
`}`

### Proposed solution

I am seeking a way to do the above by dynamically referencing each field's name, perhaps using `FieldTree.name`:
`public static disableFields($disabledFields: Signal<string[]>): Schema<SampleForm> {`
`    return schema((fieldPath: SchemaPathTree<SampleForm>) => {`
`      disabled(fieldPath, ({ fieldTree }) => $disabledFields().includes(fieldTree.name);`
`    });`
`  }`


### Alternatives considered

The ability to access properties using `fieldPath['xxx']` (similar to `.get('xxx')`) could also solve this problem, although I understand the reason this is missing as an evolution over reactive forms.
It's very possibly I'm doing something very wrong, but I've tried various things and read all of the documentation to no avail. Any help is appreciated.","['area: forms', 'forms: signals']",0
angular/angular,Incorrect XSS error shown after upgrade from angular 21.0.3 to 21.0.6,"### Which @angular/* package(s) are the source of the bug?

platform-browser

### Is this a regression?

Yes

### Description

I'm using `sanitizer.bypassSecurityTrustResourceUrl` to sanitize a url for an audio tag. This seem to have broken between angular 21.0.3 and 21.0.6, as far as I can tell. It works in this example (Angular 21.0.3) https://stackblitz.com/edit/stackblitz-starters-ffvgzhnf?file=src%2Fmain.ts, but if I then run ng update (to 21.0.6), the audio no longer plays, and I get  `src=""SafeValue must use [property]=binding: https://traffic.libsyn.com/brawlingbrothers/Brawling_Bros_-_Episode_57_-_Blobulous_Motion_-_GenCon_2017.mp3 (see https://angular.dev/best-practices/security#preventing-cross-site-scripting-xss)""` in my src.

(The repro below is update, and shows the bug).

### Please provide a link to a minimal reproduction of the bug

https://stackblitz.com/edit/stackblitz-starters-cw7peq8g?file=angular.json

### Please provide the exception or error you saw

```true

```

### Please provide the environment you discovered this bug in (run `ng version`)

```true
Angular CLI       : 21.0.4
Angular           : 21.0.6
Node.js           : 20.19.1
Package Manager   : npm 10.8.2
Operating System  : linux x64

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Package                   â”‚ Installed Version â”‚ Requested Version â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ @angular/animations       â”‚ 21.0.6            â”‚ ^21.0.3           â”‚
â”‚ @angular/build            â”‚ 21.0.4            â”‚ ^21.0.2           â”‚
â”‚ @angular/cli              â”‚ 21.0.4            â”‚ ^21.0.2           â”‚
â”‚ @angular/common           â”‚ 21.0.6            â”‚ ^21.0.3           â”‚
â”‚ @angular/compiler         â”‚ 21.0.6            â”‚ ^21.0.3           â”‚
â”‚ @angular/compiler-cli     â”‚ 21.0.6            â”‚ ^21.0.3           â”‚
â”‚ @angular/core             â”‚ 21.0.6            â”‚ ^21.0.3           â”‚
â”‚ @angular/forms            â”‚ 21.0.6            â”‚ ^21.0.3           â”‚
â”‚ @angular/platform-browser â”‚ 21.0.6            â”‚ ^21.0.3           â”‚
â”‚ @angular/router           â”‚ 21.0.6            â”‚ ^21.0.3           â”‚
â”‚ rxjs                      â”‚ 7.8.2             â”‚ ^7.8.1            â”‚
â”‚ typescript                â”‚ 5.9.3             â”‚ ^5.9.3            â”‚
â”‚ zone.js                   â”‚ 0.16.0            â”‚ ^0.16.0           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Anything else?

_No response_","['area: compiler', 'core: sanitization']",0
angular/angular,"FR: Better default debugNames for internal signals (computed / input), especially for signal forms","### Which @angular/* package(s) are relevant/related to the feature request?

_No response_

### Description

When debugging an app that uses signal forms, I tend to get error messages like

`ERROR RuntimeError: NG0950: Input ""field"" is required but no value is available yet. Find more at https://v21.angular.dev/errors/NG0950`

Example see e.g. https://stackblitz.com/~/github.com/ptandler/angular-signal-form-value-control-reactive-form-interop?file=src/main.ts

### Proposed solution

It would be amazing â¤ï¸  if `form()` and the `[field]` directive could provide a better default debugName to easier detect which field is required, e.g.
- include the component's name (or selector) in which the directive or form is defined
- include the name of the element a directive has been bound to
- if assigned to a prop (input, form), include also the prop name (i.e. `class SomeComponent { myForm = form(signal()); }` -> something like `form@SomeComponent.myForm` and then nested `form.someField.andAnother@SomeComponent.myForm`

Is it possible to get some name from the current / given injection context?

### Alternatives considered

Well, for now, I set breakpoints and try to figure out the context myself ðŸ˜ If the app is small enough, then that's fine.","['area: forms', 'forms: signals']",0
angular/angular,Inherited host bindings can write into wrong variable offset,"If one directive with host bindings containing pure functions inherits from another one, both pure function instructions will end up writing into the same offset on the `LView`. The reason is that pure functions are set up to [write to the binding root plus an offset](https://github.com/angular/angular/blob/main/packages/core/src/render3/pure_function.ts#L52), however the offsets overlap when the parent and child class host binding functions execute. This can lead to ""changed after checked"" errors.

It can be reproduced with the following tests:

```ts
it('should not throw', () => {
  @Directive({
    host: {
      '[attr.parent]': '[""parent""][0]',
    },
  })
  class ParentDir {}

  @Directive({
    host: {
      '[attr.child]': '[""child""][0]',
    },
    selector: 'some-dir',
  })
  class SomeDir extends ParentDir {}

  @Component({
    template: '<some-dir/>',
    imports: [SomeDir],
  })
  class App {}

  const fixture = TestBed.createComponent(App);
  fixture.detectChanges();
});
```

This produces two `pureFunction(2, ...)` instructions that both try to write into the same offset which eventually throw an error.","['area: compiler', 'bug', 'compiler: template pipeline']",0
angular/angular,Angular never removes style tags on destroy if any component has (animate.leave) hook,"### Which @angular/* package(s) are the source of the bug?

core

### Is this a regression?

No

### Description

If any component has a `(animate.leave)` hook - Angular does not remove style tags from head during renderer destroy

Please see reproduction example, if you remove (animate.leave) hook from `TestComponent` - background color will change when components destroys.

I expect that Angular remove style tags after all leave animations ends.

### Please provide a link to a minimal reproduction of the bug

https://stackblitz.com/edit/angular-fiddle-gqwykbhz-xf3v3hkw?file=src%2Fapp%2Ftest.component.ts

### Please provide the exception or error you saw

```true

```

### Please provide the environment you discovered this bug in (run `ng version`)

```true
Any Angular 20.2+ version
```

### Anything else?

_No response_","['area: core', 'core: animations']",0
angular/angular,RouterLinkActive: Handle undefined/null,"### Which @angular/* package(s) are relevant/related to the feature request?

router

### Description

`RouterLinkActive` and some of its inputs do not handle `undefined` or `null`.

### Proposed solution

`RouterLink` deactivates when `null` or `undefined` is passed. This is also handled in its other inputs: `fragment`, `queryParams`, `queryParamsHandling`, etc.

The same should be true for `RouterLinkActive` and by extension to its inputs: `routerLinkActiveOptions`, etc.

### Alternatives considered

None","['feature', 'area: router', 'router: directives']",0
angular/angular,Signal Forms cannot distinguish value changes from UI interaction (focus/blur),"### Which @angular/* package(s) are relevant/related to the feature request?

forms

### Description

I really appreciate the work and effort that goes into maintaining and evolving the framework â€” thank you for all the hard work.

In the current reactive model, form-related signals react to any interaction, not just real value changes.
Focus, blur, and touched state trigger the same reactions as actual data mutation, which makes common
user-editing scenarios difficult to implement cleanly.

### Minimal example with explanations

```ts
import { Component, effect, input, output, signal } from '@angular/core';
import { Field, form } from '@angular/forms/signals';

type User = {
  id: string;
  name: string;
  email: string;
};

@Component({
  selector: 'user-editor',
  imports: [Field],
  template: `
    <input placeholder=""Name"" [field]=""form.name"" />
    <input placeholder=""Email"" [field]=""form.email"" />
  `,
})
export class UserEditor {
  // Selected user (clicked from a list)
  readonly user = input.required<User>();

  // Emits when user data ""changes""
  readonly userChanged = output<Partial<User>>();

  // Form model
  readonly model = signal({ name: '', email: '' });
  readonly form = form(this.model);

  // WORKAROUNDS STATE
  private currentUserId = ''; // patch guard
  private first = true; // change guard

  constructor() {
    effect(() => {
      const user = this.user();

      /**
       * Problem #1:
       * Patching the form with incoming user data
       * triggers form reactions.
       *
       * There is no way to silence this update
       * (no equivalent to Reactive Forms `emitEvent: false`).
       */

      // Workaround: guard by user id
      if (this.currentUserId === user.id) {
        return;
      }

      this.currentUserId = user.id;
      this.first = true; // reset change guard

      // âŒ Triggers form effects even though user did not edit anything
      this.model.set({
        name: user.name,
        email: user.email,
      });
    });

    /**
     * Problem #2:
     * This effect runs on focus / blur / touched,
     * not only on real value mutation.
     */
    effect(() => {
      const form = this.form();
      const value = form.value();

      // Skip first execution (initial set / user switch)
      if (this.first) {
        this.first = false;
        return;
      }

      if (form.invalid()) {
        return;
      }

      // âŒ Emits even when no value changed
      this.userChanged.emit(value);
    });
  }
}
```

### Proposed solution

Provide a way to react **only to real value changes**, excluding UI interaction
(focus / blur / touched) and silent patching of values.

```ts
// Silence form updates when patching incoming data
form.patch(user, { emit: false });

// Subscribe only to value mutation
form.valueChanges({ only: 'value' });

// Effect that reacts only to value changes
effectOnValue(form, value => {
  userChanged.emit(value);
});
````

**Goal**

* Separate UI interaction from data mutation
* Remove the need for `first` flags and identity guards
* Make user-editing scenarios predictable and safe


### Alternatives considered

- **Manual deep-compare of `form.value()`**
  - Requires storing previous snapshots
  - Expensive and error-prone
  - Defeats the purpose of signal-based forms

- **Boolean guards (`first`, `initialized`)**
  - Must be reset on every patch
  - Easy to forget during refactors
  - Tightly couples UI flow to form logic

- **Identity guards (`currentUserId`)**
  - Leaks navigation/state logic into form code
  - Breaks when the same entity is reloaded with new data

- **Debouncing effects**
  - Still reacts to focus / blur
  - Only hides the problem, does not solve it

All alternatives are brittle and add complexity to a common user-editing use case.
","['area: forms', 'forms: signals']",0
angular/angular,"Signal forms, number input with inputMode=""decimal"" fails with localized decimal separator","### Which @angular/* package(s) are the source of the bug?

forms

### Is this a regression?

Yes

### Description

Create a signal form that contains a number attribute
Displaying that attribute with an input of type=""number"" and inputMode=""decimal"" will fail on an iPhone (iOS 26.1) when it's local OS language is french

iPhone's keyboard uses a '.' decimal separator, while the french localization seems to make the input field expect a ','. Hard coding the locale to EN from within the application has no effect

This was working fine with Reactive Forms

Switching the iPhone's OS language to english makes the bug go away.

```
type FormModel = {
  height: number;
};

model = signal<FormModel>({ height: 5.8 });

schema = schema<FormModel>((path) => {
    min(path.height, 4.13);
    max(path.height, 7);
    required(path.height);
  });

myForm = form(this.model, this.schema);
```

```
<label>Height</label>
<input [field]=""form.height"" type=""number"" inputmode=""decimal""  />
```



### Please provide a link to a minimal reproduction of the bug

_No response_

### Please provide the exception or error you saw

```true

```

### Please provide the environment you discovered this bug in (run `ng version`)

```true
Angular CLI       : 21.0.3
Angular           : 21.0.6
Node.js           : 24.11.1
Package Manager   : npm 11.7.0
Operating System  : linux x64

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Package                           â”‚ Installed Version â”‚ Requested Version â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ @angular/animations               â”‚ 21.0.6            â”‚ ^21.0.6           â”‚
â”‚ @angular/build                    â”‚ 21.0.3            â”‚ ^21.0.3           â”‚
â”‚ @angular/cdk                      â”‚ 21.0.3            â”‚ ^21.0.3           â”‚
â”‚ @angular/cli                      â”‚ 21.0.3            â”‚ ^21.0.3           â”‚
â”‚ @angular/common                   â”‚ 21.0.6            â”‚ ^21.0.6           â”‚
â”‚ @angular/compiler                 â”‚ 21.0.6            â”‚ ^21.0.6           â”‚
â”‚ @angular/compiler-cli             â”‚ 21.0.6            â”‚ ^21.0.6           â”‚
â”‚ @angular/core                     â”‚ 21.0.6            â”‚ ^21.0.6           â”‚
â”‚ @angular/forms                    â”‚ 21.0.6            â”‚ ^21.0.6           â”‚
â”‚ @angular/localize                 â”‚ 21.0.6            â”‚ ^21.0.6           â”‚
â”‚ @angular/platform-browser         â”‚ 21.0.6            â”‚ ^21.0.6           â”‚
â”‚ @angular/platform-browser-dynamic â”‚ 21.0.6            â”‚ ^21.0.6           â”‚
â”‚ @angular/router                   â”‚ 21.0.6            â”‚ ^21.0.6           â”‚
â”‚ rxjs                              â”‚ 7.8.2             â”‚ ^7.8.2            â”‚
â”‚ typescript                        â”‚ 5.9.3             â”‚ ^5.9.3            â”‚
â”‚ vitest                            â”‚ 4.0.16            â”‚ ^4.0.16           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Anything else?

_No response_","['area: forms', 'forms: signals']",0
angular/angular,Angular 21  `assertion error injectorIndex should point to a bloom filter` when using interpolation bindings on Kendo Angular Grid inputs,"### Which @angular/* package(s) are the source of the bug?

core

### Is this a regression?

Yes

### Description

After an upgrade to Angular 21, template syntax that worked in earlier versions now triggers an ASSERTION ERROR: LContainer must be defined.

The issue appears when interpolation syntax is used on component input bindings in the Kendo Angular Grid.

Example that previously worked in Angular 20:

```html
<kendo-grid [kendoGridBinding]=""data"">
  <kendo-grid-column field=""{{ 'date' }}"" title=""{{ 'Date' }}""></kendo-grid-column>
</kendo-grid>
```
The above will not work in Angular 21 unless optimization is set to true.

This will also work in Angular 21, but if you add a second Grid it will throw the error:

```html
      <kendo-grid [kendoGridBinding]=""data"">
        <kendo-grid-column title=""{{ 'bar' }}"" field=""{{ 'name' }}""></kendo-grid-column>
      </kendo-grid>
```

If[ build optimization](https://angular.dev/reference/configs/workspace-config#optimization-configuration) is enabled in the `angular.json`, the error will not be thrown.

Example where the error is reproduced and additional information in comment form:
[grid-angular21.zip](https://github.com/user-attachments/files/24193447/grid-angular21.zip)


### Please provide the exception or error you saw

```true
ERROR Error: ASSERTION ERROR: injectorIndex should point to a bloom filter [Expected=> number === string <=Actual]
    at throwError2 (_untracked-chunk.mjs:225:9)
    at assertNumber (_untracked-chunk.mjs:156:5)
    at assertNodeInjector (_untracked-chunk.mjs:1616:3)
    at lookupTokenUsingNodeInjector (_debug_node-chunk.mjs:926:20)
    at getOrCreateInjectable (_debug_node-chunk.mjs:878:19)
    at ÉµÉµdirectiveInject (_debug_node-chunk.mjs:7456:17)
    at NodeInjectorFactory.ListComponent_Factory [as factory] (progress-kendo-angular-grid.mjs:35536:102)
    at getNodeInjectable (_debug_node-chunk.mjs:1013:38)
    at instantiateAllDirectives (_debug_node-chunk.mjs:4923:23)
    at createDirectivesInstances (_debug_node-chunk.mjs:4782:3)


<img width=""960"" height=""177"" alt=""Image"" src=""https://github.com/user-attachments/assets/4b681abf-934f-461b-a634-b89d273c6913"" />
```

### Please provide the environment you discovered this bug in (run `ng version`)

```true
Angular CLI       : 21.0.0
Angular           : 21.0.0
Node.js           : 22.12.0
Package Manager   : npm 11.1.0
Operating System  : darwin arm64

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Package                   â”‚ Installed Version â”‚ Requested Version â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ @angular/build            â”‚ 21.0.0            â”‚ ^21.0.0           â”‚
â”‚ @angular/cli              â”‚ 21.0.0            â”‚ ^21.0.0           â”‚
â”‚ @angular/common           â”‚ 21.0.0            â”‚ ^21.0.0           â”‚
â”‚ @angular/compiler         â”‚ 21.0.0            â”‚ ^21.0.0           â”‚
â”‚ @angular/compiler-cli     â”‚ 21.0.0            â”‚ ^21.0.0           â”‚
â”‚ @angular/core             â”‚ 21.0.0            â”‚ ^21.0.0           â”‚
â”‚ @angular/forms            â”‚ 21.0.0            â”‚ ^21.0.0           â”‚
â”‚ @angular/localize         â”‚ 21.0.0            â”‚ ^21.0.0           â”‚
â”‚ @angular/platform-browser â”‚ 21.0.0            â”‚ ^21.0.0           â”‚
â”‚ @angular/router           â”‚ 21.0.0            â”‚ ^21.0.0           â”‚
â”‚ rxjs                      â”‚ 7.8.2             â”‚ ~7.8.0            â”‚
â”‚ typescript                â”‚ 5.9.3             â”‚ ~5.9.2            â”‚
â”‚ vitest                    â”‚ 4.0.14            â”‚ ^4.0.8
```","['area: forms', 'forms: signals']",0
angular/angular,Inject migration does not work in a multi-project workspace,"### Which @angular/* package(s) are the source of the bug?

core

### Is this a regression?

No

### Description

In [multi-project workspace](https://angular.dev/reference/configs/file-structure#multiple-projects), the inject migration doesn't work when targeting one of the projects due to either not finding any files in other projects or considering them external thus it throws a `SchematicsException`


### Minimal Reproduction
1. Create a new workspace with Angular CLI
    a. `ng new my-workspace --no-create-application`
2. Generate a new application in the workspace
    a. `ng generate application my-app`
3. Generate a new library in the workspace
    a. `ng generate library my-lib`
4. Create a new service
5. Import service into generated component in `my-lib`
6. Run inject migration targeting the library
    a. `ng generate @angular/core:inject --path=./projects/my-lib/src`

### Please provide a link to a minimal reproduction of the bug

https://stackblitz.com/edit/stackblitz-starters-ap2u31fu

### Please provide the exception or error you saw

```true
`Could not find any files to migrate under the path \\home\\projects\\stackblitz-starters-ap2u31fu\\projects\\my-lib\\src. Cannot run the inject migration.`
```

### Please provide the environment you discovered this bug in (run `ng version`)

```true
Angular CLI       : 21.0.3
Angular           : 21.0.5
Node.js           : 20.19.1
Package Manager   : npm 10.8.2
Operating System  : linux x64

 
Package				Version
------------------------------------------------
@angular/build			 21.0.3
@angular/cli			 21.0.3
@angular/common			 21.0.5
@angular/compiler		 21.0.5
@angular/compiler-cli		 21.0.5
@angular/core			 21.0.5
@angular/forms			 21.0.5
@angular/platform-browser	 21.0.5
@angular/router			 21.0.5
ng-packagr			 21.0.0
rxjs				 7.8.2
typescript			 5.9.3
vitest				 4.0.15
```

### Anything else?

I couldn't find any information if this multi-project scenario are supported by migrations so I'm not sure if this is a bug or not.

One thing I noticed is that in the [source code](https://github.com/angular/angular/blob/main/packages/core/schematics/ng-generate/inject-migration/index.ts#L62) the exception is thrown if no files are found, so maybe a naÃ¯ve fix could be just rendering a warning but continue to the next `tsconfig` file so that the whole migration process isn't interrupted (however it might still ignores due to the external library check)

### Alternatives considered
In a similar but now deprecated migration made by @eneajaho in the **ngxtension** project it contains a `--project` flag which could be a nice solution
https://ngxtension.dev/utilities/migrations/inject-migration/#options","['help wanted', 'area: migrations']",0
angular/angular,Add displayed/enabled properties as alternatives to hidden/disabled in signal forms,"### Which @angular/* package(s) are relevant/related to the feature request?

forms

### Description

When working with signal forms, relying on `hidden` and `disabled` properties often requires the use of `!field.hidden()` or `!field.disabled()`, which can make the logic harder to read and reason about.

### Proposed solution

Add positive alternatives to the existing boolean properties:

- `displayed` as the inverse of `hidden`
- `enabled` as the inverse of `disabled`

```html
@if (field.displayed() && field.enabled()) {
```

### Alternatives considered

I have a PR ready to go if the team is interested in these suggestions.","['area: forms', 'forms: signals']",0
angular/angular,Consistent and permissive style encapsulation management,"### Which @angular/* package(s) are relevant/related to the feature request?

core

### Description

It should be possible for a component to apply some style rules to its child contents.

An example of use is a reusable `<hgroup>` with consistent style but arbitrary contents. If you want to be able to pass `<p>` elements and one heading element (of arbitrary level), containing inline markup such as `<br/>, <a>, <b>, <em>`... you cannot use the component's inputs and have to pass the elements as child content. And then the trouble begins when the component needs to style that content.

With Angular's default view encapsulation mode, the style of that component's dedicated style sheet won't apply to the child content.
With the two ShadowDom view encapsulation modes, there are technical consequences which are warned against in the documentation that are probably not wanted as a general solution. Additionally, it could break the application of global style to the component's template and child contents, which is a big side-effect.
With no view encapsulation, the styles will apply globally which is definitely not a good solution for anything.

### Proposed solution

Allow (documentation-wise) and support the use of bounded `::ng-deep` to solve the issue.
What I mean by ""bounded"" means placing `ng-deep` in the middle of a CSS selector, and not at the start, for instance `:host ::ng-deep b` or `a ::ng-deep b`, such that the rule thus introduced cannot apply outside of the component's node.
Such a rule then applies to the child content of the component.

It is a simple solution, fits nicely in the style sheet of the component, and is also very versatile : as opposed to a new view encapsulation mode, you can have in the same component (even in the same style sheet), some rules applying only to the component's template, and others applying to all the component's contents (template and child).

To be perfectly clear, CSS selectors that would **start** with `::ng-deep`, or which would in any way allow styling to apply outside of the component's element, would remain as frowned upon as `::ng-deep` currently is.

Previous related discussions:
#25160
[a thread on the discord server](https://discord.com/channels/748677963142135818/1441371081020801024/1441371081020801024)
#65718
#65944

### Alternatives considered

Using ""design tokens"" or CSS variables - both kinda works the same.
CSS variables would be declared as (in the previous example) part of the `hgroup` component's API, the component's style sheet would then set values for those variables, and the component's user would have the responsibility to set all the related CSS properties, in the content passed to the component, to evaluating those variables.

I think that's several things:
- a massive bug nest with a _lot_ of moving parts
- way more complex than the use of `::ng-deep`
- requires more action from the component user's part than usually necessary
- exposing what should be private about the component : what CSS properties it affects is nobody's business, it's the component's job
- if the component doesn't want to apply the same styling to all its child contents, it needs to use CSS selectors, which the users of the component would need to copy to apply them to the child content they are passing to the component. Which makes the proposed solution even more complex.

This is the point where the discussion was left off.

---

I think that, at the very least, proposed alternatives need to:
- solve the above example
- provide concrete examples as to how allowing the ""bounded `::ng-deep`"" recipe would have worse consequences than the existing and supported `ViewEncapsulation.None` mode, in other words, why ""bounded `::ng-deep`"" should remain discouraged while that mode is not.","['area: core', 'core: stylesheets']",0
angular/angular,[signal forms]: add a way to easily access form's submitted state,"### Which @angular/* package(s) are relevant/related to the feature request?

forms

### Description

Hi, in template drive form we could find whether form is submitted by adding `#form=ngForm` on form element (using _form.submitted_), didn't find an alternative in signal based forms. 

### Proposed solution

add a way to access form submitted state like in template drive forms.

### Alternatives considered

Gemini says we should track it manually in component using hasSubmitted signal, also suggested using form `submit` helper from `@angular/forms/signals` , for submitting the form, 
but it won't be helpful in displaying error based on submitted state, so it seems manual signal in the component is the only option for now.","['area: forms', 'forms: signals']",0
angular/angular,ViewportScroller scrollToAnchor method + smooth scrolling + focus causes incorrect final scroll position when scroll-behavior: smooth is used,"### Which @angular/* package(s) are the source of the bug?

core

### Is this a regression?

No

### Description

When OS-level motion is allowed (no reduced-motion preference) and the app sets global CSS:

```
html {
  scroll-behavior: smooth;
}
```

Angularâ€™s ViewportScroller scrolls to a target and then focuses it for accessibility, specifically in the scrollToAnchor method.

```
 scrollToAnchor(target: string, options?: ScrollOptions): void {
    const elSelected = findAnchorFromDocument(this.document, target);

    if (elSelected) {
      this.scrollToElement(elSelected, options);
      // After scrolling to the element, the spec dictates that we follow the focus steps for the
      // target. Rather than following the robust steps, simply attempt focus.
      //
      // @see https://html.spec.whatwg.org/#get-the-focusable-area
      // @see https://developer.mozilla.org/en-US/docs/Web/API/HTMLOrForeignElement/focus
      // @see https://html.spec.whatwg.org/#focusable-area
      elSelected.focus();
    }
  }
```

Because smooth scrolling is asynchronous, Angularâ€™s assumption that window scrolling completes synchronously no longer holds. The subsequent focus() call can interrupt the in-progress animation, resulting in an incorrect final scroll position (the viewport ends up offset incorrectly compared to the intended anchor position).

This behavior is partially addressed in Angular 20.1, which introduces an options parameter for all ViewportScroller methods so scroll behavior can be explicitly configured and overridden. However, when behavior: 'smooth' is used (or when smooth behavior is still desired), the same glitch occurs: the focus call competes with the in-progress smooth scroll.

### Please provide a link to a minimal reproduction of the bug

https://gist.github.com/muffinman-coder/572c382e0bcc74afc6872fe605880c3c

### Please provide the exception or error you saw

```true
None. No JavaScript exceptions or console errors are thrown.

The issue is a behavioral race condition between smooth scrolling and a subsequent focus() call. When smooth scrolling is enabled, the scroll is asynchronous; Angular assumes it is synchronous and immediately focuses the target element, which interrupts the in-progress scroll animation and results in an incorrect final scroll position.
```

### Please provide the environment you discovered this bug in (run `ng version`)

```true
20.0.x
```

### Anything else?

In windows 11 to enable the smooth scrolling go to: Settings -> Ease of Use -> Show animations in windows",['area: common'],0
angular/angular,bug(material/timepicker): Usage with signal forms validation required & valid,"### Is this a regression?

- [ ] Yes, this behavior used to work in the previous version

### The previous version in which this bug was not present was

Template-driven forms, Reactive forms

### Description

When you use a `<mat-timepicker>` inside a `<mat-form-field>`, as long as you connect the field to a template-driven `[(ngModel)]` or reactive `[formControl]`, it will automatically validate that the value is a valid time, and if you input an invalid value, the `form-field` will turn red on blur.

However, if you connect the timepicker to a new Signal Forms form, it doesn't validate that the value is valid.
You can input any number like ""123"" or text like ""abc"" and the field doesn't show any red indication, and you are able to submit invalid values.

### Reproduction

StackBlitz link: https://stackblitz.com/edit/dnfb2grp?file=src%2Fexample%2Ftimepicker-validation-example.ts&file=src%2Fexample%2Ftimepicker-validation-example.html&startScript=start

<img width=""3839"" height=""1458"" alt=""Image"" src=""https://github.com/user-attachments/assets/35e9fc50-247e-4d45-92f5-d4f80d592228"" />

Steps to reproduce:
1. The StackBlitz above compares the three form types (Template-driven, Reactive, Signal forms).
2. The first two timepickers (Template-Driven & Reactive), turn red if you input an invalid value.
3. The third timepicker (Signal form), doesn't turn red if you input an invalid value.
4. In my project it also doesn't turn red when the field is clicked and blurred while empty, even though I've added a `required(fields.startTime);` to the signal form, and it's adding an asterisk to the form-field indicating that it's required. (For some reason this specific point isn't reproducing in the stackblitz above.)

### Expected Behavior

I expect two things:
- (a) When the field contains an invalid value like ""123"" or ""abc"", it should should have a red outline and the form should be ""invalid"".
- (b) When the field is empty and it's clicked then blurred, and there's a `required(...)` validator, it should turn red.



### Actual Behavior

There is no basic validation for empty/invalid values when using signal forms `[field]` with `<mat-timepicker>`.

### Environment

- Angular: 21.0.3
- CDK/Material: 21.0.2
- Browser(s): Chrome
","['area: forms', 'needs triage', 'forms: signals']",0
angular/angular,Signal Form's rawValue which includes disabled fields' values to make it compatible with Reactive Forms,"### Which @angular/* package(s) are relevant/related to the feature request?

@angular/forms/signals

### Description

As of 20.0.3, in my Signal Form example, if my field is **disabled** with schema (_SchemaPathTree_) and I try to get the form's value with `form().value()`, it will include disabled fields as well which is not compatible with Reactive Forms.

```
readonly model = signal<{ name: string; address: string }>({ name: '', address: '' });

readonly form = form(this.model, schema => {
    required(schema.name);
    disabled(schema.address);
});

console.log(form().value()); // { name: '', address: '' }

```

Usecase:
Migrating Reactive Forms to Signal Forms.

### Proposed solution

We could get the raw value from the model's _getter_.
`form().value()` could return only non-disabled value.

### Alternatives considered

Implement `rawValue() `getter as well to make it compatible with Reactive Forms.","['area: forms', 'forms: signals']",0
angular/angular,Introduce a teleport primitive for moving template fragments,"### Which @angular/* package(s) are relevant/related to the feature request?

common, compiler

### Description

Angular lacks a built-in, declarative way to move content to another location in the DOM tree (teleportation/portalling).
Developers frequently implement solutions using [CDK Portals](https://material.angular.dev/cdk/portal/overview), `@ngneat/overview`, or manual DOM manipulation.

The proposal focuses on providing a **built-in solution for ""teleportation""**. While CDK Portals/Overlays offer a powerful mechanism, they introduce additional complexity: developers must install a separate package, understand PortalOutlet concepts, and often move beyond templates to describe logic imperatively. This makes the ergonomics less intuitive, especially for beginners or for simple use cases where the goal is just â€œrender this content somewhere else in the DOMâ€.

Inspired by Vue's `<teleport>` and React's `createPortal`, this proposal introduces a declarative way to relocate template fragments in Angular, while maintaining clear data flow for the teleported part of the template. It addresses common UI challenges: rendering components outside their parent DOM hierarchy (e.g., modals, popovers, tooltips) to ensure correct stacking context and overlay behavior, while keeping content fully declarative with reactive state and DI.

### Proposed solution

The feature could be introduced as a core ""teleport"" primitive implemented as a directive. Several implementation strategies are possible:
1. **as a part of the syntax**
	- Behaves similarly to `<ng-container>` (does not create an extra DOM element)
	- Could be integrated into the compiler
2. **explicit import (like [RouterOutlet](https://angular.dev/api/router/RouterOutlet))**
	- The directive could be explicitly exported and reused as a composable building block
	- Developers could combine it with host directives when creating UI primitives (e.g., popovers, modals, dynamic overlays)

Regardless of the implementation strategy, the core behavior would be:
- to insert its children into the target element when enabled
- to insert them to their original location when disabled

Target resolution is based on a standard CSS selector to select elements directly from the DOM, meaning we do not create any separate placeholders using `<ng-container>` . This is primarily because one of the most common targets is `<body>` , where content needs to escape the root component boundaries (for example, for modals).

To emphasize that the mechanism directly belongs to the core ecosystem, Iâ€™ll use the `ng` prefix in the examples.

### Basic usage:
A simple ad-hoc modal with animation and proper stacking:
```html
<ng-teleport to=""body"">
  @if (open()) {
    <div class=""modal"" animate.enter=""fade-in"" animate.leave=""fade-out"">
      Modal content
    </div>
  }
</ng-teleport>
```

### Custom UI elements
This example illustrates a very straightforward developer workflow: I create a component â€” Popover â€” whose template will be rendered elsewhere in the DOM tree using `<ng-teleport>`. Despite being ""physically"" relocated, the template remains fully declarative: I can control its bindings using reactive state, and nodes inside the template can also inject dependencies via DI.
```ts
@Component({
  selector: 'app-popover',
  template: `
    <ng-teleport to=""body"">
      @if (open()) {
        <div class=""popover"" [style.top.px]=""y()"" [style.left.px]=""x()"">
          <ng-content />
        </div>
      }
    </ng-teleport>
  `,
})
export class Popover {
  readonly open = model(true);
  readonly anchor = input.required<HTMLElement>();
  readonly x = computed(/* ... */);
  readonly y = computed(/* ... */);
}
```

### Composition
One possible way to implement this behavior is through directive composition, assuming the teleport directive is exposed as part of the public API (and it is also expected that we will finally be able to set default values from the host):
```ts
@Component({
  selector: 'app-popover',
  template: `...`,
  hostDirectives: [
    {
      directive: Teleport,
      inputs: ['to: teleportTo'], // Iâ€™m still really waiting for the ability to set the value at this level
    },
  ],
})
export class Popover { /* ... */ }
```

### Reactive setup 
Inspired by other frameworks, we could also bind inputs reactively. For example, the previously mentioned disabled property:
```html
<ng-teleport [disabled]=""isMobile()"" [to]=""selector()"">
  ...
</ng-teleport>
```

### Ad-hoc implementation
Hereâ€™s a gist: https://gist.github.com/vs-borodin/38e87d0dbbebd7ac0cb6a9eb63e592d6, demonstrating the desired behavior at a basic level, without taking SSR considerations into account.

### Alternatives considered

To address content relocation and stacking context issues, we can consider the following alternatives:
- Since the requested mechanism primarily addresses stacking issues, overflow problems with absolutely positioned elements, as well as the limitations of fixed position when ancestors use CSS transforms, as an alternative we can rely on relatively advanced native solutions (e.g., the `<dialog>` element).
Limitations: Native elements can cover basic use cases, but for a full-featured UI component in Angularâ€”especially for complex modals or popoversâ€”JS is still required (at least for now).
- We can also leverage all the capabilities of Angular CDK portals/overlays to achieve the desired result, which, among other things, handles positioning and scroll management for overlays.
Limitations: Developers might still need to integrate third-party libraries to cover certain scenarios, especially when dealing with stacking context issuesâ€”for example, using Floating UI.",['area: core'],0
angular/angular,mat-form-fields binding to a signal form does not add readonly attribue when using the readonly validator,"### Which @angular/* package(s) are the source of the bug?

forms

### Is this a regression?

No

### Description

when using a mat-form-field with an matInput directive bound to a signal form, using a readonly validator on the field has no effect


### Please provide a link to a minimal reproduction of the bug

https://stackblitz.com/edit/stackblitz-starters-mbrurxfu?file=src%2Fapp%2Fapp.ts

### Please provide the exception or error you saw

```true
n/a
```

### Please provide the environment you discovered this bug in (run `ng version`)

```true
Angular CLI       : 21.0.2
Angular           : 21.0.3
Node.js           : 20.19.1
Package Manager   : npm 10.8.2
Operating System  : linux x64

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Package                   â”‚ Installed Version â”‚ Requested Version â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ @angular/build            â”‚ 21.0.2            â”‚ ^21.0.0           â”‚
â”‚ @angular/cdk              â”‚ 21.0.2            â”‚ ^21.0.0           â”‚
â”‚ @angular/cli              â”‚ 21.0.2            â”‚ ^21.0.0           â”‚
â”‚ @angular/common           â”‚ 21.0.3            â”‚ ^21.0.0           â”‚
â”‚ @angular/compiler         â”‚ 21.0.3            â”‚ ^21.0.0           â”‚
â”‚ @angular/compiler-cli     â”‚ 21.0.3            â”‚ ^21.0.0           â”‚
â”‚ @angular/core             â”‚ 21.0.3            â”‚ ^21.0.0           â”‚
â”‚ @angular/forms            â”‚ 21.0.3            â”‚ ^21.0.0           â”‚
â”‚ @angular/material         â”‚ 21.0.2            â”‚ ^21.0.0           â”‚
â”‚ @angular/platform-browser â”‚ 21.0.3            â”‚ ^21.0.0           â”‚
â”‚ @angular/router           â”‚ 21.0.3            â”‚ ^21.0.0           â”‚
â”‚ typescript                â”‚ 5.9.3             â”‚ ~5.9.2            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Anything else?

_No response_","['area: forms', 'forms: signals']",0
angular/angular,devtools(signal graph): internal framework signals can overwhelm application signals,"### Is this a bug report or a feature request?

- [ ] Bug Report
- [x] Feature Request

### Description [Feature Request only]

# Request: some way to filter out internal framework signals from the devtools signal graph

## Explanation + Concerns

A recent draft PR has surfaced an enhancement to the signal graph that has been on my mind

[refactor(router): Update RouterLinkActive to use signals #65892](https://github.com/angular/angular/pull/65892#issuecomment-3618988813)

My take I was told to spin into this issue: 

This is probably a larger question(s) beyond the scope of this, but this change reminds me of something I have been intending to make an issue about and this may inform/save that.

Question: Do these internal signals show up in the devtools? If so, would it be possible to opt them out with existing framework functionality?

One thing about reactive forms adopting signals internally is that the signal graph in the devtools has so much noise from internal signal state. This internal change was made before the signal devtools, but I was curious if something like this would inherit showing all those internal properties. To be honest the noise in reactive forms in particular makes it hard to see application signals, and upon seeing this PR I have some concern if this will add more noise.

For example, check out this reproduction: https://stackblitz.com/edit/stackblitz-starters-1mfuv5ek?file=src%2Fmain.ts

## Example: one user signal vs different basic reactive form signal internals

```ts
// Reproduction also includes a form group with a control and the relatively new form array with a control
// Can comment out individually to isolate + scale
@Component({
  template: `
    <p>{{applicationSignal()}}</p>

    <input [formControl]=""reactiveFormControl"" />
  `,
  imports: [ReactiveFormsModule],
})
export class App {
  applicationSignal = signal('application signal value');

  reactiveFormControl = new FormControl('form control value');
}
```

<img width=""205"" height=""196"" alt=""Signal graph of one application signal and 6 internal form signals"" src=""https://github.com/user-attachments/assets/3a2cee24-713e-4f73-b5bb-5fca52c2baaa"" />

As you can see, the one user written signal is a fraction of the six internal framework signals used for a form control.

| Form class type | Internal signals |
| --- | --- |
| 1 `FormControl` (FC) |  6 |
| 1 `FormGroup` with 1 FC | 14 |
| 1 `FormArray` with 1 FC | 14 | 

All versus a single user written signal. I do reactive forms all day and even a single control or small group could out-scale most signals I write myself in a component.

My concern is that as more signals are used internally, that it becomes harder to see my application's signals. It is already an issue IMO with reactive forms.

### Proposed solution [Feature Request only]

Four options I think could all have advantages

- Existing convention: Be able to filter out framework signals, akin to ""Hide Framework Injectors"" in the Injector Tree
- Most out of sight of user: Perhaps just not even show them? This usage of signals internally in forms predates the signal devtools, but I mention this now since it looks like more signals will be used internally in existing APIs. 
- Distinction: Place internal signals in a different cluster than application signals, or with different shapes/colors. Tbh to me this sounds reaching and plausibly just as confusing, but it came to mind.
- Potentially radical: [""Compound signals"" visualization](https://github.com/angular/angular/issues/63227). Like how the resource visualizer PR linked in that issue is like.  I do think it could be cool and useful to see internal signals, but I imagine that could be confusing and a lot of work. But I thought at least for reactive forms that it would be cool to see values in the tree in a condensed way, though I know that API in particular has nuances in those values that may not be applicable/accurate timing for users.

### Alternatives considered [Feature Request only]

Use other parts of the devtooling or continue panning around to find my authored signals. To be honest, almost all apps I make even with relatively low amount of form classes can make it really hard to find my own values.",['area: devtools'],0
angular/angular,Signal Forms: FormUiControl still doesn't allow for inputs with transform,"### Which @angular/* package(s) are the source of the bug?

forms

### Is this a regression?

No

### Description

As said in #65756, the `FormUiControl` still doesn't allow the inputs to have type-altering transforms, even after the supposed fix.

I still want to do something like:
```ts
import { BooleanInput, coerceBooleanProperty } from '@angular/cdk/coercion';
import { Component, input, model } from '@angular/core';
import { FormValueControl } from '@angular/forms/signals';

@Component({
  selector: 'custom-input',
  standalone: true,
  template: `<input [value]=""value()"" (change)=""value.set($event.target.value)"" />`,
})
export class CustomInputComponent implements FormValueControl<string> {
  readonly value = model<string>('');

  readonly disabled = input<boolean, BooleanInput>(false, {
    transform: (v) => coerceBooleanProperty(v),
  });
}
```

But this time I am getting the error:
```
Property 'readonly' in type 'CustomInputComponent' is not assignable to the same property in base type 'Pick<FormUiControl, ""disabled"" | ""readonly"">'.
  Type 'InputSignalWithTransform<boolean, BooleanInput>' is not assignable to type 'InputSignal<boolean> | InputSignalWithTransform<boolean, unknown> | undefined'.
    Type 'InputSignalWithTransform<boolean, BooleanInput>' is not assignable to type 'InputSignal<boolean> | InputSignalWithTransform<boolean, unknown>'.
      Type 'InputSignalWithTransform<boolean, BooleanInput>' is not assignable to type 'InputSignalWithTransform<boolean, unknown>'.
        The types of '[SIGNAL].transformFn' are incompatible between these types.
          Type '((value: BooleanInput) => boolean) | undefined' is not assignable to type '((value: unknown) => boolean) | undefined'.
            Type '(value: BooleanInput) => boolean' is not assignable to type '(value: unknown) => boolean'.
              Types of parameters 'value' and 'value' are incompatible.
                Type 'unknown' is not assignable to type 'BooleanInput'.ts(2416)
```

This is caused by `FormUiControl` only allowing `unknown` type for the pre-transform value type, causing only such inputs to be accepted. As I previously suggested, making the type `any` would solve this issue. The FormUiControl shouldn't care about what type the pre-transform value is or how the transformer is implemented, only that the post-transform value is boolean - thus `any` can be used instead of `unknown` to allow for type-strict transformers (such as `string | boolean` -> `boolean`).

### Please provide a link to a minimal reproduction of the bug

_No response_

Edit: Reproduction in the original Issue

### Please provide the exception or error you saw

```true
Property 'readonly' in type 'CustomInputComponent' is not assignable to the same property in base type 'Pick<FormUiControl, ""disabled"" | ""readonly"">'.
  Type 'InputSignalWithTransform<boolean, BooleanInput>' is not assignable to type 'InputSignal<boolean> | InputSignalWithTransform<boolean, unknown> | undefined'.
    Type 'InputSignalWithTransform<boolean, BooleanInput>' is not assignable to type 'InputSignal<boolean> | InputSignalWithTransform<boolean, unknown>'.
      Type 'InputSignalWithTransform<boolean, BooleanInput>' is not assignable to type 'InputSignalWithTransform<boolean, unknown>'.
        The types of '[SIGNAL].transformFn' are incompatible between these types.
          Type '((value: BooleanInput) => boolean) | undefined' is not assignable to type '((value: unknown) => boolean) | undefined'.
            Type '(value: BooleanInput) => boolean' is not assignable to type '(value: unknown) => boolean'.
              Types of parameters 'value' and 'value' are incompatible.
                Type 'unknown' is not assignable to type 'BooleanInput'.ts(2416)
```

### Please provide the environment you discovered this bug in (run `ng version`)

```true
Angular CLI       : 21.0.1
Angular           : 21.0.3
Node.js           : 22.16.0
Package Manager   : pnpm 10.9.0
Operating System  : win32 x64

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Package                           â”‚ Installed Version â”‚ Requested Version â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ @angular/animations               â”‚ 21.0.3            â”‚ ^21.0.3           â”‚
â”‚ @angular/build                    â”‚ 21.0.1            â”‚ ^21.0.1           â”‚
â”‚ @angular/cdk                      â”‚ 21.0.2            â”‚ ^21.0.2           â”‚
â”‚ @angular/cli                      â”‚ 21.0.1            â”‚ ^21.0.1           â”‚
â”‚ @angular/common                   â”‚ 21.0.3            â”‚ ^21.0.3           â”‚
â”‚ @angular/compiler                 â”‚ 21.0.3            â”‚ ^21.0.3           â”‚
â”‚ @angular/compiler-cli             â”‚ 21.0.3            â”‚ ^21.0.3           â”‚
â”‚ @angular/core                     â”‚ 21.0.3            â”‚ ^21.0.3           â”‚
â”‚ @angular/forms                    â”‚ 21.0.3            â”‚ ^21.0.3           â”‚
â”‚ @angular/platform-browser         â”‚ 21.0.3            â”‚ ^21.0.3           â”‚
â”‚ @angular/platform-browser-dynamic â”‚ 21.0.3            â”‚ ^21.0.3           â”‚
â”‚ @angular/router                   â”‚ 21.0.3            â”‚ ^21.0.3           â”‚
â”‚ ng-packagr                        â”‚ 21.0.0            â”‚ ^21.0.0           â”‚
â”‚ rxjs                              â”‚ 7.8.1             â”‚ ~7.8.0            â”‚
â”‚ typescript                        â”‚ 5.9.3             â”‚ ~5.9.2            â”‚
â”‚ vitest                            â”‚ 4.0.14            â”‚ ^4.0.8            â”‚
â”‚ zone.js                           â”‚ 0.15.1            â”‚ ~0.15.1           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Anything else?

_No response_","['area: forms', 'forms: signals']",0
angular/angular,feat(devtools): show @for loops in components tab,"### Is this a bug report or a feature request?

- [ ] Bug Report
- [x] Feature Request

### Please provide the steps to reproduce the issue [Bug Report only]

_No response_

### Please provide the expected behavior vs the actual behavior you encountered [Bug Report only]

_No response_

### Please provide a screenshot if possible [Bug Report only]

_No response_

### Please provide the exception or error you saw [Bug Report only]

```true

```

### Is this a browser-specific issue? If so, please specify the device, browser, and version. [Bug Report only]

```true

```

### Description [Feature Request only]

Seeing where @for loops are used and the current list that is being looped would be great.

### Proposed solution [Feature Request only]

-

### Alternatives considered [Feature Request only]

-",['area: devtools'],0
golang/go,x/text/internal/export/idna: TestLabelErrors/ToUnicode/*.foo.com/err:P1 failures,"```
#!watchflakes
default <- pkg == ""golang.org/x/text/internal/export/idna"" && test == ""TestLabelErrors/ToUnicode/*.foo.com/err:P1""
```

Issue created automatically to collect these failures.

Example ([log](https://ci.chromium.org/b/8691951390192671633)):

    === RUN   TestLabelErrors/ToUnicode/*.foo.com/err:P1
        idna_test.go:95: got ""*.foo.com"", no errors; want error in {P1}
    --- FAIL: TestLabelErrors/ToUnicode/*.foo.com/err:P1 (0.00s)


â€” [watchflakes](https://go.dev/wiki/Watchflakes)
",['NeedsInvestigation'],0
golang/go,context: AfterFunc (Connection) example has race condition,"### Go version

go1.25.6 playground

### Output of `go env` in your module/workspace:

```shell
N/A - reproduce found in playground
```

### What did you do?

Found race condition with test:

https://pkg.go.dev/context#example-AfterFunc-Connection

```go
fatal error: all goroutines are asleep - deadlock!

goroutine 1 [IO wait]:
internal/poll.runtime_pollWait(0x7ebd7704cc00, 0x77)
	/usr/local/go-faketime/src/runtime/netpoll.go:351 +0x85
internal/poll.(*pollDesc).wait(0xc0000f2180?, 0x516dde?, 0x0)
	/usr/local/go-faketime/src/internal/poll/fd_poll_runtime.go:84 +0x27
internal/poll.(*pollDesc).waitWrite(...)
	/usr/local/go-faketime/src/internal/poll/fd_poll_runtime.go:93
internal/poll.(*FD).WaitWrite(...)
	/usr/local/go-faketime/src/internal/poll/fd_unix.go:685
net.(*netFD).connect(0xc0000f2180, {0x547fb0, 0x6218e0}, {0x4164f4?, 0x0?}, {0x547948?, 0xc0000b6100?})
	/usr/local/go-faketime/src/net/fd_unix.go:130 +0x6b6
net.(*netFD).dial(0xc0000f2180, {0x547fb0, 0x6218e0}, {0x5485b8?, 0x0?}, {0x5485b8, 0xc0000aa630}, 0x7ebd77325470?)
	/usr/local/go-faketime/src/net/sock_posix.go:124 +0x3bc
net.socket({0x547fb0, 0x6218e0}, {0x516dde, 0x3}, 0x2, 0x1, 0xc000002380?, 0x0, {0x5485b8, 0x0}, ...)
	/usr/local/go-faketime/src/net/sock_posix.go:70 +0x29b
net.internetSocket({0x547fb0, 0x6218e0}, {0x516dde, 0x3}, {0x5485b8, 0x0}, {0x5485b8, 0xc0000aa630}, 0x1, 0x0, ...)
	/usr/local/go-faketime/src/net/ipsock_posix.go:167 +0xf8
net.(*sysDialer).doDialTCPProto(0xc0000f8000, {0x547fb0, 0x6218e0}, 0x0, 0xc0000aa630, 0x0)
	/usr/local/go-faketime/src/net/tcpsock_posix.go:85 +0xe7
net.(*sysDialer).doDialTCP(...)
	/usr/local/go-faketime/src/net/tcpsock_posix.go:75
net.(*sysDialer).dialTCP(0x28?, {0x547fb0?, 0x6218e0?}, 0xc0000789f0?, 0x4cfa66?)
	/usr/local/go-faketime/src/net/tcpsock_posix.go:71 +0x69
net.(*sysDialer).dialSingle(0xc0000f8000, {0x547fb0, 0x6218e0}, {0x547e28, 0xc0000aa630})
	/usr/local/go-faketime/src/net/dial.go:721 +0x3ce
net.(*sysDialer).dialSerial(0xc0000f8000, {0x547fb0, 0x6218e0}, {0xc0000b4130?, 0x1, 0x1c?})
	/usr/local/go-faketime/src/net/dial.go:686 +0x248
net.(*sysDialer).dialParallel(0x0?, {0x547fb0?, 0x6218e0?}, {0xc0000b4130?, 0x4f6320?, 0x516e95?}, {0x0?, 0x516dde?, 0x7ebd7731a5c0?})
	/usr/local/go-faketime/src/net/dial.go:587 +0x30a
net.(*Dialer).DialContext(0xc000078d90, {0x547fb0, 0x6218e0}, {0x516dde, 0x3}, {0xc0000be140, 0xf})
	/usr/local/go-faketime/src/net/dial.go:578 +0x6a9
net.(*Dialer).Dial(...)
	/usr/local/go-faketime/src/net/dial.go:504
net.Dial({0x516dde?, 0x490013?}, {0xc0000be140?, 0x415b77?})
	/usr/local/go-faketime/src/net/dial.go:471 +0x77
main.main()
	/tmp/sandbox917687343/prog.go:35 +0x132
```

### What did you see happen?

Occasional fatal error when trying example.

### What did you expect to see?

Examples from standard library is expected to work.",['BugReport'],0
golang/go,x/text/internal/export/idna: ExampleNew failures,"```
#!watchflakes
default <- pkg == ""golang.org/x/text/internal/export/idna"" && test == ""ExampleNew""
```

Issue created automatically to collect these failures.

Example ([log](https://ci.chromium.org/b/8691951390192671633)):

    === RUN   ExampleNew
    --- FAIL: ExampleNew (0.00s)
    got:
    *.xn--fa-hia.com <nil>
    *.fass.com <nil>
    *.faÃŸ.com idna: disallowed rune U+002A
    *.fass.com <nil>
    want:
    *.xn--fa-hia.com <nil>
    *.fass.com idna: disallowed rune U+002A
    *.faÃŸ.com idna: disallowed rune U+002A
    *.fass.com <nil>


â€” [watchflakes](https://go.dev/wiki/Watchflakes)
",['NeedsInvestigation'],0
golang/go,cmd/go: TestScript/test_vendor failures,"```
#!watchflakes
default <- pkg == ""cmd/go"" && test == ""TestScript/test_vendor""
```

Issue created automatically to collect these failures.

Example ([log](https://ci.chromium.org/b/8691922406930307265)):

    === RUN   TestScript/test_vendor
    === PAUSE TestScript/test_vendor
    === CONT  TestScript/test_vendor
        script_test.go:139: 2026-01-23T02:14:26Z
        script_test.go:141: $WORK=/Users/swarming/.swarming/w/ir/x/t/cmd-go-test-2728213404/tmpdir3044049760/test_vendor1025496841
        script_test.go:163: 
            PATH=/Users/swarming/.swarming/w/ir/x/t/cmd-go-test-2728213404/tmpdir3044049760/testbin:/Users/swarming/.swarming/w/ir/x/w/goroot/bin:/Users/swarming/.swarming/w/ir/x/w/goroot/bin:/Users/swarming/.swarming/w/ir/x/w/goroot/bin:/Users/swarming/.swarming/w/ir/cache/tools/bin:/Users/swarming/.swarming/w/ir/bbagent_utility_packages:/Users/swarming/.swarming/w/ir/bbagent_utility_packages/bin:/Users/swarming/.swarming/w/ir/cipd_bin_packages:/Users/swarming/.swarming/w/ir/cipd_bin_packages/bin:/Users/swarming/.swarming/w/ir/cache/cipd_client:/Users/swarming/.swarming/w/ir/cache/cipd_client/bin:/Users/swarming/.swarming/cipd_cache/bin:/usr/local/bin:/usr/bin:/bin:/usr/local/sbin:/usr/sbin:/sbin
            HOME=/no-home
            CCACHE_DISABLE=1
            GOARCH=arm64
    ...
            r27     0x103fc3000
            r28     0x103fe86a0
            r29     0x16d0eac50
            lr      0x194bbc894
            sp      0x16d0eabc0
            pc      0x194b7e6ec
            fault   0x194b7e6ec
            [exit status 2]
        script_test.go:163: FAIL: testdata/script/test_vendor.txt:10: context deadline exceeded
    --- FAIL: TestScript/test_vendor (997.87s)


â€” [watchflakes](https://go.dev/wiki/Watchflakes)
",['NeedsInvestigation'],0
golang/go,crypto/tls: data race accessing Conn.handshakeErr in QUICConn.NextEvent,"## What version of Go are you using (`go version`)?


$ go version
go version go1.26rc2 darwin/arm64


## Does this issue reproduce with the latest release?

No, it does not reproduce with go1.25.x. 


## What did you do?

A user observed a data race: https://github.com/quic-go/quic-go/issues/5548

## What did you expect to see?

No data race.

## What did you see instead?

The handshake error variable (`handshakeErr`) is accessed without a mutex.

Code links:
- Read: https://github.com/golang/go/blob/go1.26rc2/src/crypto/tls/quic.go#L242
- Write: https://github.com/golang/go/blob/go1.26rc2/src/crypto/tls/conn.go#L1562

This bug was introduced in the CL that added QUICErrEvent: https://go-review.googlesource.com/c/go/+/719040",[],0
golang/go,Proposal: Generic Methods for Go,"# Proposal: Generic Methods for Go

## A change of view.

## Background

For clarity, in the following we use the term *concrete method* (or just method when the context is clear) to describe a *non-interface method* declared like a function but with a receiver; and we use the term *interface method* to describe the name and signature of a *method of an [interface](https://golang.org/ref/spec#Interface_types)*.

Per the current [spec](https://golang.org/ref/spec#Method_declarations), a concrete method is a function declaration with a receiver. Syntactically this is not quite true: while functions can be generic, methods cannot. They cannot declare new type parameters themselves; but they can have a receiver of a generic type (and thus have method-local names for the type parameters of the receiver type).

A reason for this discrepancy is that we have historically viewed the primary role of methods as a means to implement an [interface](https://golang.org/ref/spec#Interface_types): permitting type parameters on concrete methods would imply that we must also permit type parameters on interface methods. Go doesn't support such generic interface methods because we don't know how to implement (calls of) them, or at least we don't know how to implement them efficiently. Specifically, because Go doesn't require a concrete type to declare the interfaces it implements, and instead this is a dynamic property, it cannot be known at compile time which of the infinite possible instantiations of concrete methods will be needed at run time. This shortcoming has long been known and was [discussed at length](https://go.googlesource.com/proposal/+/refs/heads/master/design/43651-type-parameters.md#No-parameterized-methods) in the original [Type Parameters Proposal](https://go.googlesource.com/proposal/+/refs/heads/master/design/43651-type-parameters.md).

But concrete methods are not just a means for implementing interfaces. A method is a function associated with a type, and accessed through the namespace of that type. Therefore methods are useful for organizing code even if they don't ever implement an interface. Furthermore there is a syntactic benefit: x.a().b().c() may naturally be read left to right, whereas c(b(a(x))) is evaluated inside out. Both these aspects of methods are also well known.

For these reasons, Go users have requested generic methods for a long time and the idea is widely popular. There are at least two proposals filed on the issue tracker:

* [\#49085](https://github.com/golang/go/issues/49085) (Allow type parameters in methods, Oct. 2021, with \> 900 positive emojis)  
* [\#50981](https://github.com/golang/go/issues/50981) (Add generics to methods, Feb. 2022\)

So far we have resisted adding generic concrete methods because it always implied that we also needed generic interface methods. The [Go FAQ](https://go.dev/doc/faq#generic_methods) even states that ""we do not anticipate that Go will ever add generic methods"". Perhaps a change of view is in order: concrete methods are a language feature that is useful in itself, irrespective of interfaces. If a concrete method is a function with a receiver, a generic concrete method can be a generic function with a receiver. The fact that such methods may not be invoked via an interface is an orthogonal aspect: if an interface syntactically canâ€™t include a method with type parameters, then a generic concrete method naturally plays no role in satisfying that interface because there canâ€™t be an interface method with matching type parameters.

Note that the original Type Parameters Proposal discussed [this alternative view](https://go.googlesource.com/proposal/+/refs/heads/master/design/43651-type-parameters.md#No-parameterized-methods) as well, as expressed in the following paragraph:

""Or, we could decide that parameterized methods do not, in fact, implement interfaces, but then it's much less clear why we need methods at all. If we disregard interfaces, any parameterized method can be implemented as a parameterized function.""

We may have reached some clarity on this point: generic concrete methods are useful by themselves, even if they don't implement interface methods.

## Proposal

We propose that concrete method declarations should look exactly like function declarations, but with receivers. Specifically, the syntax of a method declaration should accept type parameters as do function declarations. The syntax of function declarations is as follows:

> FunctionDecl \= ""func"" FunctionName \[ TypeParameters \] Signature \[ FunctionBody \] .

The syntax of method declarations should be changed from (old):

> MethodDecl \= ""func"" Receiver MethodName Signature \[ FunctionBody \] .

to (new):

> MethodDecl \= ""func"" Receiver MethodName **\[ TypeParameters \]** Signature \[ FunctionBody \] .

The scope of an identifier denoting a type parameter of a generic method begins after the name of the method and ends at the end of the method body. This matches existing rules for type parameters of functions and method receivers. Constraints for method type parameters may refer to type parameters declared with the method receiver because they are in scope.

Calling a generic concrete method works as expected: either, type arguments are provided as needed, or type inference determines any missing type arguments, exactly like for generic function calls.

[Method expressions](https://golang.org/ref/spec#Method_expressions) and [method values](https://golang.org/ref/spec#Method_values) continue to work as expected: if the method is generic, the resulting function is generic and the existing rules for using generic functions apply.

The grammar will need a small adjustment: currently type arguments may only be supplied to operand names which are (possibly package-qualified) identifiers for generic functions and types. Per the [syntax for operands](https://golang.org/ref/spec#Operands):

> Operand     \= Literal | **OperandName \[ TypeArgs \]** | ""("" Expression "")"" .  
> â€¦  
> OperandName \= identifier | QualifiedIdent .  
> TypeArgs \= ""\["" TypeList \[ "","" \] ""\]"" .

Methods can be attached to arbitrary user-defined types, and values of such types may be denoted by various expressions, not just qualified identifiers. To permit instantiation of generic methods, type arguments must move from the syntax for operands to the s[yntax for primary expressions](https://golang.org/ref/spec#Primary_expressions), adding an extra case to the PrimaryExpr production:

> PrimaryExpr   \= Operand | â€¦ **PrimaryExpr TypeArgs** .

(Notably, type arguments are already parsed as part of primary expressions in the current implementation. This is necessary because an instantiation such as T\[int\] and an index expression a\[i\] are syntactically indistinguishable. In the language specification, the syntactic overlap between these two is maintained solely for the sake of documentation.)

Methods of interfaces are not changed. Importantly, a generic concrete method does not match against an interface method with the same name and signature because the interface method syntactically cannot have matching type parameters.

Generic methods also won't be accessible via reflection (by name or index) for the same reason that uninstantiated generic functions are not accessible: the reflect package doesn't have a mechanism to instantiate a generic value or type (and it is unclear how one would provide such a mechanism).

This is the entirety of the proposal.

## Examples

Given a (non-interface) receiver base type S, we may associate a generic method m with it:

```go
type S struct { â€¦ }  
func (*S) m[P any](x P) { â€¦ }
```

Given a variable s of type S (or a value of type \*S), we can call m as follows:

```go
var s S  
s.m[int](42)    // explicit type argument int  
s.m(x)          // type argument P is inferred from x
```

A receiver base type may itself be generic:

```go
type G[P any] struct{ â€¦ }  
func (*G[P]) m[Q any](x Q) { â€¦ }
```

Calling m requires a variable whose type is an instantiation of G, but there is no other difference in the way m is called.

An interface I with a method m:

```go
type I interface {  
	m(string)  
}
```
may be implemented by a suitably instantiated generic type G (this is true in current Go):

```go
type G[P any] struct{ â€¦ }  
func (G[P]) m(P) { â€¦ }

var g G[string]  
var _ I = g		// valid because G[string].m has signature m(string) which matches m(string) of I
```

But I is not implemented by a type H with a generic method m:

```go
type H struct{ â€¦ }  
func (H) m[P any](P) { â€¦ }

var h H  
var _ I = h		// invalid because H.m has signature m[P any](P) which doesn't match m(string) of I
```

Here's another, less abstract example, related to the Go library. A Reader type with a generic Read method:

```go
type Reader struct{ â€¦ }  
func (*Reader) Read[E any]([]E) (int, error) { â€¦ }
```

does not implement io.Reader, even though it might if there were some way to instantiate the method as (\*Reader).Read\[byte\] (which there is not, and we are not proposing it).

Method expressions (and method values) work as expected. Given the Reader type from above, the method expression Reader.Read produces a generic function with type parameters and signature:

```go
[E any](*Reader, []E) (int, error)
```

If the type itself is generic, it must be instantiated (per existing Go [rules for generic types](https://golang.org/ref/spec#Type_declarations)) before it can be used in a method expression. For instance, given a generic List type with a generic formatting method format:

```go
type List[E any] struct { â€¦ }  
func (*List[E]) format[F any](E, F) string { â€¦ }
```

the method expression List\[string\].format will result in a function with type parameters and signature

```go
[F any](string, F) string
```

In particular, List.format is not a valid method expression (List is not instantiated), nor does it produce the generic function signature

```go
[E any, F any](E, F) string	// not the signature of List.format
```

## Implementation

### Specification

The change to the language specification is small and fully backward-compatible. The syntax changes have been outlined above. The corresponding prose changes are similarly straightforward.

Whenever there is a language change, updating the specification is a tiny part of the work needed to get documentation updated in general. This includes existing documents traditionally maintained by the Go team (which we plan to keep fresh as much as possible), to 3P documentation beyond our control (external blog post, articles, books, etc.). These will become partly out of date and may or may not be updated, but that is not avoidable if we want to make changes to the language.

### Compiler

The parser already accepts type parameters for methods for robustness but complains with an error. Removing that error is a trivial change. The parser also already handles index expressions and instantiations together. If any changes are needed at all, they will be minimal. The type checker will need various adjustments, most likely involving the removal of restrictions, the details of which will only become apparent by doing the actual work.

The compiler back-end changes are likely more significant. That said, method calls via non-interface receivers can be resolved statically (at compile time): because the type of the receiver is not an interface, its type is statically known, and thus the called method is also statically known. Conceptually, such method calls can be rewritten into function calls. If the method itself is generic, the method call can be rewritten into a call of a corresponding generic function.

For instance, given the type G, method m, and instance x of G:

```go
type G[P any] struct{ â€¦ }  
func (G[P]) m[Q any](x Q) { â€¦ }

var g G[string]
```

the method call

```go
var s string  
g.m(s)
```

can be translated into a function call f(s) of the generic function

```go
func f[Q any](g G[string], x Q) {  
	G[string].m[Q](g, x)		// using a call of method expression G[string].m  
}
```

or

```go
func f[Q any](g G[string], x Q) {  
	g.m[Q](x)			// using a call of method value g.m[Q]  
}
```

This is not to say that an implementation will be trivial. But the principal translation mechanism is understood.

The import/export data format will need to be updated to allow for type parameters on methods. This is likely the most disruptive change because of the many different exporters and importers that exist, some of which are used for language tools, reside in different repositories, and all must remain in sync with the compiler's export format. Carefully staged updates are needed in order to not break existing infrastructure. We have done import/export data format changes in the past, and while tedious, they can be done.

### Libraries and tools

No change of the existing libraries is required, but future (versions of these) libraries may want to use generic methods.

The impact on tools may be significant as they will need to handle the new language feature. The [go/types](https://pkg.go.dev/go/types) API's [Signature type](https://pkg.go.dev/go/types#Signature) already provides accessors for receiver and ordinary type parameters, so it's possible that no API changes are needed there, but [go/types](https://pkg.go.dev/go/types) clients cannot rely on the (current) fact that only one of type parameter lists is present. Judging from past experience, it may take one or two release cycles for all tools to catch up. It is difficult to judge the amount work involved without actually doing the work.

## Other considerations

This proposal adds a new feature to the language by removing a restriction; thus it is fully backward-compatible with existing Go. Importantly, it also doesn't preclude the implementation of generic interface methods at some point, should we find an acceptable implementation solution that doesn't impose a cost when the feature is not used.

Generic functions and types make it possible to write more complex code and adding generic methods will further increase that capability. But used appropriately, generic methods can simplify and generalize situations where currently more complex work-arounds are needed (see [\#49085](https://github.com/golang/go/issues/49085) for an example).",['Proposal'],0
golang/go,encoding/json/v2: remove `unknown` tag option and DiscardUnknownMembers,"### Proposal Details

(This is a counter-proposal to #76444).

The `json/v2` package provides too much functionality with unknown fields:
* A Go struct field may be [marked as `unknown`](https://pkg.go.dev/encoding/json/v2#example-package-UnknownMembers) to indicate that it holds unknown members. Note that this is nearly identical to the [existing `inline` tag option](https://pkg.go.dev/encoding/json/v2#example-package-InlinedFields), but additionally indicates intent for holding unknown members.
* [DiscardUnknownMembers](https://pkg.go.dev/encoding/json/v2#DiscardUnknownMembers) discards unknown members when marshaling by ignoring any data in Go struct fields marked as `unknown`.
* [RejectUnknownMembers](https://pkg.go.dev/encoding/json/v2#RejectUnknownMembers) rejects unknown members when unmarshaling (even if a Go struct field is marked as `unknown`) by reporting an error.

I propose removing the `unknown` tag option and the `DiscardUnknownMembers` option. Note that `RejectUnknownMembers` still needs to exist to maintain backwards compatibility with the [`jsonv1.Decoder.DisallowUnknownFields`](https://pkg.go.dev/encoding/json#Decoder.DisallowUnknownFields) feature.

We could always re-introduce the concept of Go struct fields marked as `unknown` in a future release of `json/v2`, but then we would need to think about how all related options might interoperate (see #76444). Fortunately, adding new features is always backwards compatible, but removing or changing them is not.","['Proposal', 'LibraryProposal']",0
golang/go,x/tools/gopls/internal/test/integration/misc: TestRenamePackage_InternalPackage/default failures,"```
#!watchflakes
default <- pkg == ""golang.org/x/tools/gopls/internal/test/integration/misc"" && test == ""TestRenamePackage_InternalPackage/default""
```

Issue created automatically to collect these failures.

Example ([log](https://ci.chromium.org/b/8691941815062114753)):

    === RUN   TestRenamePackage_InternalPackage/default
    panic: test timed out after 20m0s
    	running tests:
    		TestRenamePackage_InternalPackage (18m12s)
    		TestRenamePackage_InternalPackage/default (18m12s)
    
    goroutine 174948 [running]:
    testing.(*M).startAlarm.func1()
    	/Users/swarming/.swarming/w/ir/x/w/goroot/src/testing/testing.go:2802 +0x494
    created by time.goFunc
    ...
    encoding/json.(*Decoder).readValue(0xc00cb07040)
    	/Users/swarming/.swarming/w/ir/x/w/goroot/src/encoding/json/stream.go:142 +0x104
    encoding/json.(*Decoder).Decode(0xc00cb07040, {0x1065b6800, 0xc00c6ed290})
    	/Users/swarming/.swarming/w/ir/x/w/goroot/src/encoding/json/stream.go:65 +0x88
    golang.org/x/tools/internal/jsonrpc2.(*rawStream).Read(0xc008013ec0, {0x10677f848, 0xc00c023b30})
    	/Users/swarming/.swarming/w/ir/x/w/targetrepo1648819025/internal/jsonrpc2/stream.go:61 +0xac
    golang.org/x/tools/internal/jsonrpc2.(*conn).run(0xc00c023bd0, {0x10677f848, 0xc00c023b30}, 0xc016d72018)
    	/Users/swarming/.swarming/w/ir/x/w/targetrepo1648819025/internal/jsonrpc2/conn.go:195 +0xcc
    created by golang.org/x/tools/internal/jsonrpc2.(*conn).Go in goroutine 170351
    	/Users/swarming/.swarming/w/ir/x/w/targetrepo1648819025/internal/jsonrpc2/conn.go:188 +0x104


â€” [watchflakes](https://go.dev/wiki/Watchflakes)
","['NeedsInvestigation', 'gopls', 'Tools']",0
golang/go,x/build: bot linux-ppc64-power10--03 reported as broken,"The bot [linux-ppc64-power10--03](https://chromium-swarm.appspot.com/bot?id=linux-ppc64-power10--03) has been reported as broken. It is currently in ""dead"" state. Please work to resolve the issue.

@golang/ppc64
<!-- DO NOT EDIT: linux-ppc64-power10--03 -->

â€” [watchflakes](https://go.dev/wiki/Watchflakes)
","['Builders', 'NeedsInvestigation']",0
golang/go,runtime/pprof: TestGoroutineSwitch failures,"```
#!watchflakes
default <- pkg == ""runtime/pprof"" && test == ""TestGoroutineSwitch""
```

Issue created automatically to collect these failures.

Example ([log](https://ci.chromium.org/b/8691941650151044161)):

    === RUN   TestGoroutineSwitch
    --- FAIL: TestGoroutineSwitch (0.62s)
    panic: runtime error: index out of range [0] with length 0 [recovered, repanicked]
    
    goroutine 42 gp=0x2a5ba870b4a0 m=5 mp=0x2a5ba8780008 [running]:
    panic({0x7ff66ad7fe40?, 0x2a5ba87e9038?})
    	C:/b/s/w/ir/x/w/goroot/src/runtime/panic.go:879 +0x16f fp=0x2a5ba8b03a58 sp=0x2a5ba8b039a8 pc=0x7ff66abf1bcf
    testing.tRunner.func1.2({0x7ff66ad7fe40, 0x2a5ba87e9038})
    	C:/b/s/w/ir/x/w/goroot/src/testing/testing.go:1974 +0x239 fp=0x2a5ba8b03b08 sp=0x2a5ba8b03a58 pc=0x7ff66ac7b2f9
    testing.tRunner.func1()
    ...
    runtime.goparkunlock(...)
    	C:/b/s/w/ir/x/w/goroot/src/runtime/proc.go:468
    runtime.(*cleanupQueue).dequeue(0x7ff66af57340)
    	C:/b/s/w/ir/x/w/goroot/src/runtime/mcleanup.go:522 +0xd4 fp=0x2a5ba871dfa0 sp=0x2a5ba871df68 pc=0x7ff66ab90cb4
    runtime.runCleanups()
    	C:/b/s/w/ir/x/w/goroot/src/runtime/mcleanup.go:718 +0x45 fp=0x2a5ba871dfe0 sp=0x2a5ba871dfa0 pc=0x7ff66ab912c5
    runtime.goexit({})
    	C:/b/s/w/ir/x/w/goroot/src/runtime/asm_amd64.s:1771 +0x1 fp=0x2a5ba871dfe8 sp=0x2a5ba871dfe0 pc=0x7ff66abf95c1
    created by runtime.(*cleanupQueue).createGs in goroutine 69
    	C:/b/s/w/ir/x/w/goroot/src/runtime/mcleanup.go:672 +0xa5


â€” [watchflakes](https://go.dev/wiki/Watchflakes)
","['NeedsInvestigation', 'compiler/runtime']",0
golang/go,all: update to Unicode 17,,['NeedsFix'],0
golang/go,proposal: slices: CompactFunc should follow order of elements when calling eq.,"### Proposal Details

The way the algorithm is [implemented](https://cs.opensource.google/go/go/+/refs/tags/go1.25.6:src/slices/slices.go;l=394), `eq` receives elements in reverse order: `eq(s[i+1], s[i])`.

One might argue the equality function needs to be commutative. However I think because of how the function is documented (`CompactFunc keeps the first one`) it is intuitive to assume that arguments of `eq` follow slice order. That is the first arg appears before the second.

I encountered this by misusing the function. I wanted to get rid of `nil` elements in my slice using `CompactFunc` with `func(a, b any) bool { return b == nil }` instead of `DeleteFunc`. In my mind I wanted elements in {""a"", nil, nil} to appear equal and expected CompactFunc to keep just ""a"".

https://go.dev/play/p/aor4APFO3xx","['Proposal', 'LibraryProposal']",0
golang/go,x/tools/gopls: vscode shift + F12 fails to go to function's reference when the function and its reference locates in the same package,"### Go version

go version go1.24.11 darwin/arm64

### Output of `go env` in your module/workspace:

```shell
GOOS='darwin'
GOPROXY='https://go-mod-proxy.byted.org,https://goproxy.cn,https://proxy.golang.org,direct'
GOROOT='/usr/local/go'
GOSUMDB='sum.golang.google.cn'
.....
```

### What did you do?

below is a simple code with a function named ""read"", and its reference locates in the same package(`main`)

```go
package main

// go run main/string_data_race_panic.go

import (
	""fmt""
	""reflect""
	""time""
	""unsafe""
)

func main() {
	str := ""init""
	go func() {
		for i := 1; i < 10000; i++ {
			read(str)
		}
	}()

	for { 
		str = """"
		time.Sleep(10 * time.Nanosecond)
		str = ""/test/test/test""
		time.Sleep(10 * time.Nanosecond)
	}

}

func read(c string) {
	s := (*reflect.StringHeader)(unsafe.Pointer(&c))
	fmt.Printf(""ptr: 0x%x, len: %d "", s.Data, s.Len)
	fmt.Printf(""fullPath: %s\n"", c)
	// _, _ = json.Marshal(c)
}
```

when opened in vscode, you cannot goto the reference of function `read`, with gopls(server) output like this:

```bash
[Error - 17:14:56] Request textDocument/references failed.
  Message: no identifier found
  Code: 0 
```

this error message also appears in these case:

```bash
[Error - 17:15:38] Request textDocument/prepareCallHierarchy failed.
  Message: identifier not found
  Code: 0 
[Error - 17:16:09] Request textDocument/prepareTypeHierarchy failed.
  Message: not a type name
  Code: 0 
```

also only enter ""F12"" used to trip around function definition and reference, now it is bad too.

I searched the issue list, found this issue #36917, just for your reference.

### What did you see happen?

this issue may be caused by gopls v0.21.0

I used 'Go: install/update tools' to update gopls before, and I ran into this problem.

I try to reinstall the old version of gopls, and reboot vscode, the problem is solved




### What did you expect to see?

go to reference works as expected!","['WaitingForInfo', 'gopls', 'Tools']",0
golang/go,cmd/internal/testdir:7_10: Test/fixedbugs/issue41239.go failures,"```
#!watchflakes
default <- pkg == ""cmd/internal/testdir:7_10"" && test == ""Test/fixedbugs/issue41239.go""
```

Issue created automatically to collect these failures.

Example ([log](https://ci.chromium.org/b/8692034295327872209)):

    === RUN   Test/fixedbugs/issue41239.go
    === PAUSE Test/fixedbugs/issue41239.go
    === CONT  Test/fixedbugs/issue41239.go
        testdir_test.go:147: exit status 2
            unexpected fault address 0x0
            fatal error: fault
            [signal SIGBUS: bus error code=0x80 addr=0x0 pc=0xff738]
            
            goroutine 1 gp=0xeae0f81c0 m=0 mp=0xf35be0 [running]:
            runtime.throw({0x9c0418, 0x5})
    ...
            os/signal.signal_recv()
            	runtime/sigqueue.go:152 +0x190 fp=0xeae1437c0 sp=0xeae1437a0 pc=0xe2ae0
            os/signal.loop()
            	os/signal/signal_unix.go:23 +0x38 fp=0xeae1437d8 sp=0xeae1437c0 pc=0x2ce218
            runtime.goexit({})
            	runtime/asm_mips64x.s:664 +0x4 fp=0xeae1437d8 sp=0xeae1437d8 pc=0xe871c
            created by os/signal.Notify.func1.1 in goroutine 1
            	os/signal/signal.go:152 +0x50
            
    --- FAIL: Test/fixedbugs/issue41239.go (1.53s)


â€” [watchflakes](https://go.dev/wiki/Watchflakes)
",['NeedsInvestigation'],0
golang/go,cmd/go: TestScript/list_symlink_internal failures,"```
#!watchflakes
default <- pkg == ""cmd/go"" && test == ""TestScript/list_symlink_internal""
```

Issue created automatically to collect these failures.

Example ([log](https://ci.chromium.org/b/8692010861853345553)):

    === RUN   TestScript/list_symlink_internal
    === PAUSE TestScript/list_symlink_internal
    === CONT  TestScript/list_symlink_internal
        script_test.go:139: 2026-01-22T02:53:37Z
        script_test.go:141: $WORK=/Users/swarming/.swarming/w/ir/x/t/cmd-go-test-2737589760/tmpdir2279468357/list_symlink_internal2963875805
        script_test.go:163: 
            PATH=/Users/swarming/.swarming/w/ir/x/t/cmd-go-test-2737589760/tmpdir2279468357/testbin:/Users/swarming/.swarming/w/ir/x/w/goroot/bin:/Users/swarming/.swarming/w/ir/x/w/goroot/bin:/Users/swarming/.swarming/w/ir/x/w/goroot/bin:/Users/swarming/.swarming/w/ir/cache/tools/bin:/Users/swarming/.swarming/w/ir/bbagent_utility_packages:/Users/swarming/.swarming/w/ir/bbagent_utility_packages/bin:/Users/swarming/.swarming/w/ir/cipd_bin_packages:/Users/swarming/.swarming/w/ir/cipd_bin_packages/bin:/Users/swarming/.swarming/w/ir/cache/cipd_client:/Users/swarming/.swarming/w/ir/cache/cipd_client/bin:/Users/swarming/.swarming/cipd_cache/bin:/usr/local/bin:/usr/bin:/bin:/usr/local/sbin:/usr/sbin:/sbin
            HOME=/no-home
            CCACHE_DISABLE=1
            GOARCH=arm64
    ...
            > go install &
            > wait
            [background] go run p.go
            [stderr]
            signal: segmentation fault
            [exit status 1]
            [background] go build
            [background] go install
        script_test.go:163: FAIL: testdata/script/list_symlink_internal.txt:18: wait: testdata/script/list_symlink_internal.txt:14: go run p.go: exit status 1
    --- FAIL: TestScript/list_symlink_internal (2.62s)


â€” [watchflakes](https://go.dev/wiki/Watchflakes)
",['NeedsInvestigation'],0
golang/go,cmd/link/internal/ld: TestRISCVTrampolines failures,"```
#!watchflakes
default <- pkg == ""cmd/link/internal/ld"" && test == ""TestRISCVTrampolines""
```

Issue created automatically to collect these failures.

Example ([log](https://ci.chromium.org/b/8692014705681569537)):

    === RUN   TestRISCVTrampolines
    === PAUSE TestRISCVTrampolines
    === CONT  TestRISCVTrampolines
        ld_test.go:436: nm failure: signal: segmentation fault
            
    --- FAIL: TestRISCVTrampolines (5.99s)


â€” [watchflakes](https://go.dev/wiki/Watchflakes)
",['NeedsInvestigation'],0
golang/go,cmd/internal/testdir:0_10: Test/chan/select5.go failures,"```
#!watchflakes
default <- pkg == ""cmd/internal/testdir:0_10"" && test == ""Test/chan/select5.go""
```

Issue created automatically to collect these failures.

Example ([log](https://ci.chromium.org/b/8692034294106027121)):

    === RUN   Test/chan/select5.go
    === PAUSE Test/chan/select5.go
    === CONT  Test/chan/select5.go


â€” [watchflakes](https://go.dev/wiki/Watchflakes)
",['NeedsInvestigation'],0
golang/go,cmd/go: TestScript/mod_symlink failures,"```
#!watchflakes
default <- pkg == ""cmd/go"" && test == ""TestScript/mod_symlink""
```

Issue created automatically to collect these failures.

Example ([log](https://ci.chromium.org/b/8692030005800735345)):

    === RUN   TestScript/mod_symlink
    === PAUSE TestScript/mod_symlink
    === CONT  TestScript/mod_symlink
        script_test.go:139: 2026-01-21T21:49:35Z
        script_test.go:141: $WORK=C:\b\s\w\ir\x\t\cmd-go-test-958185902\tmpdir546475801\mod_symlink2801091260
        script_test.go:163: 
            PATH=C:\b\s\w\ir\x\t\cmd-go-test-958185902\tmpdir546475801\testbin;C:\b\s\w\ir\x\w\goroot\bin;C:\b\s\w\ir\x\w\goroot\bin;C:\b\s\w\ir\x\w\goroot\bin;C:\b\s\w\ir\cache\tools\bin;C:\b\s\w\ir\bbagent_utility_packages;C:\b\s\w\ir\bbagent_utility_packages\bin;C:\b\s\w\ir\cipd_bin_packages;C:\b\s\w\ir\cipd_bin_packages\bin;C:\b\s\w\ir\cache\cipd_client;C:\b\s\w\ir\cache\cipd_client\bin;C:\b\s\cipd_cache\bin;C:\Program Files\OpenSSH\;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Windows\System32\OpenSSH\;C:\Program Files\Puppet Labs\Puppet\bin;C:\Users\chrome-bot\AppData\Local\Microsoft\WindowsApps;C:\b\s\w\ir\cache\tools\cc\windows\gcc32\bin
            USERPROFILE=/no-home
            CCACHE_DISABLE=1
            GOARCH=386
    ...
            > [!symlink] skip
            [condition not met]
            # 'go get' should resolve modules of imported packages. (1.894s)
            > go get
            [stderr]
            go: downloading golang.org/x/text v0.3.0
            go: added golang.org/x/text v0.3.0
            > go list -deps -f '{{.Module}}' .
        script_test.go:163: FAIL: testdata\script\mod_symlink.txt:6: go list -deps -f {{.Module}} .: exit status 0xffffffffc0000005
    --- FAIL: TestScript/mod_symlink (2.30s)


â€” [watchflakes](https://go.dev/wiki/Watchflakes)
",['NeedsInvestigation'],0
golang/go,cmd/compile: miscompile of global array initialization [1.25 backport],"@randall77 requested issue #77193 to be considered for backport to the next 1.25 minor release.

> @gopherbot please open backport issues.
> 
> This bug can cause random crashes after constructing deep-ish array composite literals. Maybe heap corruption also? Not sure.
","['CherryPickCandidate', 'compiler/runtime']",0
golang/go,cmd/compile: miscompile of global array initialization [1.26 backport],"@randall77 requested issue #77193 to be considered for backport to the next 1.26 minor release.

> @gopherbot please open backport issues.
> 
> This bug can cause random crashes after constructing deep-ish array composite literals. Maybe heap corruption also? Not sure.
","['CherryPickCandidate', 'compiler/runtime']",0
golang/go,x/tools/gopls: SEGV in dynamic CALL (invalid pointer in itab),"```
#!stacks
""sigpanic"" && ""cache.readFile:+20""
```
Issue created by [stacks](https://pkg.go.dev/golang.org/x/telemetry/cmd/stacks).

```go
func (fs *overlayFS) ReadFile(ctx context.Context, uri protocol.DocumentURI) (file.Handle, error) {
...
	return fs.delegate.ReadFile(ctx, uri)
}
```
```
0x1948b00     488b942438010000     MOVQ   0x138(SP), DX  // load fs.delegate
0x1948b08     488b02               MOVQ 0(DX), AX  // load fs.delegate.ReadFile
0x1948b0b     ffd0                 CALL AX      // SEGV                                                                                                        
```
This stack `H_zNVA` was [reported by telemetry](https://telemetry.go.dev/data/2026-01-16):

- `crash/crash`
- [`runtime.throw:+9,+0x47`](https://cs.opensource.google/go/go/+/go1.24.10:src/runtime/panic.go;l=1101)
- [`runtime.sigpanic:+33,+0x269`](https://cs.opensource.google/go/go/+/go1.24.10:src/runtime/signal_unix.go;l=939)
- [`golang.org/x/tools/gopls/internal/cache.readFile:+20,+0x3eb`](https://cs.opensource.google/go/x/tools/+/gopls/v0.20.0:gopls/internal/cache/fs_memoized.go;l=167)
- [`golang.org/x/tools/gopls/internal/cache.(*memoizedFS).ReadFile:+46,+0x1d6`](https://cs.opensource.google/go/x/tools/+/gopls/v0.20.0:gopls/internal/cache/fs_memoized.go;l=107)
- [`golang.org/x/tools/gopls/internal/cache.(*overlayFS).ReadFile:+7,+0xf2`](https://cs.opensource.google/go/x/tools/+/gopls/v0.20.0:gopls/internal/cache/fs_overlay.go;l=50)
- [`golang.org/x/tools/gopls/internal/cache.(*Snapshot).preloadFiles.func1:+2,+0xbd`](https://cs.opensource.google/go/x/tools/+/gopls/v0.20.0:gopls/internal/cache/snapshot.go;l=1120)
- [`golang.org/x/tools/gopls/internal/cache.(*Snapshot).preloadFiles.gowrap1:+9,+0x2b`](https://cs.opensource.google/go/x/tools/+/gopls/v0.20.0:gopls/internal/cache/snapshot.go;l=1127)
- `runtime.goexit:+0,+0x0`
```
golang.org/x/tools/gopls@v0.20.0 go1.24.10 darwin/amd64 gopls.client=vscode (1)
```

Use this command to reproduce the executable:
`(HOME=$(mktemp -d); GOOS=darwin GOARCH=amd64 GOTOOLCHAIN=go1.24.10 go install golang.org/x/tools/gopls@v0.20.0 && find $HOME/go/bin -type f)`
To disassemble: `go tool objdump exe | less`

","['NeedsInvestigation', 'gopls', 'Tools', 'gopls/telemetry-wins', 'BugReport', 'gopls/memory-corruption']",0
golang/go,runtime: segmentation violation while s2.Decode,"### Go version

go1.25.3-bookworm

### Output of `go env` in your module/workspace:

```shell
AR='ar'
CC='clang'
CGO_CFLAGS='-O2 -g'
CGO_CPPFLAGS=''
CGO_CXXFLAGS='-O2 -g'
CGO_ENABLED='1'
CGO_FFLAGS='-O2 -g'
CGO_LDFLAGS='-O2 -g'
CXX='clang++'
GCCGO='gccgo'
GO111MODULE='on'
GOARCH='arm64'
GOARM64='v8.0'
GOAUTH='netrc'
GOBIN=''
GOCACHE='/Users/<redacted>/Library/Caches/go-build'
GOCACHEPROG=''
GODEBUG=''
GOENV='/Users/<redacted>/Library/Application Support/go/env'
GOEXE=''
GOEXPERIMENT=''
GOFIPS140='off'
GOFLAGS=''
GOGCCFLAGS='-fPIC -arch arm64 -pthread -fno-caret-diagnostics -Qunused-arguments -fmessage-length=0 -ffile-prefix-map=/var/folders/5n/0lsy8hnj27z5hmyrm0kc2w7h0000gn/T/go-build1323865709=/tmp/go-build -gno-record-gcc-switches -fno-common'
GOHOSTARCH='arm64'
GOHOSTOS='darwin'
GOINSECURE=''
GOMOD='/Users/<redacted>/<redacted>/go.mod'
GOMODCACHE='/Users/<redacted>/pkg/mod'
GONOPROXY=''
GONOSUMDB=''
GOOS='darwin'
GOPATH='/Users/<redacted>'
GOPRIVATE=''
GOPROXY='https://proxy.golang.org,direct'
GOROOT='/Users/<redacted>/pkg/mod/golang.org/toolchain@v0.0.1-go1.25.3.darwin-arm64'
GOSUMDB='sum.golang.org'
GOTELEMETRY='local'
GOTELEMETRYDIR='/Users/<redacted>/Library/Application Support/go/telemetry'
GOTMPDIR=''
GOTOOLCHAIN='auto'
GOTOOLDIR='/Users/<redacted>@zomato.com/pkg/mod/golang.org/toolchain@v0.0.1-go1.25.3.darwin-arm64/pkg/tool/darwin_arm64'
GOVCS=''
GOVERSION='go1.25.3'
GOWORK='/Users/<redacted>/worker-controller/go.work'
PKG_CONFIG='pkg-config'
```

### What did you do?

Not able to reproduce the crash, as we are getting this intermittently in our production system, similarly raised to upstream sometime back: https://github.com/klauspost/compress/issues/1097
```
PC=0x47642a m=22 sigcode=1 addr=0x15ff

goroutine 0 gp=0xc0010001c0 m=22 mp=0xc000781008 [idle]:
runtime.fpTracebackPCs(...)
	/usr/local/go/src/runtime/tracestack.go:267
runtime.traceStack(0xc000781008?, 0x12000?, 0x2af2390)
	/usr/local/go/src/runtime/tracestack.go:124 +0x2ca fp=0xc0007cbed8 sp=0xc0007cba70 pc=0x47642a
runtime.traceLocker.stack(...)
	/usr/local/go/src/runtime/traceevent.go:59
runtime.traceLocker.GoStop({0xc000781008?, 0x453425?}, 0x2)
	/usr/local/go/src/runtime/traceruntime.go:460 +0x85 fp=0xc0007cbf50 sp=0xc0007cbed8 pc=0x4754e5
runtime.traceLocker.GoPreempt(...)
	/usr/local/go/src/runtime/traceruntime.go:455
runtime.goschedImpl(0xc393776540, 0x1?)
	/usr/local/go/src/runtime/proc.go:4295 +0x7b fp=0xc0007cbfa0 sp=0xc0007cbf50 pc=0x4534bb
runtime.gopreempt_m(0xc393776540?)
	/usr/local/go/src/runtime/proc.go:4331 +0x18 fp=0xc0007cbfc0 sp=0xc0007cbfa0 pc=0x453818
runtime.mcall()
	/usr/local/go/src/runtime/asm_amd64.s:458 +0x55 fp=0xc0007cbfd8 sp=0xc0007cbfc0 pc=0x4885f5

goroutine 43263995 gp=0xc393776540 m=22 mp=0xc000781008 [running]:
runtime.asyncPreempt2()
	/usr/local/go/src/runtime/preempt.go:308 +0x39 fp=0xc004813100 sp=0xc0048130e0 pc=0x449e39
runtime.asyncPreempt()
	/usr/local/go/src/runtime/preempt_amd64.s:46 +0xdb fp=0xc004813288 sp=0xc004813100 pc=0x48ba5b
github.com/klauspost/compress/s2.Decode({0xc015d50000, 0xc004813348?, 0x2000}, {0xc3507f9010, 0x19e7, 0x19e7})
	/go/pkg/mod/github.com/klauspost/compress@v1.18.0/s2/decode.go:72 +0x108 fp=0xc0048132f0 sp=0xc004813288 pc=0xb2d6e8
github.com/twmb/franz-go/pkg/kgo.(*decompressor).Decompress(0xc004b9b290, {0xc3507f9010, 0x19e7, 0x19e7}, 0x2)
	/go/pkg/mod/github.com/twmb/franz-go@v1.19.1/pkg/kgo/compression.go:395 +0x487 fp=0xc004813548 sp=0xc0048132f0 pc=0xbbe4c7
github.com/twmb/franz-go/pkg/kgo.(*ProcessFetchPartitionOpts).processRecordBatch(0xc004813ae8, 0xc004813880, 0xc3638c04e0, 0x0, {0x1e05920?, 0xc004b9b290?})
	/go/pkg/mod/github.com/twmb/franz-go@v1.19.1/pkg/kgo/source.go:1629 +0xda fp=0xc004813778 sp=0xc004813548 pc=0xc2fa7a
github.com/twmb/franz-go/pkg/kgo.ProcessFetchPartition({0x0, 0x0, 0x12aa0495, {0x0}, {0xc0011246c0, 0x1c}, 0x3e, {0x0, 0x0, 0x0}}, ...)
	/go/pkg/mod/github.com/twmb/franz-go@v1.19.1/pkg/kgo/source.go:1518 +0x7d3 fp=0xc004813ae8 sp=0xc004813778 pc=0xc2f2b3
github.com/twmb/franz-go/pkg/kgo.(*cursorOffsetNext).processRespPartition(0xc363158f80, 0x0?, 0xc362dbfc80?, {0x1e05920?, 0xc004b9b290?}, {0xc0017b2730?, 0x414e00?, 0xc047a6c1c0?})
	/go/pkg/mod/github.com/twmb/franz-go@v1.19.1/pkg/kgo/source.go:1331 +0x1df fp=0xc004813c28 sp=0xc004813ae8 pc=0xc2e6bf
github.com/twmb/franz-go/pkg/kgo.(*source).handleReqResp(0xc00069f180, 0xc00244db88, 0xc3636a63c0, 0xc363177090)
	/go/pkg/mod/github.com/twmb/franz-go@v1.19.1/pkg/kgo/source.go:1115 +0x855 fp=0xc004813f50 sp=0xc004813c28 pc=0xc2ce95
github.com/twmb/franz-go/pkg/kgo.(*source).fetch.func5()
	/go/pkg/mod/github.com/twmb/franz-go@v1.19.1/pkg/kgo/source.go:902 +0x89 fp=0xc004813fe0 sp=0xc004813f50 pc=0xc2bec9
runtime.goexit({})
	/usr/local/go/src/runtime/asm_amd64.s:1693 +0x1 fp=0xc004813fe8 sp=0xc004813fe0 pc=0x48a4a1
created by github.com/twmb/franz-go/pkg/kgo.(*source).fetch in goroutine 40547232
	/go/pkg/mod/github.com/twmb/franz-go@v1.19.1/pkg/kgo/source.go:900 +0x57a

goroutine 1 gp=0xc000002380 m=nil [chan receive, 9 minutes]:
runtime.gopark(0x5acf451497?, 0x4000000000070?, 0x78?, 0x12?, 0x7f2007b56a78?)
	/usr/local/go/src/runtime/proc.go:460 +0xce fp=0xc04193ecf8 sp=0xc04193ecd8 pc=0x481a2e
runtime.chanrecv(0xc001106cb0, 0x0, 0x1)
	/usr/local/go/src/runtime/chan.go:667 +0x473 fp=0xc04193ed70 sp=0xc04193ecf8 pc=0x415373
runtime.chanrecv1(0xc001396280?, 0x0?)
	/usr/local/go/src/runtime/chan.go:509 +0x12 fp=0xc04193ed98 sp=0xc04193ed70 pc=0x414ed2
github.com/Zomato/worker-controller/pkg/node.(*Logical).Start(0xc001396280, 0xc05da046f0)
	/go/src/pkg/node/node.go:198 +0x25b fp=0xc04193ee48 sp=0xc04193ed98 pc=0x1245e5b
github.com/Zomato/worker-controller/pkg/node.(*Logical).Run(...)
	/go/src/pkg/node/node.go:165
main.main()
	/go/src/main.go:89 +0x4b9 fp=0xc04193ef50 sp=0xc04193ee48 pc=0x16147b9
runtime.main()
	/usr/local/go/src/runtime/proc.go:285 +0x29d fp=0xc04193efe0 sp=0xc04193ef50 pc=0x44b29d
runtime.goexit({})
	/usr/local/go/src/runtime/asm_amd64.s:1693 +0x1 fp=0xc04193efe8 sp=0xc04193efe0 pc=0x48a4a1

goroutine 2 gp=0xc0000028c0 m=nil [force gc (idle), 9 minutes]:
runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)
	/usr/local/go/src/runtime/proc.go:460 +0xce fp=0xc0000d6fa8 sp=0xc0000d6f88 pc=0x481a2e
runtime.goparkunlock(...)
	/usr/local/go/src/runtime/proc.go:466
runtime.forcegchelper()
	/usr/local/go/src/runtime/proc.go:373 +0xb3 fp=0xc0000d6fe0 sp=0xc0000d6fa8 pc=0x44b5d3
runtime.goexit({})
	/usr/local/go/src/runtime/asm_amd64.s:1693 +0x1 fp=0xc0000d6fe8 sp=0xc0000d6fe0 pc=0x48a4a1
created by runtime.init.7 in goroutine 1
	/usr/local/go/src/runtime/proc.go:361 +0x1a

goroutine 3 gp=0xc000002e00 m=nil [GC sweep wait]:
runtime.gopark(0x1?, 0x0?, 0x0?, 0x0?, 0x0?)
	/usr/local/go/src/runtime/proc.go:460 +0xce fp=0xc0000d7780 sp=0xc0000d7760 pc=0x481a2e
runtime.goparkunlock(...)
	/usr/local/go/src/runtime/proc.go:466
runtime.bgsweep(0xc0000f8000)
	/usr/local/go/src/runtime/mgcsweep.go:323 +0xdf fp=0xc0000d77c8 sp=0xc0000d7780 pc=0x43279f
runtime.gcenable.gowrap1()
	/usr/local/go/src/runtime/mgc.go:212 +0x25 fp=0xc0000d77e0 sp=0xc0000d77c8 pc=0x426625
runtime.goexit({})
	/usr/local/go/src/runtime/asm_amd64.s:1693 +0x1 fp=0xc0000d77e8 sp=0xc0000d77e0 pc=0x48a4a1
created by runtime.gcenable in goroutine 1
	/usr/local/go/src/runtime/mgc.go:212 +0x66

goroutine 4 gp=0xc000002fc0 m=nil [GC scavenge wait]:
runtime.gopark(0x5c6781?, 0x54ded9?, 0x0?, 0x0?, 0x0?)
	/usr/local/go/src/runtime/proc.go:460 +0xce fp=0xc0000d7f78 sp=0xc0000d7f58 pc=0x481a2e
runtime.goparkunlock(...)
	/usr/local/go/src/runtime/proc.go:466
runtime.(*scavengerState).park(0x2af0a40)
	/usr/local/go/src/runtime/mgcscavenge.go:425 +0x49 fp=0xc0000d7fa8 sp=0xc0000d7f78 pc=0x430209
runtime.bgscavenge(0xc0000f8000)
	/usr/local/go/src/runtime/mgcscavenge.go:658 +0x59 fp=0xc0000d7fc8 sp=0xc0000d7fa8 pc=0x4307b9
runtime.gcenable.gowrap2()
	/usr/local/go/src/runtime/mgc.go:213 +0x25 fp=0xc0000d7fe0 sp=0xc0000d7fc8 pc=0x4265c5
runtime.goexit({})
```

### What did you see happen?

Our container crashed with the above traceback

### What did you expect to see?

Containers should have ran normally","['NeedsInvestigation', 'compiler/runtime', 'BugReport']",0
golang/go,cmd/compile: fatal error: all goroutines are asleep - deadlock!,"### Go version

go1.25.5 windows/amd64

### Output of `go env` in your module/workspace:

```shell
set AR=ar
set CC=gcc
set CGO_CFLAGS=-O2 -g
set CGO_CPPFLAGS=
set CGO_CXXFLAGS=-O2 -g
set CGO_ENABLED=1
set CGO_FFLAGS=-O2 -g
set CGO_LDFLAGS=-O2 -g
set CXX=g++
set GCCGO=gccgo
set GO111MODULE=
set GOAMD64=v1
set GOARCH=amd64
set GOAUTH=netrc
set GOBIN=
set GOCACHE=C:\PORTABLE\DEVELOP\Go\BugProject\_internal\_z\u\AppData\Local\go-build
set GOCACHEPROG=
set GODEBUG=
set GOENV=C:\PORTABLE\DEVELOP\Go\BugProject\_internal\_z\u\AppData\Roaming\go\env
set GOEXE=.exe
set GOEXPERIMENT=
set GOFIPS140=off
set GOFLAGS=
set GOGCCFLAGS=-m64 -mthreads -Wl,--no-gc-sections -fmessage-length=0 -ffile-prefix-map=C:\PORTABLE\DEVELOP\Go\BugProject\_internal\_z\t\go-build4189164591=/tmp/go-build -gno-record-gcc-switches
set GOHOSTARCH=amd64
set GOHOSTOS=windows
set GOINSECURE=
set GOMOD=C:\PORTABLE\DEVELOP\Go\BugProject\_internal\app\go.mod
set GOMODCACHE=C:\PORTABLE\DEVELOP\Go\BugProject\_internal\_z\u\go\pkg\mod
set GONOPROXY=
set GONOSUMDB=
set GOOS=windows
set GOPATH=C:\PORTABLE\DEVELOP\Go\BugProject\_internal\_z\u\go
set GOPRIVATE=
set GOPROXY=https://proxy.golang.org,direct
set GOROOT=C:\PORTABLE\DEVELOP\Go\BugProject\_internal\GO
set GOSUMDB=sum.golang.org
set GOTELEMETRY=local
set GOTELEMETRYDIR=C:\PORTABLE\DEVELOP\Go\BugProject\_internal\_z\u\AppData\Roaming\go\telemetry
set GOTMPDIR=
set GOTOOLCHAIN=auto
set GOTOOLDIR=C:\PORTABLE\DEVELOP\Go\BugProject\_internal\GO\pkg\tool\windows_amd64
set GOVCS=
set GOVERSION=go1.25.5
set GOWORK=
set PKG_CONFIG=pkg-config
```

### What did you do?

quick description:

```go
package foo

type PBar[T comparable] = *bar[T]

type bar[T comparable] struct{}

func NewBar() PBar[int] { return &bar[int]{} }

// |func (s *bar[T]) DoSomething(other *bar[T]) { //<- compiles OK
func (s *bar[T]) DoSomething(other PBar[T]) {
}
```

Repro:

unzip [app.zip](https://github.com/user-attachments/files/24764283/app.zip)

```
cd app
app>go build
```


### What did you see happen?

```
# app
fatal error: all goroutines are asleep - deadlock!

goroutine 1 [sync.Mutex.Lock]:
internal/sync.runtime_SemacquireMutex(0x7ff79fa18e59?, 0x0?, 0xc000152be8?)
        runtime/sema.go:95 +0x25
internal/sync.(*Mutex).lockSlow(0xc000135c28)
        internal/sync/mutex.go:149 +0x15d
internal/sync.(*Mutex).Lock(...)
        internal/sync/mutex.go:70
sync.(*Mutex).Lock(...)
        sync/mutex.go:46
cmd/compile/internal/types2.(*Named).resolve(0xc000135c00)
        cmd/compile/internal/types2/named.go:176 +0x5f
cmd/compile/internal/types2.(*Named).TypeParams(...)
        cmd/compile/internal/types2/named.go:322
cmd/compile/internal/types2.(*subster).typ(0xc0001532c0, {0x7ff7a06c6e68, 0xc000135c70})
        cmd/compile/internal/types2/subst.go:245 +0x44b
cmd/compile/internal/types2.(*subster).typ(0xc0001532c0, {0x7ff7a06c6e18, 0xc0000333f0})
        cmd/compile/internal/types2/subst.go:140 +0x17d
cmd/compile/internal/types2.(*Checker).subst(0x0, {0x0?, 0x0?, 0x0?}, {0x7ff7a06c6e18, 0xc0000333f0}, 0xc0003792f0, 0x0, 0xc000065980)
        cmd/compile/internal/types2/subst.go:78 +0x1ac
cmd/compile/internal/types2.(*Checker).newAliasInstance(0x0, {0x0?, 0x160839?, 0xc0?}, 0xc0003feb80, {0xc000033430, 0x1, 0x1}, 0x0, 0xc000065980)
        cmd/compile/internal/types2/alias.go:165 +0x1c5
cmd/compile/internal/types2.(*Checker).instance(0x0, {0x0, 0x0, 0x0}, {0x7ff7a06c8620, 0xc0003feb80}, {0xc000033430, 0x1, 0x1}, 0x0, ...)
        cmd/compile/internal/types2/instantiate.go:150 +0x765
cmd/compile/internal/types2.Instantiate(0xc000065980, {0x7ff7a06c6e90, 0xc0003feb80}, {0xc000033430, 0x1, 0x1}, 0x0)
        cmd/compile/internal/types2/instantiate.go:70 +0x326
cmd/compile/internal/importer.(*reader).doTyp(0xc000153740)
        cmd/compile/internal/importer/ureader.go:258 +0xcc
cmd/compile/internal/importer.(*pkgReader).typIdx(...)
        cmd/compile/internal/importer/ureader.go:231
cmd/compile/internal/importer.(*reader).typ(0xc00040e240)
        cmd/compile/internal/importer/ureader.go:201 +0x1b9
cmd/compile/internal/importer.(*reader).param(0xc00040e240)
        cmd/compile/internal/importer/ureader.go:365 +0x65
cmd/compile/internal/importer.(*reader).params(0xc00040e240)
        cmd/compile/internal/importer/ureader.go:355 +0x74
cmd/compile/internal/importer.(*reader).signature(0xc00040e240, 0xc000135d50, {0xc000060730, 0x1, 0x1}, {0x0, 0x0, 0x0})
        cmd/compile/internal/importer/ureader.go:344 +0x72
cmd/compile/internal/importer.(*reader).method(0xc00040e240)
        cmd/compile/internal/importer/ureader.go:539 +0xbe
cmd/compile/internal/importer.ReadPackage.(*pkgReader).objIdx.func1.1(0xc000153a00?)
        cmd/compile/internal/importer/ureader.go:454 +0xa5
cmd/compile/internal/types2.(*Named).resolve(0xc000135c00)
        cmd/compile/internal/types2/named.go:215 +0xf7
cmd/compile/internal/types2.(*Named).TypeParams(...)
        cmd/compile/internal/types2/named.go:322
cmd/compile/internal/types2.(*subster).typ(0xc0001540e0, {0x7ff7a06c6e68, 0xc000135c70})
        cmd/compile/internal/types2/subst.go:245 +0x44b
cmd/compile/internal/types2.(*subster).typ(0xc0001540e0, {0x7ff7a06c6e18, 0xc0000333f0})
        cmd/compile/internal/types2/subst.go:140 +0x17d
cmd/compile/internal/types2.(*Checker).subst(0xc00037a1e0, {0xc000378990?, 0x0?, 0x0?}, {0x7ff7a06c6e18, 0xc0000333f0}, 0xc000379110, 0x0, 0xc000065980)
        cmd/compile/internal/types2/subst.go:78 +0x1ac
cmd/compile/internal/types2.(*Checker).newAliasInstance(0xc00037a1e0, {0xc000378990?, 0x160839?, 0xc0?}, 0xc0003feb80, {0xc000033400, 0x1, 0x1}, 0x0, 0xc000065980)
        cmd/compile/internal/types2/alias.go:165 +0x1c5
cmd/compile/internal/types2.(*Checker).instance(0xc00037a1e0, {0xc000378990, 0x8, 0xb}, {0x7ff7a06c8620, 0xc0003feb80}, {0xc000033400, 0x1, 0x1}, 0x0, ...)
        cmd/compile/internal/types2/instantiate.go:150 +0x765
cmd/compile/internal/types2.(*Checker).instantiatedType(0xc00037a1e0, {0x7ff7a06cb018, 0xc0003fdc20}, {0xc0000333c0, 0x1, 0x1}, 0x7ff7a052f72d?)
        cmd/compile/internal/types2/typexpr.go:474 +0x4ad
cmd/compile/internal/types2.(*Checker).typInternal(0xc00037a1e0, {0x7ff7a06cafd0, 0xc0003f7d40}, 0x0)
        cmd/compile/internal/types2/typexpr.go:294 +0x9bf
cmd/compile/internal/types2.(*Checker).definedType(0xc00037a1e0, {0x7ff7a06cafd0, 0xc0003f7d40}, 0xc000154810?)
        cmd/compile/internal/types2/typexpr.go:195 +0x2f
cmd/compile/internal/types2.(*Checker).varType(0xc00037a1e0, {0x7ff7a06cafd0, 0xc0003f7d40})
        cmd/compile/internal/types2/typexpr.go:159 +0x25
cmd/compile/internal/types2.(*Checker).structType(0xc00037a1e0, 0xc000378f00, 0xc000135650)
        cmd/compile/internal/types2/struct.go:116 +0x22c
cmd/compile/internal/types2.(*Checker).typInternal(0xc00037a1e0, {0x7ff7a06cb450, 0xc000135650}, 0xc00040e060)
        cmd/compile/internal/types2/typexpr.go:330 +0x367
cmd/compile/internal/types2.(*Checker).definedType(0xc00037a1e0, {0x7ff7a06cb450, 0xc000135650}, 0x24a93590108?)
        cmd/compile/internal/types2/typexpr.go:195 +0x2f
cmd/compile/internal/types2.(*Checker).typeDecl(0xc00037a1e0, 0xc00040e060, 0xc0003f7ce0, 0x0)
        cmd/compile/internal/types2/decl.go:564 +0x50c
cmd/compile/internal/types2.(*Checker).objDecl(0xc00037a1e0, {0x7ff7a06d0300, 0xc00040e060}, 0x0)
        cmd/compile/internal/types2/decl.go:200 +0x973
cmd/compile/internal/types2.(*Checker).packageObjects(0xc00037a1e0)
        cmd/compile/internal/types2/resolver.go:681 +0x4af
cmd/compile/internal/types2.(*Checker).checkFiles(0xc00037a1e0, {0xc0001552e0?, 0x7ff79fa2cfe5?, 0x7ff7a0d63200?})
        cmd/compile/internal/types2/check.go:497 +0x285
cmd/compile/internal/types2.(*Checker).Files(0xc00000a1c8?, {0xc0001552e0?, 0x0?, 0x0?})
        cmd/compile/internal/types2/check.go:460 +0x75
cmd/compile/internal/types2.(*Config).Check(0xc000135810, {0xc00000a1c8?, 0xc00000a360?}, {0xc0001552e0, 0x1, 0x1}, 0xc0003f7e00)
        cmd/compile/internal/types2/api.go:488 +0x19d
cmd/compile/internal/noder.checkFiles({0x0, {0x0, 0x0}}, {0xc0000606a0, 0x1, 0x7ff7a06d8280?})
        cmd/compile/internal/noder/irgen.go:95 +0x60a
cmd/compile/internal/noder.writePkgStub({0x0?, {0x0?, 0x0?}}, {0xc0000606a0, 0x1, 0x1})
        cmd/compile/internal/noder/unified.go:319 +0x6a
cmd/compile/internal/noder.unified({0x0?, {0x0?, 0x0?}}, {0xc0000606a0?, 0x7ff7a044fc40?, 0xc000155928?})
        cmd/compile/internal/noder/unified.go:195 +0xb6
cmd/compile/internal/noder.LoadPackage({0xc000116120, 0x1, 0xe})
        cmd/compile/internal/noder/noder.go:77 +0x450
cmd/compile/internal/gc.Main(0x7ff7a0576dd8)
        cmd/compile/internal/gc/main.go:208 +0xcc5
main.main()
        cmd/compile/main.go:57 +0xf9
```

### What did you expect to see?

Expected no errors.","['NeedsInvestigation', 'compiler/runtime', 'BugReport']",0
golang/go,cmd/compile: go1.22+ cmd with go.mod 1.21 generates per-loop variable when using line directive,"### Go version

go version go1.25.1 darwin/arm64

### Output of `go env` in your module/workspace:

```shell
irrelevant
```

### What did you do?

```
-- go.mod --
module test

go 1.21
-- main.go --
//line main.go:1
package main

import ""fmt""

func main() {
	var fns []func()
	for i := 0; i < 3; i++ {
		fns = append(fns, func() { fmt.Println(""loop"", i) })
	}
	for _, fn := range fns {
		fn()
	}
}
```

```
go build .
./test
```

### What did you see happen?

The output is:
```
loop 0
loop 1
loop 2
```


### What did you expect to see?

It should output `loop 3` 3 times, as the same behavior as the code without `//line main.go:1`.

```
loop 3
loop 3
loop 3
```

### Background

The background is we have some test framework internally like [goc](https://github.com/qiniu/goc) which will insert some code lines and directives(like go line directive) before go build and use it in test env.

In test env everything goes well because of this flaw, but when released the service to live without the test coverage framework(without go line directive), the code behaves different and it's not expected.

I believe we can fix it to make the behavior consistent regardless line directive exists or not. Also it will be great if we can backport it.","['compiler/runtime', 'BugReport']",0
golang/go,"cmd/link: DWARF5 output causes ~60,000 readelf warnings - incorrect DW_FORM_addrx offset calculation","### Go version

go version go1.25.6 linux/amd64

### Output of `go env` in your module/workspace:

```shell
AR='ar'
CC='gcc'
CGO_CFLAGS='-O2 -g'
CGO_CPPFLAGS=''
CGO_CXXFLAGS='-O2 -g'
CGO_ENABLED='1'
CGO_FFLAGS='-O2 -g'
CGO_LDFLAGS='-O2 -g'
CXX='g++'
GCCGO='gccgo'
GO111MODULE=''
GOAMD64='v1'
GOARCH='amd64'
GOAUTH='netrc'
GOBIN=''
GOCACHE='/home/xxx/.cache/go-build'
GOCACHEPROG=''
GODEBUG=''
GOENV='/home/xxx/.config/go/env'
GOEXE=''
GOEXPERIMENT=''
GOFIPS140='off'
GOFLAGS=''
GOGCCFLAGS='-fPIC -m64 -pthread -Wl,--no-gc-sections -fmessage-length=0 -ffile-prefix-map=/tmp/go-build3231229538=/tmp/go-build -gno-record-gcc-switches'
GOHOSTARCH='amd64'
GOHOSTOS='linux'
GOINSECURE=''
GOMOD='/home/xxx/Documents/optrollups/go-ethereum/go.mod'
GOMODCACHE='/home/xxx/go/pkg/mod'
GONOPROXY=''
GONOSUMDB=''
GOOS='linux'
GOPATH='/home/xxx/go'
GOPRIVATE=''
GOPROXY='https://proxy.golang.org,direct'
GOROOT='/usr/local/go'
GOSUMDB='sum.golang.org'
GOTELEMETRY='local'
GOTELEMETRYDIR='/home/xxx/.config/go/telemetry'
GOTMPDIR=''
GOTOOLCHAIN='auto'
GOTOOLDIR='/usr/local/go/pkg/tool/linux_amd64'
GOVCS=''
GOVERSION='go1.25.6'
GOWORK=''
PKG_CONFIG='pkg-config'
```

### What did you do?

Built a large Go project (go-ethereum) with Go 1.25.6 and analyzed the DWARF debug information with GNU readelf:
```
git clone https://github.com/ethereum/go-ethereum
cd go-ethereum
# remove the flags '--strip-all' and '-trimpath' from the /build/ci.go file, add the flag '-gcflags all=-N -l'
make geth
readelf --debug-dump=info ./build/bin/geth 2>&1 | grep -c ""readelf: Warning""
# Output: 60877
```


### What did you see happen?

Building with Go 1.25.6 (DWARF5 enabled by default) produces 60,877 readelf warnings:
```
readelf: Warning: Offset into section .debug_addr too big: 0x47002e00
readelf: Warning: Offset into section .debug_addr too big: 0x47002e08
readelf: Warning: Offset into section .debug_addr too big: 0x47002e10
...
```
Example of failing function DIE:
```
<57>   DW_AT_addr_base   : 0x8
 <1><5b>: Abbrev Number: 3 (DW_TAG_subprogram)
    <5c>   DW_AT_name        : crypto/internal/fips140/ecdh.ecdh[go.shape.*crypto/internal/fips140/nistec.P256Point]
    <b2>   DW_AT_low_pc      :readelf: Warning: Offset into section .debug_addr too big: 0x47002e00
```
The compilation unit has DW_AT_addr_base: 0x8, which is correct. However, readelf computes offset 0x47002e00 instead of the expected small offset.
If we reverse the calculation:
- readelf uses: offset = addr_base + (index Ã— 8)
- 0x47002e00 = addr_base + (index Ã— 8)
- This implies readelf is using addr_base â‰ˆ 0x47002e00 instead of 0x8

The value 0x47 is ASCII 'G', suggesting readelf may be misreading unrelated data (possibly from strings like ""github.com/..."") as the addr_base.

Tools versions:
```
$ readelf --version
GNU readelf (GNU Binutils for Ubuntu) 2.42
$ readelf --debug-dump=info ./build/bin/geth 2>/dev/null | grep ""Version:"" | head -3
   Version:       5
   Version:       5
   Version:       5
```

### What did you expect to see?

I expected readelf to parse the DWARF5 debug information without warnings and resolve function addresses correctly using the DW_AT_addr_base attribute.
Expected:
- DW_AT_low_pc should resolve to actual function addresses
- No ""Offset into section .debug_addr too big"" warnings
- DWARF consumers (debuggers, binary analysis tools) should work correctly

This breaks DWARF-based debugging and analysis tools including:
- GDB debugger
- Binary analysis frameworks (angr, Ghidra DWARF import)
- Security research tools for vulnerability analysis
- Any tool using libdwarf/libdw to parse Go binaries

Possible causes:
- The DWARF5 .debug_addr section format may not match what binutils expects
- The relationship between compilation unit boundaries and DW_AT_addr_base scope may be malformed
- There may be an issue with how DW_FORM_addrx indices are encoded or how they relate to the addr_base

Investigation notes:
- The .debug_addr section exists and has a valid DWARF5 header
- DW_AT_addr_base values are present in compilation units and appear reasonable (0x8, 0x130, etc.)
- The indices stored in DW_FORM_addrx attributes are small valid numbers
- But readelf computes wildly incorrect offsets (0x47002e00, 0x47002e08, etc.)
- This was tested with binutils 2.42 (Ubuntu current) - latest available is 2.45","['NeedsInvestigation', 'compiler/runtime', 'BugReport']",0
golang/go,cmd/compile: function type inference does not work in all assignment contexts,"The [Go 1.21 release](https://go.dev/doc/go1.21) introduced significantly improved type inference. Specifically:

> A (possibly partially instantiated generic) function may now be called with arguments that are themselves (possibly partially instantiated) generic functions. The compiler will attempt to infer the missing type arguments of the callee (as before) and, for each argument that is a generic function that is not fully instantiated, its missing type arguments (new). [ ... ] More generally, a generic function may now be used without explicit instantiation when it is assigned to a variable or returned as a result value if the type arguments can be inferred from the assignment.

As stated, this type inference works in assignments, but it doesn't work for all assignments. For instance, given ([playground](https://go.dev/play/p/DVFf0Hb3k__V)):

```go
type S struct{ f func(int) }

func g[T any](T) {}

func _(s S) {
	s.f = g          // ok
	s = S{f: g}      // error: cannot use generic function g without instantiation
	s = S{f: g[int]} // ok
}
```

The assignment of the generic function `g` to the function-typed field `f` of struct `S` works as described but the same (implicit) assignment to field `f` in the struct literal does not - explicit instantiation of `g` is required.

The same is true for assignments to array, slice, or map elements vs the same (implicit) assignment in the respective composite literals, and sends to channels ([playground](https://go.dev/play/p/7XhFxgJJRma)):

```go
type F func(int)
type A [10]F
type S []F
type M map[string]F
type C chan F

func g[T any](T) {}

func _() {
	var a A
	a[0] = g      // ok
	a = A{g}      // error: cannot use generic function g without instantiation
	a = A{g[int]} // ok

	var s S
	s[0] = g      // ok
	s = S{g}      // error: cannot use generic function g without instantiation
	s = S{g[int]} // ok

	var m M
	m[""foo""] = g         // ok
	m = M{""foo"": g}      // error: cannot use generic function g without instantiation
	m = M{""foo"": g[int]} // ok

	var c C
	c <- g      // error: cannot use generic function g without instantiation
	c <- g[int] // ok
}
```

In all these cases we have an assignment where the type on the LHS is completely known and thus the function type parameter can be inferred unambiguously, and should be inferred for consistency.

While making this change likely requires some clarifications in the spec, I believe this is arguably a bug rather than a language change because the assignments (`a[0] = g`) work, but the implicit assignments in composite literals don't (`a = A{g}`). The exception is channel sends, but even in that case the element to be sent must be [assignable](https://golang.org/ref/spec#Assignability) to the channel element type and thus establishes a clear ""target"" type.
","['compiler/runtime', 'BugReport']",0
golang/go,x/tools/go/packages: TestAbsoluteFilenames/Modules failures,"```
#!watchflakes
default <- pkg == ""golang.org/x/tools/go/packages"" && test == ""TestAbsoluteFilenames/Modules""
```

Issue created automatically to collect these failures.

Example ([log](https://ci.chromium.org/b/8692145083614651185)):

    === RUN   TestAbsoluteFilenames/Modules
    === PAUSE TestAbsoluteFilenames/Modules
    === CONT  TestAbsoluteFilenames/Modules
        invoke.go:205: starting GOROOT= GOPATH=/Users/swarming/.swarming/w/ir/x/t/TestAbsoluteFilenames_Modules846104081/modcache GO111MODULE=off GOPROXY=file:///Users/swarming/.swarming/w/ir/x/t/TestAbsoluteFilenames_Modules846104081/modproxy PWD=/Users/swarming/.swarming/w/ir/x/t/TestAbsoluteFilenames_Modules846104081/fake go list -e -f {{context.ReleaseTags}} -- unsafe
        invoke.go:205: 17.82675ms for GOROOT= GOPATH=/Users/swarming/.swarming/w/ir/x/t/TestAbsoluteFilenames_Modules846104081/modcache GO111MODULE=off GOPROXY=file:///Users/swarming/.swarming/w/ir/x/t/TestAbsoluteFilenames_Modules846104081/modproxy PWD=/Users/swarming/.swarming/w/ir/x/t/TestAbsoluteFilenames_Modules846104081/fake go list -e -f {{context.ReleaseTags}} -- unsafe
        invoke.go:205: starting GOROOT= GOPATH=/Users/swarming/.swarming/w/ir/x/t/TestAbsoluteFilenames_Modules846104081/modcache GO111MODULE=on GOPROXY=file:///Users/swarming/.swarming/w/ir/x/t/TestAbsoluteFilenames_Modules846104081/modproxy PWD=/Users/swarming/.swarming/w/ir/x/t/TestAbsoluteFilenames_Modules846104081/fake go list -e -json=Name,ImportPath,Error,Dir,GoFiles,IgnoredGoFiles,IgnoredOtherFiles,CFiles,CgoFiles,CXXFiles,MFiles,HFiles,FFiles,SFiles,SwigFiles,SwigCXXFiles,SysoFiles,TestGoFiles,XTestGoFiles,CompiledGoFiles,Export,EmbedFiles -compiled=true -test=true -export=false -deps=false -find=false -pgo=off -- golang.org/fake/a
        invoke.go:205: 18.612083ms for GOROOT= GOPATH=/Users/swarming/.swarming/w/ir/x/t/TestAbsoluteFilenames_Modules846104081/modcache GO111MODULE=on GOPROXY=file:///Users/swarming/.swarming/w/ir/x/t/TestAbsoluteFilenames_Modules846104081/modproxy PWD=/Users/swarming/.swarming/w/ir/x/t/TestAbsoluteFilenames_Modules846104081/fake go list -e -json=Name,ImportPath,Error,Dir,GoFiles,IgnoredGoFiles,IgnoredOtherFiles,CFiles,CgoFiles,CXXFiles,MFiles,HFiles,FFiles,SFiles,SwigFiles,SwigCXXFiles,SysoFiles,TestGoFiles,XTestGoFiles,CompiledGoFiles,Export,EmbedFiles -compiled=true -test=true -export=false -deps=false -find=false -pgo=off -- golang.org/fake/a
        invoke.go:205: starting GOROOT= GOPATH=/Users/swarming/.swarming/w/ir/x/t/TestAbsoluteFilenames_Modules846104081/modcache GO111MODULE=off GOPROXY=file:///Users/swarming/.swarming/w/ir/x/t/TestAbsoluteFilenames_Modules846104081/modproxy PWD=/Users/swarming/.swarming/w/ir/x/t/TestAbsoluteFilenames_Modules846104081/fake go list -e -f {{context.ReleaseTags}} -- unsafe
        invoke.go:205: 16.219625ms for GOROOT= GOPATH=/Users/swarming/.swarming/w/ir/x/t/TestAbsoluteFilenames_Modules846104081/modcache GO111MODULE=off GOPROXY=file:///Users/swarming/.swarming/w/ir/x/t/TestAbsoluteFilenames_Modules846104081/modproxy PWD=/Users/swarming/.swarming/w/ir/x/t/TestAbsoluteFilenames_Modules846104081/fake go list -e -f {{context.ReleaseTags}} -- unsafe
        invoke.go:205: starting GOROOT= GOPATH=/Users/swarming/.swarming/w/ir/x/t/TestAbsoluteFilenames_Modules846104081/modcache GO111MODULE=on GOPROXY=file:///Users/swarming/.swarming/w/ir/x/t/TestAbsoluteFilenames_Modules846104081/modproxy PWD=/Users/swarming/.swarming/w/ir/x/t/TestAbsoluteFilenames_Modules846104081/fake go list -e -json=Name,ImportPath,Error,Dir,GoFiles,IgnoredGoFiles,IgnoredOtherFiles,CFiles,CgoFiles,CXXFiles,MFiles,HFiles,FFiles,SFiles,SwigFiles,SwigCXXFiles,SysoFiles,TestGoFiles,XTestGoFiles,CompiledGoFiles,Export,EmbedFiles -compiled=true -test=true -export=false -deps=false -find=false -pgo=off -- golang.org/fake/b/vendor/a
    ...
        invoke.go:205: 28.56025ms for GOROOT= GOPATH=/Users/swarming/.swarming/w/ir/x/t/TestAbsoluteFilenames_Modules846104081/modcache GO111MODULE=on GOPROXY=file:///Users/swarming/.swarming/w/ir/x/t/TestAbsoluteFilenames_Modules846104081/modproxy PWD=/Users/swarming/.swarming/w/ir/x/t/TestAbsoluteFilenames_Modules846104081/fake go list -e -json=Name,ImportPath,Error,Dir,GoFiles,IgnoredGoFiles,IgnoredOtherFiles,CFiles,CgoFiles,CXXFiles,MFiles,HFiles,FFiles,SFiles,SwigFiles,SwigCXXFiles,SysoFiles,TestGoFiles,XTestGoFiles,CompiledGoFiles,Export,EmbedFiles -compiled=true -test=true -export=false -deps=false -find=false -pgo=off -- golang.org/fake/f
        invoke.go:205: starting GOROOT= GOPATH=/Users/swarming/.swarming/w/ir/x/t/TestAbsoluteFilenames_Modules846104081/modcache GO111MODULE=off GOPROXY=file:///Users/swarming/.swarming/w/ir/x/t/TestAbsoluteFilenames_Modules846104081/modproxy PWD=/Users/swarming/.swarming/w/ir/x/t/TestAbsoluteFilenames_Modules846104081/fake go list -e -f {{context.ReleaseTags}} -- unsafe
        invoke.go:205: 24.13925ms for GOROOT= GOPATH=/Users/swarming/.swarming/w/ir/x/t/TestAbsoluteFilenames_Modules846104081/modcache GO111MODULE=off GOPROXY=file:///Users/swarming/.swarming/w/ir/x/t/TestAbsoluteFilenames_Modules846104081/modproxy PWD=/Users/swarming/.swarming/w/ir/x/t/TestAbsoluteFilenames_Modules846104081/fake go list -e -f {{context.ReleaseTags}} -- unsafe
        invoke.go:205: starting GOROOT= GOPATH=/Users/swarming/.swarming/w/ir/x/t/TestAbsoluteFilenames_Modules846104081/modcache GO111MODULE=on GOPROXY=file:///Users/swarming/.swarming/w/ir/x/t/TestAbsoluteFilenames_Modules846104081/modproxy PWD=/Users/swarming/.swarming/w/ir/x/t/TestAbsoluteFilenames_Modules846104081/fake go list -e -json=Name,ImportPath,Error,Dir,GoFiles,IgnoredGoFiles,IgnoredOtherFiles,CFiles,CgoFiles,CXXFiles,MFiles,HFiles,FFiles,SFiles,SwigFiles,SwigCXXFiles,SysoFiles,TestGoFiles,XTestGoFiles,CompiledGoFiles,Export,EmbedFiles -compiled=true -test=true -export=false -deps=false -find=false -pgo=off -- golang.org/fake/g
        invoke.go:205: 52.832959ms for GOROOT= GOPATH=/Users/swarming/.swarming/w/ir/x/t/TestAbsoluteFilenames_Modules846104081/modcache GO111MODULE=on GOPROXY=file:///Users/swarming/.swarming/w/ir/x/t/TestAbsoluteFilenames_Modules846104081/modproxy PWD=/Users/swarming/.swarming/w/ir/x/t/TestAbsoluteFilenames_Modules846104081/fake go list -e -json=Name,ImportPath,Error,Dir,GoFiles,IgnoredGoFiles,IgnoredOtherFiles,CFiles,CgoFiles,CXXFiles,MFiles,HFiles,FFiles,SFiles,SwigFiles,SwigCXXFiles,SysoFiles,TestGoFiles,XTestGoFiles,CompiledGoFiles,Export,EmbedFiles -compiled=true -test=true -export=false -deps=false -find=false -pgo=off -- golang.org/fake/g
        invoke.go:205: starting GOROOT= GOPATH=/Users/swarming/.swarming/w/ir/x/t/TestAbsoluteFilenames_Modules846104081/modcache GO111MODULE=off GOPROXY=file:///Users/swarming/.swarming/w/ir/x/t/TestAbsoluteFilenames_Modules846104081/modproxy PWD=/Users/swarming/.swarming/w/ir/x/t/TestAbsoluteFilenames_Modules846104081/fake go list -e -f {{context.ReleaseTags}} -- unsafe
        invoke.go:205: 22.935959ms for GOROOT= GOPATH=/Users/swarming/.swarming/w/ir/x/t/TestAbsoluteFilenames_Modules846104081/modcache GO111MODULE=off GOPROXY=file:///Users/swarming/.swarming/w/ir/x/t/TestAbsoluteFilenames_Modules846104081/modproxy PWD=/Users/swarming/.swarming/w/ir/x/t/TestAbsoluteFilenames_Modules846104081/fake go list -e -f {{context.ReleaseTags}} -- unsafe
        invoke.go:205: starting GOROOT= GOPATH=/Users/swarming/.swarming/w/ir/x/t/TestAbsoluteFilenames_Modules846104081/modcache GO111MODULE=on GOPROXY=file:///Users/swarming/.swarming/w/ir/x/t/TestAbsoluteFilenames_Modules846104081/modproxy PWD=/Users/swarming/.swarming/w/ir/x/t/TestAbsoluteFilenames_Modules846104081/fake go list -e -json=Name,ImportPath,Error,Dir,GoFiles,IgnoredGoFiles,IgnoredOtherFiles,CFiles,CgoFiles,CXXFiles,MFiles,HFiles,FFiles,SFiles,SwigFiles,SwigCXXFiles,SysoFiles,TestGoFiles,XTestGoFiles,CompiledGoFiles,Export,EmbedFiles -compiled=true -test=true -export=false -deps=false -find=false -pgo=off -- golang.org/fake/h
        invoke.go:205: 47.6275ms for GOROOT= GOPATH=/Users/swarming/.swarming/w/ir/x/t/TestAbsoluteFilenames_Modules846104081/modcache GO111MODULE=on GOPROXY=file:///Users/swarming/.swarming/w/ir/x/t/TestAbsoluteFilenames_Modules846104081/modproxy PWD=/Users/swarming/.swarming/w/ir/x/t/TestAbsoluteFilenames_Modules846104081/fake go list -e -json=Name,ImportPath,Error,Dir,GoFiles,IgnoredGoFiles,IgnoredOtherFiles,CFiles,CgoFiles,CXXFiles,MFiles,HFiles,FFiles,SFiles,SwigFiles,SwigCXXFiles,SysoFiles,TestGoFiles,XTestGoFiles,CompiledGoFiles,Export,EmbedFiles -compiled=true -test=true -export=false -deps=false -find=false -pgo=off -- golang.org/fake/h
        invoke.go:205: starting GOROOT= GOPATH=/Users/swarming/.swarming/w/ir/x/t/TestAbsoluteFilenames_Modules846104081/modcache GO111MODULE=off GOPROXY=file:///Users/swarming/.swarming/w/ir/x/t/TestAbsoluteFilenames_Modules846104081/modproxy PWD=/Users/swarming/.swarming/w/ir/x/t/TestAbsoluteFilenames_Modules846104081/fake go list -e -f {{context.ReleaseTags}} -- unsafe


â€” [watchflakes](https://go.dev/wiki/Watchflakes)
","['NeedsInvestigation', 'Tools']",0
golang/go,cmd/go: SEGV causes flaky gopls test,"```
#!watchflakes
default <- pkg == ""golang.org/x/tools/gopls/internal/test/integration/misc"" && test == ""TestRenameWithUnsafeSlice/default""
```

Issue created automatically to collect these failures.

Example ([log](https://ci.chromium.org/b/8692490278749295793)):

    === RUN   TestRenameWithUnsafeSlice/default
    goroutine profile: total 10
    2 @ 0x10427edec 0x104241180 0x10427dff0 0x1042dd2b8 0x1042e2bec 0x1042e2bd1 0x10452d088 0x104549b50 0x104548498 0x104c6e8f4 0x10516d428 0x10516d371 0x104287414
    #	0x10427dfef	internal/poll.runtime_pollWait+0x9f					/Users/swarming/.swarming/w/ir/x/w/goroot/src/runtime/netpoll.go:351
    #	0x1042dd2b7	internal/poll.(*pollDesc).wait+0xb7					/Users/swarming/.swarming/w/ir/x/w/goroot/src/internal/poll/fd_poll_runtime.go:84
    #	0x1042e2beb	internal/poll.(*pollDesc).waitRead+0x35b				/Users/swarming/.swarming/w/ir/x/w/goroot/src/internal/poll/fd_poll_runtime.go:89
    #	0x1042e2bd0	internal/poll.(*FD).Accept+0x340					/Users/swarming/.swarming/w/ir/x/w/goroot/src/internal/poll/fd_unix.go:613
    #	0x10452d087	net.(*netFD).accept+0x37						/Users/swarming/.swarming/w/ir/x/w/goroot/src/net/fd_unix.go:150
    #	0x104549b4f	net.(*TCPListener).accept+0x3f						/Users/swarming/.swarming/w/ir/x/w/goroot/src/net/tcpsock_posix.go:159
    #	0x104548497	net.(*TCPListener).Accept+0x67						/Users/swarming/.swarming/w/ir/x/w/goroot/src/net/tcpsock.go:387
    ...
    
    1 @ 0x10427edec 0x10426bd70 0x104381f58 0x104287414
    #	0x104381f57	runtime/trace.(*traceMultiplexer).startLocked.func1+0xb7	/Users/swarming/.swarming/w/ir/x/w/goroot/src/runtime/trace/subscribe.go:167
    
    1 @ 0x10427edec 0x1042835e0 0x104e38960 0x104287414
    #	0x1042835df	time.Sleep+0x14f					/Users/swarming/.swarming/w/ir/x/w/goroot/src/runtime/time.go:363
    #	0x104e3895f	golang.org/x/tools/gopls/internal/filecache.gc+0x13f	/Users/swarming/.swarming/w/ir/x/w/targetrepo3574310271/gopls/internal/filecache/filecache.go:457
    
        runner.go:198: closing the sandbox: error(s) cleaning sandbox: cleaning modcache: go command failed (stdout: ) (stderr: ): signal: segmentation fault; removing files: <nil>
    --- FAIL: TestRenameWithUnsafeSlice/default (0.21s)


â€” [watchflakes](https://go.dev/wiki/Watchflakes)
","['NeedsInvestigation', 'GoCommand', 'gopls', 'Tools']",0
golang/go,x/tools/gopls: extract function produces invalid code,"### gopls version

golang.org/x/tools/gopls v0.0.0-20260120163254-613c127fdd11

### go env

```shell
AR='ar'
CC='gcc'
CGO_CFLAGS='-O2 -g'
CGO_CPPFLAGS=''
CGO_CXXFLAGS='-O2 -g'
CGO_ENABLED='1'
CGO_FFLAGS='-O2 -g'
CGO_LDFLAGS='-O2 -g'
CXX='g++'
GCCGO='gccgo'
GO111MODULE=''
GOAMD64='v1'
GOARCH='amd64'
GOAUTH='netrc'
GOBIN='/home/fata/.local/share/mise/installs/go/1.25.6/bin'
GOCACHE='/home/fata/.cache/go-build'
GOCACHEPROG=''
GODEBUG=''
GOENV='/home/fata/.config/go/env'
GOEXE=''
GOEXPERIMENT=''
GOFIPS140='off'
GOFLAGS=''
GOGCCFLAGS='-fPIC -m64 -pthread -Wl,--no-gc-sections -fmessage-length=0 -ffile-prefix-map=/tmp/go-build628802563=/tmp/go-build -gno-record-gcc-switches'
GOHOSTARCH='amd64'
GOHOSTOS='linux'
GOINSECURE=''
GOMOD='/home/fata/Documents/tools/gopls/go.mod'
GOMODCACHE='/home/fata/go/pkg/mod'
GONOPROXY=''
GONOSUMDB=''
GOOS='linux'
GOPATH='/home/fata/go'
GOPRIVATE=''
GOPROXY='https://proxy.golang.org,direct'
GOROOT='/home/fata/.local/share/mise/installs/go/1.25.6'
GOSUMDB='sum.golang.org'
GOTELEMETRY='local'
GOTELEMETRYDIR='/home/fata/.config/go/telemetry'
GOTMPDIR=''
GOTOOLCHAIN='auto'
GOTOOLDIR='/home/fata/.local/share/mise/installs/go/1.25.6/pkg/tool/linux_amd64'
GOVCS=''
GOVERSION='go1.25.6'
GOWORK=''
PKG_CONFIG='pkg-config'
```

### What did you do?

```go
func Fun(v2 int) (int, int, error) {
    // selection starts 
	switch v2 { 
	case 1:
		return doOne() 
	case 2:
		return doTwo()
	}
    // selection ends -- run extract function

	return 1, 3, nil
}

func doOne() (int, int, error) {
	return 0, 1, nil
}

func doTwo() (int, int, error) {
	return 0, 2, nil
}
```

### What did you see happen?

```go
func Fun(v2 int) (int, int, error) {
	i, i1, err, shouldReturn := newFunction(v2)
	if shouldReturn {
		return i, i1, err
	}

	return 1, 3, nil
}

func newFunction(v2 int) (int, int, error, bool) {
	switch v2 {
	case 1
		return doOne(), true  // [!] notice that this is invalid syntax
	case 2:
		return doTwo(), true  // [!] notice that this is invalid syntax
	}
	return 0, 0, nil, false
}

func doOne() (int, int, error) {
	return 0, 1, nil
}

func doTwo() (int, int, error) {
	return 0, 2, nil
}
```

### What did you expect to see?

anything as long as it produces a valid Go code. this what i will do if i do it manually:

```go
func Fun(v2 int) (int, int, error) {
	i, i1, err, shouldReturn := newFunction(v2)
	if shouldReturn {
		return i, i1, err
	} 

	return 1, 3, nil
}

func newFunction(v2 int) (int, int, error, bool) {
	switch v2 { 
	case 1:
		v1, v3, v4 := doOne()
		return v1, v3, v4, true
	case 2:
		v1, v3, v4 := doTwo()
		return v1, v3, v4, true
	}
	return 0, 0, nil, false
}

func doOne() (int, int, error) {
	return 0, 1, nil
}

func doTwo() (int, int, error) {
	return 0, 2, nil
}
```

### Editor and settings

vscode with Go extension (vanilla)

### Logs

_No response_","['gopls', 'Tools']",0
golang/go,cmd/link: binary size increase with Go 1.26,"### Go version

1.26rc2

### Output of `go env` in your module/workspace:

```shell
AR='ar'
CC='clang'
CGO_CFLAGS='-O2 -g'
CGO_CPPFLAGS=''
CGO_CXXFLAGS='-O2 -g'
CGO_ENABLED='1'
CGO_FFLAGS='-O2 -g'
CGO_LDFLAGS='-O2 -g'
CXX='clang++'
GCCGO='gccgo'
GO111MODULE='auto'
GOARCH='arm64'
GOARM64='v8.0'
GOAUTH='netrc'
GOBIN=''
GOCACHE='/Users/pierre.gimalac/Library/Caches/go-build'
GOCACHEPROG=''
GODEBUG=''
GOENV='/Users/pierre.gimalac/Library/Application Support/go/env'
GOEXE=''
GOEXPERIMENT=''
GOFIPS140='off'
GOFLAGS=''
GOGCCFLAGS='-fPIC -arch arm64 -pthread -fno-caret-diagnostics -Qunused-arguments -fmessage-length=0 -ffile-prefix-map=/var/folders/zp/m8h1x16s30g3f656sxsdt7rm0000gp/T/go-build2439715533=/tmp/go-build -gno-record-gcc-switches -fno-common'
GOHOSTARCH='arm64'
GOHOSTOS='darwin'
GOINSECURE=''
GOMOD='/Users/pierre.gimalac/pgimalac/kubernetes/go.mod'
GOMODCACHE='/Users/pierre.gimalac/go/pkg/mod'
GONOPROXY=''
GONOSUMDB='github.com/DataDog,go.ddbuild.io'
GOOS='darwin'
GOPATH='/Users/pierre.gimalac/go'
GOPRIVATE=''
GOPROXY='https://depot-read-api-go.us1.ddbuild.io/magicmirror/magicmirror/@current,https://depot-read-api-go.us1.ddbuild.io/magicmirror/testing/@current,proxy.golang.org,direct'
GOROOT='/Users/pierre.gimalac/sdk/go1.26rc2'
GOSUMDB='sum.golang.org'
GOTELEMETRY='local'
GOTELEMETRYDIR='/Users/pierre.gimalac/Library/Application Support/go/telemetry'
GOTMPDIR=''
GOTOOLCHAIN='go1.26rc2'
GOTOOLDIR='/Users/pierre.gimalac/sdk/go1.26rc2/pkg/tool/darwin_arm64'
GOVCS=''
GOVERSION='go1.26rc2'
GOWORK='/Users/pierre.gimalac/pgimalac/kubernetes/go.work'
PKG_CONFIG='pkg-config'
```

### What did you do?

I tried building various binaries with go 1.25.6 and go 1.26rc2, and consistently observed a non-negligible (between 300kiB and 1.3MiB) binary size increase, mostly proportional to code size, for binaries ranging from 20MiB to 100MiB.

https://github.com/golang/go/issues/77049#issuecomment-3705760516 (cc @cherrymui) mentions a fixed 300kiB increase, which is not what I observed.
> a small (~300K) fixed-size increase, not proportional to user code size.

I mostly want to ensure this size increase is known/expected (it's not really a blocker for me in practice) as it doesn't match what was mentioned in the other issue.

### What did you see happen?

For kubernetes binaries (darwin/arm64):

| Binary | Go 1.25.6 | Go 1.26rc2 | Delta |
|--------|-----------|------------|-------|
| kube-apiserver | 77.9 MiB | 78.9 MiB | +1081 KiB |
| kube-controller-manager | 64.6 MiB | 65.6 MiB | +1031 KiB |
| kube-proxy | 38.3 MiB | 39.1 MiB | +853 KiB |
| kube-scheduler | 43.7 MiB | 44.5 MiB | +886 KiB |
| kubelet | 48.5 MiB | 49.4 MiB | +982 KiB |

If you want to reproduce:
```sh
git clone https://github.com/kubernetes/kubernetes.git
cd kubernetes
go get google.golang.org/protobuf@cb6ac5a # fix https://github.com/golang/protobuf/issues/1704
go work vendor
GOTOOLCHAIN=go1.26rc2 make kube-apiserver kubelet kube-controller-manager kube-scheduler kube-proxy
ls -l _output/bin/
```

For datadog-agent binaries (linux/arm64):

| Binary | Go 1.25.6 | Go 1.26rc2 | Delta |
|--------|-----------|------------|-------|
| agent | 102.2 MiB | 103.5 MiB | +1356 KiB |
| process-agent | 31.1 MiB | 31.7 MiB | +589 KiB |
| security-agent | 28.8 MiB | 29.2 MiB | +461 KiB |
| system-probe | 56.5 MiB | 57.5 MiB | +973 KiB |
| trace-agent | 20.9 MiB | 21.2 MiB | +334 KiB |


### What did you expect to see?

A smaller size increase, or ideally none at all.","['NeedsInvestigation', 'binary-size', 'compiler/runtime', 'BugReport']",0
golang/go,x/playground: unit tests doesn't work when primary test file has a name,"This issue is a follow-up of previously reported bug https://github.com/golang/go/issues/32403.

### Description

The Go Playground doesn't detect unit test, if payload is supplied in txtar format and the main test file has a filename.

As result, it returns `no non-test Go files` error.

```
-- main_test.go --
package main

import (
	""testing""
	_ ""play.ground/pkg""
)

func TestFoo(t *testing.T) {}
-- go.mod --
module play.ground
-- pkg/x.go --
package pkg

```

Snippet: https://go.dev/play/p/alRl0ka0p8J

### Context

I'm author of an [alternative frontend](https://github.com/x1unix/go-playground) for Go Playground.

The app provides vscode-like UI with individual tab per file to abstract txtar format away from user.

The app under the hood, before sending a request to the upstream server, constructs a txtar payload when multiple files were provided.

As I'm not able to determine what file is actually the primary one, every file ends up having a file name in a txtar payload.



","['NeedsInvestigation', 'BugReport']",0
golang/go,runtime/metrics: Read panics when given an empty slice,"### Go version

`go version go1.25.1 linux/amd64` and `go version go1.26rc2 linux/amd64`

### Output of `go env` in your module/workspace:

```shell
AR='ar'
CC='gcc'
CGO_CFLAGS='-O2 -g'
CGO_CPPFLAGS=''
CGO_CXXFLAGS='-O2 -g'
CGO_ENABLED='1'
CGO_FFLAGS='-O2 -g'
CGO_LDFLAGS='-O2 -g'
CXX='g++'
GCCGO='gccgo'
GO111MODULE=''
GOAMD64='v1'
GOARCH='amd64'
GOAUTH='netrc'
GOBIN='/home/caleb/bin'
GOCACHE='/home/caleb/.cache/go-build'
GOCACHEPROG=''
GODEBUG=''
GOENV='/home/caleb/.config/go/env'
GOEXE=''
GOEXPERIMENT=''
GOFIPS140='off'
GOFLAGS=''
GOGCCFLAGS='-fPIC -m64 -pthread -Wl,--no-gc-sections -fmessage-length=0 -ffile-prefix-map=/tmp/go-build1915131444=/tmp/go-build -gno-record-gcc-switches'
GOHOSTARCH='amd64'
GOHOSTOS='linux'
GOINSECURE=''
GOMOD='/home/caleb/w/liftoff/go.mod'
GOMODCACHE='/home/caleb/go/pkg/mod'
GONOPROXY=''
GONOSUMDB=''
GOOS='linux'
GOPATH='/home/caleb/go'
GOPRIVATE=''
GOPROXY='https://proxy.golang.org,direct'
GOROOT='/home/caleb/3p/go'
GOSUMDB='sum.golang.org'
GOTELEMETRY='local'
GOTELEMETRYDIR='/home/caleb/.config/go/telemetry'
GOTMPDIR=''
GOTOOLCHAIN='auto'
GOTOOLDIR='/home/caleb/3p/go/pkg/tool/linux_amd64'
GOVCS=''
GOVERSION='go1.25.1'
GOWORK=''
PKG_CONFIG='pkg-config'
```

### What did you do?

Repro: https://go.dev/play/p/rUoSepQMC8K

```
package main

import (
	""runtime/metrics""
)

func main() {
	metrics.Read(nil)
}
```

### What did you see happen?

A panic:

```
panic: runtime error: index out of range [0] with length 0

goroutine 1 [running]:
runtime/metrics.Read(...)
	/usr/local/go-faketime/src/runtime/metrics/sample.go:46
main.main()
	/tmp/sandbox355581023/prog.go:8 +0x13
```

### What did you expect to see?

I expected it to succeed (it should be a no-op).

On one hand, it's pretty silly to ask for zero metrics, and likely a bug. (I discovered this because I had a bug in my code.) On the other hand, `metrics.Read` shouldn't blindly assume that there's at least one element in the input slice; I don't see anything in the documentation saying this isn't allowed.

/cc @golang/runtime @mknyszek @prattmic (per s/owners)","['NeedsInvestigation', 'compiler/runtime', 'BugReport']",0
golang/go,x/tools/gopls: add a way to export gopls metrics/logs,"The old `-ocagent` option that let us export `gopls` metrics/logs to third-party data storage was removed in [CL 653016](https://go-review.googlesource.com/c/tools/+/653016).

We actually had a wrapper for `gopls` that ran an HTTP server to receive this data and forward it to our internal storage. We had to delete this wrapper when the `-ocagent` flag was removed. At the time, it wasn't a huge deal since we didn't have the resources to build proper dashboards anyway.

I'm now getting back into monitoring, and we have a number of developers complaining that ""gopls doesn't work."" It would be extremely useful to have these data available again to properly debug and track the service.

Can we implement a (similar) way to export metrics/logs from `gopls` without patching its code?

I see there are some Prometheus counters, but they don't provide enough details, e.q duration of operations. Maybe we can extend Prometheus implementation?","['help wanted', 'FeatureRequest', 'gopls', 'Tools']",0
golang/go,proposal: net/http: add ShutdownGracePeriod to Server,"### Proposal Details

Currently, when the HTTP/2 stack is gracefully severing a connection, either due to idleness or because the server is shutting down via `Server.Shutdown`, it sends a `GOAWAY` frame with the latest stream ID, waits for any in progress requests to finish, and then closes the connection.

While this is not directly a problem, it does mean that requests that are currently in flight over the network will need to be retried (on a different connection), which is undesirable.

RFC 7540 says the following on the matter:
> A server that is attempting to gracefully shut down a
   connection SHOULD send an initial GOAWAY frame with the last stream
   identifier set to 2^31-1 and a NO_ERROR code.  This signals to the
   client that a shutdown is imminent and that initiating further
   requests is prohibited.  After allowing time for any in-flight stream
   creation (at least one round-trip time), the server can send another
   GOAWAY frame with an updated last stream identifier.  This ensures
   that a connection can be cleanly shut down without losing requests.

To that end, it would be helpful if net/http added another configuration value to `Server` (tentatively called `ShutdownGracePeriod` but I don't care much about the particular name). Then when gracefully severing a connection, the HTTP/2 stack will:
1. send a `GOAWAY` frame with a stream ID of 2^31 - 1
2. sleep for `ShutdownGracePeriod`
3. send a `GOAWAY` frame with the latest stream ID
4. wait for in progress requests to finish
5. close the connection

If `ShutdownGracePeriod` is 0, then it may skip steps 1 and 2.",['Proposal'],0
golang/go,"x/exp/trace,internal/trace: support event writing","_Note: This is not a proposal in the sense of Go's proposal process because it targets the API of an internal package (that is exported to x/exp/trace)._

## Overview

This proposal adds a `Writer` type to the `internal/trace` package that enables writing execution traces in the Go trace format v2 (Go 1.22+). The primary use case is filtering traces to reduce data volume. For example, extracting events for a subset of goroutines or removing certain event types.

Another use case is validating events produced via the [event constructor API](https://github.com/golang/go/issues/74826). Events created programmatically can be written via `Writer` and read back via `Reader` to verify they pass the reader's validation logic.

Last but not least, this API might allow the creation of obfuscation tools that can sanitize traces of sensitive information to make them suitable for sharing in public.

```go
w, err := trace.NewWriter(file, version.Go126)
if err != nil {
    // handle error
}
defer w.Close()

r, err := trace.NewReader(existingTrace)
for {
    e, err := r.ReadEvent()
    if err == io.EOF {
        break
    }
    if keep, newEvent := processEvent(e); keep {
        if err := w.WriteEvent(newEvent); err != nil {
            // handle error
        }
    }
}
```

## API

```go
// Writer writes execution traces in the Go trace format v2 (Go 1.22+).
type Writer struct {}

// NewWriter creates a new Writer that writes a trace to w with the specified version.
func NewWriter(w io.Writer, v version.Version) (*Writer, error)

// WriteEvent writes a single event to the trace.
func (w *Writer) WriteEvent(e Event) error

// Close writes all buffered events to the underlying writer.
// After Close returns, subsequent calls to WriteEvent or Close will return an error.
func (w *Writer) Close() error
```

Note: Maybe instead of the `version.Version` argument we should accept a `WriterConfig` struct that can be extended?

## Requirements

Correctness is verified through round-trip testing: traces read via `Reader`, written via `Writer`, and read back must produce equivalent event sequences. Specifically, the `String()` output of each event must be identical before and after the round-trip. The binary representation of the trace may differ due to string/stack ID reassignment and batch reordering.

## Implementation

A prototype is available in [CL 737340](https://go-review.googlesource.com/c/go/+/737340).

_Full disclosure: AI was heavily used for the prototype. I plan to further polish this (or rebuild fully by hand) before I expect any serious review. But the code does pass the round-trip tests, including some gnarly gc-stress and proc stealing cases. It still fails the stress tests using data generated on the fly._

cc @mknyszek @prattmic ","['NeedsInvestigation', 'compiler/runtime', 'LibraryProposal']",0
golang/go,proposal: os/exec: support closing stdin/stdout pipes easily,"### Proposal Details

Ref: os/exec: Unable to close pipes properly if the caller can't close the pipes ahead #76057

----

If Stdin/Stdout/Stderr isn't an os pipe, Golang will call ""awaitGoroutines"" in ""Wait"" to wait for the pipes (or readers/writers) to be closed. It could easily cause deadlock if the reader/writer isn't handled carefully.

Although there are alternatives:

* Using `StdinPipe` and `StdoutPipe` makes code much more complicated than it should be in many cases
    * developers need to maintain their own goroutines to do the same ""io.Copy"" as ""Cmd"" does
* Using `os/exec.Cmd.Process` is too difficult to use for daily work (there are a lot of ""process"" related details)


The real problem is: `exec.Cmd` should be able to close the pipes like `Stdin` if it is a Closer, or make callers able to close them, it will make code much simpler and bug-free.

The real world example is like this:

```
// it is a io.Closer, and its Close will unblock the Stdin's ""Read"" in the ""io.Copy"" loop
// otherwise, the `io.Copy` in Cmd's goroutines won't stop even if the process exits
cmd.Stdin = readerWithBlocking
....
```

So maybe it should be good enough to add a `Closer` check and call in `Cmd.awaitGoroutines`: 

```
if closer, ok := c.Stdin.(io.Closer); ok {
    _ = closer.Close()
}
```

Or also add an option for this behavior.

Or add a callback in `Cmd.Wait` before `awaitGoroutines` to let callers close their pipes.","['Proposal', 'LibraryProposal']",0
golang/go,x/tools/go/packages: Load returns incomplete package data when given a long list of files,"### What version of Go are you using (`go version`)?

<pre>
$ go version
go version go1.25.6 darwin/arm64
</pre>

### Does this issue reproduce with the latest release?

Yes, reproduced with `golang.org/x/tools` v0.39.0.

### What operating system and processor architecture are you using (`go env`)?

<details><summary><code>go env</code> Output</summary>



<pre>
$ go env
AR='ar'
CC='cc'
CGO_CFLAGS='-O2 -g'
CGO_CPPFLAGS=''
CGO_CXXFLAGS='-O2 -g'
CGO_ENABLED='1'
CGO_FFLAGS='-O2 -g'
CGO_LDFLAGS='-O2 -g'
CXX='c++'
GCCGO='gccgo'
GO111MODULE='on'
GOARCH='arm64'
GOARM64='v8.0'
GOAUTH='netrc'
GOBIN=''
GOCACHE='/Users/eliranmarom/Library/Caches/go-build'
GOCACHEPROG=''
GODEBUG=''
GOENV='/Users/eliranmarom/Library/Application Support/go/env'
GOEXE=''
GOEXPERIMENT=''
GOFIPS140='off'
GOFLAGS=''
GOGCCFLAGS='-fPIC -arch arm64 -pthread -fno-caret-diagnostics -Qunused-arguments -fmessage-length=0 -ffile-prefix-map=/var/folders/99/fg0pm57x4h52qjw6_ssm00540000gn/T/go-build4261233807=/tmp/go-build -gno-record-gcc-switches -fno-common'
GOHOSTARCH='arm64'
GOHOSTOS='darwin'
GOINSECURE=''
GOMOD='/Users/eliranmarom/repos/wiz/go.mod'
GOMODCACHE='/Users/eliranmarom/go/pkg/mod'
GONOPROXY='[github.com/beyondnetworks,github.com/wiz-sec](https://www.google.com/search?q=https://github.com/beyondnetworks,github.com/wiz-sec)'
GONOSUMDB='[github.com/beyondnetworks,github.com/wiz-sec](https://www.google.com/search?q=https://github.com/beyondnetworks,github.com/wiz-sec)'
GOOS='darwin'
GOPATH='/Users/eliranmarom/go'
GOPRIVATE='[github.com/beyondnetworks,github.com/wiz-sec](https://www.google.com/search?q=https://github.com/beyondnetworks,github.com/wiz-sec)'
GOPROXY='[https://proxy.golang.org](https://proxy.golang.org),direct'
GOROOT='/opt/homebrew/Cellar/go/1.25.6/libexec'
GOSUMDB='sum.golang.org'
GOTELEMETRY='local'
GOTELEMETRYDIR='/Users/eliranmarom/Library/Application Support/go/telemetry'
GOTMPDIR=''
GOTOOLCHAIN='local'
GOTOOLDIR='/opt/homebrew/Cellar/go/1.25.6/libexec/pkg/tool/darwin_arm64'
GOVCS=''
GOVERSION='go1.25.6'
GOWORK='/Users/eliranmarom/repos/wiz/go.work'
PKG_CONFIG='pkg-config'
uname -v: Darwin Kernel Version 25.2.0: Tue Nov 18 21:09:40 PST 2025; root:xnu-12377.61.12~1/RELEASE_ARM64_T6000
ProductName:		macOS
ProductVersion:		26.2
BuildVersion:		25C56
lldb --version: lldb-1703.0.236.21
Apple Swift version 6.2.3 (swiftlang-6.2.3.3.21 clang-1700.6.3.2)
</pre></details>

### What did you do?

I called `packages.Load()` with a list of Go file paths. The list was very long - over 200 files - which made the total command line arguments longer than 16KB.

When the arguments are too long, the go/packages library automatically splits them into smaller chunks, each chunk is processed separately and then the results are merged back together.

Here's a simple test that reproduces the issue:

```go
package main

import (
	""fmt""
	""os""
	""path/filepath""
	""testing""

	""golang.org/x/tools/go/packages""
	""golang.org/x/tools/go/packages/packagestest""
)

func TestPackageLoadChunking(t *testing.T) {
	exported := packagestest.Export(t, packagestest.GOPATH, []packagestest.Module{{
		Name: ""golang.org/fake"",
		Files: map[string]any{
			""a/def.go"": `package a; const SharedConstant = 42`,
		}}})
	defer exported.Cleanup()

	// Get the actual file paths
	defFile := exported.File(""golang.org/fake"", ""a/def.go"")

	// Create enough filler files to force chunking (exceed ~16KB arg limit)
	// Each file path is ~100 bytes, so we need ~200 files to exceed 16KB
	var files []string
	files = append(files, defFile)

	for i := 0; i < 200; i++ {
		fillerPath := filepath.Join(filepath.Dir(defFile), fmt.Sprintf(""filler_%03d.go"", i))
		if err := os.WriteFile(fillerPath, []byte(""package a\n""), 0644); err != nil {
			t.Fatal(err)
		}
		files = append(files, fillerPath)
	}

	// Load by individual file paths (not package pattern)
	// This will cause the same package to appear in multiple chunks
	exported.Config.Mode = packages.NeedFiles
	pkgs, err := packages.Load(exported.Config, files...)
	if err != nil {
		t.Fatal(err)
	}

	if len(pkgs) != 1 {
		t.Fatal(""package golang.org/fake/a not found"")
	}
	// Check that all expected files are present (def.go and 200 filler files)
	if len(pkgs[0].GoFiles) != len(files) {
		t.Errorf(""expected %d files, got %d"", len(files), len(pkgs[0].GoFiles))
	}
}
```

### What did you expect to see?
a list of all files in the package

### What did you see instead?
partial list of files in the package
","['NeedsInvestigation', 'Tools', 'BugReport']",0
golang/go,reflect: commit e8fdfeb7 made the DCE impossible,"### Go version

go version go1.26rc1 linux/amd64

### Output of `go env` in your module/workspace:

```shell
irrelevant to the issue
```

### What did you do?

Compile the following program with `go build -tags grpcnotrace -ldflags '-dumpdep'`.

1. bug.go
```
package main

import (
        _ ""cloud.google.com/go/storage""
)

func main() {
        println(""oops"")
}
```

2. go.mod (do a `go mod tidy` to add indirect dependencies):
```
module bug

go 1.24.0

require cloud.google.com/go/storage v1.59.1
```

### What did you see happen?

`-dumpdep` detects the following chain of dependencies:
```
...
cloud.google.com/go/storage/internal/apiv2/storagepb.file_google_storage_v2_storage_proto_init -> type:*reflect.rtype <UsedInIface>
...
type:*reflect.rtype <UsedInIface> -> reflect.(*rtype).Methods
...
reflect.(*rtype).Methods -> reflect.(*rtype).Methods.func1 <ReflectMethod>
```


### What did you expect to see?

The example program must compile with DCE enabled.

Compiling the same example program with go 1.25.6 finds no `<ReflectMethod>`s.","['WaitingForInfo', 'compiler/runtime', 'BugReport']",0
golang/go,cmd/go: clean should honor the -o flag as a build parameter so it can remove the final built artifacts,"<!-- Please answer these questions before submitting your issue. Thanks! -->

### What version of Go are you using (`go version`)?

<pre>
$ go version
go version go1.25.6 darwin/arm64
</pre>

### Does this issue reproduce with the latest release?
Yes

### What operating system and processor architecture are you using (`go env`)?

<details><summary><code>go env</code> Output</summary><br><pre>
$ go env
AR='ar'
CC='cc'
CGO_CFLAGS='-O2 -g'
CGO_CPPFLAGS=''
CGO_CXXFLAGS='-O2 -g'
CGO_ENABLED='1'
CGO_FFLAGS='-O2 -g'
CGO_LDFLAGS='-O2 -g'
CXX='c++'
GCCGO='gccgo'
GO111MODULE=''
GOARCH='arm64'
GOARM64='v8.0'
GOAUTH='netrc'
GOBIN=''
GOCACHE='/Users/norm/Library/Caches/go-build'
GOCACHEPROG=''
GODEBUG=''
GOENV='/Users/norm/Library/Application Support/go/env'
GOEXE=''
GOEXPERIMENT=''
GOFIPS140='off'
GOFLAGS=''
GOGCCFLAGS='-fPIC -arch arm64 -pthread -fno-caret-diagnostics -Qunused-arguments -fmessage-length=0 -ffile-prefix-map=/var/folders/jx/038678ld7hx14_ys0r2nh_tm0000gn/T/go-build3841041466=/tmp/go-build -gno-record-gcc-switches -fno-common'
GOHOSTARCH='arm64'
GOHOSTOS='darwin'
GOINSECURE=''
GOMOD='/Users/norm/Projects/CloudFoundry/ocf-scheduler/ocf-scheduler/go.mod'
GOMODCACHE='/Users/norm/go/pkg/mod'
GONOPROXY=''
GONOSUMDB=''
GOOS='darwin'
GOPATH='/Users/norm/go'
GOPRIVATE=''
GOPROXY='https://proxy.golang.org,direct'
GOROOT='/opt/homebrew/Cellar/go/1.25.6/libexec'
GOSUMDB='sum.golang.org'
GOTELEMETRY='local'
GOTELEMETRYDIR='/Users/norm/Library/Application Support/go/telemetry'
GOTMPDIR=''
GOTOOLCHAIN='auto'
GOTOOLDIR='/opt/homebrew/Cellar/go/1.25.6/libexec/pkg/tool/darwin_arm64'
GOVCS=''
GOVERSION='go1.25.6'
GOWORK=''
PKG_CONFIG='pkg-config'
uname -v: Darwin Kernel Version 25.2.0: Tue Nov 18 21:07:05 PST 2025; root:xnu-12377.61.12~1/RELEASE_ARM64_T6020
ProductName:		macOS
ProductVersion:		26.2
BuildVersion:		25C56
lldb --version: lldb-1703.0.236.21
Apple Swift version 6.2.3 (swiftlang-6.2.3.3.21 clang-1700.6.3.2)
</pre></details>

### What did you do?

The following command builds multiple main packages and places their final executables in the directory specified by the -o parameter.
`go build -o . ./...`

I would have expected the following command to remove the final executables from the directory specified by -o. 
go clean -o . ./...

Multiple main packages were located in subdirectories structured as follows:
cmd/app1/main.go
cmd/app2/main.go
cmd/appn/main.go

### What did you expect to see?
It was expected the final executables would be removed from where the -o parameter was set to.


### What did you see instead?
go clean -o . ./...
flag provided but not defined: -o
","['NeedsInvestigation', 'FeatureRequest', 'GoCommand']",0
golang/go,doc/install/source: does not document `GOARM64`,"Hi!  In reading through https://go.dev/doc/install/source#environment, I was surprised to note that the `GOARM64` variable added back in https://github.com/golang/go/issues/60905 does not appear to be documented. ðŸ™‡ :heart:","['Documentation', 'NeedsInvestigation', 'GoCommand']",0
golang/go,cmd/compile: reproducible builds with cgo,"### Go version

go version go1.25.5 X:nodwarf5 linux/amd64

### Output of `go env` in your module/workspace:

```shell
AR='ar'
CC='gcc'
CGO_CFLAGS='-O2 -g'
CGO_CPPFLAGS=''
CGO_CXXFLAGS='-O2 -g'
CGO_ENABLED='1'
CGO_FFLAGS='-O2 -g'
CGO_LDFLAGS='-O2 -g'
CXX='g++'
GCCGO='gccgo'
GO111MODULE=''
GOAMD64='v1'
GOARCH='amd64'
GOAUTH='netrc'
GOBIN=''
GOCACHE='/home/res/.cache/go-build'
GOCACHEPROG=''
GODEBUG=''
GOENV='/home/res/.config/go/env'
GOEXE=''
GOEXPERIMENT='nodwarf5'
GOFIPS140='off'
GOFLAGS=''
GOGCCFLAGS='-fPIC -m64 -pthread -Wl,--no-gc-sections -fmessage-length=0 -ffile-prefix-map=/tmp/go-build1033279940=/tmp/go-build -gno-record-gcc-switches'
GOHOSTARCH='amd64'
GOHOSTOS='linux'
GOINSECURE=''
GOMOD='/dev/null'
GOMODCACHE='/home/res/.local/share/go/pkg/mod'
GONOPROXY=''
GONOSUMDB=''
GOOS='linux'
GOPATH='/home/res/.local/share/go'
GOPRIVATE=''
GOPROXY='https://proxy.golang.org,direct'
GOROOT='/usr/lib/go'
GOSUMDB='sum.golang.org'
GOTELEMETRY='local'
GOTELEMETRYDIR='/home/res/.config/go/telemetry'
GOTMPDIR=''
GOTOOLCHAIN='auto'
GOTOOLDIR='/usr/lib/go/pkg/tool/linux_amd64'
GOVCS=''
GOVERSION='go1.25.5 X:nodwarf5'
GOWORK=''
PKG_CONFIG='pkg-config'
```

### What did you do?

Using the same example code listed on https://github.com/golang/go/issues/75253 on both linux/386 and linux/amd64

1. Build a single shared library
2. Build a cgo application that links against that library
3. Remove the cache directory (`~/.cache/go-build/`)
4. Build the same cgo application again using the same shared library

### What did you see happen?

The `.note.go.buildid` and subsequently the `.note.gnu.build-id` sections of the ELF file will be different.

### What did you expect to see?

Given the same library, source files, build path, and build arguments that the output binary should be the same.","['NeedsInvestigation', 'compiler/runtime']",0
golang/go,crypto/tls: certificate chains aren't re-checked on resumption,"As part of the report which led to #77113, @rbqvq, also noted that we don't reverify certificate chains during resumption. This has been the default behavior of Go since 1.21 (see #31641 for the discussion of this at the time), when we resolved our previous inconsistent behavior and landed on not reverifying certificates during resumption (and as such documented `VerifyPeerCertificates` was not called for resumed connections).

The issue as reported was that if the underlying trust configuration changed after an initial connection, this could cause session tickets to be used to resume a connection based on previous information that would otherwise be rejected if used during a fresh handshake. It is our belief that this does not represent a significant security issue, and that this behavior matches the behavior of a number of other implementations (namely BoringSSL and OpenSSL).

Firstly, it is documented to be a API violation to mutate a `tls.Config` once it has been passed to a TLS function (""After [a tls.Config] has been passed to a TLS function it must not be modified""), so mutating any of the `Config` fields which define trust (`Config.Roots`, `Config.ClientCAs`, `Config.ClientAuth`, `Config.VerifyConnection`, etc) while a `Config` is in-use is already a misuse of crypto/tls.

There was a related security issue with respect to using `Config.Clone` to clone a `Config` for mutation that retained the automatically generated session ticket keys for the cloned config (see #77113) that has since been resolved.

We also provide an advanced API, `Config.SetSessionTicketKeys`, which allows explicit user control over the session ticket keys, disabling our automatic generation and rotation of said keys. We believe making changes to the trust configuration of a `Config` but retaining the same keys is a misuse of this API (i.e. by not calling `Config.SetSessionTicketKeys` with a new set of keys after mutating the relevant trust configurations fields). Additionally, using the same set of session ticket keys across backends that are explicitly configured to have different trust configurations is a misuse of the API. I believe our documentation here is somewhat lacking and should be improved to detail that this is a rather sharp edged API, and should be used carefully with respect to these properties.

Skipping certificate chain verification is one of the main wins we get from session resumption, but it is worth considering if there is anything we can do to improve the security of resumption without reducing the performance. @rbqvq had some ideas here which are worth investigating.

cc @golang/security @FiloSottile 
","['NeedsDecision', 'LibraryProposal']",0
golang/go,"cmd/compile, cmd/asm, cmd/cgo: -trimpath with relative paths has no effect","The flag `-trimpath=.` has no effect because relative paths are not resolved.
This forces build systems to generate commands with absolute paths in the `-trimpath` flag:

```shell
$ go tool compile -trimpath=$(pwd) foo/foo.go && go tool objdump foo.o
TEXT <unlinkable>.Foo(SB) foo/foo.go        # trimmed

$ go tool compile -trimpath=. foo/foo.go && go tool objdump foo.o
TEXT <unlinkable>.Foo(SB) /Users/user/project/foo/foo.go   # not trimmed
```

## Why this matters

Build systems like Buck generate build actions that are independent from the host machine. Using `-trimpath=$(pwd)` embeds the absolute path into the build action, breaking reproducibility across different executor machines. With `-trimpath=.`, build actions remain portable and produce identical artifacts regardless of where they run.

Currently, we work around this by using wrapper scripts for the compiler that resolve absolute paths on the build machine. This adds unnecessary latency and complexity.

## Proposed fix

In `cmd/internal/objabi/line.go`, convert relative prefixes to absolute:

```go
if prefix != """" && !filepath.IsAbs(prefix) {
    if abs, err := filepath.Abs(prefix); err == nil {
        prefix = abs
    }
}
```

## Test plan

```shell
$ (cd $GO/src && ./make.bash)
$ $GO/bin/go tool compile -trimpath=. foo/foo.go && go tool objdump foo.o
TEXT <unlinkable>.Foo(SB) foo/foo.go        # trimmed
```","['NeedsInvestigation', 'compiler/runtime']",0
golang/go,x/build/cmd/relui: automate process of upstreaming private-track security CLs on release day (for release branches),"By default, Go releases are built from the tip of their corresponding release branch, which gets modified only to update the content of the VERSION file during the release process. Doing this isn't viable whenever a Go release contains private-track security fixes, since details of said security fixes cannot be made public until the pre-announced release day, we want to ensure the release is built and tested ahead of that time.

To facilitate this, relui supports specifying a ref in an internal repository, and we arrange for that ref to contain the contents of the public release branch with the private-track security fix CLs applied on top. On the pre-announced release day, those private-track security fix CLs are upstreamed to the public release branch, tagged, and the release is published.

This issue tracks having relui automate the upstreaming process. Currently, relui relies on the release coordinator to handle the step of upstreaming security fix CLs out of band, and it merely checks that the eventual public release branch content matches that which was specified in the custom ref (to catch the possibility of a mistake or skew). 

This was something we were going to get to after having security metadata, but there seems to be a good opportunity reduce quite a bit of toil for release coordinators even before we have that. We know that the security ref has the exact git tree (ignoring the VERSION file) that the public release branch needs to be come, and Roland's already built out the process to upstream a single security fix for x/ repos in [CL 559295](https://go.dev/cl/559295) (CC @rolandshoemaker). It should be viable to handle a stack of CLs by mailing the releases' specified ref to refs/for/release-branch.go1.NN. After security metadata is added (tracked in issue #76157, CC @thatnealpatel) we can do more, like verify commit message issue references, but until then this automation can handle upstreaming CL content as is.

Leaving a few things explicitly out of scope to make this issue more actionable:
- upstreaming to main branch - this has a lot in common to release branches, but we don't have an existing ref for this and there are more considerations for how to handle potential merge conflicts. This makes for a good followup.
- dealing with vendored golang.org/x packages (https://go.dev/wiki/MinorReleases#cherry-pick-cls-for-vendored-golangorgx-packages) - this would involve quite different additional steps, but it happens rarely, and maybe will be even more rare in the future. Leaving it out for now, can be a followup if needed.

CC @golang/release, @prattmic.","['Builders', 'NeedsFix']",0
golang/go,"x/tools/gopls: when editing Go files in GOMODCACHE, offer to clone module and add replace directive","Being able to quickly edit the standard library (e.g. by inserting a print statement) is extremely useful during debugging. However, editing source files from dependencies within the module cache is not as convenient. Gopls could detect an attempt to modify such a file and offer to clone the module, add a [replace directive](https://go.dev/ref/mod#go-mod-file-replace) to the main module, and move the cursor to the corresponding position in the cloned file, which can then be freely edited. The replace directive could be flagged with a `// FIXME: debugging` comment, with a code action to revert it.

(Credit to @prattmic for the idea.)","['help wanted', 'FeatureRequest', 'gopls', 'Tools']",0
golang/go,"x/tools/gopls: spurious ""connection closed"" in mcptools/symbol_references.txt marker test","Typical symptom ([from CL 737000](https://logs.chromium.org/logs/golang/buildbucket/cr-buildbucket/8692496879752621297/+/u/step/35/log/2)):
```
=== RUN   Test/mcptools/symbol_references.txt
=== PAUSE Test/mcptools/symbol_references.txt
=== CONT  Test/mcptools/symbol_references.txt
    marker_test.go:2627: go.mod:6:3 (mcptools/symbol_references.txt:13:3): failed to call mcp tool: calling ""tools/call"": connection closed
    marker_test.go:2627: go.mod:7:3 (mcptools/symbol_references.txt:14:3): failed to call mcp tool: calling ""tools/call"": connection closed
    marker_test.go:2627: go.mod:8:3 (mcptools/symbol_references.txt:15:3): failed to call mcp tool: calling ""tools/call"": connection closed
--- FAIL: Test/mcptools/symbol_references.txt (0.36s)
```
","['gopls', 'Tools', 'BugReport']",0
golang/go,cmd/go: TestScript/mod_get_insecure_redirect failures,"```
#!watchflakes
default <- pkg == ""cmd/go"" && test == ""TestScript/mod_get_insecure_redirect""
```

Issue created automatically to collect these failures.

Example ([log](https://ci.chromium.org/b/8692527742095428353)):

    === RUN   TestScript/mod_get_insecure_redirect
    === PAUSE TestScript/mod_get_insecure_redirect
    === CONT  TestScript/mod_get_insecure_redirect
        script_test.go:139: 2026-01-16T16:02:28Z
        script_test.go:141: $WORK=/home/swarming/.swarming/w/ir/x/t/cmd-go-test-216576930/tmpdir1772520228/mod_get_insecure_redirect218758677
        script_test.go:163: 
            PATH=/home/swarming/.swarming/w/ir/x/t/cmd-go-test-216576930/tmpdir1772520228/testbin:/home/swarming/.swarming/w/ir/x/w/goroot/bin:/home/swarming/.swarming/w/ir/x/w/goroot/bin:/home/swarming/.swarming/w/ir/x/w/goroot/bin:/home/swarming/.swarming/w/ir/cache/tools/bin:/home/swarming/.swarming/w/ir/bbagent_utility_packages:/home/swarming/.swarming/w/ir/bbagent_utility_packages/bin:/home/swarming/.swarming/w/ir/cipd_bin_packages:/home/swarming/.swarming/w/ir/cipd_bin_packages/bin:/home/swarming/.swarming/w/ir/cache/cipd_client:/home/swarming/.swarming/w/ir/cache/cipd_client/bin:/home/swarming/.swarming/cipd_cache/bin:/command:/usr/local/bin:/usr/local/sbin:/usr/pkg/bin:/usr/pkg/sbin:/bin:/sbin:/usr/bin:/usr/sbin:/usr/X11R6/bin
            HOME=/no-home
            CCACHE_DISABLE=1
            GOARCH=arm64
    ...
            [condition not met]
            > env GOPRIVATE=vcs-test.golang.org
            > ! go get -d vcs-test.golang.org/insecure/go/insecure
            [stderr]
            go: -d flag is deprecated. -d=true is a no-op
            go: unrecognized import path ""vcs-test.golang.org/insecure/go/insecure"": https fetch: Get ""https://127.0.0.1:64809/insecure/go/insecure?go-get=1"": net/http: TLS handshake timeout
            [exit status 1]
            > stderr 'redirected .* to insecure URL'
        script_test.go:163: FAIL: testdata/script/mod_get_insecure_redirect.txt:9: stderr 'redirected .* to insecure URL': no match for `(?m)redirected .* to insecure URL` in stderr
    --- FAIL: TestScript/mod_get_insecure_redirect (16.04s)


â€” [watchflakes](https://go.dev/wiki/Watchflakes)
",['NeedsInvestigation'],0
golang/go,cmd/link/internal/riscv64: wrong dynamic loader path on Linux,"On linux/riscv64, the dynamic loader path hardcoded into binaries using cgo and the internal linker is `/lib/ld.so.1`. The reality is that no major distro puts the dynamic loader in there.

The canonical path is `/lib/ld-linux-riscv64-lp64d.so.1`. You can test this, for example, using the official Go docker image for linux/riscv64:

```
docker run --platform linux/riscv64 golang:1.25 stat /lib/ld-linux-riscv64-lp64d.so.1 // succeeds
docker run --platform linux/riscv64 golang:1.25 stat /lib/ld.so.1 // fails
```

@golang/riscv64 ","['OS-Linux', 'NeedsInvestigation', 'arch-riscv', 'compiler/runtime', 'BugReport']",0
golang/go,cmd/cgo/internal/testerrors: TestReportsTypeErrors/err2.go failures,"```
#!watchflakes
default <- pkg == ""cmd/cgo/internal/testerrors"" && test == ""TestReportsTypeErrors/err2.go""
```

Issue created automatically to collect these failures.

Example ([log](https://ci.chromium.org/b/8692592288432963137)):

    === RUN   TestReportsTypeErrors/err2.go
    === PAUSE TestReportsTypeErrors/err2.go


â€” [watchflakes](https://go.dev/wiki/Watchflakes)
",['NeedsInvestigation'],0
golang/go,internal/trace: TestTraceStacks/Stress failures,"```
#!watchflakes
default <- pkg == ""internal/trace"" && test == ""TestTraceStacks/Stress""
```

Issue created automatically to collect these failures.

Example ([log](https://ci.chromium.org/b/8692584763684013057)):

    === RUN   TestTraceStacks/Stress
        reader_test.go:122: sync 274 has non-increasing trace time: 10872849644928 vs 10872849652608
        helpers.go:38: Trace Go1.26
            EventBatch gen=1 m=18446744073709551615 time=169886663181 size=25
            Sync
            Frequency freq=15625000
            ClockSnapshot dt=31 mono=10872746445625 sec=1768506162 nsec=474306000
            EventBatch gen=1 m=6097367040 time=169886667310 size=43
            ProcStatus dt=23 p=1 pstatus=2
            ProcStart dt=1 p=1 p_seq=1
    ...
            String id=89
            	data=""/Users/swarming/.swarming/w/ir/x/w/goroot/src/runtime/mgc.go""
            String id=90
            	data=""runtime.runFinalizers""
            String id=91
            	data=""/Users/swarming/.swarming/w/ir/x/w/goroot/src/runtime/mfinal.go""
            EndOfGeneration
            
        helpers.go:39: Convert this to a raw trace with `go test internal/trace/testtrace -convert in.tracetxt -out out.trace`
    --- FAIL: TestTraceStacks/Stress (1.24s)


â€” [watchflakes](https://go.dev/wiki/Watchflakes)
","['NeedsInvestigation', 'compiler/runtime']",0
PySimpleGUI/PySimpleGUI,[Bug?] [Solved] Menu not generating an event on latest Raspbian Pi4 ,"Bug?
----------------------------------------
#### Operating System
bookworm on Pi4
glibc 2.36
3.11.2 (main, Nov 30 2024, 21:22:50) [GCC 12.2.0]3.11.2 (main, Nov 30 2024, 21:22:50) [GCC 12.2.0]

#### PySimpleGUI 5.0.8 
#### tkinter 8.6.13 

#### Python version 3.11.2 (main, Nov 30 2024, 21:22:50) [GCC 12.2.0]

---------------------

#### Your Experience In Months or Years (optional)

Years Python programming experience
20

Years Programming experience overall
40

Have used another Python GUI Framework? (tkinter, Qt, etc) (yes/no is fine)
yes

---------------------

#### Troubleshooting

These items may solve your problem. Please check those you've done by changing - [ ] to - [X]

- [X] Searched main docs for your problem [PySimpleGUI Documenation](https://docs.PySimpleGUI.com)
- [X] Looked for Demo Programs that are similar to your goal. It is recommend you use the Demo Browser! [Demo Programs](Demos.PySimpleGUI.com)
- [X] None of your GUI code was generated by an AI algorithm like GPT
- [X] If not tkinter - looked for Demo Programs for specific port
- [X] For non tkinter - Looked at readme for your specific port if not PySimpleGUI (Qt, WX, Remi)
- [X] Run your program outside of your debugger (from a command line)
- [X] Searched through Issues (open and closed) to see if already reported Issues.PySimpleGUI.com
- [X] Have upgraded to the latest release of PySimpleGUI on PyPI (lastest official version)
- [ ] Tried running the Development Build.  Your problem may have already been fixed but not released. Check Home Window for release notes and upgrading capability
- [ ] For licensing questions please email license@PySimpleGUI.com

#### Detailed Description
JUST A WHOLE LOT OF NOTHING returned on menu click.
Sub-menu pops up and goes away just fine, but no event generated.
Works OK on my Ubuntu desktop - just not here on my Pi4.


```python
import PySimpleGUI as sg

layoutPRINT = [
    [sg.Button('Print your TAB',  disabled=False, key='-tab_print-')],
    [sg.Button('and Finalize it', disabled=False, key='-tab_erase-')],
]

menu_def=[['New User',['Register']]]

layout=[
    [sg.Menu(menu_def)],
    [sg.Frame("""", layoutPRINT)],
]

window = sg.Window('menu-play', [layout], finalize=True, keep_on_top=True)

while True:

    event, values = window.read(timeout=100)

    if event != sg.TIMEOUT_EVENT:
        print(event)

    if event == sg.WIN_CLOSED:
        break

window.close()
```
",[],0
PySimpleGUI/PySimpleGUI,[ Bug ]  License Key Validation Failed dialog....  Commercial license incorrectly flagged as expired,"### Type of Issue (Enhancement, Error, Bug, Question)

Bug
----------------------------------------

#### Operating System

Windows, Linux, Mac


#### PySimpleGUI Port (tkinter, Qt, Wx, Web)

tkinter

----------------------------------------

## Versions (NOTE - PSG4 is no longer supported)

Version information can be obtained by calling `sg.main_get_debug_data()`
Or you can print each version shown in ()


#### Python version (`sg.sys.version`)

3.*

#### PySimpleGUI Version (`sg.__version__`)

3.0.9 and below

#### GUI Version  (tkinter (`sg.tclversion_detailed`), PySide2, WxPython, Remi)

8.6.*

### Priority Support Code - Only Commercially Licensed Users Receive Support as of Feb 2025

ekydJxM3


---------------------

#### Your Experience In Months or Years (optional)



---------------------

#### Troubleshooting

These items may solve your problem. Please check those you've done by changing - [ ] to - [X]

- [ X  ] Searched main docs for your problem [PySimpleGUI Documenation](https://docs.PySimpleGUI.com)
- [ ] Looked for Demo Programs that are similar to your goal. It is recommend you use the Demo Browser! [Demo Programs](Demos.PySimpleGUI.com)
- [ X ] None of your GUI code was generated by an AI algorithm like GPT
- [ ] If not tkinter - looked for Demo Programs for specific port
- [ ] For non tkinter - Looked at readme for your specific port if not PySimpleGUI (Qt, WX, Remi)
- [ ] Run your program outside of your debugger (from a command line)
- [ ] Searched through Issues (open and closed) to see if already reported Issues.PySimpleGUI.com
- [ X ] Have upgraded to the latest release of PySimpleGUI on PyPI (lastest official version)
- [ X ] Tried running the Development Build.  Your problem may have already been fixed but not released. Check Home Window for release notes and upgrading capability
- [ X ] For licensing questions please email license@PySimpleGUI.com

#### Detailed Description

There's a bug in the PySimpleGUI 5 code that is flagging Commercial Developer Licenses as ""expired"".  If you've purchased a Commercial License, then the license to run your code does not expire.  If you see an error dialog when using your Commercial Key, then you've hit this bug.  Note that Hobbyist keys DO expire.

A FIX was released this morning in version 5.0.10 that you'll find both as a maintenance release as well as on the PySimpleGUI Private PyPI server.

Reminder that this is the command upgrade to the latest from the Private Server:
`python -m pip install --upgrade --extra-index-url https://PySimpleGUI.net/install  PySimpleGUI`

or for Mac/Linux

`python3 -m pip install --upgrade --extra-index-url https://PySimpleGUI.net/install  PySimpleGUI`




#### Screenshot, Sketch, or Drawing

![Image](https://github.com/user-attachments/assets/84e46a1f-115a-4235-a4dd-21642badb257)
","['Done - Install Dev Build (see docs for how)', 'Port - TK', 'PySimpleGUI 5', 'License']",0
PySimpleGUI/PySimpleGUI,[Question] Verticalseperator bar does not extend for 3 rows,"## Type of Issue (Enhancement, Error, Bug, Question)
Question

----------------------------------------

#### Operating System
windows 11

#### PySimpleGUI Port (tkinter, Qt, Wx, Web)

Qt

----------------------------------------

## Versions (NOTE - PSG4 is no longer supported)

#### Python version (`sg.sys.version`)

Python 3.12.8

#### PySimpleGUI Version (`sg.__version__`)

PySimpleGUIQt      5.0.0


### Priority Support Code (Commercial License Users)

#### Your Experience In Months or Years (optional)

10 months Python programming experience

64 Years Programming experience overall

Have used another Python GUI Framework? (tkinter, Qt, etc) (yes/no is fine)

no

---------------------

#### Troubleshooting

These items may solve your problem. Please check those you've done by changing - [ ] to - [X]

- [x] Searched main docs for your problem [PySimpleGUI Documenation](https://docs.PySimpleGUI.com)
- [x] Looked for Demo Programs that are similar to your goal. It is recommend you use the Demo Browser! [Demo Programs](Demos.PySimpleGUI.com)
- [x] None of your GUI code was generated by an AI algorithm like GPT
- [x] If not tkinter - looked for Demo Programs for specific port
- [x] For non tkinter - Looked at readme for your specific port if not PySimpleGUI (Qt, WX, Remi)
- [x] Run your program outside of your debugger (from a command line)
- [x] Searched through Issues (open and closed) to see if already reported Issues.PySimpleGUI.com
- [x] Have upgraded to the latest release of PySimpleGUI on PyPI (lastest official version)
- [x] Tried running the Development Build.  Your problem may have already been fixed but not released. Check Home Window for release notes and upgrading capability
- [x] For licensing questions please email license@PySimpleGUI.com

#### Detailed Description

verticalseperator bar does not extend for 3 rows

documentation say pad = 0 on column, i get an error.
expand = expamf_y on vertical seperator gets error


#### Code To Duplicate

```python
import PySimpleGUIQt as sg

col1 = sg.Column([[sg.Text('Line 1')],
                  [sg.Text('Line 2')],    
                  [sg.Text('Line 3')]], pad=(0,0))

col2 = sg.Column([[sg.Text('Line 1')],
                  [sg.Text('Line 2')],
                  [sg.Text('Line 3')]], pad=(0,0))

layout = [[col1, sg.VerticalSeparator(), col2]]

window = sg.Window('File Browse Example', layout)
event, values = window.read()

window.close()
```

#### Screenshot, Sketch, or Drawing

--------------------
![image](https://github.com/user-attachments/assets/2d4d3774-c12c-4b2a-a55f-ba8772b61593)

-

### Watcha Makin?

If you care to share something about your project, it would be awesome to hear what you're building.
",['Port - Qt'],0
PySimpleGUI/PySimpleGUI,[Error]  Website typos,"### Type of Issue (Enhancement, Error, Bug, Question)

Error
----------------------------------------

#### Operating System

n/a

#### PySimpleGUI Port (tkinter, Qt, Wx, Web)

n/a

----------------------------------------

## Versions (NOTE - PSG4 is no longer supported)

Version information can be obtained by calling `sg.main_get_debug_data()`
Or you can print each version shown in ()


#### Python version (`sg.sys.version`)

n/a

#### PySimpleGUI Version (`sg.__version__`)

n/a

#### GUI Version  (tkinter (`sg.tclversion_detailed`), PySide2, WxPython, Remi)

n/a

#### GUI Version  (tkinter (`sg.tclversion_detailed`), PySide2, WxPython, Remi)

n/a

### Priority Support Code (Commercial License Users)

038ab42367c1

---------------------

#### Your Experience In Months or Years (optional)

Years Python programming experience

Years Programming experience overall

Have used another Python GUI Framework? (tkinter, Qt, etc) (yes/no is fine)

Anything else you think would be helpful?


---------------------

#### Troubleshooting

These items may solve your problem. Please check those you've done by changing - [ ] to - [X]

- [X] Searched main docs for your problem [PySimpleGUI Documenation](https://docs.PySimpleGUI.com)
- [ ] Looked for Demo Programs that are similar to your goal. It is recommend you use the Demo Browser! [Demo Programs](Demos.PySimpleGUI.com)
- [ ] None of your GUI code was generated by an AI algorithm like GPT
- [ ] If not tkinter - looked for Demo Programs for specific port
- [ ] For non tkinter - Looked at readme for your specific port if not PySimpleGUI (Qt, WX, Remi)
- [ ] Run your program outside of your debugger (from a command line)
- [X] Searched through Issues (open and closed) to see if already reported Issues.PySimpleGUI.com
- [ ] Have upgraded to the latest release of PySimpleGUI on PyPI (lastest official version)
- [ ] Tried running the Development Build.  Your problem may have already been fixed but not released. Check Home Window for release notes and upgrading capability
- [ ] For licensing questions please email license@PySimpleGUI.com

#### Detailed Description

Typo on [Who Is PySimpleGUI Designed For?](https://docs.pysimplegui.com/en/latest/documentation/what_is_it/) page: in paragraph following GUI Bar image, the last sentence fails a semantic check. This text: ""The requirement is not only to understand how to use classes, but programmers are required to write their own classes <ins>***was***</ins> well when utilizing the other popular Python GUI frameworks.""  should probably be: ""The requirement is not only to understand how to use classes, but programmers are required to write their own classes <ins>***as***</ins> well when utilizing the other popular Python GUI frameworks."", instead.


#### Code To Duplicate

A **short** program that isolates and demonstrates the problem (Do not paste your massive program, but instead 10-20 lines that clearly show the problem)

This pre-formatted code block is all set for you to paste in your bit of code:

```python

# Paste your code here
print('Not applicable')

```

#### Screenshot, Sketch, or Drawing

---------------------

### Watcha Makin?

If you care to share something about your project, it would be awesome to hear what you're building.
","['documentation', 'PySimpleGUI 5']",0
PySimpleGUI/PySimpleGUI,"[ Bug]  TclError with PySimpleGUI, Python 3.13 and tcl-tk 9.0.0_1","### Type of Issue (Enhancement, Error, Bug, Question)

Bug

----------------------------------------

#### Operating System

MacOS 14.7 (arm)

#### PySimpleGUI Port (tkinter, Qt, Wx, Web)

tkinter

----------------------------------------

## Versions (NOTE - PSG4 is no longer supported)

Python 3.13.0
PySimpleGUI version: 5.0.7
tcl-tk 9.0.0_1
python-tk  3.13.0

---------------------

#### Your Experience In Months or Years (optional)

5+

Years Programming experience overall

10+

Anything else you think would be helpful?

Error reported: `_tkinter.TclError: bad option ""variable"": must be add, info, or remove`

---------------------

#### Troubleshooting

These items may solve your problem. Please check those you've done by changing - [ ] to - [X]

- [x] Searched main docs for your problem [PySimpleGUI Documenation](https://docs.PySimpleGUI.com)
- [x] Looked for Demo Programs that are similar to your goal. It is recommend you use the Demo Browser! [Demo Programs](Demos.PySimpleGUI.com)
- [x] None of your GUI code was generated by an AI algorithm like GPT
- [x] If not tkinter - looked for Demo Programs for specific port
- [x] For non tkinter - Looked at readme for your specific port if not PySimpleGUI (Qt, WX, Remi)
- [x] Run your program outside of your debugger (from a command line)
- [x] Searched through Issues (open and closed) to see if already reported Issues.PySimpleGUI.com
- [x] Have upgraded to the latest release of PySimpleGUI on PyPI (latest official version)
- [ ] Tried running the Development Build.  Your problem may have already been fixed but not released. Check Home Window for release notes and upgrading capability
- [ ] For licensing questions please email license@PySimpleGUI.com

#### Detailed Description

Using Python 3.13 with tcl-tk version 9.0.0_1 causes the error  `_tkinter.TclError: bad option ""variable"": must be add, info, or remove` 

Reverting to Using Python 3.11 with tcl-tk version 8.6.15 works without error


#### Code To Duplicate

Installing the older version of tcl-tk with python 3.11 allows PySimpleGUI to work, it doesn't seem to work when python 3.12 or python3.13 is used with tcl-tk 9.0.0_1 

Sample code from the site was used to test:

```python

import PySimpleGUI as sg

# All the stuff inside your window.
layout = [  [sg.Text('Some text on Row 1')],
            [sg.Text('Enter something on Row 2'), sg.InputText()],
            [sg.Button('Ok'), sg.Button('Cancel')] ]

# Create the Window
window = sg.Window('Window Title', layout)

# Event Loop to process ""events"" and get the ""values"" of the inputs
while True:
    event, values = window.read()
    if event == sg.WIN_CLOSED or event == 'Cancel':	# if user closes window or clicks cancel
        break
    print('You entered ', values[0])

window.close()


```

---------------------

### Watcha Makin?

Testing PySimpleGUI 
","['Done - Install Dev Build (see docs for how)', 'Platform Specific Issue - Mac', 'Port - TK']",0
PySimpleGUI/PySimpleGUI,[Bug]  Crash with PySimpleGUI + psgtray + PyGame,"### Type of Issue (Enhancement, Error, Bug, Question)
Bug/Crash

----------------------------------------

#### Operating System
Windows 11

#### PySimpleGUI Port (tkinter, Qt, Wx, Web)
tkintr

----------------------------------------

## Versions (NOTE - PSG4 is no longer supported)
Python Interpeter: C:\Users\vmix\GIL-crash\GIL-crash\.venv\Scripts\python.exe
Python version: 3.12.3
Platform: Windows
Platform version: ('11', '10.0.22631', 'SP0', 'Multiprocessor Free')
Port: PySimpleGUI
tkinter version: 8.6.13
PySimpleGUI version: 5.0.6
PySimpleGUI filename: C:\Users\vmix\GIL-crash\GIL-crash\.venv\Lib\site-packages\PySimpleGUI\PySimpleGUI.py

---------------------

#### Your Experience In Months or Years (optional)

Years Python programming experience
5+

Years Programming experience overall
30+

Have used another Python GUI Framework? (tkinter, Qt, etc) (yes/no is fine)
no

Anything else you think would be helpful?

This may be related to https://github.com/PySimpleGUI/PySimpleGUI/issues/4880 because it only happens when window.read(timeout), not with window.read()

---------------------

#### Troubleshooting

These items may solve your problem. Please check those you've done by changing - [ ] to - [X]

- [X] None of your GUI code was generated by an AI algorithm like GPT
- [X] Searched through Issues (open and closed) to see if already reported Issues.PySimpleGUI.com
- [X] Have upgraded to the latest release of PySimpleGUI on PyPI (lastest official version)
- [X] duplicated the problem running python from the command line
- [X] problem originally found using Python 3.10, reproduced with Python 3.12

#### Detailed Description

Problem: Program crashes with error
```
Fatal Python error: PyEval_RestoreThread: the function must be called with the GIL held, but the GIL is released (the current Python thread state is NULL)
```
The full error dump is below

Duplicating the problem requires all of the following conditions:
1. import pygame and call pygame.init()â€¨
2. add a menu to the main PySimpleGUI windowâ€¨
3. create a psgtray SystemTray 
4. â€¨call window.read(timeout) - the crash does not occur if no timeoutâ€¨
5. call pygame.events.get() - the crash does not occur is this is not calledâ€¨
6. select an item from the window menu - the crash occurs when the item is selected, the window.read() callâ€¨   does not returnâ€¨

#### Code To Duplicate

I was able to duplicate the problem by inserting code for the above into a reduced version of the psgtray sample program. I have not been able to reduce it to 10-20 lines

```python

import PySimpleGUI as sg
from psgtray import SystemTray
import pygame

""""""
    Demonstrate GIL Crash with PyGame + psgtray
    the following are necessary for the crash:
    1. import pygame and call pygame.init()
    2. add a menu to the main PySimpleGUI window
    3. create a psgtray SystemTray 
    4. call window.read(timeout) - the crash does not occur if no timeout
    5. call pygame.events.get() - the crash does not occur is this is not called
    7. select an item from the window menu - the crash occurs when the item is selected, the window.read() call
       does not return
""""""

def main():

    pygame.init( )

    menu = ['',
            ['Show Window', 'Hide Window', 'Exit']]
    tooltip = 'Tooltip'

    winmenu = [['Menu',
            ['Show Window', 'Hide Window', 'Exit']]]

    layout = [[sg.Menu(winmenu)],
              [sg.Text('My PySimpleGUI Celebration Window - X will minimize to tray')],
              [sg.T('Double clip icon to restore or right click and choose Show Window')],
              [sg.Multiline(size=(60, 10), reroute_stdout=False, reroute_cprint=True, write_only=True, key='-OUT-')],
              [sg.Button('Go'), sg.B('Hide Icon'), sg.B('Show Icon'), sg.B('Hide Window'), sg.Button('Exit')]]

    window = sg.Window('Window Title', layout, finalize=True, enable_close_attempted_event=True)

    tray = SystemTray(menu, single_click_events=False, window=window, tooltip=tooltip, icon=sg.DEFAULT_BASE64_ICON)

    sg.cprint(sg.main_get_debug_data())
    sg.cprint(sg.get_versions())

    while True:
        event, values = window.read(10)

        if event == sg.TIMEOUT_EVENT:
            events = pygame.event.get()
            continue

        # IMPORTANT step. It's not required, but convenient. Set event to value from tray
        # if it's a tray event, change the event variable to be whatever the tray sent
        if event == tray.key:
            sg.cprint(f'System Tray Event = ', values[event], c='white on red')
            event = values[event]  # use the System Tray's event as if was from the window

        if event in (sg.WIN_CLOSED, 'Exit'):
            break

        sg.cprint(event, values)
        tray.show_message(title=event, message=values)

        if event in ('Show Window', sg.EVENT_SYSTEM_TRAY_ICON_DOUBLE_CLICKED):
            window.un_hide()
            window.bring_to_front()
        elif event in ('Hide Window', sg.WIN_CLOSE_ATTEMPTED_EVENT):
            window.hide()
            tray.show_icon()  # if hiding window, better make sure the icon is visible
            # tray.notify('System Tray Item Chosen', f'You chose {event}')


    tray.close()  # optional but without a close, the icon may ""linger"" until moused over
    window.close()

if __name__ == '__main__':
    main()

```

#### Screenshot, Sketch, or Drawing

---------------------

### Watcha Makin?

I am working on an enhanced version of the project https://github.com/International-Anglican-Church/visca-joystick , adding better window support, configuration, controller hot plugging ...

### Full Error dump

```
pygame 2.6.1 (SDL 2.28.4, Python 3.10.11)
Hello from the pygame community. https://www.pygame.org/contribute.html
Fatal Python error: PyEval_RestoreThread: the function must be called with the GIL held, but the GIL is released (the current Python thread state is NULL)
Python runtime state: initialized

Thread 0x00001164 (most recent call first):
  File ""C:\Users\vmix\psgtray-test\psgtray_test\.venv\lib\site-packages\pystray\_win32.py"", line 147 in _mainloop
  File ""C:\Users\vmix\psgtray-test\psgtray_test\.venv\lib\site-packages\pystray\_win32.py"", line 128 in _run
  File ""C:\Users\vmix\psgtray-test\psgtray_test\.venv\lib\site-packages\pystray\_base.py"", line 212 in run
  File ""C:\Users\vmix\psgtray-test\psgtray_test\.venv\lib\site-packages\psgtray\psgtray.py"", line 324 in _pystray_thread
  File ""C:\Users\vmix\AppData\Local\Programs\Python\Python310\lib\threading.py"", line 953 in run
  File ""C:\Users\vmix\AppData\Local\Programs\Python\Python310\lib\threading.py"", line 1016 in _bootstrap_inner
  File ""C:\Users\vmix\AppData\Local\Programs\Python\Python310\lib\threading.py"", line 973 in _bootstrap

Current thread 0x00003d8c (most recent call first):
  File ""C:\Users\vmix\psgtray-test\psgtray_test\psgtray-test.py"", line 70 in main
  File ""C:\Users\vmix\psgtray-test\psgtray_test\psgtray-test.py"", line 110 in <module>

Extension modules: PIL._imaging, pygame.base, pygame.constants, pygame.rect, pygame.rwobject, pygame.surflock, pygame.bufferproxy, pygame.math, pygame.surface, pygame.display, pygame.draw, pygame.event, pygame.imageext, pygame.image, pygame.joystick, pygame.key, pygame.mouse, pygame.time, pygame.mask, pygame.pixelcopy, pygame.transform, pygame.font, pygame.mixer_music, pygame.mixer, pygame.scrap, pygame._freetype, PIL._imagingmath, PIL._webp (total: 28)
(.venv) VMIX:psgtray_test vmix$ .venv/Scripts/python psgtray-test.py
pygame 2.6.1 (SDL 2.28.4, Python 3.10.11)
Hello from the pygame community. https://www.pygame.org/contribute.html
Fatal Python error: PyEval_RestoreThread: the function must be called with the GIL held, but the GIL is released (the current Python thread state is NULL)
Python runtime state: initialized

Thread 0x00003bd0 (most recent call first):
  File ""C:\Users\vmix\psgtray-test\psgtray_test\.venv\lib\site-packages\pystray\_win32.py"", line 147 in _mainloop
  File ""C:\Users\vmix\psgtray-test\psgtray_test\.venv\lib\site-packages\pystray\_win32.py"", line 128 in _run
  File ""C:\Users\vmix\psgtray-test\psgtray_test\.venv\lib\site-packages\pystray\_base.py"", line 212 in run
  File ""C:\Users\vmix\psgtray-test\psgtray_test\.venv\lib\site-packages\psgtray\psgtray.py"", line 324 in _pystray_thread
  File ""C:\Users\vmix\AppData\Local\Programs\Python\Python310\lib\threading.py"", line 953 in run
  File ""C:\Users\vmix\AppData\Local\Programs\Python\Python310\lib\threading.py"", line 1016 in _bootstrap_inner
  File ""C:\Users\vmix\AppData\Local\Programs\Python\Python310\lib\threading.py"", line 973 in _bootstrap

Current thread 0x00004404 (most recent call first):
  File ""C:\Users\vmix\psgtray-test\psgtray_test\psgtray-test.py"", line 70 in main
  File ""C:\Users\vmix\psgtray-test\psgtray_test\psgtray-test.py"", line 110 in <module>

Extension modules: PIL._imaging, pygame.base, pygame.constants, pygame.rect, pygame.rwobject, pygame.surflock, pygame.bufferproxy, pygame.math, pygame.surface, pygame.display, pygame.draw, pygame.event, pygame.imageext, pygame.image, pygame.joystick, pygame.key, pygame.mouse, pygame.time, pygame.mask, pygame.pixelcopy, pygame.transform, pygame.font, pygame.mixer_music, pygame.mixer, pygame.scrap, pygame._freetype, PIL._imagingmath, PIL._webp (total: 28)
(.venv) VMIX:psgtray_test vmix$ .venv/Scripts/python psgtray-test.py
pygame 2.6.1 (SDL 2.28.4, Python 3.10.11)
Hello from the pygame community. https://www.pygame.org/contribute.html
Fatal Python error: PyEval_RestoreThread: the function must be called with the GIL held, but the GIL is released (the current Python thread state is NULL)
Python runtime state: initialized

Thread 0x000041e0 (most recent call first):
  File ""C:\Users\vmix\psgtray-test\psgtray_test\.venv\lib\site-packages\pystray\_win32.py"", line 147 in _mainloop
  File ""C:\Users\vmix\psgtray-test\psgtray_test\.venv\lib\site-packages\pystray\_win32.py"", line 128 in _run
  File ""C:\Users\vmix\psgtray-test\psgtray_test\.venv\lib\site-packages\pystray\_base.py"", line 212 in run
  File ""C:\Users\vmix\psgtray-test\psgtray_test\.venv\lib\site-packages\psgtray\psgtray.py"", line 324 in _pystray_thread
  File ""C:\Users\vmix\AppData\Local\Programs\Python\Python310\lib\threading.py"", line 953 in run
  File ""C:\Users\vmix\AppData\Local\Programs\Python\Python310\lib\threading.py"", line 1016 in _bootstrap_inner
  File ""C:\Users\vmix\AppData\Local\Programs\Python\Python310\lib\threading.py"", line 973 in _bootstrap

Current thread 0x000045b4 (most recent call first):
  File ""C:\Users\vmix\psgtray-test\psgtray_test\psgtray-test.py"", line 70 in main
  File ""C:\Users\vmix\psgtray-test\psgtray_test\psgtray-test.py"", line 110 in <module>

Extension modules: PIL._imaging, pygame.base, pygame.constants, pygame.rect, pygame.rwobject, pygame.surflock, pygame.bufferproxy, pygame.math, pygame.surface, pygame.display, pygame.draw, pygame.event, pygame.imageext, pygame.image, pygame.joystick, pygame.key, pygame.mouse, pygame.time, pygame.mask, pygame.pixelcopy, pygame.transform, pygame.font, pygame.mixer_music, pygame.mixer, pygame.scrap, pygame._freetype, PIL._imagingmath, PIL._webp (total: 28)
```",['Interoperability'],0
PySimpleGUI/PySimpleGUI,[Bug] Keyboard spontaneously stops entering text into PSG fields,"### Type of Issue (Enhancement, Error, Bug, Question)
Bug

----------------------------------------

#### Operating System

Alma Linux 9

#### PySimpleGUI Port (tkinter, Qt, Wx, Web)

tkinter

----------------------------------------

## Versions (NOTE - PSG4 is no longer supported)

Python Interpeter: /usr/bin/python3
Python version: 3.9.18
Platform: Linux
Platform version: ('glibc', '2.34')
Port: PySimpleGUI
tkinter version: 8.6.10
PySimpleGUI version: 5.0.6

### Priority Support Code (Commercial License Users)

n/a

---------------------

#### Your Experience In Months or Years (optional)

Years Python programming experience: 8

Years Programming experience overall: 9

Have used another Python GUI Framework? (tkinter, Qt, etc) (yes/no is fine) yes

Anything else you think would be helpful?


---------------------

#### Troubleshooting

These items may solve your problem. Please check those you've done by changing - [ ] to - [X]

- [X] Searched main docs for your problem [PySimpleGUI Documenation](https://docs.PySimpleGUI.com)
- [X] Looked for Demo Programs that are similar to your goal. It is recommend you use the Demo Browser! [Demo Programs](Demos.PySimpleGUI.com)
- [X] None of your GUI code was generated by an AI algorithm like GPT
- [ ] If not tkinter - looked for Demo Programs for specific port
- [ ] For non tkinter - Looked at readme for your specific port if not PySimpleGUI (Qt, WX, Remi)
- [X] Run your program outside of your debugger (from a command line)
- [X] Searched through Issues (open and closed) to see if already reported Issues.PySimpleGUI.com
- [x] Have upgraded to the latest release of PySimpleGUI on PyPI (lastest official version)
- [ ] Tried running the Development Build.  Your problem may have already been fixed but not released. Check Home Window for release notes and upgrading capability
- [ ] For licensing questions please email license@PySimpleGUI.com

#### Detailed Description

I am running a GUI in Alma Linux 9 that interfaces with several classes I have written, which interface with several external systems. Sometimes, after the GUI has been in use for a bit, the keyboard spontaneously stops working. I have not been able to understand what causes this, nor been able to predict the issue. The keyboard still works in other windows, and I can still interact with the GUI in other ways (pressing buttons, etc.), so I believe it is an issue between PSG and the keyboard. I recently upgraded the operating system of the PC from Centos 7 to Alma 9; in Centos 7 this issue was not observed, and the code has not changed. 

In debugging this, I have added `enable_events=True` to a few of the text boxes and told the GUI to print every event it sees. I have also added the following block which runs a keyboard listener while the GUI event loop runs:
```python
from pynput import keyboard
def on_press(key):
    try:
        print('alphanumeric key {0} pressed'.format(
            key.char))
    except AttributeError:
        print('special key {0} pressed'.format(
            key))

def on_release(key):
    print('{0} released'.format(
        key))
    if key == keyboard.Key.esc:
        # Stop listener                                                                                                                                                                                            
        return False

listener = keyboard.Listener(
    on_press=on_press,
    on_release=on_release)
listener.start()
```
When the keyboard issue I am discussing occurs, the events generated by buttons and checkboxes still appear in the terminal output, but no events generated by `sg.Input` fields show up. However, the listener I pasted above still prints all key presses and releases that occur while the GUI window is in focus, but no text is entered into the GUI and no `sg.Input` events appear.

#### Code To Duplicate

I have not been able to understand nor isolate the issue, so I am not sure I can give duplication code. I know how helpful that is, but I am running out of ideas for how to solve this.

Thanks for any help you can provide!
",['Platform Specific Issue - Linux'],0
PySimpleGUI/PySimpleGUI,[Bug] Titlebars Cannot be Disabled in Android/PyDroid3,"### Type of Issue (Enhancement, Error, Bug, Question)
Bug

----------------------------------------

#### Operating System
Android 14 - PyDroid3


#### PySimpleGUI Port (tkinter, Qt, Wx, Web)
tkinter


----------------------------------------

## Versions (NOTE - PSG4 is no longer supported)

Version information can be obtained by calling `sg.main_get_debug_data()`
Or you can print each version shown in ()


#### Python version (`sg.sys.version`)
3.11.4


#### PySimpleGUI Version (`sg.__version__`)
5.0.6

#### GUI Version  (tkinter (`sg.tclversion_detailed`), PySide2, WxPython, Remi)
tkinter 8.6.10

#### GUI Version  (tkinter (`sg.tclversion_detailed`), PySide2, WxPython, Remi)


### Priority Support Code (Commercial License Users)



---------------------

#### Your Experience In Months or Years (optional)
1 year
Years Python programming experience
1 year
Years Programming experience overall
50 years
Have used another Python GUI Framework? (tkinter, Qt, etc) (yes/no is fine)
tkinter (briefly)
Anything else you think would be helpful?


---------------------

#### Troubleshooting

These items may solve your problem. Please check those you've done by changing - [ ] to - [X]

- [ x] Searched main docs for your problem [PySimpleGUI Documenation](https://docs.PySimpleGUI.com)
- [ ] Looked for Demo Programs that are similar to your goal. It is recommend you use the Demo Browser! [Demo Programs](Demos.PySimpleGUI.com)
- [x ] None of your GUI code was generated by an AI algorithm like GPT
- [ ] If not tkinter - looked for Demo Programs for specific port
- [ ] For non tkinter - Looked at readme for your specific port if not PySimpleGUI (Qt, WX, Remi)
- [ ] Run your program outside of your debugger (from a command line)
- [x ] Searched through Issues (open and closed) to see if already reported Issues.PySimpleGUI.com
- [ ] Have upgraded to the latest release of PySimpleGUI on PyPI (lastest official version)
- [ ] Tried running the Development Build.  Your problem may have already been fixed but not released. Check Home Window for release notes and upgrading capability
- [ ] For licensing questions please email license@PySimpleGUI.com

#### Detailed Description
This is a low priority, aesthetic, issue restricted to PySimpleGUI in PyDroid3. It is mainly 'for your information'. I enjoy using PyDroid3 to write 'hobby' programs on my phone, and after struggling with tkinter GUIs, I was very happy to discover PySimpleGUI and how well it works overall in PyDroid3. 
	
Having Popups (and custom Window popups) always show a titlebar and frame looks a little messy on the small phone screen. At least they are functional, and a maximized Window does lose its titlebar and frame.




#### Code To Duplicate

A **short** program that isolates and demonstrates the problem (Do not paste your massive program, but instead 10-20 lines that clearly show the problem)

This pre-formatted code block is all set for you to paste in your bit of code:

```python
import PySimpleGUI as sg

HDG = """"""
================================================
 TitlebarsOnAndroid.py            v.2024-09-20b
 '''''''''''''''''''''
 When running a PySimpleGUI script on Android
 with PyDroid3, the 'no_titlebar' parameter for
 Windows and Popups is ignored. Only maximized
 Windows will not have a titlebar/frame.
 
 NB: This Window has 'no_titlebar=True'.
================================================
""""""
sg.theme('DarkGray16')
ISMAX=False
FNTL=(None,9)
FNTS=(None,5)
MONS=('Droid Sans Mono',5)

def ui():
  rowT = [
    sg.Text("" Titlebars On Android"",
            pad=0, font=FNTL,
            background_color='#4a5459',
            expand_x=True),
    sg.Text(""  x  "", key=""-CLOSE-"", 
            pad=0, font=FNTL,
            enable_events=True, 
            background_color='Maroon')
  ]
  
  rowD = [sg.Text(HDG, font=MONS, expand_x=True)]

  rowB = [
    sg.Button(""Toggle Max"", key='-MAX-',
              font=FNTS, expand_x=True),
    sg.Button('Popups', key='-POPUPS-',
              font=FNTS, expand_x=True)
  ]

  layout = [rowT, rowD, rowB]
  window = sg.Window(""Titlebars On Android"", 
                     layout,
                     size=(1000, 650),
                     location=(15,20),
                     grab_anywhere=True,
                     no_titlebar=True,
                     resizable=True, 
                     finalize=True)
  return window
# ==============================================
if __name__ == '__main__':
  print(HDG)
  win = ui()

  while True:
    ev, val = win.read()

    if ev in (sg.WIN_CLOSED, '-CLOSE-'):
      break

    if ev == '-MAX-':
      if ISMAX:
        win.normal()
        win.refresh()
        win.move(15,20)
      else:
        win.maximize()
      ISMAX=not ISMAX

    if ev == '-POPUPS-':
      sg.popup(""no_titlebar = False"",
               title=""Popup"",
               font=(None,6),
               non_blocking=True,
               location=(100,800),
               no_titlebar=False)
      sg.popup(""no_titlebar = True"",
               title=""Popup"",
               font=(None,6),
               non_blocking=True,
               location=(600,800),
               no_titlebar=True)

  # loop
  win.close()
  print(""\nDone.\n"")
# ==============================================
```

#### Screenshot, Sketch, or Drawing

![Screenshot_Titlebars_Pydroid3 (s)](https://github.com/user-attachments/assets/4c8cbc9c-7a9b-44cc-b005-7579847ce17c)


### Watcha Makin?

If you care to share something about your project, it would be awesome to hear what you're building.
",[],0
PySimpleGUI/PySimpleGUI,[ Enhancement/Bug/Question]  NOTE - you can also call sg.main() or sg.main_open_github_issue() to post an issue,"### Type of Issue (Enhancement, Error, Bug, Question)


----------------------------------------

#### Operating System



#### PySimpleGUI Port (tkinter, Qt, Wx, Web)



----------------------------------------

## Versions (NOTE - PSG4 is no longer supported)

Version information can be obtained by calling `sg.main_get_debug_data()`
Or you can print each version shown in ()


#### Python version (`sg.sys.version`)



#### PySimpleGUI Version (`sg.__version__`)


#### GUI Version  (tkinter (`sg.tclversion_detailed`), PySide2, WxPython, Remi)


#### GUI Version  (tkinter (`sg.tclversion_detailed`), PySide2, WxPython, Remi)


### Priority Support Code (Commercial License Users)



---------------------

#### Your Experience In Months or Years (optional)

Years Python programming experience

Years Programming experience overall

Have used another Python GUI Framework? (tkinter, Qt, etc) (yes/no is fine)

Anything else you think would be helpful?


---------------------

#### Troubleshooting

These items may solve your problem. Please check those you've done by changing - [ ] to - [X]

- [x] Searched main docs for your problem [PySimpleGUI Documenation](https://docs.PySimpleGUI.com)
- [x] Looked for Demo Programs that are similar to your goal. It is recommend you use the Demo Browser! [Demo Programs](Demos.PySimpleGUI.com)
- [x] None of your GUI code was generated by an AI algorithm like GPT
- [x] If not tkinter - looked for Demo Programs for specific port
- [x] For non tkinter - Looked at readme for your specific port if not PySimpleGUI (Qt, WX, Remi)
- [x] Run your program outside of your debugger (from a command line)
- [x] Searched through Issues (open and closed) to see if already reported Issues.PySimpleGUI.com
- [x] Have upgraded to the latest release of PySimpleGUI on PyPI (lastest official version)
- [x] Tried running the Development Build.  Your problem may have already been fixed but not released. Check Home Window for release notes and upgrading capability
- [x] For licensing questions please email license@PySimpleGUI.com

#### Detailed Description




#### Code To Duplicate

A **short** program that isolates and demonstrates the problem (Do not paste your massive program, but instead 10-20 lines that clearly show the problem)

This pre-formatted code block is all set for you to paste in your bit of code:

```python

# Paste your code here


```

#### Screenshot, Sketch, or Drawing

---------------------

### Watcha Makin?

If you care to share something about your project, it would be awesome to hear what you're building.
",[],0
PySimpleGUI/PySimpleGUI,[Bug] Custom Titlebar window focus problems,"## Type of Issue (Enhancement, Error, Bug, Question)

Bug

----------------------------------------

## Environment

#### Operating System

Linux  version ('glibc', '2.35')

#### PySimpleGUI Port (tkinter, Qt, Wx, Web)

tkinter

----------------------------------------



## Versions


#### Python version (`sg.sys.version`)

3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]

#### PySimpleGUI Version (`sg.__version__`)

5.0.4.15

#### GUI Version  (tkinter (`sg.tclversion_detailed`), PySide2, WxPython, Remi)

8.6.12



---------------------

## Your Experience In Months or Years (optional)

 Years Python programming experience
 Years Programming experience overall
No Have used another Python GUI Framework? (tkinter, Qt, etc) (yes/no is fine)


---------------------

## Troubleshooting

These items may solve your problem. Please check those you've done by changing - [ ] to - [X]
- [X] Searched main docs for your problem https://Docs.PySimpleGUI.com
- [X] Looked for Demo Programs that are similar to your goal.
It is recommend you use the Demo Browser! https://Demos.PySimpleGUI.com
- [ ] If not tkinter - looked for Demo Programs for specific port 
- [ ] For non tkinter - Looked at readme for your specific port if not PySimpleGUI (Qt, WX, Remi) 
- [ ] None of your GUI code was generated by an AI algorithm like GPT 
- [ ] Run your program outside of your debugger (from a command line) 
- [ ] Searched through Issues (open and closed) to see if already reported https://Issues.PySimpleGUI.com
- [ ] Have upgraded to the latest release of PySimpleGUI on PyPI (latest official version) https://Upgrading.PySimpleGUI.com
- [ ] Tried running the Maintenance Release. Use Home Window to access Maint Releases 
- [ ] For licensing questions please email mailto:license@PySimpleGUI.com

## Detailed Description

When multiple windows are open and overlapping, moving the mouse to the parent window sends child to the back. I have 2 windows, the first has a button to call the second window. If the second window overlaps the parent window in any way, moving the mouse into the parent window immediately gives focus to that, hiding the overlapping portion of window 2. It should require a mouse click in the parent window before giving it focus.
If I comment out the 'use custom titlebar' option it works correctly but I then have borders around the window, so things like the button bar, status bar are indented on both sides, looking pretty ordinary. I have tried defining the titlebar separately & not setting the option within the window call but the result is the same.

#### Code To Duplicate


```python
import PySimpleGUI as sg

def main_window():
    winsize = (800, 600)
    toolbar_buttons = [[sg.Button('Test', key='-TEST-')]]
    layout = [[sg.Frame('', toolbar_buttons, title_color='white', size=(winsize[0], 30),
                        pad=(0,0), background_color=sg.COLOR_SYSTEM_DEFAULT)]]
    win1 = sg.Window('test', size=winsize,
                     use_custom_titlebar=True,
                     layout=layout)

    while True:
        event, values = win1.read()
        print('main event= ', event)
        if event == '-TEST-':
            second_window()
        if event in (sg.WIN_CLOSED, '-CANCEL', 'CANCELB', '-EXIT-'):
            break
    win1.close()


def second_window():
    winsize = (600, 400)
    layout = [[sg.Text('Input',pad=(5, 0))],
              [sg.Input()]
              ]
    win2 = sg.Window('test', layout=layout, size=winsize,
                     keep_on_top=True, use_custom_titlebar=True)
    while True:
        event, values = win2.read()
        if event in (sg.WIN_CLOSED, '-CANCEL', 'CANCELB', '-EXIT-'):
            break
    win2.close()


if __name__ == '__main__':
    main_window()


```

#### Screenshot, Sketch, or Drawing",[],0
PySimpleGUI/PySimpleGUI,[Enhancement] Udemy Course Enhancement Request: Context Menu Course Module,"### Type of Issue (Enhancement, Error, Bug, Question)
Enhancement

----------------------------------------

#### Operating System

Windows 10

#### PySimpleGUI Port (tkinter, Qt, Wx, Web)
PySimpleGui Udemy Course

----------------------------------------

#### Your Experience In Months or Years (optional)

Years Python programming experience:  Since 3.9

Years Programming experience overall:  20+ years


---------------------

#### Troubleshooting

These items may solve your problem. Please check those you've done by changing - [ ] to - [X]

- [ ] Searched main docs for your problem [PySimpleGUI Documenation](https://docs.PySimpleGUI.com)
- [ ] Looked for Demo Programs that are similar to your goal. It is recommend you use the Demo Browser! [Demo Programs](Demos.PySimpleGUI.com)
- [ ] None of your GUI code was generated by an AI algorithm like GPT
- [ ] If not tkinter - looked for Demo Programs for specific port
- [ ] For non tkinter - Looked at readme for your specific port if not PySimpleGUI (Qt, WX, Remi)
- [ ] Run your program outside of your debugger (from a command line)
- [ ] Searched through Issues (open and closed) to see if already reported Issues.PySimpleGUI.com
- [ ] Have upgraded to the latest release of PySimpleGUI on PyPI (lastest official version)
- [ ] Tried running the Development Build.  Your problem may have already been fixed but not released. Check Home Window for release notes and upgrading capability
- [ ] For licensing questions please email license@PySimpleGUI.com

#### Detailed Description
I was asked by Mike on Udemy to post this as an issue on Github:

Do you have or plan to have a module on context menus? Are they/will they be available for individual components?
",[],0
PySimpleGUI/PySimpleGUI,[Question] PySide cannot accept a list of objects as the values for the Combobox element,"### Type of Issue (Enhancement, Error, Bug, Question)
Question

----------------------------------------

#### Operating System
WIN10


#### PySimpleGUI Port (tkinter, Qt, Wx, Web)
Qt


----------------------------------------

## Versions

Version information can be obtained by calling `sg.main_get_debug_data()`
Or you can print each version shown in ()


#### Python version (`sg.sys.version`)
Python 3.9.13


#### PySimpleGUI Version (`sg.__version__`)
5.0.0


#### GUI Version  (tkinter (`sg.tclversion_detailed`), PySide2, WxPython, Remi)
PySide2/PySide6

#### GUI Version  (tkinter (`sg.tclversion_detailed`), PySide2, WxPython, Remi)
PySide2/PySide6

### Priority Support Code (Commercial Users)



---------------------

#### Your Experience In Months or Years (optional)

5+ Years Python programming experience

10+ Years Programming experience overall

Have used another Python GUI Framework? (tkinter, Qt, etc) (yes/no is fine)
tkinter

Anything else you think would be helpful?


---------------------

#### Troubleshooting

These items may solve your problem. Please check those you've done by changing - [ ] to - [X]

- [x] Searched main docs for your problem [PySimpleGUI Documenation](https://docs.PySimpleGUI.com)
- [x] Looked for Demo Programs that are similar to your goal. It is recommend you use the Demo Browser! [Demo Programs](Demos.PySimpleGUI.com)
- [x] None of your GUI code was generated by an AI algorithm like GPT
- [x] If not tkinter - looked for Demo Programs for specific port
- [x] For non tkinter - Looked at readme for your specific port if not PySimpleGUI (Qt, WX, Remi)
- [x] Run your program outside of your debugger (from a command line)
- [x] Searched through Issues (open and closed) to see if already reported Issues.PySimpleGUI.com
- [x] Have upgraded to the latest release of PySimpleGUI on PyPI (lastest official version)
- [x] Tried running the Development Build.  Your problem may have already been fixed but not released. Check Home Window for release notes and upgrading capability
- [ ] For licensing questions please email license@PySimpleGUI.com

#### Detailed Description

> PySide2.QtWidgets.QComboBox.addItems(texts)
>
> Parameters:
>
>     texts â€“ list of strings

PySimpleGUI/tkinter can accept objects as values of widget, but PySimpleGUIQt/PySide cannot.

- Source code when initiate a Combobox, and it's fine.

```python
items_as_strings = [str(v) for v in element.Values]
# element.QT_ComboBox.addItems(element.Values)
element.QT_ComboBox.addItems(items_as_strings)
```

- Source code when update a Combobox

```python
self.QT_ComboBox.addItems(values)
```
You can find that there's no code to converts values to a list of `str` in PySimpleGUIQt.py when update, so we need to do it by ourself, or new values will be shown as blank lines in the Combobox element after updated.

#### Code To Duplicate

```python
import PySimpleGUIQt as sg

class Point:
    def __init__(self, location) -> None:
        self.location = location

    def __str__(self) -> str:
        return f""Location {self.location}""

points = [Point(i) for i in range(5)]

window = sg.Window(""Title"", [[sg.Combo([0,1,2,3,4], key='COMBO')]], finalize=True)
window['COMBO'].update(values=points)
# window['COMBO'].update(values=tuple(map(str, points)))
window.read(close=True)
```

#### Screenshot, Sketch, or Drawing

---------------------

### Watcha Makin?

If you care to share something about your project, it would be awesome to hear what you're building.
","['Question', 'Port - Qt']",0
PySimpleGUI/PySimpleGUI,Bug: Error dialog when programatically updating PySimpleGUIQt Checkbox,"### Type of Issue (Enhancement, Error, Bug, Question)
Bug

----------------------------------------

#### Operating System
Windows 10 and Windows 11

#### PySimpleGUI Port (tkinter, Qt, Wx, Web)
Qt

----------------------------------------

## Versions

#### Python version (`sg.sys.version`)
'3.12.3 (tags/v3.12.3:f6650f9, Apr  9 2024, 14:05:25) [MSC v.1938 64 bit (AMD64)]'

#### PySimpleGUI Version (`sg.__version__`)
'5.0.0'
PySimpleGUI               5.0.4
PySimpleGUIQt             5.0.0


#### GUI Version  (tkinter (`sg.tclversion_detailed`), PySide2, WxPython, Remi)
PySide6                   6.7.0
PySide6_Addons            6.7.0
PySide6_Essentials        6.7.0

### Priority Support Code (Commercial Users)

---------------------

#### Your Experience In Months or Years (optional)

Years Python programming experience:
1

Years Programming experience overall:
30+

Have used another Python GUI Framework? (tkinter, Qt, etc) (yes/no is fine)
Yes

Anything else you think would be helpful?


---------------------

#### Troubleshooting

These items may solve your problem. Please check those you've done by changing - [ ] to - [X]

- [X ] Searched main docs for your problem [PySimpleGUI Documenation](https://docs.PySimpleGUI.com)
- X ] Looked for Demo Programs that are similar to your goal. It is recommend you use the Demo Browser! [Demo Programs](Demos.PySimpleGUI.com)
- [X ] None of your GUI code was generated by an AI algorithm like GPT
- [X] If not tkinter - looked for Demo Programs for specific port
- [X] For non tkinter - Looked at readme for your specific port if not PySimpleGUI (Qt, WX, Remi)
- [X] Run your program outside of your debugger (from a command line)
- [X] Searched through Issues (open and closed) to see if already reported Issues.PySimpleGUI.com
- [X] Have upgraded to the latest release of PySimpleGUI on PyPI (lastest official version)
- [X] Tried running the Development Build.  Your problem may have already been fixed but not released. Check Home Window for release notes and upgrading capability
- [X] For licensing questions please email license@PySimpleGUI.com

#### Detailed Description
When trying to change values of checkboxes programatically, I get an error stating that Read or Finalize must be called first. As can be seen below I use ""Finalize=True"" when creating window, and Read has been called (i.e. the layout and window has been instantiated). I get the error dialogue once for each checkbox, but after clicking the error button the action of checking/unchecing the checkbox has actually been completed correctly.

c:\xxxx\Python\Python312\Lib\site-packages\PySimpleGUIQt\PySimpleGUIQt.py:562: UserWarning: You cannot Update element with key = test2 until the window has been Read or Finalized
warnings.warn('You cannot Update element with key = {} until the window has been Read or Finalized'.format(self.Key), UserWarning)

#### Code To Duplicate

A **short** program that isolates and demonstrates the problem (Do not paste your massive program, but instead 10-20 lines that clearly show the problem)

This pre-formatted code block is all set for you to paste in your bit of code:

```python

import PySimpleGUIQt as psg

if __name__ == '__main__':
    
    TRANS_IDX = ['test1', 'test2']

    layout = [  [psg.Checkbox('Testbutton', key='test1')],
                [psg.Checkbox('Testbutton', key='test2')],
                [psg.Button('Select all', key='-sel-', size_px=(110,25)),
                 psg.Button('Deselect all', key='-desel-', size_px=(110,25))]]

    window = psg.Window('Checkbox test', layout, finalize=True)

    while (True):
        event, values = window.read(timeout=1000)
                    
        match (event):
            case '-sel-':
                for tr in TRANS_IDX: 
                    window[tr].update(value=True) # type: psg.Checkbox()
            case '-desel-':
                for tr in TRANS_IDX:
                   window[tr].update(value=False)  # type: psg.Checkbox()

```

#### Screenshot, Sketch, or Drawing
![image](https://github.com/PySimpleGUI/PySimpleGUI/assets/171613829/25b60818-1b90-4c4e-9a3d-bdeda6fdc6ee)
![image](https://github.com/PySimpleGUI/PySimpleGUI/assets/171613829/a08e699a-c8b2-4703-9335-89a354356545)

---------------------

### Watcha Makin?

If you care to share something about your project, it would be awesome to hear what you're building.
Building a utility to download blobs from an Azure storage account. Need the checkboxes to select, which blob folders (transaction types) to download. The above example is a subset of code illustrating my problem, which with the above simplified excerpt is identical to what I experience in the actual utility.
","['Bug', 'Port - Qt']",0
PySimpleGUI/PySimpleGUI,[ Bug ]  Problem with HID device at first fire in the edit focused in pysimplegui tkinter.,"Bug 

Problem with HID device at first fire in the edit focused in pysimplegui tkinter.
----------------------------------------

#### Operating System

Android  

#### PySimpleGUI Port (tkinter, Qt, Wx, Web)

Tkinter

----------------------------------------

## Versions

Version information can be obtained by calling `sg.main_get_debug_data()`
Or you can print each version shown in ()

Android 13 (the phone is samsung).
PSG is 5, i reinstall and i have inserted my developer key (take care, the screen is too much large in orizontal, it has been difficult to insert the passcode, but i set it).

#### Python version (`sg.sys.version`)

Pydroid ... i have not the cell in my hand now ... but seems thae pydroid has been update because running the pysimplegui app, library pusimplegui wasn't installed (they have been lost by pydroid). So is a recent pydrod i mean.

#### PySimpleGUI Version (`sg.__version__`)

5 with developer key inserted correctly

#### GUI Version  (tkinter (`sg.tclversion_detailed`), PySide2, WxPython, Remi)

tkinter

---------------------

#### Your Experience In Months or Years (optional)

6 Years Python programming experience

Since Commodore Vic 20 ... :-D ... Years Programming experience overall

Have used another Python GUI Framework? (tkinter, Qt, etc) (yes/no is fine)

No only the simpler tkinter

Anything else you think would be helpful?

I'm sorry is not simple to handle with warehouse workers ... it seems to me that the issue is due to some difference in recent android, with HID devices. Before, runned the psg app, it instantly fire barcode into che edit field. Now it's behaviour is changed, i do not know why, the app is the same. Some other phone has not this problem by i can't get now to see what android they have ...
If you can't give me some advice, I'll investigate further, but if in the meantime someone already had the solution to the problem... that's why I wrote.
---------------------

#### Troubleshooting

These items may solve your problem. Please check those you've done by changing - [ ] to - [X]

- [ ] Searched main docs for your problem [PySimpleGUI Documenation](https://docs.PySimpleGUI.com)
- [ ] Looked for Demo Programs that are similar to your goal. It is recommend you use the Demo Browser! [Demo Programs](Demos.PySimpleGUI.com)
- [ ] None of your GUI code was generated by an AI algorithm like GPT
- [ ] If not tkinter - looked for Demo Programs for specific port
- [ ] For non tkinter - Looked at readme for your specific port if not PySimpleGUI (Qt, WX, Remi)
- [ ] Run your program outside of your debugger (from a command line)
- [ ] Searched through Issues (open and closed) to see if already reported Issues.PySimpleGUI.com
- [ x] Have upgraded to the latest release of PySimpleGUI on PyPI (lastest official version)
- [ ] Tried running the Development Build.  Your problem may have already been fixed but not released. Check Home Window for release notes and upgrading capability
- [ ] For licensing questions please email license@PySimpleGUI.com

#### Detailed Description

Hi, I have a problem on an android cell where I run pydroid with pysimplegui. This is the second time this has happened to me, because it had already happened to me on another cell phone. So: the mobile phone is connected to a barcode reader. When I fire the barcode, and I'm in an edit field, clearly the read code was inserted immediately (the barcode is in keyboard emulation, connected via bluetooth). The barcode does not give any errors: if I run it into any text editor, it transcribes the barcode.
Now, I launch the program on Android and I see a pysimplegui window, with the cursor flashing inside an edit field, until recently it didn't cause problems. If I fired, it immediately inserted into the edit field. Now, however, if I shoot, nothing happens. Only if with my real finger I click inside the edit field and the Android virtual keyboard is shown, then from that moment on, by shooting in the reader the barcode is inserted into the edit. It therefore seems that the first time I launch the window, even though the edit field is in focus, Android doesn't feel that focus until I click in it with my finger. It's a problem because the warehouse workers don't understand what's happening, they hear the reader beeping, but they don't enter the code. Unfortunately, the warehouses often have staff who come and go and so it is problematic that even though the cursor flashing inside the edi field is there, when shooting it doesn't read anything... has anyone had the same problem? How can it be solved? I've tried various things at the android level, but I can't get past it. If you focus on an edit field, Android should feel like the edit field has been clicked, or not? Why didn't this happen a while ago? Maybe it's a problem on Android versions? cell phones are quite recent.


#### Code To Duplicate

A simple form with one edit field get the issue ... i think it is not necessary to paste code ...
Again, i will do more tests... but is not so easy o go to customers ....so if you can help me the same, i appreciate ... other wise, i will do more tests by miself ...


#### Screenshot, Sketch, or Drawing

---------------------

### Watcha Makin?

Well, I have done a 3D autoprinted case to contain 1) an android phone 2) and a HID barcode connected via bluetooth. With pydroid and PSG - what a powerful couple !! - , I now have the most powerful app to read barcode in the world ... i am joking, but the intergration with my ERP is fanatstic ... but i had this issue recently ... unfortunately warehouse workers are often a category of demanding users: either things work perfectly, or they don't use them at all...
","['Port - TK', 'Platform Specific Issue - Android', 'PySimpleGUI 5']",0
PySimpleGUI/PySimpleGUI,[Bug] Event handling is broken on the macOS Sonoma 14.2.1 with tkinter backend,"### Type of Issue Bug


----------------------------------------

#### Operating System
macOS Sonoma 14.2.1


#### PySimpleGUI Port (tkinter, Qt, Wx, Web)



----------------------------------------

## Versions
*** Version information copied to your clipboard. Paste into your GitHub Issue. ***

Python Interpeter: /opt/homebrew/Caskroom/miniconda/base/envs/biquad_designer_env/bin/python
Python version: 3.12.0
Platform: Mac
Platform version: ('14.2.1', ('', '', ''), 'arm64')
Port: PySimpleGUI
tkinter version: 8.6.12
PySimpleGUI version: 5.0.2
PySimpleGUI filename: /opt/homebrew/Caskroom/miniconda/base/envs/biquad_designer_env/lib/python3.12/site-packages/PySimpleGUI/PySimpleGUI.py
---------------------

#### Your Experience In Months or Years (optional)

Years Python programming experience
>2 years
Years Programming experience overall
>6 years
Have used another Python GUI Framework? (tkinter, Qt, etc) (yes/no is fine)
Qt/QML, Win32API, a few attempts were made to use GTK4

Anything else you think would be helpful?
It seems that there is an existing issue in another project which describes the actual issue. Also, we have the support key assigned, but it's not clear in this issue form where to apply it.


---------------------

#### Troubleshooting

These items may solve your problem. Please check those you've done by changing - [ ] to - [X]

- [X] Searched main docs for your problem [PySimpleGUI Documenation](https://docs.PySimpleGUI.com)
- [X] Looked for Demo Programs that are similar to your goal. It is recommend you use the Demo Browser! [Demo Programs](Demos.PySimpleGUI.com)
- [X] None of your GUI code was generated by an AI algorithm like GPT
- [ ] If not tkinter - looked for Demo Programs for specific port
- [ ] For non tkinter - Looked at readme for your specific port if not PySimpleGUI (Qt, WX, Remi)
- [X] Run your program outside of your debugger (from a command line)
- [X] Searched through Issues (open and closed) to see if already reported Issues.PySimpleGUI.com
- [X] Have upgraded to the latest release of PySimpleGUI on PyPI (lastest official version)
- [X] Tried running the Development Build.  Your problem may have already been fixed but not released. Check Home Window for release notes and upgrading capability
- [ ] For licensing questions please email license@PySimpleGUI.com

#### Detailed Description

On the latest macOS version for M1 platform PySimpleGui behaves incorrectly with tkinter backend. There is an existing issue which describes the problem similar to our one:
https://github.com/pure-data/pure-data/issues/2105

The issue is simple. It looks like handling of events is broken either in PySimpleGui or tkinter backend. It's necessary to move the mouse pointer several times around the element for being able to pass a click to it. It seems to be either regression of tkinter or PySimpleGui backend. If any video/whatever help is required we can assist with this as well.

#### Code To Duplicate
I've tried to use this example:
https://github.com/PySimpleGUI/PySimpleGUI/blob/master/DemoPrograms/Demo_All_Elements.py


#### Screenshot, Sketch, or Drawing
<img width=""432"" alt=""image"" src=""https://github.com/PySimpleGUI/PySimpleGUI/assets/150831218/6f042bff-9d32-4945-bc57-6abd922cd273"">


#### Extras

Thanks for your handling the bug. Just a small question-where should we put our support code for the faster processing of the issue?

---------------------","['Platform Specific Issue - Mac', 'Port - TK']",0
PySimpleGUI/PySimpleGUI,[Error Message] alloc:Invalid block,"## Type of Issue (Enhancement, Error, Bug, Question)

Error Message

----------------------------------------

## Environment 

#### Operating System

Windows  version win11

#### PySimpleGUI Port (tkinter, Qt, Wx, Web)

tkinter

----------------------------------------

## Versions


#### Python version (`sg.sys.version`)

3.11.0 (main, Oct 24 2022, 18:26:48) [MSC v.1933 64 bit (AMD64)]

#### PySimpleGUI Version (`sg.__version__`)

4.60.4

#### GUI Version  (tkinter (`sg.tclversion_detailed`), PySide2, WxPython, Remi)

8.6.12



---------------------

## Your Experience In Months or Years (optional)

 Years Python programming experience
 Years Programming experience overall
No Have used another Python GUI Framework? (tkinter, Qt, etc) (yes/no is fine)


---------------------

## Troubleshooting

These items may solve your problem. Please check those you've done by changing - [ ] to - [X]

- [X] Searched main docs for your problem  www.PySimpleGUI.org
- [X] Looked for Demo Programs that are similar to your goal Demos.PySimpleGUI.org
- [ ] If not tkinter - looked for Demo Programs for specific port
- [X] For non tkinter - Looked at readme for your specific port if not PySimpleGUI (Qt, WX, Remi)
- [X] Run your program outside of your debugger (from a command line)
- [X] Searched through Issues (open and closed) to see if already reported Issues.PySimpleGUI.org
- [X] Tried using the PySimpleGUI.py file on GitHub. Your problem may have already been fixed but not released

## Detailed Description

alloc: invalid block: 00000293E7AC94A0: c0 e7

#### Code To Duplicate


```python
# Paste your code here


```

#### Screenshot, Sketch, or Drawing",['Port - TK'],0
PySimpleGUI/PySimpleGUI,"[ Bug]  sg.WIN_CLOSED event is not generated, after catching mouse down events in same window for read_all_windows calls","### Type of Issue (Enhancement, Error, Bug, Question)

Bug

----------------------------------------

#### Operating System
MacOs 10.14.6

#### PySimpleGUI Port (tkinter, Qt, Wx, Web)
tkinter

----------------------------------------

## Versions

Version information can be obtained by calling `sg.main_get_debug_data()`
Or you can print each version shown in ()

Python version: 3.12.2 | packaged by conda-forge | (main, Feb 16 2024, 21:00:12) [Clang 16.0.6 ]
        port: tkinter
        tkinter version: 8.6.13
        PySimpleGUI version: 4.60.5
        PySimpleGUI filename: /Users/cm/anaconda3/envs/sg/lib/python3.12/site-packages/PySimpleGUI/PySimpleGUI.py


#### Python version (`sg.sys.version`)
3.12.2 | packaged by conda-forge | (main, Feb 16 2024, 21:00:12) [Clang 16.0.6 ]

#### PySimpleGUI Version (`sg.__version__`)
4.60.5

#### GUI Version  (tkinter (`sg.tclversion_detailed`), PySide2, WxPython, Remi)
8.6.13


---------------------

#### Your Experience In Months or Years (optional)

Years Python programming experience
4 years

Years Programming experience overall
30 years

Have used another Python GUI Framework? (tkinter, Qt, etc) (yes/no is fine)
no

Anything else you think would be helpful?


---------------------

#### Troubleshooting

These items may solve your problem. Please check those you've done by changing - [ ] to - [X]

- [x] Searched main docs for your problem  www.PySimpleGUI.org
- [x] Looked for Demo Programs that are similar to your goal. It is recommend you use the Demo Browser! Demos.PySimpleGUI.org
- [x] None of your GUI code was generated by an AI algorithm like GPT
- [ ] If not tkinter - looked for Demo Programs for specific port
- [ ] For non tkinter - Looked at readme for your specific port if not PySimpleGUI (Qt, WX, Remi)
- [x] Run your program outside of your debugger (from a command line)
- [x] Searched through Issues (open and closed) to see if already reported Issues.PySimpleGUI.org
- [x] Have upgraded to the latest release of PySimpleGUI on PyPI (lastest official version)
- [ ] Tried running the Development Build.  Your problem may have already been fixed but not released

#### Detailed Description

The sg.WIN_CLOSED event is not generated, after catching mouse down events in same window.
If I open a window and if I hit the ""window close"" icon in the left upper corner (MacOsX), the close event is generated and the window closes and in the event loop is exited.

If I click inside of the window before clicking the close window icon, it will not accept the close window icon any more. If I click on the close icon I get the window, event, value as follows:
<PySimpleGUI.PySimpleGUI.Window object at 0x10a700a10> -GRAPH2-+UP {'-GRAPH2-': (3, 293)}

The coordinate is a coordinate inside of the close window icon.
This only happens after generating a mouse down event in the window.



#### Code To Duplicate


This pre-formatted code block is all set for you to paste in your bit of code:

```python

import PySimpleGUI as sg
from PIL import Image, ImageDraw, ImageTk
import io

## Indeed, it should be possible to combine the following two create window functions to one general function:
def make_overview_window(h, w):
    layout_overview_window = [[sg.Graph(
        canvas_size=(h, w),
        graph_bottom_left=(0, 0),
        graph_top_right=(h, w),
        key=""-GRAPH1-"",
        change_submits=True,  # mouse click events
        background_color='white',
        drag_submits=True), ],
        [sg.Text(key='info1', size=(60, 1))]]

    return sg.Window(""Main zoom selection window"", layout_overview_window, finalize=True)

if __name__ == '__main__':
    # Load JPG images using Pillow
    image1 = Image.open('image1.jpg')
    h0, w0 = image1.size

    print(""Size: "", h0, "" "", w0)

    # Convert images to bytes
    img_bytes1 = io.BytesIO()
    image1.save(img_bytes1, format='PNG')
    window_overview = make_overview_window(h0, w0)

    # get the graph element for ease of use later
    graph_overview_window = window_overview[""-GRAPH1-""]
    graph_overview_window.draw_image(data=img_bytes1.getvalue(), location=(0, h0))

    while True:
        window, event, values = sg.read_all_windows() # timeout=10)

        print(window, event, values)
        if window == window_overview:
            if event == sg.WIN_CLOSED:
                break  # exit

            if event == ""-GRAPH1-"":  # if there's a ""Graph"" event, then it's a mouse
                x, y = values[""-GRAPH1-""]
                print(event, "" "", x, "" "", y)

    # Close the window
    window_overview.close()

```

#### Screenshot, Sketch, or Drawing

---------------------

### Watcha Makin?

If you care to share something about your project, it would be awesome to hear what you're building.
",[],0
PySimpleGUI/PySimpleGUI,"[Question] Input and Multiline elements with option do_not_clear=False, cleared when CalendarButton clicked","### Type of Issue (Enhancement, Error, Bug, Question)
Question

----------------------------------------

#### Operating System
WIN10


#### PySimpleGUI Port (tkinter, Qt, Wx, Web)
tkinter


----------------------------------------

## Versions


#### Python version (`sg.sys.version`)
3.10.2


#### PySimpleGUI Version (`sg.__version__`)
5.0.2.2


#### GUI Version  (tkinter (`sg.tclversion_detailed`), PySide2, WxPython, Remi)
8.6.12


---------------------

#### Your Experience In Months or Years (optional)

5+ Years Python programming experience

10+ Years Programming experience overall

Have used another Python GUI Framework? (tkinter, Qt, etc) (yes/no is fine)
tkinter

Anything else you think would be helpful?
None

---------------------

#### Troubleshooting

These items may solve your problem. Please check those you've done by changing - [ ] to - [X]

- [x] Searched main docs for your problem  www.PySimpleGUI.org
- [x] Looked for Demo Programs that are similar to your goal. It is recommend you use the Demo Browser! Demos.PySimpleGUI.org
- [x] None of your GUI code was generated by an AI algorithm like GPT
- [ ] If not tkinter - looked for Demo Programs for specific port
- [ ] For non tkinter - Looked at readme for your specific port if not PySimpleGUI (Qt, WX, Remi)
- [x] Run your program outside of your debugger (from a command line)
- [x] Searched through Issues (open and closed) to see if already reported Issues.PySimpleGUI.org
- [x] Have upgraded to the latest release of PySimpleGUI on PyPI (lastest official version)
- [x] Tried running the Development Build.  Your problem may have already been fixed but not released

#### Detailed Description

Content of Input and Multiline elements with option do_not_clear=False, cleared when CalendarButton clicked.

IMO, they should not be cleared.


#### Code To Duplicate

```python
import PySimpleGUI as sg

layout = [
    [sg.Input(""Input Element"", expand_x=True, do_not_clear=False)],
    [sg.Multiline(""Multiline Element"", size=(40, 5), expand_x=True, do_not_clear=False)],
    [sg.Input(key='-DATE-', expand_x=True),
     sg.CalendarButton(""Date"")],
    [sg.Button(""Add task"")],
]
window = sg.Window(""Title"", layout)

while True:

    event, values = window.read()

    if event == sg.WIN_CLOSED:
        break

window.close()
```

#### Screenshot, Sketch, or Drawing
In the beginning, 
![image](https://github.com/PySimpleGUI/PySimpleGUI/assets/55352169/b913e196-36c7-492d-97fe-e38c598e792f)
After CalendarButton `Date` clicked,
![image](https://github.com/PySimpleGUI/PySimpleGUI/assets/55352169/5d066967-f5b4-44e8-99aa-a732913a905e)
Content of Input and Multiline elements cleared !",[],0
PySimpleGUI/PySimpleGUI,[Enhancement] Add support for `Tree().Update(select_rows=values)`,"### Type of Issue - Enhancement


----------------------------------------

#### Operating System - Windows 10



#### PySimpleGUI Port  - tkinter



----------------------------------------

## Versions


#### Python version - 3.12.1



#### PySimpleGUI Version - 4.60.5



#### GUI Version - 8.6.13



---------------------

#### Your Experience In Months or Years (optional)

Years Python programming experience - 2.5y

Years Programming experience overall - 2.5y

Have used another Python GUI Framework? - No

Anything else you think would be helpful?


---------------------

#### Troubleshooting

These items may solve your problem. Please check those you've done by changing - [ ] to - [X]

- [x] Searched main docs for your problem  www.PySimpleGUI.org
- [x] Looked for Demo Programs that are similar to your goal. It is recommend you use the Demo Browser! Demos.PySimpleGUI.org
- [x] None of your GUI code was generated by an AI algorithm like GPT
- [x] If not tkinter - looked for Demo Programs for specific port
- [x] For non tkinter - Looked at readme for your specific port if not PySimpleGUI (Qt, WX, Remi)
- [x] Run your program outside of your debugger (from a command line)
- [x] Searched through Issues (open and closed) to see if already reported Issues.PySimpleGUI.org
- [x] Have upgraded to the latest release of PySimpleGUI on PyPI (lastest official version)
- [x] Tried using the PySimpleGUI.py file on GitHub. Your problem may have already been fixed but not released

#### Detailed Description
Right now, `Tree().Update` has no support for the `select_rows` parameter.  It would be nice to have this for consistency with the `Table` element, as I am using them semi-interchangeably (I have to use `Tree` because I am using the -1 column for small image based indicators).

##### More context: 
I am using a set of tabs, each tab having a table of some kind, which I would like to coordinate selections across, since there is so much data that it needs to be split across multiple tabs. All items are already added to each table, and they are updated and sorted as a group, however I now need to have them be able to be selected as a group.  For the tables I am using the `Update` method, but since the `Tree` object has no `select_rows` parameter, this doesn't work for it.  I have also considered trying to update `Tree().SelectedRows` and replicate some of the code inside the `Tree()._table_clicked()` method, but that is pretty janky, and I would rather have first party support directly in the library.

If there is a preference towards not adding this, then an idea of the best way to do this for the `Tree` element would be great as well.

#### Code To Duplicate

`sg.Tree().Update(select_rows=[])`

#### Screenshot, Sketch, or Drawing

---------------------

### Watcha Makin?

https://github.com/UpstreamData/upstream_config_util
","['enhancement', 'Port - TK']",0
PySimpleGUI/PySimpleGUI,[Question]  How to close the right_click_menu without selection of one of the items,"### Type of Issue (Enhancement, Error, Bug, Question) 
I wonder how to close the right_click_menu (after it has been opened with a right mouse click), by a left click on another part of the window or screen. Just as is common in Linux or Windows. It turns out that this is not happening when I e.g. run the demo 
[Demo_Multiline_Right_Click_Menu_Clipboard.py](https://github.com/PySimpleGUI/PySimpleGUI/blob/master/DemoPrograms/Demo_Multiline_Right_Click_Menu_Clipboard.py)
After appearance of the right_click_menu, it does not close and disappear when e.g. selecting another window. It even stays on top. 

----------------------------------------

#### Operating System
Ubuntu 22.04.3 LTS


#### PySimpleGUI Port (tkinter, Qt, Wx, Web)



----------------------------------------

## Versions
*** Version information copied to your clipboard. Paste into your GitHub Issue. ***

Python version: 3.9.16 (main, Jan 11 2023, 16:05:54) 
[GCC 11.2.0]
        tkinter version: 8.6.12
        PySimpleGUI version: 4.60.4
---------------------

#### Your Experience In Months or Years (optional)

Years Python programming experience 8

Years Programming experience overall 40

Have used another Python GUI Framework? (tkinter, Qt, etc) (yes/no is fine)

Anything else you think would be helpful?


---------------------

#### Troubleshooting

These items may solve your problem. Please check those you've done by changing - [ ] to - [X]

- [x ] Searched main docs for your problem  www.PySimpleGUI.org
- [x] Looked for Demo Programs that are similar to your goal. It is recommend you use the Demo Browser! Demos.PySimpleGUI.org
- [x ] None of your GUI code was generated by an AI algorithm like GPT
- [ ] If not tkinter - looked for Demo Programs for specific port
- [ ] For non tkinter - Looked at readme for your specific port if not PySimpleGUI (Qt, WX, Remi)
- [ x] Run your program outside of your debugger (from a command line)
- [x ] Searched through Issues (open and closed) to see if already reported Issues.PySimpleGUI.org
- [ ] Have upgraded to the latest release of PySimpleGUI on PyPI (lastest official version)
- [ ] Tried using the PySimpleGUI.py file on GitHub. Your problem may have already been fixed but not released

#### Detailed Description




#### Code To Duplicate

A **short** program that isolates and demonstrates the problem (Do not paste your massive program, but instead 10-20 lines that clearly show the problem)

This pre-formatted code block is all set for you to paste in your bit of code:

```python

# Paste your code here

import PySimpleGUI as sg

""""""
    Demo - Adding a right click menu to perform multiline element common operations

    Sometimes Multiline Elements can benefit from a right click menu. There are no default menu
    that come with tkinter, so you'll need to create your own.

    Some common clipboard types of operations
        Select all
        Copy
        Paste
        Cut

    The underlying Widget is accessed several times in this code because setting selections,
    getting their values, and clipboard operations are not currently exposed in the APIs

    NOTE - With tkinter, if you use the built-in clipboard, you must keep your program
    running in order to access the clipboard.  Upon exit, your clipboard will be deleted.
    You can get around this by using other clipboard packages.

    Copyright 2021 PySimpleGUI
""""""

right_click_menu = ["""", [""Copy"", ""Paste"", ""Select All"", ""Cut""]]
MLINE_KEY = ""-MLINE-""


def do_clipboard_operation(event, window, element):
    if event == ""Select All"":
        element.Widget.selection_clear()
        element.Widget.tag_add(""sel"", ""1.0"", ""end"")
    elif event == ""Copy"":
        try:
            text = element.Widget.selection_get()
            window.TKroot.clipboard_clear()
            window.TKroot.clipboard_append(text)
        except:
            print(""Nothing selected"")
    elif event == ""Paste"":
        element.Widget.insert(sg.tk.INSERT, window.TKroot.clipboard_get())
    elif event == ""Cut"":
        try:
            text = element.Widget.selection_get()
            window.TKroot.clipboard_clear()
            window.TKroot.clipboard_append(text)
            element.update("""")
        except:
            print(""Nothing selected"")


def main():
    layout = [
        [sg.Text(""Using a custom right click menu with Multiline Element"")],
        [sg.Multiline(size=(60, 20), key=MLINE_KEY, right_click_menu=right_click_menu)],
        [sg.B(""Go""), sg.B(""Exit"")],
    ]

    window = sg.Window(""Right Click Menu Multiline"", layout)

    mline: sg.Multiline = window[MLINE_KEY]

    while True:
        event, values = window.read()  # type: (str, dict)
        print(""Event: "", event, "" Values: "", values)
        if event in (sg.WIN_CLOSED, ""Exit""):
            break

        # if event is a right click menu for the multiline, then handle the event in func
        if event in right_click_menu[1]:
            do_clipboard_operation(event, window, mline)

    window.close()


if __name__ == ""__main__"":
    main()
#### Screenshot, Sketch, or Drawing

---------------------

### Watcha Makin?

If you care to share something about your project, it would be awesome to hear what you're building.
A gui interface for plotting graphs","['Question', 'Port - TK', 'Platform Specific Issue - Linux']",0
PySimpleGUI/PySimpleGUI,[Enhancement] Focus first button in popup() calls,"## Type of Issue (Enhancement, Error, Bug, Question)

Enhancement

----------------------------------------

## Environment 

#### Operating System

Windows  version ('10', '10.0.19045', 'SP0', 'Multiprocessor Free')

#### PySimpleGUI Port (tkinter, Qt, Wx, Web)

tkinter

----------------------------------------

## Versions


#### Python version (`sg.sys.version`)

3.11.6 (tags/v3.11.6:8b6ee5b, Oct  2 2023, 14:57:12) [MSC v.1935 64 bit (AMD64)]

#### PySimpleGUI Version (`sg.__version__`)

4.61.0.198

#### GUI Version  (tkinter (`sg.tclversion_detailed`), PySide2, WxPython, Remi)

8.6.12



---------------------

## Your Experience In Months or Years (optional)

2 Years Python programming experience
10 Years Programming experience overall
Yes Have used another Python GUI Framework? (tkinter, Qt, etc) (yes/no is fine)


---------------------

## Troubleshooting

These items may solve your problem. Please check those you've done by changing - [ ] to - [X]

- [X] Searched main docs for your problem  www.PySimpleGUI.org
- [X] Looked for Demo Programs that are similar to your goal. It is recommend you use the Demo Browser! Demos.PySimpleGUI.org
- [X] If not tkinter - looked for Demo Programs for specific port
- [X] For non tkinter - Looked at readme for your specific port if not PySimpleGUI (Qt, WX, Remi)
- [X] Run your program outside of your debugger (from a command line)
- [X] Searched through Issues (open and closed) to see if already reported Issues.PySimpleGUI.org
- [X] Upgraded to the latest official release of PySimpleGUI on PyPI
- [X] Tried using the PySimpleGUI.py file on GitHub. Your problem may have already been fixed but not released

## Detailed Description

When using the popup() calls, it would be really nice if there was a way to focus the first button by default.
I encountered this when checking for some input and creating popups if they were malformed. It's really annoying always having to touch the moouse or try tabbing through the popup if it only contains text and one button.

Something like the following would be enough:

```python
sg.popup_ok('text', title='title', focus_first_button=True)
```",['Port - TK'],0
PySimpleGUI/PySimpleGUI,[Question]  How do you apply themes to user defined elements,"### Type of Issue (Enhancement, Error, Bug, Question)
Question

----------------------------------------

#### Operating System

MacOS 12.5.1 Monterey

#### PySimpleGUI Port (tkinter, Qt, Wx, Web)

tkinter

----------------------------------------

## Versions

Version information can be obtained by calling `sg.main_get_debug_data()`
Python version: 3.11.5 (main, Sep 11 2023, 08:19:27) [Clang 14.0.6 ]
        port: tkinter
        tkinter version: 8.6.12
        PySimpleGUI version: 4.60.5
        PySimpleGUI filename: /Users/ddunbar/anaconda3/lib/python3.11/site-packages/PySimpleGUI/PySimpleGUI.py

Or you can print each version shown in ()


#### Python version (`sg.sys.version`)



#### PySimpleGUI Version (`sg.__version__`)



#### GUI Version  (tkinter (`sg.tclversion_detailed`), PySide2, WxPython, Remi) 
Tkinter



---------------------

#### Your Experience In Months or Years (optional)

Years Python programming experience 5/6y dabbling and more for data science!

Years Programming experience overall 8/9y

Have used another Python GUI Framework? (tkinter, Qt, etc) (yes/no is fine) No

Anything else you think would be helpful?


---------------------

#### Troubleshooting

These items may solve your problem. Please check those you've done by changing - [ ] to - [X]

- [x] Searched main docs for your problem  www.PySimpleGUI.org
- [x] Looked for Demo Programs that are similar to your goal. It is recommend you use the Demo Browser! Demos.PySimpleGUI.org
- [x] None of your GUI code was generated by an AI algorithm like GPT
- [x] If not tkinter - looked for Demo Programs for specific port
- [x] For non tkinter - Looked at readme for your specific port if not PySimpleGUI (Qt, WX, Remi)
- [x] Run your program outside of your debugger (from a command line)
- [x] Searched through Issues (open and closed) to see if already reported Issues.PySimpleGUI.org
- [x] Have upgraded to the latest release of PySimpleGUI on PyPI (lastest official version)
- [x] Tried using the PySimpleGUI.py file on GitHub. Your problem may have already been fixed but not released

#### Detailed Description

I'm trying to use a user defined element (UDE) similar what you had in your ""Dashboard Mockup"" example (here: https://pysimplegui.trinket.io/demo-programs#/graph-element/dashboard-mockup). I can apply the theme to all ""sg.xxx"" elements in the layout, however the theme doesn't apply to the used defined elements? And I cannot work it out, I may be doing something wrong, but not sure if this is the case or if there is a bug here? I'm trying to get the front end made before I add in the mechanics, as they will need the inputs from the gui to function. I changed the radio buttons in your UDE in the example above and the same thing happens, the spinner is dark and cannot be changed. If you place a spinner in the regular layout it picks up the theme. I can't even manually change the text box colour in the UDE. 

#### Code To Duplicate

A **short** program that isolates and demonstrates the problem (Do not paste your massive program, but instead 10-20 lines that clearly show the problem)

This pre-formatted code block is all set for you to paste in your bit of code:

```python

import sys
import os
import pandas as pd
#import tkinter as tk
import PySimpleGUI as sg 

sampleNum = [i for i in range(0,100)]
bloodList = {'Heparin blood':'-H-', 'Heparin plasma':'-Hp-', 'EDTA blood':'-E-', 'EDTA plasma':'-Ep-', 'Serum clotted':'-SC-',
            'Serum gel':'-SG-', 'Fluoride Oxalate':'-FO-', 'Glucose':'-Glu-',}
fluidList = {'Ascities (plain)':'-ASCp-', 'Ascities (EDTA)':'-ASCe-', 'Pleural effusion (plain)':'-PEp-', 'Pleural effusion (EDTA)':'-PEe-',
             'Pericaridal effusion (plain)':'-PCEp-','Pericaridal effusion (EDTA)':'-PCEe-', 'Aqueous humour (plain)':'-AHp-',
             'Aqueous humour (EDTA)':'-AHe-'}
swabList = {'VTM oropharyngeal':'-VTMo', 'VTM conjunctival':'-VTMc-', 'VTM nasal':'-VTMn-', 'VTM vaginal':'-VTMv-', 'VTM scab':'-VTMs-',
            'Charcoal oropharyngeal':'-CHARo-', 'Charcoal conjunctival':'-CHARc-', 'Charcoal nasal':'-CHARn-', 'Charcoal vaginal':'-CHARv-',
            'Aimes oropharyngeal':'-AIMo-', 'Aimes conjunctival':'-AIMc-', 'Aimes nasal':'-AIMn-', 'Aimes vaginal':'-AIMv-',
            'Dry oropharyngeal':'-DRYo-', 'Dry conjunctival':'-DRYc-', 'Dry nasal':'-DRYn-', 'Dry vaginal':'-DRYv-'}
faecalList = {'Faeces':'-FAEC-', 'Faeces (3d pool)':'-FAEC3-', 'Faecal swab (any)':'-FAECsw-'}
cytologyList = {'Blood smear':'-SMEARbl-', 'Fluid smear':'-SMEARfl-', 'FNA smear (add site)':'-SMEARfna-'}
otherList = {'Fresh tissue (add site)':'-TISS-', 'Fixed tissue (add site)':'-FTISS-'}

sampleArray = [""2H"",""1E"",""1eASC""]
sampleList = ', '.join(sampleArray)

def typeFrames(title, sample_dict):
    layout = [[sg.Text(title,  size=(26,1), justification='center', font='Current 16 underline')]]
    for item, key in sample_dict.items():
        sg.theme('BlueMono'),
        layout += [[sg.Spin(sampleNum, initial_value=0, size=(4,2), readonly=True, enable_events=True, key=key, font='Current 14'), sg.Text(item, font='Current 14')]]
    return sg.Frame('',layout,)

sg.theme('BlueMono')

sampleCol1 = [
            [typeFrames(""Bloods"", bloodList)], 
            [typeFrames(""Fluids"", fluidList)],
            [typeFrames(""Faecal"", faecalList)],
            [typeFrames(""Other"", otherList)]]

sampleCol2 = [
            [typeFrames(""Swabs"", swabList)],
            [typeFrames(""Cytology"", cytologyList)],
            [sg.Button('Generate sample lsit and copy to clipboard', key='-GEN-', size=35, font='Current 14')],
            [sg.Multiline((""add output array here as a string""+"".""), size=(35,4), font='Current 14')]]

clinhxLayout = [[sg.T('Clinical history selector')]]

sampleLayout = [ 
        [sg.Text('Select samples submitted: ', font='Current 14')],
        [sg.Spin(sampleNum, initial_value=0)],
        [sg.Column(sampleCol1), sg.Column(sampleCol2)]]

layout = [[sg.Column([[sg.Text('VDS Snippets ', font='Current 30')]]),
        sg.Column([[sg.Button('Exit', font='Current 14')]], key='-EXIT-', element_justification='r', expand_x=True)]]


layout +=[[sg.TabGroup([[sg.Tab('Samples submitted', sampleLayout),
                         sg.Tab('Clinical history', clinhxLayout)]], key='-TAB GROUP-', expand_x=True, expand_y=True, font='Current 14')]]

window = sg.Window(""VDS Snippets"", layout, keep_on_top=True)

while True:
            event, values = window.read()
            print(event, values)
            out = print(sampleList, end=""."")
            #window['-S_OUT-'].update(out)
            if event == sg.WIN_CLOSED or event == 'Exit':
                break
            if event == '-GEN-':
                print('You pressed the button')

window.close()
exit(0)

```

#### Screenshot, Sketch, or Drawing
<img width=""1260"" alt=""code_screengrab"" src=""https://github.com/PySimpleGUI/PySimpleGUI/assets/46853209/e4c3913e-8ffc-49b0-814b-49eb0a76f43b"">
<img width=""669"" alt=""gui_screengrab"" src=""https://github.com/PySimpleGUI/PySimpleGUI/assets/46853209/b144fe61-9491-428b-8b26-50a3bc670e63"">

---------------------

### Watcha Makin?

I'm building a tool to be used for standardising the input of details about samples submitted to our laboratory, to save errors, style and format differences between users. It'll hopefully make it easier to use the data from the database. 
","['enhancement', 'Port - TK']",0
PySimpleGUI/PySimpleGUI,[Bug] Error code in example code for Desktop Floating Widget - Timer,"### Type of Issue (Enhancement, Error, Bug, Question)
Bug

----------------------------------------

#### Operating System
Ubuntu Linux 23.10


#### PySimpleGUI Port (tkinter, Qt, Wx, Web)
tkinter


----------------------------------------

## Versions

Version information can be obtained by calling `sg.main_get_debug_data()`
Or you can print each version shown in ()


#### Python version (`sg.sys.version`)
3.11.6


#### PySimpleGUI Version (`sg.__version__`)
4.60.5


#### GUI Version  (tkinter (`sg.tclversion_detailed`), PySide2, WxPython, Remi)
8.6.13


---------------------

#### Your Experience In Months or Years (optional)

2 Years Python programming experience

40 Years Programming experience overall

Have used another Python GUI Framework? (tkinter, Qt, etc) (yes/no is fine)
No

Anything else you think would be helpful?


---------------------

#### Troubleshooting

These items may solve your problem. Please check those you've done by changing - [ ] to - [X]

- [ ] Searched main docs for your problem  www.PySimpleGUI.org
- [ ] Looked for Demo Programs that are similar to your goal. It is recommend you use the Demo Browser! Demos.PySimpleGUI.org
- [ ] None of your GUI code was generated by an AI algorithm like GPT
- [ ] If not tkinter - looked for Demo Programs for specific port
- [ ] For non tkinter - Looked at readme for your specific port if not PySimpleGUI (Qt, WX, Remi)
- [ ] Run your program outside of your debugger (from a command line)
- [ ] Searched through Issues (open and closed) to see if already reported Issues.PySimpleGUI.org
- [ ] Have upgraded to the latest release of PySimpleGUI on PyPI (lastest official version)
- [ ] Tried using the PySimpleGUI.py file on GitHub. Your problem may have already been fixed but not released

#### Detailed Description

I'm simply trying to help with your Timer example on the https://github.com/PySimpleGUI page.

When I copied the code into Visual Studio Code, saved it and ran it using the version of Python above, I got the following errors:

```python
  File ""/home/user/projects/python-gui/timer-example.py"", line 46
    event = window[event).GetText()
                        ^
SyntaxError: closing parenthesis ')' does not match opening parenthesis '['

  File ""/home/user/projects/python-gui/timer-example.py"", line 57
    element = window['button')
                             ^
SyntaxError: closing parenthesis ')' does not match opening parenthesis '['
```

```python
/home/user/projects/python-gui/timer-example.py:50: SyntaxWarning: ""is"" with a literal. Did you mean ""==""?
  if event is 'Reset':
```

I believe that there was an open square bracket-close parenthesis match problem as well. I fixed these for myself, but thought you should know that they're in the code on the webpage...

#### Code To Duplicate

None

#### Screenshot, Sketch, or Drawing

---------------------

### Watcha Makin?","['Bug', 'documentation', 'Port - TK']",0
PySimpleGUI/PySimpleGUI,[Bug] Tabgroup element generate different kind of event value after new Tab added,"### Type of Issue (Enhancement, Error, Bug, Question)
Bug

----------------------------------------

#### Operating System

WIN10


#### PySimpleGUI Port (tkinter, Qt, Wx, Web)
tkinter


----------------------------------------

## Versions


#### Python version (`sg.sys.version`)
3.10.2


#### PySimpleGUI Version (`sg.__version__`)
4.61.0.206


#### GUI Version  (tkinter (`sg.tclversion_detailed`), PySide2, WxPython, Remi)
8.6.12


---------------------

#### Your Experience In Months or Years (optional)

5 Years Python programming experience

10+ Years Programming experience overall

Have used another Python GUI Framework? (tkinter, Qt, etc) (yes/no is fine)
tkinter
Anything else you think would be helpful?
None

---------------------

#### Troubleshooting

These items may solve your problem. Please check those you've done by changing - [ ] to - [X]

- [x] Searched main docs for your problem  www.PySimpleGUI.org
- [x] Looked for Demo Programs that are similar to your goal. It is recommend you use the Demo Browser! Demos.PySimpleGUI.org
- [x] None of your GUI code was generated by an AI algorithm like GPT
- [ ] If not tkinter - looked for Demo Programs for specific port
- [ ] For non tkinter - Looked at readme for your specific port if not PySimpleGUI (Qt, WX, Remi)
- [x] Run your program outside of your debugger (from a command line)
- [x] Searched through Issues (open and closed) to see if already reported Issues.PySimpleGUI.org
- [x] Have upgraded to the latest release of PySimpleGUI on PyPI (lastest official version)
- [x] Tried using the PySimpleGUI.py file on GitHub. Your problem may have already been fixed but not released

#### Detailed Description

The event value of the TabGroup element is different between existing Tabs and new Tabs which is added by call method `add_tab`.

It should be caused by that the information of the new added Tab element missed to add into `element.tab_index_to_key` in method `TabGroup.add_tab`.

#### Code To Duplicate

```python
import PySimpleGUI as sg

def tab(index):
    return sg.Tab(f""TAB {index}"", [[sg.Text(f""This is Tab {index}"")]], key=f""{index} TAB"")

total = 2
layout = [[sg.TabGroup([[tab(index) for index in range(total)]], enable_events=True, key='TABGROUP')]]
window = sg.Window('Title', layout, finalize=True)
window['TABGROUP'].add_tab(tab(total))

while True:

    event, values = window.read()

    if event == sg.WIN_CLOSED:
        break
    print(event, values)

window.close()
```

Click each Tab to check the value for TabGroup element

```python
TABGROUP {'TABGROUP': '1 TAB'}    # Key of Tab
TABGROUP {'TABGROUP': '0 TAB'}    # Key of Tab
TABGROUP {'TABGROUP': 'TAB 2'}    # Title of Tab
```
#### Screenshot, Sketch, or Drawing
![image](https://github.com/PySimpleGUI/PySimpleGUI/assets/55352169/2bcc98a3-55df-4977-b87b-7075cb9844d2)
","['Bug', 'Port - TK']",0
PySimpleGUI/PySimpleGUI,"[Enhancement] Yield the ""right click menu"" event before the ""cell clicked"" event when right_click_selects=True","## Type of Issue (Enhancement, Error, Bug, Question)

Enhancement

----------------------------------------

## Environment 

#### Operating System

Windows  version ('10', '10.0.19045', 'SP0', 'Multiprocessor Free')

#### PySimpleGUI Port (tkinter, Qt, Wx, Web)

tkinter

----------------------------------------

## Versions


#### Python version (`sg.sys.version`)

3.11.3 (tags/v3.11.3:f3909b8, Apr  4 2023, 23:49:59) [MSC v.1934 64 bit (AMD64)]

#### PySimpleGUI Version (`sg.__version__`)

4.60.5

#### GUI Version  (tkinter (`sg.tclversion_detailed`), PySide2, WxPython, Remi)

8.6.12



---------------------

## Your Experience In Months or Years (optional)

 Years Python programming experience
 Years Programming experience overall
No Have used another Python GUI Framework? (tkinter, Qt, etc) (yes/no is fine)


---------------------

## Troubleshooting

These items may solve your problem. Please check those you've done by changing - [ ] to - [X]

- [X] Searched main docs for your problem  www.PySimpleGUI.org
- [X] Looked for Demo Programs that are similar to your goal. It is recommend you use the Demo Browser! Demos.PySimpleGUI.org
- [x] If not tkinter - looked for Demo Programs for specific port
- [x] For non tkinter - Looked at readme for your specific port if not PySimpleGUI (Qt, WX, Remi)
- [X] Run your program outside of your debugger (from a command line)
- [ ] Searched through Issues (open and closed) to see if already reported Issues.PySimpleGUI.org
- [ ] Upgraded to the latest official release of PySimpleGUI on PyPI
- [ ] Tried using the PySimpleGUI.py file on GitHub. Your problem may have already been fixed but not released

## Detailed Description

When using the right_click_menu options with the Table element, and right_click_selects=True, the event for the click happens before the event for the right click. This is counterintuitive because the right click is the primary action, so it should be emitted first.

I also can't figure out if there's a way to get the cell that was right clicked, though I can work around that.

#### Code To Duplicate


```python
import PySimpleGUI as sg

menu = ["""", [""Option 1"", ""Option 2""]]
table = sg.Table(
    values=[[1.0, ""foo""], [2.0, ""bar""]],
    headings=[""Reading"", ""Name""],
    right_click_menu=menu,
    enable_click_events=True
)

layout = [[table]]

window = sg.Window(layout=layout, title=""Right Click Menu Bug for Table"")

while True:
    event, values = window.read()

    if event in [sg.WIN_CLOSED]:
        break

    print(event, values[event])


```

#### Screenshot, Sketch, or Drawing","['enhancement', 'Port - TK']",0
PySimpleGUI/PySimpleGUI,[Question] Is there a clear 'sg.FolderBrowse' for Android?,"### Type of Issue (Enhancement, Error, Bug, Question)

Question

----------------------------------------

#### Operating System

Windows 11
Making for Android

#### PySimpleGUI Port (tkinter, Qt, Wx, Web)

Tkinter

----------------------------------------

## Versions

Version information can be obtained by calling `sg.main_get_debug_data()`
Or you can print each version shown in ()


#### Python version (`sg.sys.version`)

3.11.6

#### PySimpleGUI Version (`sg.__version__`)

4.60.5

#### GUI Version  (tkinter (`sg.tclversion_detailed`), PySide2, WxPython, Remi)

8.6.12

---------------------

#### Your Experience In Months or Years (optional)

Years Python programming experience
Almost 1 year

Years Programming experience overall
Almost 1 year

Have used another Python GUI Framework? (tkinter, Qt, etc) (yes/no is fine)
no

Anything else you think would be helpful?
No

---------------------

#### Troubleshooting

These items may solve your problem. Please check those you've done by changing - [ ] to - [X]

- [X] Searched main docs for your problem  www.PySimpleGUI.org
- [ ] Looked for Demo Programs that are similar to your goal. It is recommend you use the Demo Browser! Demos.PySimpleGUI.org
- [ ] None of your GUI code was generated by an AI algorithm like GPT
- [ ] If not tkinter - looked for Demo Programs for specific port
- [ ] For non tkinter - Looked at readme for your specific port if not PySimpleGUI (Qt, WX, Remi)
- [ ] Run your program outside of your debugger (from a command line)
- [X] Searched through Issues (open and closed) to see if already reported Issues.PySimpleGUI.org
- [X] Have upgraded to the latest release of PySimpleGUI on PyPI (lastest official version)
- [ ] Tried using the PySimpleGUI.py file on GitHub. Your problem may have already been fixed but not released

#### Detailed Description

I tried just copying the code from my IDE on windows (Pycharm) to my IDE on Android (Pydroid3) and executing the code, and the window showed and worked. When I tried to browse for the file location using 'sg.FolderBrowse', it showed me a menu with the path to the .py file that is running, but not just a file browser where i can click around and find the folder. Is there a way to get the system file browser to open instead of the PySimpleGUI one?

#### Screenshot, Sketch, or Drawing

<a href='https://postimg.cc/tnb3zpz5' target='_blank'><img src='https://i.postimg.cc/tnb3zpz5/1703946752779.jpg' border='0' alt='1703946752779'/></a>

---------------------

### Watcha Makin?

I'm making a Youtube video downloader with additional GUI. It supports whole Youtube playlists and the option to just download the audio.
","['Question', 'Platform Specific Issue - Android']",0
PySimpleGUI/PySimpleGUI,[ Bug] Some elements are not being reported by screen readers,"### Type of Issue (Enhancement, Error, Bug, Question)

Bug

----------------------------------------

#### Operating System

Windows 11


#### PySimpleGUI Port (tkinter, Qt, Wx, Web)

Wx


----------------------------------------

## Versions

Version information can be obtained by calling `sg.main_get_debug_data()`
Or you can print each version shown in ()


#### Python version (`sg.sys.version`)

Python 3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64bit (AMD64)]


#### PySimpleGUI Version (`sg.__version__`)

4.60.5

#### GUI Version  (tkinter (`sg.tclversion_detailed`), PySide2, WxPython, Remi)

4.2.1

---------------------

#### Your Experience In Months or Years (optional)

Years Python programming experience
About 4 Months I guess...
Years Programming experience overall
About 2.5 years now
Have used another Python GUI Framework? (tkinter, Qt, etc) (yes/no is fine)
Nope

---------------------

#### Troubleshooting

These items may solve your problem. Please check those you've done by changing - [ ] to - [X]

- [X] Searched main docs for your problem  www.PySimpleGUI.org
- [X] Looked for Demo Programs that are similar to your goal. It is recommend you use the Demo Browser! Demos.PySimpleGUI.org
- [X] None of your GUI code was generated by an AI algorithm like GPT
- [X] If not tkinter - looked for Demo Programs for specific port
- [X] For non tkinter - Looked at readme for your specific port if not PySimpleGUI (Qt, WX, Remi)
- [X] Run your program outside of your debugger (from a command line)
- [X] Searched through Issues (open and closed) to see if already reported Issues.PySimpleGUI.org
- [X] Have upgraded to the latest release of PySimpleGUI on PyPI (lastest official version)
- [ ] Tried using the PySimpleGUI.py file on GitHub. Your problem may have already been fixed but not released

#### Detailed Description
I'm using PySimpleGUIWx, and, while working with it, I noticed that some elements are not being reported by screen readers (I tested with NVDA at least). I know buttons, input fields and popups  do get reported, but some other elements like dropdowns and texts don't. I don't know the full list of unreported elements, because I didn't test all of them. As discussed in:
<https://nvda.groups.io/g/nvda/topic/how_does_nvda_make_its_gui/87411360?p=>
According to one of NVDA developers, Wx elements are accessible, so I don't exactly know why using the Wx port to ""communicate"" with Wx doesn't produce the same output. I know that it may be the way PySimpleGUIWx is implemented, but I didn't take a look at the code, nor I think I have enough experience and knowledge, neither in python nor in Wx to discover or even guess why this happens. If I were to suggest something, I would tell you to test which elements don't get reported. I also will try doing the same tests with other screen readers and post here, but to be honest, NVDA does a very good job most of the time, and if something is not being reported by NVDA, it's very unlikely that it will get reported by other screen readers in most of cases.

Anyway, I hope this contribution will be useful somehow and will help devs create accessible apps really easy.

#### Code To Duplicate

A **short** program that isolates and demonstrates the problem (Do not paste your massive program, but instead 10-20 lines that clearly show the problem)

This pre-formatted code block is all set for you to paste in your bit of code:

```python

# Paste your code here

There's no relevant code to show i guess...
```

#### Screenshot, Sketch, or Drawing

---------------------

### Watcha Makin?

If you care to share something about your project, it would be awesome to hear what you're building.
Just a chess client...",[],0
PySimpleGUI/PySimpleGUI,[Enhancement] Nativ support for getting and setting indices in e.g. Combo,"## Type of Issue (Enhancement, Error, Bug, Question)

Enhancement

----------------------------------------

## Environment 

#### Operating System

Windows  version ('10', '10.0.19045', 'SP0', 'Multiprocessor Free')

#### PySimpleGUI Port (tkinter, Qt, Wx, Web)

tkinter

----------------------------------------

## Versions


#### Python version (`sg.sys.version`)

3.11.6 (tags/v3.11.6:8b6ee5b, Oct  2 2023, 14:57:12) [MSC v.1935 64 bit (AMD64)]

#### PySimpleGUI Version (`sg.__version__`)

4.61.0.198

#### GUI Version  (tkinter (`sg.tclversion_detailed`), PySide2, WxPython, Remi)

8.6.12



---------------------

## Your Experience In Months or Years (optional)

2 Years Python programming experience
10 Years Programming experience overall
Yes Have used another Python GUI Framework? (tkinter, Qt, etc) (yes/no is fine)


---------------------

## Troubleshooting

These items may solve your problem. Please check those you've done by changing - [ ] to - [X]

- [X] Searched main docs for your problem  www.PySimpleGUI.org
- [X] Looked for Demo Programs that are similar to your goal. It is recommend you use the Demo Browser! Demos.PySimpleGUI.org
- [X] If not tkinter - looked for Demo Programs for specific port
- [X] For non tkinter - Looked at readme for your specific port if not PySimpleGUI (Qt, WX, Remi)
- [X] Run your program outside of your debugger (from a command line)
- [X] Searched through Issues (open and closed) to see if already reported Issues.PySimpleGUI.org
- [X] Upgraded to the latest official release of PySimpleGUI on PyPI
- [X] Tried using the PySimpleGUI.py file on GitHub. Your problem may have already been fixed but not released

## Detailed Description

In the application I am currently working on, the user can choose between several options in a ```Combo```. Depending on their choice, some data is displayed in a ```Listbox``` (different for each option in the Combo). Now the user can change the displayed data and the ```Listbox``` is updated accordingly.

All the data is saved internally in my application and displayed in a heavily formatted way. Therefore, I only need the indices of the ```Combo``` and the ```Listbox```, which for the ```Combo``` I have to get through ```tkinter``` calls via ```window[combo_key].TKCombo.current()```, and never their content.

Since one of my long-term goals for this application is to write a port for the ```PySimpleGUIWeb``` port, this problem contradicts the line ""just replace ```import PySimpleGUI``` by ```PySimpleGUIWeb```"".

Also, everything that is stored in ```values[combo_key]``` is useless for me and only wastes memory.

There are two possible solutions for this problem which come to mind (not only for ```Combo``` but also for similar index-'supporting' objects):
 - Add methods to get and set the index. (```Listbox``` has those methods for the selected lines which I use already.)
 - Add an option to the constructor s.t. not the selected item but the selected index is contained in ```values[element_key]``` (or both, i.e. as ```(index, value)```). For backwards compatibility this can default to the old behaviour.

I far prefer the second solution, but the first one would already be really nice.","['enhancement', 'Port - TK']",0
PySimpleGUI/PySimpleGUI,[ Enhancement] High Contrast Themes for PySimpleGUI,"### Type of Issue (Enhancement, Error, Bug, Question)

Enhancement

----------------------------------------

#### Operating System

Windows 11


#### PySimpleGUI Port (tkinter, Qt, Wx, Web)

Wx


----------------------------------------

## Versions

Version information can be obtained by calling `sg.main_get_debug_data()`
Or you can print each version shown in ()


#### Python version (`sg.sys.version`)

Python 3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64bit (AMD64)]


#### PySimpleGUI Version (`sg.__version__`)

4.60.5

#### GUI Version  (tkinter (`sg.tclversion_detailed`), PySide2, WxPython, Remi)

4.2.1

---------------------

#### Your Experience In Months or Years (optional)

Years Python programming experience
About 4 Months I guess...
Years Programming experience overall
About 2.5 years now
Have used another Python GUI Framework? (tkinter, Qt, etc) (yes/no is fine)
Nope
Anything else you think would be helpful?
Just to say that I'm in love with PySimpleGUI. Even someone who is totally blind can do good guis lol. Congrats for the awesome job!

---------------------

#### Troubleshooting

These items may solve your problem. Please check those you've done by changing - [ ] to - [X]

- [X] Searched main docs for your problem  www.PySimpleGUI.org
- [X] Looked for Demo Programs that are similar to your goal. It is recommend you use the Demo Browser! Demos.PySimpleGUI.org
- [X] None of your GUI code was generated by an AI algorithm like GPT
- [X] If not tkinter - looked for Demo Programs for specific port
- [X] For non tkinter - Looked at readme for your specific port if not PySimpleGUI (Qt, WX, Remi)
- [X] Run your program outside of your debugger (from a command line)
- [X] Searched through Issues (open and closed) to see if already reported Issues.PySimpleGUI.org
- [X] Have upgraded to the latest release of PySimpleGUI on PyPI (lastest official version)
- [ ] Tried using the PySimpleGUI.py file on GitHub. Your problem may have already been fixed but not released

#### Detailed Description

It's just that. While looking through the list of themes, I thought it would be a nice idea to have some high contrast theme options to make it easier for devs to make their programs more accessible to everyone.
A random article I found on the web (probably not necessary but anyway) about the topic:
<https://uxdesign.cc/high-contrast-when-you-think-the-dark-mode-is-enough-d190218d4bba>

Anyway, I hope this contribution will be useful somehow and will help devs create accessible apps really easy.

#### Code To Duplicate

A **short** program that isolates and demonstrates the problem (Do not paste your massive program, but instead 10-20 lines that clearly show the problem)

This pre-formatted code block is all set for you to paste in your bit of code:

```python

# Paste your code here

There's no relevant code to show i guess...
```

#### Screenshot, Sketch, or Drawing

---------------------

### Watcha Makin?

If you care to share something about your project, it would be awesome to hear what you're building.
Just a chess client...","['enhancement', 'Port - TK']",0
PySimpleGUI/PySimpleGUI,[ Bug]  set_vscroll_position on multiline broken in version 4.61.0.206,"### Type of Issue (Enhancement, Error, Bug, Question)

Bug

----------------------------------------

#### Operating System

Windows 10

#### PySimpleGUI Port (tkinter, Qt, Wx, Web)

tkinter

----------------------------------------

## Versions

Python version: 3.12.0 (tags/v3.12.0:0fb18b0, Oct  2 2023, 13:03:39) [MSC v.1935 64 bit (AMD64)]
port: tkinter
tkinter version: 8.6.13
PySimpleGUI version: 4.60.5
PySimpleGUI filename: C:\Users\Simon\AppData\Roaming\Python\Python312\site-packages\PySimpleGUI\PySimpleGUI.py

---------------------

#### Your Experience In Months or Years (optional)

Years Python programming experience - 1 year

Years Programming experience overall - 40 plus years (doesn't mean I'm any good though)

Have used another Python GUI Framework? (tkinter, Qt, etc) (yes/no is fine) - No

Anything else you think would be helpful?


---------------------

#### Troubleshooting

These items may solve your problem. Please check those you've done by changing - [ ] to - [X]

- [X ] Searched main docs for your problem  www.PySimpleGUI.org
- [ ] Looked for Demo Programs that are similar to your goal. It is recommend you use the Demo Browser! Demos.PySimpleGUI.org
- [X ] None of your GUI code was generated by an AI algorithm like GPT
- [ ] If not tkinter - looked for Demo Programs for specific port
- [ ] For non tkinter - Looked at readme for your specific port if not PySimpleGUI (Qt, WX, Remi)
- [ X] Run your program outside of your debugger (from a command line)
- [ ] Searched through Issues (open and closed) to see if already reported Issues.PySimpleGUI.org
- [ ] Have upgraded to the latest release of PySimpleGUI on PyPI (lastest official version)
- [X ] Tried using the PySimpleGUI.py file on GitHub. Your problem may have already been fixed but not released

#### Detailed Description

Looks like a bug, but it might just be the way I am using it.

Using cprint to print to multiline element. Using set_vscroll_position when output has more lines than multiline can show.

Worked beautifully in pysimplegui 4.60.5

Upgraded to pysimplegui 4.61.0.206 to take advantage of something... can't remember what it was. Some days later realised that
my cprint to multiline does not scroll / scrolls by a tiny amount in pysimplegui 4.61.0.206. It can be made to work with an additional call to window.refresh(), but this makes the multiline flash in a most unattractive manner.

Reverted to version 4.60.5 and it works beautifully again. No window.refresh required and no flashing.


#### Code To Duplicate

A **short** program that isolates and demonstrates the problem (Do not paste your massive program, but instead 10-20 lines that clearly show the problem)

This pre-formatted code block is all set for you to paste in your bit of code:

```python

# Paste your code here
# It will take some effort to create a short program to isolate the issue. If you are not already aware of this I will try to produce something.

```

#### Screenshot, Sketch, or Drawing

---------------------

### Watcha Makin?

A program to replay chess games from my games database. I use cprint to get the current move highlighted in red in the multiline.
![screenshot](https://github.com/PySimpleGUI/PySimpleGUI/assets/98977276/25b504a3-3109-4679-a7c7-1790a7572fa9)

","['Bug', 'Port - TK']",0
PySimpleGUI/PySimpleGUI,[ Bug] Python crash when do win.read(),"### Type of Issue (Enhancement, Error, Bug, Question)


----------------------------------------

#### Operating System

Mac Sonoma 14.1

#### PySimpleGUI Port (tkinter, Qt, Wx, Web)

tkinter

----------------------------------------

## Versions

Version information can be obtained by calling `sg.main_get_debug_data()`
Or you can print each version shown in ()
Python version: 3.12.1 (v3.12.1:2305ca5144, Dec  7 2023, 17:23:38) [Clang 13.0.0 (clang-1300.0.29.30)]
        port: tkinter
        tkinter version: 8.6.13
        PySimpleGUI version: 4.60.5
        PySimpleGUI filename: /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/PySimpleGUI/PySimpleGUI.py

---------------------

#### Your Experience In Months or Years (optional)

Years Python programming experience

Years Programming experience overall

Have used another Python GUI Framework? (tkinter, Qt, etc) (yes/no is fine)

Anything else you think would be helpful?


---------------------

#### Troubleshooting

These items may solve your problem. Please check those you've done by changing - [ ] to - [X]

- [x] Searched main docs for your problem  www.PySimpleGUI.org
- [x] Looked for Demo Programs that are similar to your goal. It is recommend you use the Demo Browser! Demos.PySimpleGUI.org
- [x] None of your GUI code was generated by an AI algorithm like GPT
- [ ] If not tkinter - looked for Demo Programs for specific port
- [ ] For non tkinter - Looked at readme for your specific port if not PySimpleGUI (Qt, WX, Remi)
- [x] Run your program outside of your debugger (from a command line)
- [x] Searched through Issues (open and closed) to see if already reported Issues.PySimpleGUI.org
- [x] Have upgraded to the latest release of PySimpleGUI on PyPI (lastest official version)
- [x] Tried using the PySimpleGUI.py file on GitHub. Your problem may have already been fixed but not released

#### Detailed Description
If I use Python3.12, then python will crash when do ""evt, val = win.read()""
with same script, If I use Python3.9, then UI will launch successfully for  ""evt, val = win.read()""


#### Code To Duplicate

A **short** program that isolates and demonstrates the problem (Do not paste your massive program, but instead 10-20 lines that clearly show the problem)

This pre-formatted code block is all set for you to paste in your bit of code:

```python

# Paste your code here
win = sg.Window(window_title, main_layout, alpha_channel=0.95, grab_anywhere=False, resizable=True)
while True:
     evt, val = win.read()

```

#### Screenshot, Sketch, or Drawing
*** Terminating app due to uncaught exception 'NSInvalidArgumentException', reason: '-[NSApplication macOSVersion]: unrecognized selector sent to instance 0x12ca1c050'
*** First throw call stack:
(
	0   CoreFoundation                      0x0000000189f92800 __exceptionPreprocess + 176
	1   libobjc.A.dylib                     0x0000000189a89eb4 objc_exception_throw + 60
	2   CoreFoundation                      0x000000018a0443bc -[NSObject(NSObject) __retain_OA] + 0
	3   CoreFoundation                      0x0000000189efca84 ___forwarding___ + 1572
	4   CoreFoundation                      0x0000000189efc3a0 _CF_forwarding_prep_0 + 96
	5   libtk8.6.dylib                      0x000000013ccc63e8 GetRGBA + 64
	6   libtk8.6.dylib                      0x000000013ccc6168 TkpGetColor + 476
	7   libtk8.6.dylib                      0x000000013cc144b4 Tk_GetColor + 152
	8   libtk8.6.dylib                      0x000000013cc05c98 Tk_Get3DBorder + 136
	9   libtk8.6.dylib                      0x000000013cc05ae8 Tk_Alloc3DBorderFromObj + 144
	10  libtk8.6.dylib                      0x000000013cc159b4 DoObjConfig + 832
	11  libtk8.6.dylib                      0x000000013cc1557c Tk_InitOptions + 372
	12  libtk8.6.dylib                      0x000000013cc15460 Tk_InitOptions + 88
	13  libtk8.6.dylib                      0x000000013cc419a8 CreateFrame + 1472
	14  libtk8.6.dylib                      0x000000013cc41ca8 TkListCreateFrame + 156
	15  libtk8.6.dylib                      0x000000013cc3a690 Initialize + 1856
	16  _tkinter.cpython-312-darwin.so      0x000000011ca26390 Tcl_AppInit + 92
	17  _tkinter.cpython-312-darwin.so      0x000000011ca2138c Tkapp_New + 548
	18  _tkinter.cpython-312-darwin.so      0x000000011ca21164 _tkinter_create_impl + 268
	19  _tkinter.cpython-312-darwin.so      0x000000011ca20c3c _tkinter_create + 236
	20  Python                              0x0000000105d1d820 _PyEval_EvalFrameDefault + 50696
	21  Python                              0x0000000105bd70fc _PyObject_FastCallDictTstate + 96
	22  Python                              0x0000000105c742c8 slot_tp_init + 208
	23  Python                              0x0000000105c6914c type_call + 148
	24  Python                              0x0000000105bd7378 _PyObject_MakeTpCall + 128
	25  Python                              0x0000000105d1ca40 _PyEval_EvalFrameDefault + 47144
	26  Python                              0x0000000105d10f00 PyEval_EvalCode + 304
	27  Python                              0x0000000105d91448 run_mod + 176
	28  Python                              0x0000000105d904d8 pyrun_file + 148
	29  Python                              0x0000000105d8f43c _PyRun_SimpleFileObject + 304
	30  Python                              0x0000000105d8f0ac _PyRun_AnyFileObject + 248
	31  Python                              0x0000000105dbd330 pymain_run_file_obj + 220
	32  Python                              0x0000000105dbd070 pymain_run_file + 72
	33  Python                              0x0000000105dbc9a8 Py_RunMain + 1464
	34  Python                              0x0000000105dbcb48 pymain_main + 36
	35  Python                              0x0000000105dbcbc0 Py_BytesMain + 40
	36  dyld                                0x0000000189ac50e0 start + 2360
)
libc++abi: terminating due to uncaught exception of type NSException

---------------------

### Watcha Makin?

If you care to share something about your project, it would be awesome to hear what you're building.
","['Platform Specific Issue - Mac', 'Missing Code To Duplicate']",0
PySimpleGUI/PySimpleGUI,[ Enhancement]  Add dash line and dotted line for graph element draw_line and draw_lines,"### Type of Issue (Enhancement, Error, Bug, Question)
Enhancement

----------------------------------------

#### Operating System
Operating System: openSUSE Tumbleweed 20231114
KDE Plasma Version: 5.27.9
KDE Frameworks Version: 5.112.0
Qt Version: 5.15.11
Kernel Version: 6.6.1-1-default (64-bit)
Graphics Platform: X11
Processors: 12 Ã— AMD Ryzen 5 4600H with Radeon Graphics
Memory: 7.6 GiB of RAM
Graphics Processor: NVIDIA GeForce GTX 1650 Ti/PCIe/SSE2
Manufacturer: LENOVO
Product Name: 82B5
System Version: Lenovo Legion 5 15ARH05


#### PySimpleGUI Port (tkinter, Qt, Wx, Web)
## Versions
Python version: 3.11.5 (main, Nov 10 2011, 15:00:00) [GCC 13.2.0]
        port: tkinter
        tkinter version: 8.6.13
        PySimpleGUI version: 4.60.5

---------------------

#### Your Experience In Months or Years (optional)
1
Years Python programming experience
1
Years Programming experience overall
1
Have used another Python GUI Framework? (tkinter, Qt, etc) (yes/no is fine)
no
Anything else you think would be helpful?


---------------------

#### Troubleshooting

These items may solve your problem. Please check those you've done by changing - [ ] to - [X]

- [x] Searched main docs for your problem  www.PySimpleGUI.org
- [x] Looked for Demo Programs that are similar to your goal. It is recommend you use the Demo Browser! Demos.PySimpleGUI.org
- [x] None of your GUI code was generated by an AI algorithm like GPT
- [ ] If not tkinter - looked for Demo Programs for specific port
- [ ] For non tkinter - Looked at readme for your specific port if not PySimpleGUI (Qt, WX, Remi)
- [x] Run your program outside of your debugger (from a command line)
- [x] Searched through Issues (open and closed) to see if already reported Issues.PySimpleGUI.org
- [x] Have upgraded to the latest release of PySimpleGUI on PyPI (lastest official version)
- [x] Tried using the PySimpleGUI.py file on GitHub. Your problem may have already been fixed but not released

#### Detailed Description

From my knowledge, the draw_line and draw_lines graph elements do not have an option to draw a dash line.
I think it would be convenient to have an option similar to Matplotlib.
Something like this

`graph.draw_line((x1,y1),(x2,y2),width=3,color='red',linestyle='--')`

The code below is my attempt to replicate a dash line.
However, the downside is that I could not delete the whole dash line. The dash line is made of many smaller lines; only the latest small line was deleted.

#### Code To Duplicate

A **short** program that isolates and demonstrates the problem (Do not paste your massive program, but instead 10-20 lines that clearly show the problem)

This pre-formatted code block is all set for you to paste in your bit of code:

```
import PySimpleGUI as sg
import numpy as np

layout = [
    [sg.Graph(canvas_size=(400, 400), graph_bottom_left=(-1, -1), graph_top_right=(1, 1), background_color='white', key='graph', enable_events=True)],
    [sg.Button('Delete Dash Line')]
]

window = sg.Window('Dashed Line Example', layout,finalize=True)

start = (-0.8,-0.6)
end = (0.5,0.7)  

dash_length = 0.05

graph = window['graph']
window_key = 'graph'
def dash_line(window_key,start,end,dash_length):
    slope = (end[1]-start[1])/(end[0]-start[0])
    b = start[1] - slope*start[0]
    for x1 in np.linspace(start[0], end[0],num=10):
        x2 = x1 + dash_length
        y1 = slope*x1+b
        y2 = slope*x2+b  
        id_name = window[window_key].draw_line((x1,y1),(x2,y2),width=3,color='red')
    return id_name

id_name = dash_line(window_key,start,end,dash_length)

# window['graph'].erase()

while True:
    event, values = window.read()
    if event in (sg.WIN_CLOSED, 'Exit'):
        break
    if event == 'Delete Dash Line':
       graph.delete_figure(id_name) 
window.close()
```

#### Screenshot, Sketch, or Drawing

---------------------

### Watcha Makin?

If you care to share something about your project, it would be awesome to hear what you're building.
PySimpleCV
https://github.com/kevinsmia1939/PySimpleCV","['enhancement', 'Port - TK']",0
PySimpleGUI/PySimpleGUI,[Enhancement]  Add read_all_windows to PySimpleGUIWx,"### Type of Issue Enhancement


----------------------------------------

#### Operating System

Linux, but should be irrelevant.

#### PySimpleGUI Port (tkinter, Qt, Wx, Web)

Wx

----------------------------------------

## Versions

Version information can be obtained by calling `sg.main_get_debug_data()`
Or you can print each version shown in ()

#### Python version (`sg.sys.version`)

3.10.12

#### PySimpleGUI Version (`sg.__version__`)

4.60.5
PySimpleGUIWx version 0.17.1

#### GUI Version  (tkinter (`sg.tclversion_detailed`), PySide2, WxPython, Remi)

wxPython 4.2.0 gtk3

---------------------

#### Your Experience In Months or Years (optional)

Years Python programming experience: 20

Years Programming experience overall: 50

Have used another Python GUI Framework? (tkinter, Qt, etc) (yes/no is fine): Yes, many

Anything else you think would be helpful?

---------------------

#### Troubleshooting

These items may solve your problem. Please check those you've done by changing - [ ] to - [X]

- [X] Searched main docs for your problem  www.PySimpleGUI.org
- [X] Looked for Demo Programs that are similar to your goal. It is recommend you use the Demo Browser! Demos.PySimpleGUI.org
- [X] None of your GUI code was generated by an AI algorithm like GPT
- [ ] If not tkinter - looked for Demo Programs for specific port
- [X] For non tkinter - Looked at readme for your specific port if not PySimpleGUI (Qt, WX, Remi)
- [X] Run your program outside of your debugger (from a command line)
- [X] Searched through Issues (open and closed) to see if already reported Issues.PySimpleGUI.org
- [ ] Have upgraded to the latest release of PySimpleGUI on PyPI (lastest official version)
- [ ] Tried using the PySimpleGUI.py file on GitHub. Your problem may have already been fixed but not released

#### Detailed Description

PySimpleGUIiWx  does not have the ""read_all_windows"" method that allows a single dispatch loop to handle multiple windows, and I could not figure out a simple way to simulate it or a simple patch to the code to implement it.

#### Code To Duplicate

A **short** program that isolates and demonstrates the problem (Do not paste your massive program, but instead 10-20 lines that clearly show the problem)

This pre-formatted code block is all set for you to paste in your bit of code:

```python

# Paste your code here


```

#### Screenshot, Sketch, or Drawing

---------------------

### Watcha Makin?

I'm making a simple CRUD front-end for my invoice database.
","['enhancement', 'Port - Wx']",0
PySimpleGUI/PySimpleGUI,[ Enhancement] I suggest adding an international language to the study guide.,"### Type of Issue (Enhancement)

add international translation
----------------------------------------

#### Operating System



#### PySimpleGUI Port (tkinter, Qt, Wx, Web)



----------------------------------------

## Versions

Version information can be obtained by calling `sg.main_get_debug_data()`
Or you can print each version shown in ()


#### Python version (`sg.sys.version`)



#### PySimpleGUI Version (`sg.__version__`)



#### GUI Version  (tkinter (`sg.tclversion_detailed`), PySide2, WxPython, Remi)



---------------------

#### Your Experience In Months or Years (optional)

Years Python programming experience

Years Programming experience overall

Have used another Python GUI Framework? (tkinter, Qt, etc) (yes/no is fine)

Anything else you think would be helpful?


---------------------

#### Troubleshooting

These items may solve your problem. Please check those you've done by changing - [ ] to - [X]

- [ ] Searched main docs for your problem  www.PySimpleGUI.org
- [ ] Looked for Demo Programs that are similar to your goal. It is recommend you use the Demo Browser! Demos.PySimpleGUI.org
- [ ] None of your GUI code was generated by an AI algorithm like GPT
- [ ] If not tkinter - looked for Demo Programs for specific port
- [ ] For non tkinter - Looked at readme for your specific port if not PySimpleGUI (Qt, WX, Remi)
- [ ] Run your program outside of your debugger (from a command line)
- [ ] Searched through Issues (open and closed) to see if already reported Issues.PySimpleGUI.org
- [ ] Have upgraded to the latest release of PySimpleGUI on PyPI (lastest official version)
- [ ] Tried using the PySimpleGUI.py file on GitHub. Your problem may have already been fixed but not released

#### Detailed Description




#### Code To Duplicate

A **short** program that isolates and demonstrates the problem (Do not paste your massive program, but instead 10-20 lines that clearly show the problem)

This pre-formatted code block is all set for you to paste in your bit of code:

```python

# Paste your code here


```

#### Screenshot, Sketch, or Drawing

---------------------

### Watcha Makin?

If you care to share something about your project, it would be awesome to hear what you're building.
",[],0
PySimpleGUI/PySimpleGUI,"[Enhancement] Keyboard events with modifiers, and Tkinter Key events","### Type of Issue (Enhancement, Error, Bug, Question)

Enhancement
----------------------------------------

#### Operating System

Windows 11 pro

#### PySimpleGUI Port (tkinter, Qt, Wx, Web)


tkinter
----------------------------------------

## Versions

Version information can be obtained by calling `sg.main_get_debug_data()`
Or you can print each version shown in ()


#### Python version (`sg.sys.version`)

3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]

#### PySimpleGUI Version (`sg.__version__`)

4.60.5

#### GUI Version  (tkinter (`sg.tclversion_detailed`), PySide2, WxPython, Remi)

8.6.12

---------------------

#### Your Experience In Months or Years (optional)

Years Python programming experience
10+
Years Programming experience overall
40+
Have used another Python GUI Framework? (tkinter, Qt, etc) (yes/no is fine)
wxPython
Anything else you think would be helpful?


---------------------

#### Troubleshooting

These items may solve your problem. Please check those you've done by changing - [ ] to - [X]

- [X] Searched main docs for your problem  www.PySimpleGUI.org
- [X] Looked for Demo Programs that are similar to your goal. It is recommend you use the Demo Browser! Demos.PySimpleGUI.org
- [X] None of your GUI code was generated by an AI algorithm like GPT
- [ ] If not tkinter - looked for Demo Programs for specific port
- [ ] For non tkinter - Looked at readme for your specific port if not PySimpleGUI (Qt, WX, Remi)
- [X] Run your program outside of your debugger (from a command line)
- [X] Searched through Issues (open and closed) to see if already reported Issues.PySimpleGUI.org
- [ ] Have upgraded to the latest release of PySimpleGUI on PyPI (lastest official version)
- [ ] Tried using the PySimpleGUI.py file on GitHub. Your problem may have already been fixed but not released

#### Detailed Description
I need to handle quite a few keyboard events for a project; let's say I need to recognize any letter [a-z] with any combination of  `<Ctrl>` and `<Alt>`.

The behaviour of `return_keyboard_events=True` is a bit of a surprise to me; I expected a ""Ctrl-a"" event, not two subsequent events, one for the ""a"" and then one for Ctrl. But this is something that can be partly managed, since the event for ""a"" is different in the two cases: just ""a"" when the key is pressed alone, and ""a:65"" when it is pressed with Ctrl. So when we get ""a:65"" we know we need to wait for the next event, although if the user typed something like ""Ctrl-a-b"", without releasing Ctrl, the probem would arise again (they would not see the effect of their first command until they input the second one).
But for ""Alt"" the behaviour is different: in that case the event for the normal key is ""a"" again.

So this brings to the enhancement proposal: it would be very nice if we had a single event, showing the key AND the modifier(s); but at least Ctrl and Alt (and Shift) should behave in the same way.

At this point I turned to binding the Tkinter event, and found the error: it didn't work for any of `<Key>`, `<KeyPress>`, `<KeyUp>`. Just to check I tried `<Enter>` and `<Button-3>` and they worked like a charm.

#### Code To Duplicate

A **short** program that isolates and demonstrates the problem (Do not paste your massive program, but instead 10-20 lines that clearly show the problem)

This pre-formatted code block is all set for you to paste in your bit of code:

```python
import PySimpleGUI as sg

grid = [[sg.Graph(canvas_size=(450,450), graph_bottom_left=(0.0,9.0), graph_top_right=(9.0,0.0),
				  background_color=""light grey"", key=""the_grid"", float_values=True,
				  enable_events=True)],
		[sg.Ok(key=""OK"")]
		]
text_elem = sg.Text(size=(30, 1))
grid[-1].append(text_elem)

##window = sg.Window('The Grid', grid, return_keyboard_events=True, use_default_focus=False, finalize=True)
window = sg.Window('The Grid', grid, finalize=True)

window[""the_grid""].bind(""<KeyUp>"", ""+keypress"") #if we replace the event with <Enter> it works
window[""the_grid""].bind(""<Button-3>"", ""+rightclick"")

for r in range(9):
	for c in range(9):
		window[""the_grid""].draw_rectangle(top_left=(r+0.05,c+0.05), bottom_right=(r+0.95,c+0.95), fill_color=""white"")

window.refresh()

cnt = 0
while True:
	event, values = window.read() #a right click is intercepted, a keypress is not
	cnt += 1
	if event in [sg.WIN_CLOSED, ""OK""]:
		break
	text_elem.update(f'{event} {values}')
	print(cnt, event)
	if '+' in event:
		print('got!', window[""the_grid""].user_bind_event)
window.close()

```

#### Screenshot, Sketch, or Drawing

---------------------

### Watcha Makin?

If you care to share something about your project, it would be awesome to hear what you're building.
",[],0
PySimpleGUI/PySimpleGUI,"[Enhancement]  adding rotate_figure for graph element lines, polygons, arcs, etc (texts would also be nice to be able to rotate)","### Type of Issue (Enhancement, Error, Bug, Question)
Enhancement
----------------------------------------
#### Operating System
Pi OS

#### PySimpleGUI Port (tkinter, Qt, Wx, Web)
Tkinter

----------------------------------------
## Versions

#### Python version (`sg.sys.version`)
3.11.2

#### PySimpleGUI Version (`sg.__version__`)
8.6.11

#### GUI Version  (tkinter (`sg.tclversion_detailed`), PySide2, WxPython, Remi)
4.60.5

---------------------

#### Your Experience In Months or Years (optional)

Years Python programming experience
1
Years Programming experience overall
20
Have used another Python GUI Framework? (tkinter, Qt, etc) (yes/no is fine)
no
Anything else you think would be helpful?


---------------------

#### Troubleshooting

These items may solve your problem. Please check those you've done by changing - [ ] to - [X]

- [ ] Searched main docs for your problem  www.PySimpleGUI.org
- [ ] Looked for Demo Programs that are similar to your goal. It is recommend you use the Demo Browser! Demos.PySimpleGUI.org
- [ ] None of your GUI code was generated by an AI algorithm like GPT
- [ ] If not tkinter - looked for Demo Programs for specific port
- [ ] For non tkinter - Looked at readme for your specific port if not PySimpleGUI (Qt, WX, Remi)
- [ ] Run your program outside of your debugger (from a command line)
- [ ] Searched through Issues (open and closed) to see if already reported Issues.PySimpleGUI.org
- [ ] Have upgraded to the latest release of PySimpleGUI on PyPI (lastest official version)
- [ ] Tried using the PySimpleGUI.py file on GitHub. Your problem may have already been fixed but not released

#### Detailed Description
It would be nice and helpful to have a:
rotate_figure(figure_id, center_of_rotation, rotation_angle)
for the graph element items.

","['enhancement', 'Port - TK']",0
PySimpleGUI/PySimpleGUI,[Enhancement] Want to use PySimpleGUI in pyscript application.,"### Type of Issue (Enhancement, Error, Bug, Question)
Enhancement request
Need a pure python wheel for PySide2
----------------------------------------

#### Operating System
Kubuntu 20.04


#### PySimpleGUI Port (tkinter, Qt, Wx, Web)
PySimpleGUIQt


----------------------------------------

## Versions

Version information can be obtained by calling `sg.main_get_debug_data()`
Or you can print each version shown in ()


#### Python version (`sg.sys.version`)
3.8.10


#### PySimpleGUI Version (`sg.__version__`)
PySimpleGUIQt 0.35.0


#### GUI Version  (tkinter (`sg.tclversion_detailed`), PySide2, WxPython, Remi)
pyside2-5.15.2.1


---------------------

#### Your Experience In Months or Years (optional)

Years Python programming experience
20
Years Programming experience overall
30
Have used another Python GUI Framework? (tkinter, Qt, etc) (yes/no is fine)
yes
Anything else you think would be helpful?
no

---------------------

#### Troubleshooting

These items may solve your problem. Please check those you've done by changing - [ ] to - [X]

- [x ] Searched main docs for your problem  www.PySimpleGUI.org
- [ ] Looked for Demo Programs that are similar to your goal. It is recommend you use the Demo Browser! Demos.PySimpleGUI.org
- [x ] None of your GUI code was generated by an AI algorithm like GPT
- [x ] If not tkinter - looked for Demo Programs for specific port
- [ ] For non tkinter - Looked at readme for your specific port if not PySimpleGUI (Qt, WX, Remi)
- [ ] Run your program outside of your debugger (from a command line)
- [ ] Searched through Issues (open and closed) to see if already reported Issues.PySimpleGUI.org
- [ ] Have upgraded to the latest release of PySimpleGUI on PyPI (lastest official version)
- [ ] Tried using the PySimpleGUI.py file on GitHub. Your problem may have already been fixed but not released

#### Detailed Description
I would like to use PySimpleGUIQt in a 'pyscript' application.  PyScript attempts to use PySimpleGui but fails because there is no pure python wheel for PySide2.  I think that the addition of PySimpleGUIQt would be a great enhancement for PyScript.  I suspect that Mike would agree; hence the effort to develop PySimpleGUIWeb.  PyScript indicates that I could build a wheel for  PySide2 which could be used.  I don't know  how to do that.  QUESTION: Would you(psg) have any interest in generating a pure python wheel for PySide2 if it would mean that the psg code could be used with  PyScript?  Think about psg running in the browser with out a server.



#### Code To Duplicate

A **short** program that isolates and demonstrates the problem (Do not paste your massive program, but instead 10-20 lines that clearly show the problem)

This pre-formatted code block is all set for you to paste in your bit of code:

```python

# Paste your code here


```

#### Screenshot, Sketch, or Drawing

---------------------

### Watcha Makin?

If you care to share something about your project, it would be awesome to hear what you're building.
",['enhancement'],0
PySimpleGUI/PySimpleGUI,[Bug] Multiline element not generating events for pressing enter when enter_submits=True,"### Type of Issue (Enhancement, Error, Bug, Question)

Bug

----------------------------------------

#### Operating System

Windows 11

----------------------------------------

#### PySimpleGUI Port (tkinter, Qt, Wx, Web)

tkinter

----------------------------------------

## Versions

Python version: 3.11.6 (tags/v3.11.6:8b6ee5b, Oct  2 2023, 14:57:12) [MSC v.1935 64 bit (AMD64)]
        port: tkinter
        tkinter version: 8.6.12
        PySimpleGUI version: 4.60.5
        PySimpleGUI filename: c:\Users\XXX\venv\Lib\site-packages\PySimpleGUI\PySimpleGUI.py


---------------------


#### Troubleshooting

- [X] Searched main docs for your problem  www.PySimpleGUI.org
- [X] Looked for Demo Programs that are similar to your goal. It is recommend you use the Demo Browser! Demos.PySimpleGUI.org
- [X] None of your GUI code was generated by an AI algorithm like GPT
- [ ] If not tkinter - looked for Demo Programs for specific port
- [ ] For non tkinter - Looked at readme for your specific port if not PySimpleGUI (Qt, WX, Remi)
- [X] Run your program outside of your debugger (from a command line)
- [X] Searched through Issues (open and closed) to see if already reported Issues.PySimpleGUI.org
- [X] Have upgraded to the latest release of PySimpleGUI on PyPI (lastest official version)
- [X] Tried using the PySimpleGUI.py file on GitHub. Your problem may have already been fixed but not released

#### Detailed Description

The argument `enter_submits=True` in Multiline is not causing events to be generated upon hitting the Return key.

 - When `enable_events=True` and `enter_submits=False` the Return key generates an event like all other key-presses.
 - When `enable_events=True` and `enter_submits=True` no event is generated by the Return key.  A new line is generated in the window.

#### Code To Duplicate

```python
import PySimpleGUI as sg

def make_layout(enable_events: bool, enter_submits: bool) -> sg.Multiline:
    value = f""{enable_events=}, {enter_submits=}""
    return sg.Multiline(
        value,
        size=(20, 5),
        enable_events=enable_events,
        enter_submits=enter_submits,
        key=value,
    )

layout = [
    [make_layout(False, False), make_layout(False, True)],
    [make_layout(True, False), make_layout(True, True)],
]

window = sg.Window(
    title=""Various combinations of multiline options"", layout=layout, finalize=True
)

while True:
    event, values = window.read() 
    if event == sg.WIN_CLOSED:
        break
    print(f""Got event: {event}, with value: '{values[event]}'"")

window.close()
```

#### Screenshot, Sketch, or Drawing

![image](https://github.com/PySimpleGUI/PySimpleGUI/assets/794108/b318f65a-ea78-4991-9c65-10f871e7127e)

Neither of the multiline boxes in the right column generate events when the Enter key is pressed. The `enable_events=True, enter_submits=False` box in the lower left does.

Apologies if this is user error and not a bug.

Thanks!

","['Bug', 'documentation', 'Port - TK']",0
PySimpleGUI/PySimpleGUI,[Bug] custom_text in popup display wrong size for some language,"### Type of Issue (Enhancement, Error, Bug, Question)
Bug

----------------------------------------

#### Operating System
WIN10


#### PySimpleGUI Port (tkinter, Qt, Wx, Web)
tkinter


----------------------------------------

## Versions

Version information can be obtained by calling `sg.main_get_debug_data()`
Or you can print each version shown in ()


#### Python version (`sg.sys.version`)
3.11.2


#### PySimpleGUI Version (`sg.__version__`)
4.61.0.201

#### GUI Version  (tkinter (`sg.tclversion_detailed`), PySide2, WxPython, Remi)
8.6.12


---------------------

#### Your Experience In Months or Years (optional)

Years Python programming experience
5 yrs
Years Programming experience overall
10+ yrs
Have used another Python GUI Framework? (tkinter, Qt, etc) (yes/no is fine)
tkinter
Anything else you think would be helpful?
None

---------------------

#### Troubleshooting

These items may solve your problem. Please check those you've done by changing - [ ] to - [X]

- [x] Searched main docs for your problem  www.PySimpleGUI.org
- [x] Looked for Demo Programs that are similar to your goal. It is recommend you use the Demo Browser! Demos.PySimpleGUI.org
- [x] None of your GUI code was generated by an AI algorithm like GPT
- [x] If not tkinter - looked for Demo Programs for specific port
- [x] For non tkinter - Looked at readme for your specific port if not PySimpleGUI (Qt, WX, Remi)
- [x] Run your program outside of your debugger (from a command line)
- [x] Searched through Issues (open and closed) to see if already reported Issues.PySimpleGUI.org
- [x] Have upgraded to the latest release of PySimpleGUI on PyPI (lastest official version)
- [x] Tried using the PySimpleGUI.py file on GitHub. Your problem may have already been fixed but not released

#### Detailed Description

custom_text in popup display wrong size for some language because they occupied more space than in English.
Although we can build one popup by user, maybe it will be better to remove the setting `size` from popup when layout because it is not required.

```python
size=(len(custom_text), 1)
size=(len(custom_text[0]), 1)
size=(len(custom_text[1]), 1)
```


#### Code To Duplicate

```python
import PySimpleGUI as sg

sg.popup('ç¡®å®šè¦ä¿®æ”¹å¯†ç å—ï¼Ÿ', custom_text=('ç¡®è®¤', 'å–æ¶ˆ'))
```

#### Screenshot, Sketch, or Drawing
![image](https://github.com/PySimpleGUI/PySimpleGUI/assets/55352169/d4af57b9-fd01-4bea-993f-b543531dcebb)
","['Bug', 'Port - TK']",0
PySimpleGUI/PySimpleGUI,[Bug]  Broken Linux symlink crashes Demo_Tree_Element.py,"### Type of Issue (Enhancement, Error, Bug, Question)
[Bug]  Broken Linux symlink crashes Demo_Tree_Element.py
When running the program against a directory tree that contains a broken file symbolic link, it crashes with a ""file not found"" error. Python throws an exception that isn't handled:
Traceback (most recent call last):
  File ""Demo_Tree_Element.py"", line 38, in <module>
    add_files_in_folder('', starting_path)
  File ""Demo_Tree_Element.py"", line 34, in add_files_in_folder
    add_files_in_folder(fullname, fullname)
  File ""Demo_Tree_Element.py"", line 34, in add_files_in_folder
    add_files_in_folder(fullname, fullname)
  File ""Demo_Tree_Element.py"", line 36, in add_files_in_folder
    treedata.Insert(parent, fullname, f, values=[os.stat(fullname).st_size], icon=file_icon)
FileNotFoundError: [Errno 2] No such file or directory: '/home/ran/projects/net_tools/samplicator-master/depcomp'

This was obvious when I ran it from the command line, but from the Demo Browser it just silently disappeared


----------------------------------------

#### Operating System
Ubuntu 20.04



#### PySimpleGUI Port (tkinter, Qt, Wx, Web)
>>> print(sg)
<module 'PySimpleGUI' from '/usr/local/lib/python3.8/dist-packages/PySimpleGUI/__init__.py'>
>>> (sg.version)
'4.60.5 Released 21-May-2023'



----------------------------------------

## Versions
*** Version information copied to your clipboard. Paste into your GitHub Issue. ***

Python version: 3.8.10 (default, May 26 2023, 14:05:08) 
[GCC 9.4.0]
        port: tkinter
        tkinter version: 8.6.10
        PySimpleGUI version: 4.60.5
        PySimpleGUI filename: /usr/local/lib/python3.8/dist-packages/PySimpleGUI/PySimpleGUI.py

Version information can be obtained by calling `sg.main_get_debug_data()`
Or you can print each version shown in ()


#### Python version (`sg.sys.version`)
'3.8.10 (default, May 26 2023, 14:05:08) \n[GCC 9.4.0]'


#### PySimpleGUI Version (`sg.__version__`)
'4.60.5'


#### GUI Version  (tkinter (`sg.tclversion_detailed`), PySide2, WxPython, Remi)
tkinter


---------------------

#### Your Experience In Months or Years (optional)

Years Python programming experience 2

Years Programming experience overall 54

Have used another Python GUI Framework? (tkinter, Qt, etc) (yes/no is fine)

Anything else you think would be helpful?


---------------------

#### Troubleshooting

These items may solve your problem. Please check those you've done by changing - [ ] to - [X]

- [ ] Searched main docs for your problem  www.PySimpleGUI.org
- [ ] Looked for Demo Programs that are similar to your goal. It is recommend you use the Demo Browser! Demos.PySimpleGUI.org
- [ ] None of your GUI code was generated by an AI algorithm like GPT
- [ ] If not tkinter - looked for Demo Programs for specific port
- [ ] For non tkinter - Looked at readme for your specific port if not PySimpleGUI (Qt, WX, Remi)
- [ ] Run your program outside of your debugger (from a command line)
- [ ] Searched through Issues (open and closed) to see if already reported Issues.PySimpleGUI.org
- [ ] Have upgraded to the latest release of PySimpleGUI on PyPI (lastest official version)
- [ ] Tried using the PySimpleGUI.py file on GitHub. Your problem may have already been fixed but not released

#### Detailed Description
When running the program against a directory tree that contains a broken file symbolic link, it crashes with a ""file not found"" error. Python throws an exception that isn't handled:
Traceback (most recent call last):
  File ""Demo_Tree_Element.py"", line 38, in <module>
    add_files_in_folder('', starting_path)
  File ""Demo_Tree_Element.py"", line 34, in add_files_in_folder
    add_files_in_folder(fullname, fullname)
  File ""Demo_Tree_Element.py"", line 34, in add_files_in_folder
    add_files_in_folder(fullname, fullname)
  File ""Demo_Tree_Element.py"", line 36, in add_files_in_folder
    treedata.Insert(parent, fullname, f, values=[os.stat(fullname).st_size], icon=file_icon)
FileNotFoundError: [Errno 2] No such file or directory: '/home/ran/projects/net_tools/samplicator-master/depcomp'

This was obvious when I ran it from the command line, but from the Demo Browser it just silently disappeared





#### Code To Duplicate

The Demo_Tree_Element.py file included with the package.


This pre-formatted code block is all set for you to paste in your bit of code:

```python

# Paste your code here

To fix, replace line 36 with something like:

            try:
                treedata.Insert(parent, fullname, f, values=[os.stat(fullname).st_size], icon=file_icon)
            except:
                treedata.Insert(parent, fullname, f, values=[""???? error""], icon=file_icon)

```

#### Screenshot, Sketch, or Drawing

---------------------

### Watcha Makin?

If you care to share something about your project, it would be awesome to hear what you're building.
",['Demo Programs'],0
PySimpleGUI/PySimpleGUI,[Bug/Question]  Radio element size parameter,"### Type of Issue ( Bug, Question)


Bug/Question

#### Operating System

Windows

#### PySimpleGUI Port (tkinter, Qt, Wx, Web)


tkinter
----------------------------------------

## Versions

Version information can be obtained by calling `sg.main_get_debug_data()`
Or you can print each version shown in ()


#### Python version (`sg.sys.version`)

3.10.12

#### PySimpleGUI Version (`sg.__version__`)

4.60.5

#### GUI Version  (tkinter (`sg.tclversion_detailed`), PySide2, WxPython, Remi)

tkinter version: 8.6.12

---------------------

#### Your Experience In Months or Years (optional)

Years Python programming experience

5

Years Programming experience overall

7

Have used another Python GUI Framework? (tkinter, Qt, etc) (yes/no is fine)

no

Anything else you think would be helpful?


---------------------

#### Troubleshooting

These items may solve your problem. Please check those you've done by changing - [ ] to - [X]

- [x] Searched main docs for your problem  www.PySimpleGUI.org
- [x] Looked for Demo Programs that are similar to your goal. It is recommend you use the Demo Browser! Demos.PySimpleGUI.org
- [x] None of your GUI code was generated by an AI algorithm like GPT
- [ ] If not tkinter - looked for Demo Programs for specific port
- [ ] For non tkinter - Looked at readme for your specific port if not PySimpleGUI (Qt, WX, Remi)
- [ ] Run your program outside of your debugger (from a command line)
- [x] Searched through Issues (open and closed) to see if already reported Issues.PySimpleGUI.org
- [x] Have upgraded to the latest release of PySimpleGUI on PyPI (lastest official version)
- [ ] Tried using the PySimpleGUI.py file on GitHub. Your problem may have already been fixed but not released

#### Detailed Description

I am using the radio button element, but the size parameter doesn't do what i thought it should. I thought it should make the button itself larger, but it only affects the size of the text next to the button. Also it seems as if the text is only effected by the width parameter and not the height. This may not be the case and the height only gets triggered by text overflow in the width.


#### Code To Duplicate

A **short** program that isolates and demonstrates the problem (Do not paste your massive program, but instead 10-20 lines that clearly show the problem)

This pre-formatted code block is all set for you to paste in your bit of code:

```python

import PySimpleGUI as sg


sg.theme('SystemDefaultForReal')  # please make your windows colorful
header_font_4 = {'font': ('Calibri',20,'bold')}
radio_format = {'group_id': 1, 'size': (1,1)}
radio_1 = [[sg.Radio('Hello', group_id=1,size=(2,2))],
         [sg.Radio('Hello Again', group_id=1,size=(10,10))],
         [sg.Radio('Goodbye', group_id=1,size=(100,100))]]
layout = [[sg.Frame('',radio_1)]]

window = sg.Window('Window Title', layout).Finalize()

while True:  # Event Loop
    event, values = window.read()
    print(event, values)
    if event == sg.WIN_CLOSED or event == 'Exit':
        break
    if event == 'Show':
        # change the ""output"" element to be the value of ""input"" element
        window['-OUTPUT-'].update(values['-IN-'])

window.close()


```

#### Screenshot, Sketch, or Drawing

---------------------

### Watcha Makin?

If you care to share something about your project, it would be awesome to hear what you're building.
![bug](https://github.com/PySimpleGUI/PySimpleGUI/assets/148901824/ff8f18dc-a71c-459a-8567-96c2c4ccb1a5)
","['Bug', 'Port - TK']",0
PySimpleGUI/PySimpleGUI,[Bug] Dynamic TabGroups Cause Shrinking Layout,"
## Type of Issue (Enhancement, Error, Bug, Question)

Bug

----------------------------------------

## Environment 

#### Operating System

Windows  version ('10', '10.0.19044', 'SP0', 'Multiprocessor Free')

#### PySimpleGUI Port (tkinter, Qt, Wx, Web)

tkinter

----------------------------------------

## Versions


#### Python version (`sg.sys.version`)

3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]

#### PySimpleGUI Version (`sg.__version__`)

Tried on 4.60.5 and 4.61.0.201.

It would be nice to find a workaround for 4.60.5

#### GUI Version  (tkinter (`sg.tclversion_detailed`), PySide2, WxPython, Remi)

8.6.12



---------------------

## Your Experience In Months or Years (optional)

5 Years Python programming experience
7 Years Programming experience overall
No Have used another Python GUI Framework? (tkinter, Qt, etc) (yes/no is fine)


---------------------

## Troubleshooting

These items may solve your problem. Please check those you've done by changing - [ ] to - [X]

- [X] Searched main docs for your problem  www.PySimpleGUI.org
- [X] Looked for Demo Programs that are similar to your goal. It is recommend you use the Demo Browser! Demos.PySimpleGUI.org
- [ ] If not tkinter - looked for Demo Programs for specific port
- [ ] For non tkinter - Looked at readme for your specific port if not PySimpleGUI (Qt, WX, Remi)
- [X] Run your program outside of your debugger (from a command line)
- [X] Searched through Issues (open and closed) to see if already reported Issues.PySimpleGUI.org
- [X] Upgraded to the latest official release of PySimpleGUI on PyPI
- [X] Tried using the PySimpleGUI.py file on GitHub. Your problem may have already been fixed but not released

## Detailed Description

When I dynamically create new Tabs and add them into a new TabGroup inside of an existing TabGroup or Column it causes the bottom of the whole layout to shrink. Specifically the lowest part of the window's layout seems to shrink.

#### Code To Duplicate

tabgroup_inception_minimal.py:

```python
import PySimpleGUI as sg

outertabgroup = sg.TabGroup([[]], expand_x=True, expand_y=True)
tabcount = 0

layout = [
    [sg.Button(""New Tab"", key=f""new_tab"")],
    [outertabgroup]
]

window = sg.Window(""New Tabs"", layout, resizable=True, finalize=True, enable_close_attempted_event=True)

while True :
    event, values = window.read(timeout=100)

    if event == sg.WINDOW_CLOSE_ATTEMPTED_EVENT :
        break

    if event == ""new_tab"" :
        newtabs = [
            sg.Tab(f""Tab {i}"", [[sg.T(f""Tab {i}"")]])
            for i in range(5)
        ]
        newtabgroup = sg.TabGroup([newtabs], expand_x=True, expand_y=True)

        newtab = sg.Tab(f""TabGroup {tabcount}"", [[newtabgroup]])
        tabcount += 1

        outertabgroup.add_tab(newtab)

window.close()


```

tabgroup_extend_layout_minimal.py:

```python

import PySimpleGUI as sg

addCol = sg.Col([[]], key=""add_col"", expand_x=True, expand_y=True)

btntab = sg.Tab(""Button"", [[sg.Text(""Some Text""), sg.Button(""Add TabGroup"", key=""add"")]])
coltab = sg.Tab(""Col"", [[addCol]])
tabs = [
    btntab,
    coltab
]

outertabgroup = sg.TabGroup([tabs], expand_x=True, expand_y=True)

layout = [[outertabgroup]]

window = sg.Window(""Extend Column"", layout, resizable=True, finalize=True, enable_close_attempted_event=True)


while True :
    event, values = window.read(timeout=100)

    if event == sg.WINDOW_CLOSE_ATTEMPTED_EVENT :
        break

    if event == ""add"" :
        tabs = [
            sg.Tab(f""Tab {i}"", [[sg.T(f""Tab {i}"")]])
            for i in range(5)
        ]
        tabgroup = sg.TabGroup([tabs], expand_x=True, expand_y=True)

        window.extend_layout(addCol, [[tabgroup]])


window.close()
```


#### Screenshot, Sketch, or Drawing


**TabGroup Inception**

At the beginning
![image](https://github.com/PySimpleGUI/PySimpleGUI/assets/35788257/e056ad83-d5db-42b0-abdd-a9112ec64b79)

After adding several tabs: 
![image](https://github.com/PySimpleGUI/PySimpleGUI/assets/35788257/047874d9-0ad5-48e7-a72c-4edcb052afb1)

After adding a lot of tabs: 
![image](https://github.com/PySimpleGUI/PySimpleGUI/assets/35788257/0e575793-733c-4956-999d-d882897bf6f3)


**Extending Layout**
Something similar happens when I add the new TabGroups into a Column (The eventual idea being to hide different TabGroups at different times)

At the beginning: 
![image](https://github.com/PySimpleGUI/PySimpleGUI/assets/35788257/9997ad1b-639c-4f33-a342-c99b62bdf186)

After adding several tabs:
![image](https://github.com/PySimpleGUI/PySimpleGUI/assets/35788257/a09b2181-0246-4b35-9c61-65a5737d9bd5)

After adding even more tabs:
![image](https://github.com/PySimpleGUI/PySimpleGUI/assets/35788257/f1130090-75e8-4441-95e6-6c222dba080e)


------------------------
## Watcha Makin?
I am trying to create a program that presents data to the user. The nature of the data is that it makes sense to organize it into tabs, and subtabs, and subsubtabs. However, all of the data is not known at window creation so it needs to be added dynamically, possibly quite frequently.

",[],0
PySimpleGUI/PySimpleGUI,[Question] Unmodifiable (seemingly) Column padding,"### Question

----------------------------------------

#### Operating System

macOS 14.0

#### PySimpleGUI Port (tkinter, Qt, Wx, Web)

Not sure, just installed via pip

----------------------------------------

## Versions

Version information can be obtained by calling `sg.main_get_debug_data()`
Or you can print each version shown in ()


#### Python version (`sg.sys.version`)



#### PySimpleGUI Version (`sg.__version__`)

3.11.6

#### GUI Version  (tkinter (`sg.tclversion_detailed`), PySide2, WxPython, Remi)

8.6.13

---------------------

#### Your Experience In Months or Years (optional)

Years Python programming experience

5

Years Programming experience overall

7

Have used another Python GUI Framework? (tkinter, Qt, etc) (yes/no is fine)

no

Anything else you think would be helpful?


---------------------

#### Troubleshooting

These items may solve your problem. Please check those you've done by changing - [ ] to - [X]

- [X] Searched main docs for your problem  www.PySimpleGUI.org
- [X] Looked for Demo Programs that are similar to your goal. It is recommend you use the Demo Browser! Demos.PySimpleGUI.org
- [X] None of your GUI code was generated by an AI algorithm like GPT
- [ ] If not tkinter - looked for Demo Programs for specific port
- [ ] For non tkinter - Looked at readme for your specific port if not PySimpleGUI (Qt, WX, Remi)
- [X] Run your program outside of your debugger (from a command line)
- [X] Searched through Issues (open and closed) to see if already reported Issues.PySimpleGUI.org
- [X] Have upgraded to the latest release of PySimpleGUI on PyPI (lastest official version)
- [X] Tried using the PySimpleGUI.py file on GitHub. Your problem may have already been fixed but not released

#### Detailed Description

The `sg.Column` element seems to have some unmodifiable padding that is not removed by setting `sg.Window`'s `element_padding` parameter to `(0, 0)`. If same-width `sg.Column`'s are nested with colored backgrounds (code attached), it becomes easily observable by naked eye.

#### Code To Duplicate

A **short** program that isolates and demonstrates the problem (Do not paste your massive program, but instead 10-20 lines that clearly show the problem)

```python

import PySimpleGUI as psg


layout = [[
    psg.TabGroup(
        layout=[[
            psg.Tab(
                'Test',
                layout=[[
                    psg.Column(
                        layout=[[
                            psg.Column(
                                layout=[[psg.Column(
                                    layout=[[psg.Text('test', pad=(0, 0))]],
                                    size=(400, 50),
                                    background_color='green',
                                    pad=(0, 0),
                                )]],
                                size=(400, 100),
                                background_color='blue',
                                pad=(0, 0),
                            )
                        ]],
                        size=(400, 400),
                        background_color='red',
                        pad=(0, 0),
                    )
                ]],
            ),
        ]],
        background_color='violet',
        pad=(0, 0)
    )
]]

window = psg.Window('Test', layout, size=(500, 500), margins=(0, 0), border_depth=0, element_padding=(0, 0))

while True:
    event, values = window.read()
    if event in (psg.WIN_CLOSED, 'Exit'):
        break
window.close()

```

#### Screenshot, Sketch, or Drawing

---------------------

### Watcha Makin?

A simple resource tracker for a PC game.
",[],0
PySimpleGUI/PySimpleGUI,[Bug] Elements unclickable when opening a new Window,"### Bug
Elements do not respond to clicks when a new Window opens

----------------------------------------

#### Operating System
macOS 14.0 Sonoma



#### PySimpleGUI Port (tkinter, Qt, Wx, Web)
tkinter



----------------------------------------

## Versions

#### Python version: 3.11.3 (v3.11.3:f3909b8bc8, Apr  4 2023, 20:12:10) [Clang 13.0.0 (clang-1300.0.29.30)]
        port: tkinter
        tkinter version: 8.6.12
        PySimpleGUI version: 4.60.5
        PySimpleGUI filename: /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/PySimpleGUI/PySimpleGUI.py




---------------------

#### Your Experience In Months or Years (optional)

Years Python programming experience 6 years (hobby)

Years Programming experience overall 10 years

Have used another Python GUI Framework? (tkinter, Qt, etc) (yes/no is fine) Yes

Anything else you think would be helpful?
I believe the issue is specific to macOS Sonoma as the issue started immediately after upgrading. 

---------------------

#### Troubleshooting

These items may solve your problem. Please check those you've done by changing - [ ] to - [X]

- [x] Searched main docs for your problem  www.PySimpleGUI.org
- [x] Looked for Demo Programs that are similar to your goal. It is recommend you use the Demo Browser! Demos.PySimpleGUI.org
- [x] None of your GUI code was generated by an AI algorithm like GPT
- [ ] If not tkinter - looked for Demo Programs for specific port
- [ ] For non tkinter - Looked at readme for your specific port if not PySimpleGUI (Qt, WX, Remi)
- [x] Run your program outside of your debugger (from a command line)
- [x] Searched through Issues (open and closed) to see if already reported Issues.PySimpleGUI.org
- [x] Have upgraded to the latest release of PySimpleGUI on PyPI (lastest official version)
- [ ] Tried using the PySimpleGUI.py file on GitHub. Your problem may have already been fixed but not released

#### Detailed Description
App has multiple windows which should hide/display as needed. When the app opens, elements in the window are unresponsive to clicks. I've been able to activate the elements by clicking the title bar for the window first, or by holding my click and dragging the mouse a bit. 

My built app has a few more elements than seen in my example code, but the effect is the same in the example. 



#### Code To Duplicate

A **short** program that isolates and demonstrates the problem (Do not paste your massive program, but instead 10-20 lines that clearly show the problem)

This pre-formatted code block is all set for you to paste in your bit of code:

```python

import PySimpleGUI as sg

def build_main_window():
    layoutMain = [
        [
            sg.Text(
                ""Buttons won't click until you click the title bar"",
                auto_size_text=True,
                size=(40, 2),
                expand_x=True,
                font=(""Roboto"", 18)
            )
        ],
        [
            sg.Button(
                button_text=""New Window"",
                enable_events=True,
                border_width=5,
                key='btnNew',
                font=(""Roboto"", 18),
                pad=(10, 20),
                button_color=""green""
            ),
            sg.Button(
                button_text=""Exit"",
                enable_events=True,
                border_width=5,
                key='exit',
                font=(""Roboto"", 18),
                pad=(10, 20)
            ),
        ]
    ],
    windowMain = sg.Window(
        title=""GUI Test"",
        layout=layoutMain,
        margins=(15, 10),
        finalize=True
        )
    return windowMain


def new_window():
    layoutNew = [
        [
            sg.Text(
                text=""But once clicking the title bar, all buttons will work"",
                auto_size_text=True,
                size=(60, 2),
                expand_x=True,
                font=(""Roboto"", 18)
            )
        ],

        [
            sg.Button(
                button_text=""Go Back"",
                enable_events=True,
                border_width=5,
                key='btnMain',
                font=(""Roboto"", 18),
                pad=(10, 20)
            ),
            sg.Button(
                button_text=""Submit"",
                enable_events=True,
                border_width=5,
                key='btnSubmitNew',
                font=(""Roboto"", 18),
                pad=(10, 20),
                button_color=""green"",
                bind_return_key=True
            ),
            sg.Text(
                text="""",
                size=(16, 1),
                font=(""Roboto"", 14),
                key=""log"",
                expand_x=True,
                expand_y=True
                )
        ]

    ]
    windowNew = sg.Window(
        title=""secondary window"",
        layout=layoutNew,
        margins=(15, 10),
        finalize=True
        )
    return windowNew

def main():
    windowMain, windowNew = build_main_window(), None

    while True:
        window, event, values = sg.read_all_windows()

        if event == sg.WIN_CLOSED or event == 'exit':
            window.close()
            if window == windowNew:
                windowNew = None
                windowMain = build_main_window()
            elif window == windowMain:
                break
        elif event == 'btnNew' and not windowNew:
            windowMain.disappear()
            windowNew = new_window()
        elif event == 'btnSubmitNew':
            window['log'].update(value='Working')
            window.read(timeout=0)
        elif event == 'btnMain' and windowNew:
            windowNew.close()
            windowNew = None
            windowMain.reappear()

    window.close()

if __name__ == '__main__':
    main()



```

#### Screenshot, Sketch, or Drawing

---------------------

### Watcha Makin?

I built myself a little app to automate some employee on/off-boarding tasks across various systems. 

","['duplicate', 'Platform Specific Issue - Mac']",0
PySimpleGUI/PySimpleGUI,[ Bug]  Mac OS 14 bug with button clicks and combo selection,"### Type of Issue (Enhancement, Error, Bug, Question)
Bug

----------------------------------------

#### Operating System
Mac OS 14


#### PySimpleGUI Port (tkinter, Qt, Wx, Web)
tkinter

----------------------------------------

## Versions

Python version: 3.11.4 (v3.11.4:d2340ef257, Jun  6 2023, 19:15:51) [Clang 13.0.0 (clang-1300.0.29.30)]
port: tkinter
tkinter version: 8.6.12
PySimpleGUI version: 4.60.5
PySimpleGUI filename: .virtualenvs/ll/lib/python3.11/site-packages/PySimpleGUI/PySimpleGUI.py

---------------------

#### Your Experience In Months or Years (optional)

Years Python programming experience
5

Years Programming experience overall
5

Have used another Python GUI Framework? (tkinter, Qt, etc) (yes/no is fine)
no

Anything else you think would be helpful?
Everything worked fine until updating to Mac OS 14. Also get this error when running python scripts now (not sure if its related to either of these issues or not).

`2023-10-05 20:42:32.916 Python[4310:3325882] WARNING: Secure coding is not enabled for restorable state! Enable secure coding by implementing NSApplicationDelegate.applicationSupportsSecureRestorableState: and returning YES.`

---------------------

#### Troubleshooting

These items may solve your problem. Please check those you've done by changing - [ ] to - [X]

- [X] Searched main docs for your problem  www.PySimpleGUI.org
- [X] Looked for Demo Programs that are similar to your goal. It is recommend you use the Demo Browser! Demos.PySimpleGUI.org
- [X] None of your GUI code was generated by an AI algorithm like GPT
- [ ] If not tkinter - looked for Demo Programs for specific port
- [ ] For non tkinter - Looked at readme for your specific port if not PySimpleGUI (Qt, WX, Remi)
- [X] Run your program outside of your debugger (from a command line)
- [X] Searched through Issues (open and closed) to see if already reported Issues.PySimpleGUI.org
- [X] Have upgraded to the latest release of PySimpleGUI on PyPI (lastest official version)
- [ ] Tried using the PySimpleGUI.py file on GitHub. Your problem may have already been fixed but not released

#### Detailed Description
Recently updated to Mac OS Sonomoa (OS 14) and i started experiencing unreliable button pressing and combo dropdown selection with the mouse. Nothing changed in the code between updating the operating system. I have verified proper execution of the code on Mac OS 13.x and there are no issues. I've included a gif to try to show the issue.


#### Code To Duplicate

A **short** program that isolates and demonstrates the problem (Do not paste your massive program, but instead 10-20 lines that clearly show the problem)

I only include pynput for cross-checking the detection of mouse clicks. the problem exists without that import.

```
import PySimpleGUI as sg
from pynput import mouse


def on_click(x, y, button, pressed):
    if pressed:
        print(""Mouse Click Registered"")


listener = mouse.Listener(on_click=on_click)
listener.start()

layout = [
    [sg.Text(""This is a button test"")],
    [
        sg.Combo(
            [""option1"", ""option2"", ""option3"", ""option4""],
            enable_events=True,
            key=""Combo"",
        )
    ],
    [sg.Button(""Submit"")],
    [sg.Multiline(autoscroll=True, size=(30, 20), reroute_stdout=True)],
]

window = sg.Window(""Test"", layout=layout, finalize=True)

while True:
    event, values = window.read()

    if event in (None, ""Quit"", sg.WIN_CLOSED):
        window.close()
        listener.stop()
        break

    if event == ""Submit"":
        print(""Button Clicked"")

    if event == ""Combo"":
        print(""Combo Selection"", values[event])

```

#### Screenshot, Sketch, or Drawing

---------------
![C323990C-EF20-4598-B6F0-6AD9F1F2D4B5](https://github.com/PySimpleGUI/PySimpleGUI/assets/13004111/9cfc8df7-cbed-46e9-bdd4-4a0c34944d40)
------
","['workaround available', 'Platform Specific Issue - Mac', 'Port - TK']",0
PySimpleGUI/PySimpleGUI,"[ Enhancement] Set focus to text input widget for popup_get_text(), get_file(), and get_folder()","### Type of Issue (Enhancement, Error, Bug, Question)
Enhancement

----------------------------------------

#### Operating System
All


#### PySimpleGUI Port (tkinter, Qt, Wx, Web)
tkinter


----------------------------------------

## Versions


#### Python version (`sg.sys.version`)
3.11.3


#### PySimpleGUI Version (`sg.__version__`)
4.60.4


#### GUI Version  (tkinter (`sg.tclversion_detailed`), PySide2, WxPython, Remi)
8.6.13


---------------------

#### Your Experience In Months or Years (optional)

Years Python programming experience 
5 years

Years Programming experience overall
30 years

Have used another Python GUI Framework? (tkinter, Qt, etc) (yes/no is fine)
Yes, several

Anything else you think would be helpful?


---------------------

#### Troubleshooting

These items may solve your problem. Please check those you've done by changing - [ ] to - [X]

- [ ] Searched main docs for your problem  www.PySimpleGUI.org
- [ ] Looked for Demo Programs that are similar to your goal. It is recommend you use the Demo Browser! Demos.PySimpleGUI.org
- [ ] None of your GUI code was generated by an AI algorithm like GPT
- [ ] If not tkinter - looked for Demo Programs for specific port
- [ ] For non tkinter - Looked at readme for your specific port if not PySimpleGUI (Qt, WX, Remi)
- [ ] Run your program outside of your debugger (from a command line)
- [ ] Searched through Issues (open and closed) to see if already reported Issues.PySimpleGUI.org
- [ ] Have upgraded to the latest release of PySimpleGUI on PyPI (lastest official version)
- [ ] Tried using the PySimpleGUI.py file on GitHub. Your problem may have already been fixed but not released

#### Detailed Description

I have made several changes to the popup dialogs for my own use. I thought about sending a Pull Request but thought
I would mention it here first. One thing I prefer is to Push() the buttons and reverse Okay and Cancel buttons. I think I saw an option for this and It's not as important as my issue with popup_get_text, get_file, and get_folder.

These dialogs should have the input box focused so you can immediately start typing. I cannot think of any reason for it not to be in the popup_get_text() dialog at least, and don't see any issues with compatability.

Would appreciate peoples opinions on this. Would be happy to send a pull request. Delete this issue if you want.

Thanks


#### Code To Duplicate

N/A

This pre-formatted code block is all set for you to paste in your bit of code:

```python

import PySimpleGUI as sg
sg.popup_get_text('Hello world\nHow are you?')


```

#### Screenshot, Sketch, or Drawing

---------------------

### Watcha Makin?

Right now I'm working on a tool to edit and maintain config files for the Pegasus retro game frontend, which also updates/alters/fixes windows shortcuts using pylnk3.
",[],0
PySimpleGUI/PySimpleGUI,[Bug]  Menu Element visibility parameter does not work,"### Type of Issue (Enhancement, Error, Bug, Question)
Bug

----------------------------------------

#### Operating System
Windows 10


#### PySimpleGUI Port (tkinter, Qt, Wx, Web)
tkinter


----------------------------------------

## Versions


Python version: 3.11.5 (tags/v3.11.5:cce6ba9, Aug 24 2023, 14:38:34) [MSC v.1936 64 bit (AMD64)]
        port: tkinter
        tkinter version: 8.6.12
        PySimpleGUI version: 4.60.5
        PySimpleGUI filename: C:\Users\stephen.cholvat\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\PySimpleGUI\PySimpleGUI.py
---------------------

#### Troubleshooting

These items may solve your problem. Please check those you've done by changing - [ ] to - [X]

- [X] Searched main docs for your problem  www.PySimpleGUI.org
- [X] Looked for Demo Programs that are similar to your goal. It is recommend you use the Demo Browser! Demos.PySimpleGUI.org
- [X] None of your GUI code was generated by an AI algorithm like GPT
- [x] If not tkinter - looked for Demo Programs for specific port
- [X] For non tkinter - Looked at readme for your specific port if not PySimpleGUI (Qt, WX, Remi)
- [X] Run your program outside of your debugger (from a command line)
- [X] Searched through Issues (open and closed) to see if already reported Issues.PySimpleGUI.org
- [X] Have upgraded to the latest release of PySimpleGUI on PyPI (lastest official version)
- [X] Tried using the PySimpleGUI.py file on GitHub. Your problem may have already been fixed but not released

#### Detailed Description

I tried to initialize a menu bar using the following line:
sg.Menu(menu_def, visible=False, key='-MENUBAR-')

However, the menu bar was still visible. Using the .update(visible=False) function did properly hide the menu bar.


#### Code To Duplicate

```python

# Paste your code here
import PySimpleGUI as sg

menu_def = [['&File', ['&Open     Ctrl-O', '&Save       Ctrl-S', '&Properties', 'E&xit']]]

layout = [
    #This visible=False parameter has no effect
    [sg.Menu(menu_def, visible=False, key='-MENUBAR-')],
    [sg.Button('Enable Visibility', key=""enable-visibility"")],
    [sg.Button('Disable Visibility', key=""disable-visibility"")]
]
window = sg.Window(""Menu Visibility Test"", layout)

while True:
    event, values = window.read()
    if event in (sg.WIN_CLOSED, 'Exit'):
        break
    elif event == ""enable-visibility"":
        window['-MENUBAR-'].update(visible = True)
    elif event == ""disable-visibility"":
        window['-MENUBAR-'].update(visible = False)
window.close()

```

#### Screenshot, Sketch, or Drawing
![image](https://github.com/PySimpleGUI/PySimpleGUI/assets/10137206/2de1beb1-a624-49a2-976e-e60f0d270ee8)

---------------------

### Watcha Makin?

If you care to share something about your project, it would be awesome to hear what you're building.
","['Bug', 'Port - TK']",0
PySimpleGUI/PySimpleGUI,[Question]  Getting Errors on Multiple Attributes with PySimpleGUIWeb,"### Type of Issue (Enhancement, Error, Bug, Question)

Error
----------------------------------------

#### Operating System
macOS Ventura 12.4.1


#### PySimpleGUI Port (tkinter, Qt, Wx, Web)
tkinter


----------------------------------------

## Versions

Version information can be obtained by calling `sg.main_get_debug_data()`
Or you can print each version shown in ()

Python version: 3.11.3 (main, Apr  7 2023, 20:13:31) [Clang 14.0.0 (clang-1400.0.29.202)]
        port: tkinter
        tkinter version: 8.6.13
        PySimpleGUI version: 4.60.5

#### Python version (`sg.sys.version`)

#### PySimpleGUI Version (`sg.__version__`)

#### GUI Version  (tkinter (`sg.tclversion_detailed`), PySide2, WxPython, Remi)
8.6.13

---------------------

#### Your Experience In Months or Years (optional)
5

Years Python programming experience
5

Years Programming experience overall
5+

Have used another Python GUI Framework? (tkinter, Qt, etc) (yes/no is fine)
no

Anything else you think would be helpful?


---------------------

#### Troubleshooting

These items may solve your problem. Please check those you've done by changing - [ ] to - [X]

- [x] Searched main docs for your problem  www.PySimpleGUI.org
- [x] Looked for Demo Programs that are similar to your goal. It is recommend you use the Demo Browser! Demos.PySimpleGUI.org
- [x] None of your GUI code was generated by an AI algorithm like GPT
- [ ] If not tkinter - looked for Demo Programs for specific port
- [ ] For non tkinter - Looked at readme for your specific port if not PySimpleGUI (Qt, WX, Remi)
- [x] Run your program outside of your debugger (from a command line)
- [x] Searched through Issues (open and closed) to see if already reported Issues.PySimpleGUI.org
- [x] Have upgraded to the latest release of PySimpleGUI on PyPI (lastest official version)
- [ ] Tried using the PySimpleGUI.py file on GitHub. Your problem may have already been fixed but not released

#### Detailed Description
I need a list of what doesn't work in PySimpleGUIWeb.

On my first attempt running my code with PSGW I got: AttributeError: 'HTMLParser' object has no attribute 'unescape'
I searched the issues and did the upgrade to remi, and added 'Attributes' to the ImportError except statement. That seemed to work. At least the error changed.

Now it was complaining about: use_readonly_for_disable=True. I thought maybe that was too advanced, and I could work around it, so I took it out.

Now it is complaining: AttributeError: module 'PySimpleGUIWeb' has no attribute 'Push'

This seems pretty basic, so before I try changing all Push-es to Sizers - can you give me some advice? Or a list of what won't work?

Error:
(venv) PSGweb> python -m scratch
Traceback (most recent call last):
  File ""<frozen runpy>"", line 198, in _run_module_as_main
  File ""<frozen runpy>"", line 88, in _run_code
  File ""/Users/scott.freemire/Code/PSGweb/scratch.py"", line 5, in <module>
    [sg.Push(),
     ^^^^^^^
AttributeError: module 'PySimpleGUIWeb' has no attribute 'Push'

I tried the code below with no Push-es, and a couple of extra buttons to get the version info and a web page popped up. Even then, once I clicked one button, no buttons did anything after that.

Am I missing something basic?

#### Code To Duplicate

A **short** program that isolates and demonstrates the problem (Do not paste your massive program, but instead 10-20 lines that clearly show the problem)

This pre-formatted code block is all set for you to paste in your bit of code:

```python

import PySimpleGUIWeb as sg

layout = [
   [sg.Push(),
   sg.Text('Test text'),
   sg.Push()],
   [sg.Button('Exit')]
]

window = sg.Window('PySimpleGUI Web Test', layout,
   keep_on_top=False, finalize=True)

while True:
   event, values = window.read()    # type: (str, dict)
   print(event, values)
   if event in ('Exit', sg.WIN_CLOSED):
      break

window.close()


```

#### Screenshot, Sketch, or Drawing

---------------------

### Watcha Makin?

If you care to share something about your project, it would be awesome to hear what you're building.

IP Network Calculator

",[],0
PySimpleGUI/PySimpleGUI,[Question] - Extending tkinter table to drag rows around,"## Type of Issue (Enhancement, Error, Bug, Question)

Question

----------------------------------------

## Environment 

#### Operating System

Windows  version ('7', '6.1.7601', 'SP1', 'Multiprocessor Free')

#### PySimpleGUI Port (tkinter, Qt, Wx, Web)

tkinter

----------------------------------------

## Versions


#### Python version (`sg.sys.version`)

3.8.6 (tags/v3.8.6:db45529, Sep 23 2020, 15:52:53) [MSC v.1927 64 bit (AMD64)]

#### PySimpleGUI Version (`sg.__version__`)

4.61.0.196

#### GUI Version  (tkinter (`sg.tclversion_detailed`), PySide2, WxPython, Remi)

8.6.9



---------------------

## Your Experience In Months or Years (optional)

 Years Python programming experience
 Years Programming experience overall
No Have used another Python GUI Framework? (tkinter, Qt, etc) (yes/no is fine)


---------------------

## Troubleshooting

These items may solve your problem. Please check those you've done by changing - [ ] to - [X]

- [X] Searched main docs for your problem  www.PySimpleGUI.org
- [X] Looked for Demo Programs that are similar to your goal. It is recommend you use the Demo Browser! Demos.PySimpleGUI.org
- [ ] If not tkinter - looked for Demo Programs for specific port
- [ ] For non tkinter - Looked at readme for your specific port if not PySimpleGUI (Qt, WX, Remi)
- [X] Run your program outside of your debugger (from a command line)
- [X] Searched through Issues (open and closed) to see if already reported Issues.PySimpleGUI.org
- [X] Upgraded to the latest official release of PySimpleGUI on PyPI
- [X] Tried using the PySimpleGUI.py file on GitHub. Your problem may have already been fixed but not released

## Detailed Description
I've provided some mock code where I'm using a table and want to drag rows around. I found this [code](https://pythonguides.com/python-tkinter-drag-and-drop/#Python_Tkinter_Drag_and_Drop_ide) and it works when I just copy over the functions and bindings, but I'm unsure how to get it working with the other code and updating `tabledata` when the mouse button is released, since PSG's table only generates the click event when mouse is released.

## EDIT
Also, I can't get it to scroll up/down when i move up/down to the end of the table, either.

#### Code To Duplicate


```python
import PySimpleGUI as sg


row_num_clicked = None


menu_def = [
    [
        ""File"",
        [
            ""Open"",
            ""Save"",
            ""Exit"",
        ],
    ],
    [
        ""Edit"",
        [
            ""Paste"",
            ""Undo"",
        ],
    ],
    [""Help"", ""About...""],
]

layout0 = [
    [sg.Button(sg.SYMBOL_UP)],
    [sg.Button(sg.SYMBOL_DOWN)],
]

tabledata = [
    [""One"", ""name1"", ""loc1""],
    [""Two"", ""name2"", ""loc2""],
    [""Three"", ""name3"", ""loc3""],
    [""Four"", ""name4"", ""loc4""],
    [""Five"", ""name5"", ""loc5""],
    [""Six"", ""name6"", ""loc6""],
    [""Seven"", ""name7"", ""loc7""],
    [""Eight"", ""name8"", ""loc8""],
    [""Nine"", ""name9"", ""loc9""],
    [""Ten"", ""name10"", ""loc10""],
    [""Eleven"", ""name11"", ""loc11""],
    [""Twelve"", ""name12"", ""loc12""],
    [""Thirteen"", ""name13"", ""loc13""],
    [""Fourteen"", ""name14"", ""loc14""],
]

layout1 = [
    [
        sg.Table(
            tabledata,
            headings=[""Title"", ""Names"", ""Locations""],
            enable_click_events=True,
            key=""-TABLE-"",
            display_row_numbers=True,
            starting_row_number=1,
        ),
        sg.Column(layout0),
    ],
]

layout2 = [
    [sg.Text(""Selected: ""), sg.Text(""None"", key=""-ROWSELECT-"")],
    [sg.Text(""Move to:""), sg.Spin([0, 1, 2], initial_value=0)],
]

layout = [
    [
        sg.Titlebar(
            ""Playlist Editor"",
            background_color=""#141417"",
            font=(
                ""Gill Sans MT"",
                18,
            ),
        )
    ],
    [
        sg.MenubarCustom(
            menu_def,
            bar_background_color=""#2d2d30"",
            bar_font=(
                ""Gill Sans MT"",
                12,
            ),
            font=(
                ""Gill Sans MT"",
                12,
            ),
            bar_text_color=""#f1f1f1"",
            key=""-MENUBAR-"",
        )
    ],
    [sg.Text(""Load a file"")],
    [sg.Column(layout1)],
    [sg.Column(layout2), sg.Button(""Add entries"")],
]

window = sg.Window(""Demo"", layout, finalize=True)


while True:
    event, values = window.read()
    print(event)
    if event in (sg.WIN_CLOSED, ""Exit""):
        break
    if isinstance(event, tuple):
        if event[2][0] != -1 and event[2][0] is not None:
            row_num_clicked = event[2][0]
            window[""-ROWSELECT-""].update(row_num_clicked + 1)


window.close()

```

#### Screenshot, Sketch, or Drawing




","['Question', 'Port - TK']",0
PySimpleGUI/PySimpleGUI,[Enhancement] psgdemo doesn't report missing module dependencies,"## Type of Issue (Enhancement, Error, Bug, Question)

Enhancement

----------------------------------------

## Environment 

#### Operating System

Windows  version 11 10.0.22621 SP0 {Multiprocessor Free}

#### PySimpleGUI Port (tkinter, Qt, Wx, Web)

tkinter

----------------------------------------

## Versions


#### Python version (`sg.sys.version`)

3.11.5 (tags/v3.11.5:cce6ba9, Aug 24 2023, 14:38:34) [MSC v.1936 64 bit (AMD64)]

#### PySimpleGUI Version (`sg.__version__`)

4.60.5

#### GUI Version  (tkinter (`sg.tclversion_detailed`), PySide2, WxPython, Remi)

8.6.12



---------------------

## Your Experience In Months or Years (optional)

 Years Python programming experience
 Years Programming experience overall
No Have used another Python GUI Framework? (tkinter, Qt, etc) (yes/no is fine)


---------------------

## Troubleshooting

These items may solve your problem. Please check those you've done by changing - [ ] to - [X]

- [X] Searched main docs for your problem  www.PySimpleGUI.org
- [X] Looked for Demo Programs that are similar to your goal. It is recommend you use the Demo Browser! Demos.PySimpleGUI.org
- [X] If not tkinter - looked for Demo Programs for specific port
- [X] For non tkinter - Looked at readme for your specific port if not PySimpleGUI (Qt, WX, Remi)
- [X] Run your program outside of your debugger (from a command line)
- [X] Searched through Issues (open and closed) to see if already reported Issues.PySimpleGUI.org
- [X] Upgraded to the latest official release of PySimpleGUI on PyPI
- [X] Tried using the PySimpleGUI.py file on GitHub. Your problem may have already been fixed but not released

## Detailed Description

I have a fresh install of python and psgdemos UI. In psgdemos, when I try to run the Demo_Desktop_Widget_CPU_Dashboard.py or any other CPU demos, nothing happens. When I run the python file in a cmd window, it states that I needed to install psutils module. Once I install psutils the demo works as intended. Psgdemos should give an error or notification for missing modules.

#### Code To Duplicate


```python
In psgdemos, when I try to run the Demo_Desktop_Widget_CPU_Dashboard.py or any other CPU demos, nothing happens. When I run the python file in a cmd window, it states that I needed to install psutils module.


```

#### Screenshot, Sketch, or Drawing","['enhancement', 'Demo Programs']",0
manual,Fix typo in README,Just a small spelling error in the intro.,['documentation'],1
manual,Update installation guide,The link to the wiki is broken.,['docs'],1
manual,Add comments to function,"The code is hard to understand, needs comments.",['good first issue'],1
manual,Rename variable for clarity,Change x to velocity in main.py,['refactor'],1
manual,Delete unused file,test_old.py is no longer used.,['cleanup'],1
manual,Fix indentation error,Line 45 has wrong spaces.,['bug'],1
manual,Update dependencies,Upgrade numpy to version 1.21,['dependencies'],1
manual,Broken link in footer,Footer link goes to 404 page.,['web'],1
manual,Add missing docstring,Function get_velocity needs a docstring.,['docs'],1
manual,Remove print statements,Leftover debugging print found in loop.,['cleanup'],1
manual,Fix typo in README,Just a small spelling error in the intro.,['documentation'],1
manual,Update installation guide,The link to the wiki is broken.,['docs'],1
manual,Add comments to function,"The code is hard to understand, needs comments.",['good first issue'],1
manual,Rename variable for clarity,Change x to velocity in main.py,['refactor'],1
manual,Delete unused file,test_old.py is no longer used.,['cleanup'],1
manual,Fix indentation error,Line 45 has wrong spaces.,['bug'],1
manual,Update dependencies,Upgrade numpy to version 1.21,['dependencies'],1
manual,Broken link in footer,Footer link goes to 404 page.,['web'],1
manual,Add missing docstring,Function get_velocity needs a docstring.,['docs'],1
manual,Remove print statements,Leftover debugging print found in loop.,['cleanup'],1
manual,Fix typo in README,Just a small spelling error in the intro.,['documentation'],1
manual,Update installation guide,The link to the wiki is broken.,['docs'],1
manual,Add comments to function,"The code is hard to understand, needs comments.",['good first issue'],1
manual,Rename variable for clarity,Change x to velocity in main.py,['refactor'],1
manual,Delete unused file,test_old.py is no longer used.,['cleanup'],1
manual,Fix indentation error,Line 45 has wrong spaces.,['bug'],1
manual,Update dependencies,Upgrade numpy to version 1.21,['dependencies'],1
manual,Broken link in footer,Footer link goes to 404 page.,['web'],1
manual,Add missing docstring,Function get_velocity needs a docstring.,['docs'],1
manual,Remove print statements,Leftover debugging print found in loop.,['cleanup'],1
manual,Fix typo in README,Just a small spelling error in the intro.,['documentation'],1
manual,Update installation guide,The link to the wiki is broken.,['docs'],1
manual,Add comments to function,"The code is hard to understand, needs comments.",['good first issue'],1
manual,Rename variable for clarity,Change x to velocity in main.py,['refactor'],1
manual,Delete unused file,test_old.py is no longer used.,['cleanup'],1
manual,Fix indentation error,Line 45 has wrong spaces.,['bug'],1
manual,Update dependencies,Upgrade numpy to version 1.21,['dependencies'],1
manual,Broken link in footer,Footer link goes to 404 page.,['web'],1
manual,Add missing docstring,Function get_velocity needs a docstring.,['docs'],1
manual,Remove print statements,Leftover debugging print found in loop.,['cleanup'],1
manual,Fix typo in README,Just a small spelling error in the intro.,['documentation'],1
manual,Update installation guide,The link to the wiki is broken.,['docs'],1
manual,Add comments to function,"The code is hard to understand, needs comments.",['good first issue'],1
manual,Rename variable for clarity,Change x to velocity in main.py,['refactor'],1
manual,Delete unused file,test_old.py is no longer used.,['cleanup'],1
manual,Fix indentation error,Line 45 has wrong spaces.,['bug'],1
manual,Update dependencies,Upgrade numpy to version 1.21,['dependencies'],1
manual,Broken link in footer,Footer link goes to 404 page.,['web'],1
manual,Add missing docstring,Function get_velocity needs a docstring.,['docs'],1
manual,Remove print statements,Leftover debugging print found in loop.,['cleanup'],1
manual,Add documentations ,Just a small spelling error in the intro.,['documentation'],1
manual,docs ,Just a small spelling error in the intro.,['documentation'],1
manual,Add documentations ,Just a small spelling error in the intro.,['documentation'],1
manual,docs ,Just a small spelling error in the intro.,['documentation'],1
manual,Add documentations ,Just a small spelling error in the intro.,['documentation'],1
manual,docs ,Just a small spelling error in the intro.,['documentation'],1
manual,Add documentations ,Just a small spelling error in the intro.,['documentation'],1
manual,docs ,Just a small spelling error in the intro.,['documentation'],1
manual,Add documentations ,Just a small spelling error in the intro.,['documentation'],1
manual,docs ,Just a small spelling error in the intro.,['documentation'],1
manual,Fix typo in README,Just a small spelling error in the intro.,['documentation'],1
manual,Update installation guide,The link to the wiki is broken.,['docs'],1
manual,Add comments to function,"The code is hard to understand, needs comments.",['good first issue'],1
manual,Rename variable for clarity,Change x to velocity in main.py,['refactor'],1
manual,Delete unused file,test_old.py is no longer used.,['cleanup'],1
manual,Fix indentation error,Line 45 has wrong spaces.,['bug'],1
manual,Update dependencies,Upgrade numpy to version 1.21,['dependencies'],1
manual,Broken link in footer,Footer link goes to 404 page.,['web'],1
manual,Add missing docstring,Function get_velocity needs a docstring.,['docs'],1
manual,Remove print statements,Leftover debugging print found in loop.,['cleanup'],1
manual,Add documentations,Leftover debugging print found in loop.,['docs'],1
manual,Fix typo in README,Just a small spelling error in the intro.,['documentation'],1
manual,Update installation guide,The link to the wiki is broken.,['docs'],1
manual,Add comments to function,"The code is hard to understand, needs comments.",['good first issue'],1
manual,Rename variable for clarity,Change x to velocity in main.py,['refactor'],1
manual,Delete unused file,test_old.py is no longer used.,['cleanup'],1
manual,Fix indentation error,Line 45 has wrong spaces.,['bug'],1
manual,Update dependencies,Upgrade numpy to version 1.21,['dependencies'],1
manual,Broken link in footer,Footer link goes to 404 page.,['web'],1
manual,Add missing docstring,Function get_velocity needs a docstring.,['docs'],1
manual,Remove print statements,Leftover debugging print found in loop.,['cleanup'],1
manual,Add documentations,Leftover debugging print found in loop.,['docs'],1
manual,Fix typo in README,Just a small spelling error in the intro.,['documentation'],1
manual,Update installation guide,The link to the wiki is broken.,['docs'],1
manual,Add comments to function,"The code is hard to understand, needs comments.",['good first issue'],1
manual,Rename variable for clarity,Change x to velocity in main.py,['refactor'],1
manual,Delete unused file,test_old.py is no longer used.,['cleanup'],1
manual,Fix indentation error,Line 45 has wrong spaces.,['bug'],1
manual,Update dependencies,Upgrade numpy to version 1.21,['dependencies'],1
manual,Broken link in footer,Footer link goes to 404 page.,['web'],1
manual,Add missing docstring,Function get_velocity needs a docstring.,['docs'],1
manual,Remove print statements,Leftover debugging print found in loop.,['cleanup'],1
manual,Add documentations,Leftover debugging print found in loop.,['docs'],1
manual,Fix typo in README,Just a small spelling error in the intro.,['documentation'],1
manual,Update installation guide,The link to the wiki is broken.,['docs'],1
manual,Add comments to function,"The code is hard to understand, needs comments.",['good first issue'],1
manual,Rename variable for clarity,Change x to velocity in main.py,['refactor'],1
manual,Delete unused file,test_old.py is no longer used.,['cleanup'],1
manual,Fix indentation error,Line 45 has wrong spaces.,['bug'],1
manual,Update dependencies,Upgrade numpy to version 1.21,['dependencies'],1
manual,Broken link in footer,Footer link goes to 404 page.,['web'],1
manual,Add missing docstring,Function get_velocity needs a docstring.,['docs'],1
manual,Remove print statements,Leftover debugging print found in loop.,['cleanup'],1
manual,Add documentations,Leftover debugging print found in loop.,['docs'],1
manual,Fix typo in README,Just a small spelling error in the intro.,['documentation'],1
manual,Update installation guide,The link to the wiki is broken.,['docs'],1
manual,Add comments to function,"The code is hard to understand, needs comments.",['good first issue'],1
manual,Rename variable for clarity,Change x to velocity in main.py,['refactor'],1
manual,Delete unused file,test_old.py is no longer used.,['cleanup'],1
manual,Fix indentation error,Line 45 has wrong spaces.,['bug'],1
manual,Update dependencies,Upgrade numpy to version 1.21,['dependencies'],1
manual,Broken link in footer,Footer link goes to 404 page.,['web'],1
manual,Add missing docstring,Function get_velocity needs a docstring.,['docs'],1
manual,Remove print statements,Leftover debugging print found in loop.,['cleanup'],1
manual,Add documentations,Leftover debugging print found in loop.,['docs'],1
manual,Fix typo in README,Just a small spelling error in the intro.,['documentation'],1
manual,Update installation guide,The link to the wiki is broken.,['docs'],1
manual,Add comments to function,"The code is hard to understand, needs comments.",['good first issue'],1
manual,Rename variable for clarity,Change x to velocity in main.py,['refactor'],1
manual,Delete unused file,test_old.py is no longer used.,['cleanup'],1
manual,Fix indentation error,Line 45 has wrong spaces.,['bug'],1
manual,Update dependencies,Upgrade numpy to version 1.21,['dependencies'],1
manual,Broken link in footer,Footer link goes to 404 page.,['web'],1
manual,Add missing docstring,Function get_velocity needs a docstring.,['docs'],1
manual,Remove print statements,Leftover debugging print found in loop.,['cleanup'],1
manual,Add documentation for setting up IAM links,Leftover debugging print found in loop.,['docs'],1
manual,Fix typo in README,Just a small spelling error in the intro.,['documentation'],1
manual,Update installation guide,The link to the wiki is broken.,['docs'],1
manual,Add comments to function,"The code is hard to understand, needs comments.",['good first issue'],1
manual,Rename variable for clarity,Change x to velocity in main.py,['refactor'],1
manual,Delete unused file,test_old.py is no longer used.,['cleanup'],1
manual,Fix indentation error,Line 45 has wrong spaces.,['bug'],1
manual,Update dependencies,Upgrade numpy to version 1.21,['dependencies'],1
manual,Broken link in footer,Footer link goes to 404 page.,['web'],1
manual,Add missing docstring,Function get_velocity needs a docstring.,['docs'],1
manual,Remove print statements,Leftover debugging print found in loop.,['cleanup'],1
manual,Add documentation for setting up IAM links,Leftover debugging print found in loop.,['docs'],1
manual,Fix typo in README,Just a small spelling error in the intro.,['documentation'],1
manual,Update installation guide,The link to the wiki is broken.,['docs'],1
manual,Add comments to function,"The code is hard to understand, needs comments.",['good first issue'],1
manual,Rename variable for clarity,Change x to velocity in main.py,['refactor'],1
manual,Delete unused file,test_old.py is no longer used.,['cleanup'],1
manual,Fix indentation error,Line 45 has wrong spaces.,['bug'],1
manual,Update dependencies,Upgrade numpy to version 1.21,['dependencies'],1
manual,Broken link in footer,Footer link goes to 404 page.,['web'],1
manual,Add missing docstring,Function get_velocity needs a docstring.,['docs'],1
manual,Remove print statements,Leftover debugging print found in loop.,['cleanup'],1
manual,Add documentation for setting up IAM links,Leftover debugging print found in loop.,['docs'],1
manual,Fix typo in README,Just a small spelling error in the intro.,['documentation'],1
manual,Update installation guide,The link to the wiki is broken.,['docs'],1
manual,Add comments to function,"The code is hard to understand, needs comments.",['good first issue'],1
manual,Rename variable for clarity,Change x to velocity in main.py,['refactor'],1
manual,Delete unused file,test_old.py is no longer used.,['cleanup'],1
manual,Fix indentation error,Line 45 has wrong spaces.,['bug'],1
manual,Update dependencies,Upgrade numpy to version 1.21,['dependencies'],1
manual,Broken link in footer,Footer link goes to 404 page.,['web'],1
manual,Add missing docstring,Function get_velocity needs a docstring.,['docs'],1
manual,Remove print statements,Leftover debugging print found in loop.,['cleanup'],1
manual,Add documentation for setting up IAM links,Leftover debugging print found in loop.,['docs'],1
manual,Fix typo in README,Just a small spelling error in the intro.,['documentation'],1
manual,Update installation guide,The link to the wiki is broken.,['docs'],1
manual,Add comments to function,"The code is hard to understand, needs comments.",['good first issue'],1
manual,Rename variable for clarity,Change x to velocity in main.py,['refactor'],1
manual,Delete unused file,test_old.py is no longer used.,['cleanup'],1
manual,Fix indentation error,Line 45 has wrong spaces.,['bug'],1
manual,Update dependencies,Upgrade numpy to version 1.21,['dependencies'],1
manual,Broken link in footer,Footer link goes to 404 page.,['web'],1
manual,Add missing docstring,Function get_velocity needs a docstring.,['docs'],1
manual,Remove print statements,Leftover debugging print found in loop.,['cleanup'],1
manual,Add documentation for setting up IAM links,Leftover debugging print found in loop.,['docs'],1
